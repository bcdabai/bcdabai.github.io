<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HDFS编程实践 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="HDFS编程实践" />
<meta property="og:description" content="HDFS编程实践 1.利用Shell命令与HDFS进行交互1.1 目录操作1.2 文件操作 2.利用Web界面管理HDFS3.利用Java API与HDFS进行交互3.1 在Ubuntu中安装Eclipse3.2 使用Eclipse开发调试HDFS Java程序3.2.1 在Eclipse创建项目 3.4 编译运行程序 首先开启hadoop hadoop 时，无需进行 NameNode 的初始化 cd /usr/local/hadoop ./sbin/start-dfs.sh #启动hadoop 1.利用Shell命令与HDFS进行交互 有三种shell命令方式。
hadoop fs 适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统hadoop dfs 只能适用于HDFS文件系统hdfs dfs 跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统 1.1 目录操作 HDFS中为hadoop用户创建一个用户目录
cd /usr/local/hadoop ./bin/hdfs dfs -mkdir -p /user/hadoop 显示HDFS中与当前用户hadoop对应的用户目录下的内容：
./bin/hdfs dfs -ls . 上面的命令和下面的命令是等价的：
./bin/hdfs dfs -ls /user/hadoop 如果要列出HDFS上的所有目录，可以使用如下命令，“.”表示HDFS中的当前用户目录，也就是“/user/hadoop”目录
./bin/hdfs dfs -ls . 可以使用如下命令创建一个input目录，这个input目录创建成功以后，它在HDFS中的完整路径是“/user/hadoop/input”。
./bin/hdfs dfs -mkdir input 在HDFS的根目录下创建一个名称为input的目录
./bin/hdfs dfs -mkdir /input 可以使用rm命令删除一个目录，比如，可以使用如下命令删除刚才在HDFS中创建的“/input”目录（不是“/user/hadoop/input”目录）：
./bin/hdfs dfs -rm -r /input -r是递归删除（删除目录及其子目录所有内容）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/7671983dae9d9b913047f76ce9291ac4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-26T22:14:41+08:00" />
<meta property="article:modified_time" content="2023-03-26T22:14:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">HDFS编程实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>HDFS编程实践</h4> 
 <ul><li><a href="#1ShellHDFS_8" rel="nofollow">1.利用Shell命令与HDFS进行交互</a></li><li><ul><li><a href="#11%09_14" rel="nofollow">1.1 目录操作</a></li><li><a href="#12__66" rel="nofollow">1.2 文件操作</a></li></ul> 
  </li><li><a href="#2WebHDFS_129" rel="nofollow">2.利用Web界面管理HDFS</a></li><li><a href="#3Java_APIHDFS_132" rel="nofollow">3.利用Java API与HDFS进行交互</a></li><li><ul><li><a href="#31_UbuntuEclipse_134" rel="nofollow">3.1 在Ubuntu中安装Eclipse</a></li><li><a href="#32_EclipseHDFS_Java_149" rel="nofollow">3.2 使用Eclipse开发调试HDFS Java程序</a></li><li><ul><li><a href="#321_Eclipse_153" rel="nofollow">3.2.1 在Eclipse创建项目</a></li></ul> 
   </li><li><a href="#34__248" rel="nofollow">3.4 编译运行程序</a></li></ul> 
 </li></ul> 
</div> 
<br> 首先开启hadoop 
<br> hadoop 时，无需进行 NameNode 的初始化 
<p></p> 
<pre><code>cd /usr/local/hadoop
./sbin/start-dfs.sh #启动hadoop
</code></pre> 
<h2><a id="1ShellHDFS_8"></a>1.利用Shell命令与HDFS进行交互</h2> 
<p>有三种shell命令方式。</p> 
<ol><li>hadoop fs 适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统</li><li>hadoop dfs 只能适用于HDFS文件系统</li><li>hdfs dfs 跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统</li></ol> 
<h3><a id="11%09_14"></a>1.1 目录操作</h3> 
<p>HDFS中为hadoop用户创建一个用户目录</p> 
<pre><code>cd /usr/local/hadoop
./bin/hdfs dfs -mkdir -p /user/hadoop
</code></pre> 
<p>显示HDFS中与当前用户hadoop对应的用户目录下的内容：</p> 
<pre><code>./bin/hdfs dfs -ls .
</code></pre> 
<p><img src="https://images2.imgbox.com/f5/78/r9j9FzyA_o.png" alt="在这里插入图片描述"></p> 
<p>上面的命令和下面的命令是等价的：</p> 
<pre><code>./bin/hdfs dfs -ls /user/hadoop
</code></pre> 
<p><img src="https://images2.imgbox.com/d7/41/wOzsNO5J_o.png" alt="在这里插入图片描述"></p> 
<p>如果要列出HDFS上的所有目录，可以使用如下命令，“.”表示HDFS中的当前用户目录，也就是“/user/hadoop”目录</p> 
<pre><code>./bin/hdfs dfs -ls .
</code></pre> 
<p>可以使用如下命令创建一个input目录，这个input目录创建成功以后，它在HDFS中的完整路径是“/user/hadoop/input”。</p> 
<pre><code>./bin/hdfs dfs -mkdir input
</code></pre> 
<p>在HDFS的根目录下创建一个名称为input的目录</p> 
<pre><code>./bin/hdfs dfs -mkdir /input
</code></pre> 
<p>可以使用rm命令删除一个目录，比如，可以使用如下命令删除刚才在HDFS中创建的“/input”目录（不是“/user/hadoop/input”目录）：</p> 
<pre><code>./bin/hdfs dfs -rm -r /input
</code></pre> 
<p>-r是递归删除（删除目录及其子目录所有内容）</p> 
<h3><a id="12__66"></a>1.2 文件操作</h3> 
<p>在实际应用中，经常需要从本地文件系统向HDFS中上传文件，或者把HDFS中的文件下载到本地文件系统中。<br> 在本地Linux文件系统的“/home/hadoop/”目录下创建一个文件myLocalFile.txt</p> 
<pre><code>cd /home/Hadoop/
gedit myLocalFile.txt
</code></pre> 
<p>里面可以随意输入一些单词，比如，输入如下三行：</p> 
<pre><code>Hadoop
Spark
XMU DBLAB
</code></pre> 
<p>把本地文件系统的“/home/hadoop/myLocalFile.txt”上传到HDFS中的当前用户目录的input目录下，也就是上传到HDFS的“/user/hadoop/input/”目录下：</p> 
<pre><code>./bin/hdfs dfs -put /home/hadoop/myLocalFile.txt  input
</code></pre> 
<p>使用ls命令查看一下文件是否成功上传到HDFS中</p> 
<pre><code>./bin/hdfs dfs -ls input
 
</code></pre> 
<p>下面使用如下命令查看HDFS中的myLocalFile.txt这个文件的内容：</p> 
<pre><code>./bin/hdfs dfs -cat input/myLocalFile.txt
</code></pre> 
<p>把HDFS中的myLocalFile.txt文件下载到本地文件系统中的“/home/hadoop/下载/”这个目录下，命令如下：</p> 
<pre><code>./bin/hdfs dfs -get input/myLocalFile.txt  /home/hadoop/下载
</code></pre> 
<p>到本地文件系统查看下载下来的文件myLocalFile.txt：</p> 
<pre><code>cd ~
cd 下载
ls
cat myLocalFile.txt
</code></pre> 
<p>了解一下如何把文件从HDFS中的一个目录拷贝到HDFS中的另外一个目录。比如，如果要把HDFS的“/user/hadoop/input/myLocalFile.txt”文件，拷贝到HDFS的另外一个目录“/input”中（注意，这个input目录位于HDFS根目录下），可以使用如下命令：</p> 
<pre><code>./bin/hdfs dfs -cp input/myLocalFile.txt  /input
</code></pre> 
<h2><a id="2WebHDFS_129"></a>2.利用Web界面管理HDFS</h2> 
<p>WEB界面的访问地址是<a href="http://localhost:9870" rel="nofollow">http://localhost:9870</a><br> <img src="https://images2.imgbox.com/6b/43/MiHIWO3o_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="3Java_APIHDFS_132"></a>3.利用Java API与HDFS进行交互</h2> 
<p><a href="https://hadoop.org.cn/docs/" rel="nofollow">Apache Hadoop中文文档</a></p> 
<h3><a id="31_UbuntuEclipse_134"></a>3.1 在Ubuntu中安装Eclipse</h3> 
<p>可以到Eclipse官网（<a href="https://www.eclipse.org/downloads/" rel="nofollow">https://www.eclipse.org/downloads/</a>）下载安装包。文章是eclipse-4.7.0-linux.gtk.x86_64.tar.gz。假设安装文件下载后保存在了Linux系统的目录“~/Downloads”下，下面执行如下命令对文件进行解压缩：</p> 
<pre><code>cd ~/Downloads
sudo tar -zxvf ./eclipse-4.7.0-linux.gtk.x86_64.tar.gz -C /usr/local
</code></pre> 
<p>启动eclipse</p> 
<pre><code>cd /usr/local/eclipse
./eclipse
</code></pre> 
<h3><a id="32_EclipseHDFS_Java_149"></a>3.2 使用Eclipse开发调试HDFS Java程序</h3> 
<p>Hadoop采用Java语言开发的，提供了Java API与HDFS进行交互。上面介绍的Shell命令，在执行时实际上会被系统转换成Java API调用。<a href="http://hadoop.apache.org/docs/stable/api/" rel="nofollow">Hadoop API文档</a></p> 
<p>假设在目录“hdfs://localhost:9000/user/hadoop”下面有几个文件，分别是file1.txt、file2.txt、file3.txt、file4.abc和file5.abc，现在需要从该目录中过滤出所有后缀名不为“.abc”的文件，对过滤之后的文件进行读取，并将这些文件的内容合并到文件“hdfs://localhost:9000/user/hadoop/merge.txt”中。</p> 
<h4><a id="321_Eclipse_153"></a>3.2.1 在Eclipse创建项目</h4> 
<p>“File–&gt;New–&gt;Java Project”<br> 新项目设置<br> <img src="https://images2.imgbox.com/91/fe/FTX9rC91_o.png" alt="在这里插入图片描述"></p> 
<p>创建之后，在File-&gt;Properties找到Java Build Path，点Libraries<br> <img src="https://images2.imgbox.com/be/3e/e82vQwQP_o.png" alt="在这里插入图片描述"></p> 
<p>然后Add External JARs</p> 
<p>向Java工程中添加以下JAR包：<br> （1）“/usr/local/hadoop/share/hadoop/common”目录下的所有JAR包，包括hadoop-common-3.1.3.jar、hadoop-common-3.1.3-tests.jar、haoop-nfs-3.1.3.jar和haoop-kms-3.1.3.jar，注意，不包括目录jdiff、lib、sources和webapps；<br> （2）“/usr/local/hadoop/share/hadoop/common/lib”目录下的所有JAR包；<br> （3）“/usr/local/hadoop/share/hadoop/hdfs”目录下的所有JAR包，注意，不包括目录jdiff、lib、sources和webapps；<br> （4）“/usr/local/hadoop/share/hadoop/hdfs/lib”目录下的所有JAR包。<br> <img src="https://images2.imgbox.com/33/b9/5ACSUfnl_o.png" alt="在这里插入图片描述"></p> 
<p>3.3 编写Java应用程序<br> “Package Explorer”面板中（如下图所示），找到刚才创建好的工程名称“HDFSExample”，然后在该工程名称上点击鼠标右键，在弹出的菜单中选择“New–&gt;Class”菜单。</p> 
<p><img src="https://images2.imgbox.com/59/bc/HEgQ0lgm_o.png" alt="在这里插入图片描述"><br> 只需要在“Name”后面输入新建的Java类文件的名称，这里采用名称“MergeFile”，其他都可以采用默认设置</p> 
<p>“MergeFile.java”的源代码文件，请在该文件中输入以下代码：</p> 
<pre><code>import java.io.IOException;
import java.io.PrintStream;
import java.net.URI;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
 
/**
 * 过滤掉文件名满足特定条件的文件 
 */
class MyPathFilter implements PathFilter {
     String reg = null; 
     MyPathFilter(String reg) {
          this.reg = reg;
     }
     public boolean accept(Path path) {
        if (!(path.toString().matches(reg)))
            return true;
        return false;
    }
}
/***
 * 利用FSDataOutputStream和FSDataInputStream合并HDFS中的文件
 */
public class MergeFile {
    Path inputPath = null; //待合并的文件所在的目录的路径
    Path outputPath = null; //输出文件的路径
    public MergeFile(String input, String output) {
        this.inputPath = new Path(input);
        this.outputPath = new Path(output);
    }
    public void doMerge() throws IOException {
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS","hdfs://localhost:9000");
          conf.set("fs.hdfs.impl","org.apache.hadoop.hdfs.DistributedFileSystem");
        FileSystem fsSource = FileSystem.get(URI.create(inputPath.toString()), conf);
        FileSystem fsDst = FileSystem.get(URI.create(outputPath.toString()), conf);
                //下面过滤掉输入目录中后缀为.abc的文件
        FileStatus[] sourceStatus = fsSource.listStatus(inputPath,
                new MyPathFilter(".*\\.abc")); 
        FSDataOutputStream fsdos = fsDst.create(outputPath);
        PrintStream ps = new PrintStream(System.out);
        //下面分别读取过滤之后的每个文件的内容，并输出到同一个文件中
        for (FileStatus sta : sourceStatus) {
            //下面打印后缀不为.abc的文件的路径、文件大小
            System.out.print("路径：" + sta.getPath() + "    文件大小：" + sta.getLen()
                    + "   权限：" + sta.getPermission() + "   内容：");
            FSDataInputStream fsdis = fsSource.open(sta.getPath());
            byte[] data = new byte[1024];
            int read = -1;
 
            while ((read = fsdis.read(data)) &gt; 0) {
                ps.write(data, 0, read);
                fsdos.write(data, 0, read);
            }
            fsdis.close();          
        }
        ps.close();
        fsdos.close();
    }
    public static void main(String[] args) throws IOException {
        MergeFile merge = new MergeFile(
                "hdfs://localhost:9000/user/hadoop/",
                "hdfs://localhost:9000/user/hadoop/merge.txt");
        merge.doMerge();
    }
}
</code></pre> 
<h3><a id="34__248"></a>3.4 编译运行程序</h3> 
<p>在开始编译运行程序之前，请一定确保Hadoop已经启动运行，如果还没有启动，需要打开一个Linux终端，输入以下命令启动Hadoop：</p> 
<pre><code>cd /usr/local/hadoop
./sbin/start-dfs.sh
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3d80fdffaf336e062926d1f3070ff1b1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Jetson嵌入式系列模型部署-2</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7020944e377d403b08957907457e9843/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">51单片机笔记二</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>