<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《大数据开发》Hive - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《大数据开发》Hive" />
<meta property="og:description" content="Hive 是基于 Hadoop 的一个数据仓库工具；提供Sql（hive Sql）查询功能；数据是存储在hdfs上，hive本身不存储数据，构建表的逻辑存在指定数据库（mysql ）。本质是将 SQL 语句转换为 MapReduce 任务执行。离线大数据计算。可以将结构化的数据文件映射成为一张数据库表。 官方文档
流程图 HiveSql与Sql相比
Hive字段类型
1. 建表 三种方式
直接建表法： create table movies (uid string,iid string,score string , ts string) row format delimited fields terminated by &#39;\t&#39; tblproperties(&#34;skip.header.line.count&#34;=&#34;1&#34;); 查询建表法： create table movies_select as select * from movies limit 1000; like建表法 create table movies_like like movies_select ; 2. 执行顺序 在hive的执行语句当中的执行查询的顺序：这是一条sql:select … from … where … group by … having … order by …" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/590aca7c5cb3f73ae2a7b078bac93115/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-19T15:00:15+08:00" />
<meta property="article:modified_time" content="2021-06-19T15:00:15+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《大数据开发》Hive</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Hive_0"></a>Hive</h3> 
<ol><li>是基于 Hadoop 的一个数据仓库工具；</li><li>提供Sql（hive Sql）查询功能；</li><li>数据是存储在hdfs上，hive本身不存储数据，构建表的逻辑存在指定数据库（mysql ）。</li><li>本质是将 SQL 语句转换为 MapReduce 任务执行。</li><li>离线大数据计算。</li><li>可以将结构化的数据文件映射成为一张数据库表。</li></ol> 
<p><a href="https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference" rel="nofollow">官方文档</a></p> 
<h5><a id="_11"></a>流程图</h5> 
<p><img src="https://images2.imgbox.com/48/dd/M1WmsHTs_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/00/b4/mnO7wFQ6_o.png" alt="在这里插入图片描述"><br> HiveSql与Sql相比</p> 
<p><img src="https://images2.imgbox.com/d5/29/F6sWWGN5_o.png" alt="在这里插入图片描述"><br> Hive字段类型<br> <img src="https://images2.imgbox.com/fe/2e/yfUYdcti_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ae/1c/gBcuS5LJ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="1__21"></a>1. 建表</h4> 
<p>三种方式</p> 
<ol><li>直接建表法：</li></ol> 
<pre><code class="prism language-sql"> <span class="token keyword">create</span> <span class="token keyword">table</span> movies <span class="token punctuation">(</span>uid string<span class="token punctuation">,</span>iid string<span class="token punctuation">,</span>score string <span class="token punctuation">,</span> ts string<span class="token punctuation">)</span>  
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span> tblproperties<span class="token punctuation">(</span><span class="token string">"skip.header.line.count"</span><span class="token operator">=</span><span class="token string">"1"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<ol start="2"><li>查询建表法：</li></ol> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> movies_select  <span class="token keyword">as</span>    <span class="token keyword">select</span> <span class="token operator">*</span>  <span class="token keyword">from</span> movies <span class="token keyword">limit</span> <span class="token number">1000</span><span class="token punctuation">;</span>
</code></pre> 
<ol start="3"><li>like建表法</li></ol> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> movies_like   <span class="token operator">like</span>  movies_select <span class="token punctuation">;</span>
</code></pre> 
<h4><a id="2__41"></a>2. 执行顺序</h4> 
<p>在hive的执行语句当中的执行查询的顺序：这是一条sql:select … from … where … group by … having … order by …<br> 执行顺序：<br> from … where … select … group by … having … order by …<br> 其实总结hive的执行顺序也是总结mapreduce的执行顺序：MR程序的执行顺序：</p> 
<p><strong>map阶段</strong>：<br> 1.执行from加载，进行表的查找与加载<br> 2.执行where过滤，进行条件过滤与筛选<br> 3.执行select查询：进行输出项的筛选<br> 4.map端文件合并：</p> 
<p><strong>reduce阶段</strong>：map端本地溢出写文件的合并操作，每个map最终形成一个临时文件。 然后按列映射到对应的Reduce阶段：<br> 1.group by：对map端发送过来的数据进行分组并进行计算。<br> 2.having：最后过滤列用于输出结果<br> 3.order by排序后进行结果输出到HDFS文件</p> 
<p>所以通过上面的例子我们可以看到，在进行select之后我们会形成一张表，在这张表当中做分组排序这些操作。</p> 
<h4><a id="3__60"></a>3. 常用解析函数</h4> 
<p>over(partition by col1 order by col2 asc/desc)</p> 
<p>1.Partition by col1 按哪列进行分组，如果不指定,默认全局排序，如果指定一<br> 列，则按照指定列进行分组，然后进行排序<br> 2.Order by 按哪一列进行排序，这个不指定就会报错。<br> 3. asc/desc按升序或者降序进行排序，默认升序。</p> 
<p>row_number()<br> select *, row_number() over(order by score desc) as ra from t1<br> 此处是排序，即使排序的列里面有相同的值，他的排名也会不同。</p> 
<p>rank()<br> select *, rank() over(order by score desc) as ra from t1<br> 此处是排序，排序的列里面有相同的值，他的排名会相同。</p> 
<p>Collect_set() 和 collect_list()<br> 列转行，会将其变为集合或者数组，set会去重，list不会去重<br> <img src="https://images2.imgbox.com/b0/08/T3WDMM2a_o.png" alt="在这里插入图片描述"><br> Case when (条件) Then () else () end as col1 ,</p> 
<p><img src="https://images2.imgbox.com/34/36/eMfKBpMr_o.png" alt="在这里插入图片描述"><br> Explode函数和lateral view函数<br> Explode：UDTF，一行变多行<br> lateral view：虚表<br> <img src="https://images2.imgbox.com/99/7d/69eZHSgF_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Hivesort_byorder_bycluster_bydistribute_by_86"></a>Hive中sort by、order by、cluster by、distribute by的区别</h4> 
<p>1、sort by：不是全局排序，其在数据进入reducer完成排序。<br> 2、order by：会对输入做全局排序，因此只有一个reducer，如果有多个reducer无法保证全局的排序。计算规模较大，时间可能会很长,慎用。<br> 3、cluster by：除了具有distribute by的功能，还具有了sort by的功能select * from t1 cluster by rank 。:cluster_by 是能升<br> 4、distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。distribute by是控制map的输出在reducer是如何划分的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。举个例子select * from t1 distribute by rank sort by rank,time desc limit 5;</p> 
<h5><a id="hivestrict_92"></a>hive的strict模式</h5> 
<p>(1) 有partition的表查询需要加上where子句，筛选部分数据实现分区裁剪，即不允许全表全分区扫描，防止数据过大<br> (2) order by 执行时只产生一个reduce，必须加上limit限制结果的条数，防止数据量过大造成1个reduce超负荷，否则会报错。<br> (3) join时，如果只有一个reduce，则不支持笛卡尔积查询。也就是说必须要有on语句的关联条件。</p> 
<p>group by和order by 同时使用，不会按组进行排序where,group by,having,order by同时使用，执行顺序为<br> （1）where过滤数据<br> （2）对筛选结果集group by分组，group by 执行顺序是在select 之前的。因此group by中不能使用select 后面字段的别名的。<br> （3）对每个分组进行select查询，提取对应的列，有几组就执行几次<br> （4）再进行having筛选每组数据<br> （5）最后整体进行order by排序</p> 
<h5><a id="__104"></a>内部表 外部表</h5> 
<p>Hive建表分为外部表和内部表，语句如下：<br> 内部表:create table [表名] （默认内部表）<br> 外部表:create external table [表名] location ‘hdfs_path ’ (hdfs_path必须是文件夹，否则会报错 )</p> 
<p>为什么需要区分内部表、外部表？<br> 1.内部表在构建的时候，其会将数据拉到对应的hive/warehous/xx.db之下，而外部表是在指定的hdfs_path,并未在对应的hive/warehouse上，只不过是mysql保存了外部表的一些元数据。<br> 2.在进行删除表的时候(drop)，Hive会把所有的元数据和hdfs上的数据全部删除。而删除外部表的时候，只删除外部表的元数据，数据不删除。<br> 经验： 每天收集到的网站数据,需要做大量的统计数据分析,所以在数据源上可以使用外部表进行存储，方便数据的共享，在做统计分析时候用到的中间表，结果表可以使用内部表，因为这些数据不需要共享，使用内部表更为合适。</p> 
<h5><a id="Hive_113"></a>Hive表的分区</h5> 
<p>为什么要分区 ：随着MR的任务越来越多，表的数据量会越来越大，而Hive查询数据的数据的时候通常使用的是全表扫描，这样将会导致大量不必要的数据进行扫描，从而查询效率会大大的降低。 从而Hive引进了分区技术，使用分区技术：避免Hive全表扫描，提升查询效率<br> 分区:将整个表的数据在存储时划分成多个子目录来存储(子目录就是以分区名来名称, partitioned by(分区名 数据类型)),比如按星期几进行划分数据data，当然也可以建立多分区:<br> user/hive/warehouse/xx.db/data/Monday/xxx.dt<br> user/hive/warehouse/xx.db/data/Tuesday/xxx.dt<br> .<br> .<br> .<br> user/hive/warehouse/xx.db/data/Sunday/xxx.dt</p> 
<h5><a id="Hive_123"></a>Hive表的动态分区</h5> 
<p>Hive表的动态分区<br> 上述分区都是静态分区，插入的时候知道分区类型，而且每个分区写一个load data，很繁琐。使用动态分区可解决以上问题，其可以根据查询得到的数据动态分配到分区里。其实动态分区与静态分区区别就是不指定分区目录，由系统自己选择。<br> 首先，启动动态分区功能<br> 注意，动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区。<br> 动态分区可以允许所有的分区列都是动态分区列，但是要设置一个参数：<br> set hive.exec.dynamic.partition=true;<br> set hive.exec.dynamic.partition.mode=nostrick;</p> 
<p><img src="https://images2.imgbox.com/af/63/Zp0EUPDB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/82/06/qjf4VoYr_o.png" alt="在这里插入图片描述"><br> hive的分区使用的是外表字段，并且是一个伪列，可以用于查询过滤条件，但不会存储实际的值<br> hive的分区名不准使用中文</p> 
<h5><a id="Hive_136"></a>Hive表的分桶</h5> 
<p>一、什么是数据分桶？</p> 
<p>二、数据分桶的作用</p> 
<p>三、如何创建一个分桶表</p> 
<p>四、对分桶表进行的数据抽样</p> 
<p><strong>一、什么是数据分桶？</strong><br> Hive是基于Hadoop的一个数据仓库，可将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。Hive的本质是将HiveSQL语句转化成MapReduce任务执行。<br> Hive中，分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可形成合理的分区，尤其是需要确定合适大小的分区划分方式(有的数据分区数据过大，有的很少，即我们常说的数据倾斜)<br> 我们可以将Hive中的分桶原理理解成MapReduce中的HashPartitioner的原理。都是基于hash值对数据进行分桶。<br> MR：按照key的hash值除以reduceTask个数进行取余(reduce_id = key.hashcode % reduce.num)<br> Hive:按照分桶字段(列)的hash值除以分桶的个数进行取余(bucket_id = column.hashcode % bucket.num)</p> 
<p><strong>二、数据分桶的作用</strong></p> 
<p>2.1 数据抽样<br> 在处理大规模数据集时，尤其载数据挖掘的阶段，可以用一份数据验证一下，代码是否可以运行成功，进行局部测试，也可以抽样进行一些代表性统计分析。<br> 2.2 map-side join<br> 可以获得更高的查询处理效率。桶为表加上了额外的结构，（利用原有字段进行分桶），Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。<br> 查看sampling数据：select * from student tablesample(bucket 1 out of 32 on id);</p> 
<p>三、创建分桶表：</p> 
<ol><li>‘set hive.enforce.bucketing = true’(临时设置，退出终端，再打开就会恢复false)。这个打开后，会自动根据bucket个数自动分配Reduce task个数，reduce个数和buckert个数一样。<br> 2.建表<br> <img src="https://images2.imgbox.com/87/df/lipSBNHV_o.png" alt="在这里插入图片描述"><br> 3.导入数据：insert into test_bucket select * from need_bucket cluster by (id)<br> 4、对分桶表进行的数据抽样<br> <img src="https://images2.imgbox.com/76/8b/WqVZIBKj_o.png" alt="在这里插入图片描述"><br> y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了12份，当y=24<br> 时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64<em>1/128=)1/2个bucket的数据。x表示从哪个bucket开始<br> 抽取。例如，table总bucket数为32，tablesample(bucket 2 out of 16)，表示总共抽取（32/16=）2个bucket的数<br> 据，分别为第1个bucket和 ]m 第(1+16)17个bucket的数据。 4</em>1/32=1/8buckets。</li></ol> 
<p>桶的概念就是MapReduce的分区的概念，两者完全相同。物理上每个桶就是目录里的一个文件，一个作业产生的桶（输出文件）数量和reduce任务个数相同。<br> 而分区表的概念，则是新的概念。分区代表了数据的仓库，也就是文件夹目录。每个文件夹下面可以放不同的数据文件。通过文件夹可以查询里面存放的文件。但文件夹本身和数据的内容毫无关系。<br> 桶则是按照数据内容的某个值进行分桶，把一个大文件散列称为一个个小文件。</p> 
<p>这些小文件可以单独排序。如果另外一个表也按照同样的规则分成了一个个小文件。两个表join的时候，就不必要扫描整个表，只需要匹配相同分桶的数据即可。效率当然大大提升。<br> 同样，对数据抽样的时候，也不需要扫描整个文件。只需要对每个分区按照相同规则抽取一部分数据即可。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fedb75b2f9dc77be0c56c3bc4dee1554/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">node.js学习笔记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7cbddeee046a23cf55d651bbe8e6ba0c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Altium Designer 20 （11）——封装的IPC创建</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>