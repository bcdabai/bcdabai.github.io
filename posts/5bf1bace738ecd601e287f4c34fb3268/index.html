<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《动手学深度学习》Day3:多层感知机 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《动手学深度学习》Day3:多层感知机" />
<meta property="og:description" content="文章目录 一、多层感知机的基本知识1.1 隐藏层1.2 表达公式1.3 激活函数1.4 多层感知机 二、多层感知机从零开始的实现2.1 获取训练集2.2 定义模型参数2.3 定义激活函数2.4 定义网络2.5 定义损失函数2.6 训练 三、多层感知机pytorch实现3.1 初始化模型和各个参数3.2 训练 一、多层感知机的基本知识 深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。
1.1 隐藏层 下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。
1.2 表达公式 具体来说，给定一个小批量样本 X ∈ R n × d \textbf{X}\in R^{n\times d} X∈Rn×d其批量大小为n，输入个数为d。假设多层感知机只有一个隐藏层，其中隐藏单元个数为h。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为 H \textbf{H} H,有 H ∈ R n × h \textbf{H}\in R^{n\times h} H∈Rn×h。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为 W h ∈ R d × h \textbf{W}_{h}\in R^{d\times h} Wh​∈Rd×h和 b h ∈ R 1 × h \textbf{b}_{h}\in R^{1\times h} bh​∈R1×h,输出层的权重和偏差参数分别为 W o ∈ R h × q \textbf{W}_{o}\in R^{h\times q} Wo​∈Rh×q和 b o ∈ R 1 × q \textbf{b}_{o}\in R^{1\times q} bo​∈R1×q." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5bf1bace738ecd601e287f4c34fb3268/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-14T20:27:48+08:00" />
<meta property="article:modified_time" content="2020-02-14T20:27:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《动手学深度学习》Day3:多层感知机</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <hr> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_5" rel="nofollow">一、多层感知机的基本知识</a></li><li><ul><li><a href="#11__8" rel="nofollow">1.1 隐藏层</a></li><li><a href="#12__11" rel="nofollow">1.2 表达公式</a></li><li><a href="#13__20" rel="nofollow">1.3 激活函数</a></li><li><a href="#14__86" rel="nofollow">1.4 多层感知机</a></li></ul> 
  </li><li><a href="#_91" rel="nofollow">二、多层感知机从零开始的实现</a></li><li><ul><li><a href="#21__104" rel="nofollow">2.1 获取训练集</a></li><li><a href="#22__111" rel="nofollow">2.2 定义模型参数</a></li><li><a href="#23__127" rel="nofollow">2.3 定义激活函数</a></li><li><a href="#24__134" rel="nofollow">2.4 定义网络</a></li><li><a href="#25__143" rel="nofollow">2.5 定义损失函数</a></li><li><a href="#26__149" rel="nofollow">2.6 训练</a></li></ul> 
  </li><li><a href="#pytorch_190" rel="nofollow">三、多层感知机pytorch实现</a></li><li><ul><li><a href="#31__203" rel="nofollow">3.1 初始化模型和各个参数</a></li><li><a href="#32__219" rel="nofollow">3.2 训练</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_5"></a>一、多层感知机的基本知识</h2> 
<p>深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p> 
<h3><a id="11__8"></a>1.1 隐藏层</h3> 
<p>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。<br> <img src="https://images2.imgbox.com/a0/c7/xGCdH3e4_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12__11"></a>1.2 表达公式</h3> 
<p>具体来说，给定一个小批量样本<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           n 
          
         
           × 
          
         
           d 
          
         
        
       
      
        \textbf{X}\in R^{n\times d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72521em; vertical-align: -0.0391em;"></span><span class="mord text"><span class="mord textbf">X</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>其批量大小为n，输入个数为d。假设多层感知机只有一个隐藏层，其中隐藏单元个数为h。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
      
        \textbf{H} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68611em; vertical-align: 0em;"></span><span class="mord text"><span class="mord textbf">H</span></span></span></span></span></span>,有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           n 
          
         
           × 
          
         
           h 
          
         
        
       
      
        \textbf{H}\in R^{n\times h} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72521em; vertical-align: -0.0391em;"></span><span class="mord text"><span class="mord textbf">H</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span>。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          h 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           d 
          
         
           × 
          
         
           h 
          
         
        
       
      
        \textbf{W}_{h}\in R^{d\times h} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83611em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
        
          h 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           1 
          
         
           × 
          
         
           h 
          
         
        
       
      
        \textbf{b}_{h}\in R^{1\times h} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span>,输出层的权重和偏差参数分别为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          o 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           h 
          
         
           × 
          
         
           q 
          
         
        
       
      
        \textbf{W}_{o}\in R^{h\times q} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83611em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
        
          o 
         
        
       
         ∈ 
        
        
        
          R 
         
         
         
           1 
          
         
           × 
          
         
           q 
          
         
        
       
      
        \textbf{b}_{o}\in R^{1\times q} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span></span></span></span></span></span></span></span></span></span>.</p> 
<p>我们先来看一种含单隐藏层的多层感知机的设计。其输出<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ∈ 
        
        
        
          R 
         
         
         
           n 
          
         
           × 
          
         
           q 
          
         
        
       
      
        \textbf{O}\in R^{n\times q} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72521em; vertical-align: -0.0391em;"></span><span class="mord text"><span class="mord textbf">O</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.771331em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span></span></span></span></span></span></span></span></span></span>的计算为<br> <img src="https://images2.imgbox.com/56/c5/pRDrRyhG_o.png" alt="在这里插入图片描述"><br> 也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到<br> <img src="https://images2.imgbox.com/cd/79/r5aKP1FG_o.png" alt="在这里插入图片描述"><br> 从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          h 
         
        
        
        
          W 
         
        
          o 
         
        
       
      
        \textbf{W}_{h}\textbf{W}_{o} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83611em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，偏差参数为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
        
          h 
         
        
        
        
          W 
         
        
          o 
         
        
       
         + 
        
        
        
          b 
         
        
          o 
         
        
       
      
        \textbf{b}_{h}\textbf{W}_{o}+\textbf{b}_{o} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord text"><span class="mord textbf">W</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord text"><span class="mord textbf">b</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p> 
<h3><a id="13__20"></a>1.3 激活函数</h3> 
<p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p> 
<p>下面我们介绍几个常用的激活函数：</p> 
<p><strong>1、ReLU函数</strong></p> 
<p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>，该函数定义为<br> <img src="https://images2.imgbox.com/1b/9c/ZFOMjEC4_o.png" alt="在这里插入图片描述"><br> 可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p> 
<pre><code>%matplotlib inline
import torch
import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.append("/home/kesci/input")
import d2lzh1981 as d2l
print(torch.__version__)
</code></pre> 
<p>1.3.0</p> 
<pre><code>def xyplot(x_vals, y_vals, name):
    # d2l.set_figsize(figsize=(5, 2.5))
    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())
    plt.xlabel('x')
    plt.ylabel(name + '(x)')
</code></pre> 
<pre><code>x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = x.relu()
xyplot(x, y, 'relu')
</code></pre> 
<p><img src="https://images2.imgbox.com/7c/39/E8d4xsQw_o.png" alt="在这里插入图片描述"></p> 
<pre><code>y.sum().backward()
xyplot(x, x.grad, 'grad of relu')
</code></pre> 
<p><img src="https://images2.imgbox.com/66/c7/UnFfwDFO_o.png" alt="在这里插入图片描述"><br> <strong>2、Sigmoid函数</strong></p> 
<p>sigmoid函数可以将元素的值变换到0和1之间：<br> <img src="https://images2.imgbox.com/c7/0d/qjqYLil7_o.png" alt="在这里插入图片描述"></p> 
<pre><code>y = x.sigmoid()
xyplot(x, y, 'sigmoid')
</code></pre> 
<p><img src="https://images2.imgbox.com/1f/b1/IMFYQS3z_o.png" alt="在这里插入图片描述"><br> 依据链式法则，sigmoid函数的导数<br> <img src="https://images2.imgbox.com/78/67/hvIResUt_o.png" alt="在这里插入图片描述"><br> 下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p> 
<pre><code>x.grad.zero_()
y.sum().backward()
xyplot(x, x.grad, 'grad of sigmoid')
</code></pre> 
<p><img src="https://images2.imgbox.com/9e/5d/HTMvAlIq_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="14__86"></a>1.4 多层感知机</h3> 
<p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：<br> <img src="https://images2.imgbox.com/93/b1/W2W7tjfL_o.png" alt="在这里插入图片描述"><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ϕ 
        
       
      
        \phi 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">ϕ</span></span></span></span></span>表示激活函数。</p> 
<h2><a id="_91"></a>二、多层感知机从零开始的实现</h2> 
<pre><code>import torch
import numpy as np
import sys
sys.path.append("/home/kesci/input")
import d2lzh1981 as d2l
print(torch.__version__)

</code></pre> 
<p>1.3.0</p> 
<h3><a id="21__104"></a>2.1 获取训练集</h3> 
<pre><code>batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='/home/kesci/input/FashionMNIST2065')
</code></pre> 
<h3><a id="22__111"></a>2.2 定义模型参数</h3> 
<pre><code>num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
b1 = torch.zeros(num_hiddens, dtype=torch.float)
W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
b2 = torch.zeros(num_outputs, dtype=torch.float)

params = [W1, b1, W2, b2]
for param in params:
    param.requires_grad_(requires_grad=True)

</code></pre> 
<h3><a id="23__127"></a>2.3 定义激活函数</h3> 
<pre><code>def relu(X):
    return torch.max(input=X, other=torch.tensor(0.0))
</code></pre> 
<h3><a id="24__134"></a>2.4 定义网络</h3> 
<pre><code>def net(X):
    X = X.view((-1, num_inputs))
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2
</code></pre> 
<h3><a id="25__143"></a>2.5 定义损失函数</h3> 
<pre><code>loss = torch.nn.CrossEntropyLoss()
</code></pre> 
<h3><a id="26__149"></a>2.6 训练</h3> 
<pre><code>num_epochs, lr = 5, 100.0
# def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
#               params=None, lr=None, optimizer=None):
#     for epoch in range(num_epochs):
#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
#         for X, y in train_iter:
#             y_hat = net(X)
#             l = loss(y_hat, y).sum()
#             
#             # 梯度清零
#             if optimizer is not None:
#                 optimizer.zero_grad()
#             elif params is not None and params[0].grad is not None:
#                 for param in params:
#                     param.grad.data.zero_()
#            
#             l.backward()
#             if optimizer is None:
#                 d2l.sgd(params, lr, batch_size)
#             else:
#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到
#             
#             
#             train_l_sum += l.item()
#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
#             n += y.shape[0]
#         test_acc = evaluate_accuracy(test_iter, net)
#         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))

d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)
</code></pre> 
<p>epoch 1, loss 0.0030, train acc 0.712, test acc 0.806<br> epoch 2, loss 0.0019, train acc 0.821, test acc 0.806<br> epoch 3, loss 0.0017, train acc 0.847, test acc 0.825<br> epoch 4, loss 0.0015, train acc 0.856, test acc 0.834<br> epoch 5, loss 0.0015, train acc 0.863, test acc 0.847</p> 
<h2><a id="pytorch_190"></a>三、多层感知机pytorch实现</h2> 
<pre><code>import torch
from torch import nn
from torch.nn import init
import numpy as np
import sys
sys.path.append("/home/kesci/input")
import d2lzh1981 as d2l

print(torch.__version__)
</code></pre> 
<h3><a id="31__203"></a>3.1 初始化模型和各个参数</h3> 
<pre><code>num_inputs, num_outputs, num_hiddens = 784, 10, 256
    
net = nn.Sequential(
        d2l.FlattenLayer(),
        nn.Linear(num_inputs, num_hiddens),
        nn.ReLU(),
        nn.Linear(num_hiddens, num_outputs), 
        )
    
for params in net.parameters():
    init.normal_(params, mean=0, std=0.01)
</code></pre> 
<h3><a id="32__219"></a>3.2 训练</h3> 
<pre><code>batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root='/home/kesci/input/FashionMNIST2065')
loss = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)

num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)
</code></pre> 
<p>epoch 1, loss 0.0031, train acc 0.701, test acc 0.774<br> epoch 2, loss 0.0019, train acc 0.821, test acc 0.806<br> epoch 3, loss 0.0017, train acc 0.841, test acc 0.805<br> epoch 4, loss 0.0015, train acc 0.855, test acc 0.834<br> epoch 5, loss 0.0014, train acc 0.866, test acc 0.840</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6f0a7deb9357d8d6e29f538f1dd06431/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何获取网易云音频地址</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/30f99ac9ce58be586a94cc9f5a3cad60/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">利用python爬取新冠肺炎疫情实时数据&#43;可视化展示</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>