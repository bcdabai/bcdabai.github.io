<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Swin-Transformer论文解析 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Swin-Transformer论文解析" />
<meta property="og:description" content="目录 Swin-TransformerAttention机制的发展历程Attention中Q、K、V的概念Attention的计算过程swin-transformer 与VIT的区别swin-transformer整体架构Patch MergingSwin-transformer BlockSwin-transformer中的掩码机制自注意力的复杂度计算Swin-Transformer的详细架构效果比较ImageNet数据集COCO数据集语义分割数据集ADE20K Swin-Transformer Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang Stephen Lin, Baining Guo
Computer Vision and Pattern Recognition (2021)
Microsoft Research Asia
https://github.com/microsoft/Swin-Transformer
Swin: Shifted Windows
swin-transformer是什么？
swin-transformer是可以用于计算机视觉任务的通用主干网络，可以用于图像分类、图像分割、目标检测等一系列视觉下游任务。
下面是swin-transformer在各个领域的一些应用
解决了什么问题？
与NLP领域不同，视觉领域同类的物体，在不同图像上/同一图像上的尺度会相差巨大；相较于文本，图像的尺寸过大，计算复杂度较高； 优点是什么？
提出了一种层级式网络结构，解决视觉图像的多尺度问题；提出Shifted Windows，极大降低了transformer的计算复杂度；可以广泛应用到所有计算机视觉领域； 结论是什么？
Transformer完全可以在各个领域取代CNN
效果怎么样？
在ImageNet上并非SOTA，仅与EfficientNet的性能差不多
swin-transformer的优点不是在于分类，在分类上的提升不是太多，而在检测、分割等下游任务中，有巨大的提升.
在目标检测数据集COCO上，比当时的SOTA提高了2.7点左右的mAP
下面是paper with code上的最新截图，几乎全是Swin-Transformer的变体
在语义分割数据集ADE20K上，比当时的SOTA提高了3.2点左右的mAP
下面是paper with code上的最新截图，也几乎全是Swin-Transformer的变体
Attention机制的发展历程 Bengio团队于2014年提出Attention机制，用于机器翻译中，使模型自动寻找原文本中与预测目标单词相关的内容
Bahdanau D, Cho K, Bengio Y." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/59f9d91d37150e2c685c77322aefb0d2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-04T15:10:57+08:00" />
<meta property="article:modified_time" content="2022-08-04T15:10:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Swin-Transformer论文解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#SwinTransformer_2" rel="nofollow">Swin-Transformer</a></li><li><ul><li><a href="#Attention_38" rel="nofollow">Attention机制的发展历程</a></li><li><a href="#AttentionQKV_52" rel="nofollow">Attention中Q、K、V的概念</a></li><li><a href="#Attention_57" rel="nofollow">Attention的计算过程</a></li><li><a href="#swintransformer_VIT_68" rel="nofollow">swin-transformer 与VIT的区别</a></li><li><a href="#swintransformer_90" rel="nofollow">swin-transformer整体架构</a></li><li><a href="#Patch_Merging_104" rel="nofollow">Patch Merging</a></li><li><a href="#Swintransformer_Block_114" rel="nofollow">Swin-transformer Block</a></li><li><a href="#Swintransformer_121" rel="nofollow">Swin-transformer中的掩码机制</a></li><li><a href="#_141" rel="nofollow">自注意力的复杂度计算</a></li><li><a href="#SwinTransformer_149" rel="nofollow">Swin-Transformer的详细架构</a></li><li><a href="#_153" rel="nofollow">效果比较</a></li><li><ul><li><a href="#ImageNet_154" rel="nofollow">ImageNet数据集</a></li><li><a href="#COCO_160" rel="nofollow">COCO数据集</a></li><li><a href="#ADE20K_173" rel="nofollow">语义分割数据集ADE20K</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="SwinTransformer_2"></a>Swin-Transformer</h2> 
<p><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.</strong><br> Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang Stephen Lin, Baining Guo<br> Computer Vision and Pattern Recognition (2021)<br> Microsoft Research Asia<br> https://github.com/microsoft/Swin-Transformer<br> Swin: Shifted Windows</p> 
<p>swin-transformer<strong>是什么</strong>？<br> swin-transformer是可以用于计算机视觉任务的通用主干网络，可以用于图像分类、图像分割、目标检测等一系列视觉下游任务。<br> 下面是swin-transformer在各个领域的一些应用<br> <img src="https://images2.imgbox.com/14/73/NUYIbjP0_o.png" alt="在这里插入图片描述"></p> 
<p><strong>解决了什么问题</strong>？</p> 
<ol><li>与NLP领域不同，视觉领域同类的物体，在不同图像上/同一图像上的尺度会相差巨大；</li><li>相较于文本，图像的尺寸过大，计算复杂度较高；</li></ol> 
<p><strong>优点是什么</strong>？</p> 
<ol><li>提出了一种层级式网络结构，解决视觉图像的多尺度问题；</li><li>提出Shifted Windows，极大降低了transformer的计算复杂度；</li><li>可以广泛应用到所有计算机视觉领域；</li></ol> 
<p><strong>结论是什么</strong>？<br> Transformer完全可以在各个领域取代CNN</p> 
<p><strong>效果怎么样</strong>？<br> 在ImageNet上并非SOTA，仅与EfficientNet的性能差不多<br> swin-transformer的优点不是在于分类，在分类上的提升不是太多，而在检测、分割等下游任务中，有巨大的提升.<br> <img src="https://images2.imgbox.com/ad/e6/8bek7MTU_o.png" alt="在这里插入图片描述"><br> 在目标检测数据集COCO上，比当时的SOTA提高了2.7点左右的mAP<br> 下面是paper with code上的最新截图，几乎全是Swin-Transformer的变体<br> <img src="https://images2.imgbox.com/8e/40/JRc0anBa_o.png" alt="在这里插入图片描述"><br> 在语义分割数据集ADE20K上，比当时的SOTA提高了3.2点左右的mAP<br> 下面是paper with code上的最新截图，也几乎全是Swin-Transformer的变体<br> <img src="https://images2.imgbox.com/46/89/snh45Xxb_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Attention_38"></a>Attention机制的发展历程</h3> 
<ol><li> <p>Bengio团队于2014年提出Attention机制，用于机器翻译中，使模型自动寻找原文本中与预测目标单词相关的内容<br> Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.</p> </li><li> <p>谷歌团队于2017年提出基于注意力机制的Transformer架构代替RNN<br> Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need [C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.</p> </li><li> <p>2018年Momenta提出SENet，开始将Attention机制与CNN结合，用于图像领域<br> Jie Hu et al. Squeeze-and-Excitation Networks. Computer vision and Pattern recognition (2018)</p> </li><li> <p>谷歌大脑于2020年提出第一篇应用于图像领域纯Attention网络ViT<br> Alexey Dosovitskiy et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” arXiv: Computer Vision and Pattern Recognition (2020)</p> </li></ol> 
<h3><a id="AttentionQKV_52"></a>Attention中Q、K、V的概念</h3> 
<p>以商品搜索为例<br> Query：搜索引擎上输入的内容，例如商品名称<br> Key：搜索引擎根据Query为你匹配Key，例如商品的种类，颜色，描述<br> Value：搜索引擎根据Query和Key的相似度得到匹配的内容</p> 
<h3><a id="Attention_57"></a>Attention的计算过程</h3> 
<p>参考链接：https://jalammar.github.io/illustrated-transformer</p> 
<ol><li>将输入单词转化成嵌入向量；</li><li>嵌入向量通过全连接层，得到q、k、v三个向量；</li><li>为每个向量计算一个score：score=k*v attention</li><li>为保持梯度稳定，score除以 (d_k为q, k, v的向量维度)</li><li>对score施以softmax激活函数；<br> softmax分数决定每个单词对编码当下位置（“Thinking”）的贡献</li><li>softmax(score)点乘v，得到加权v向量</li><li>v相加之后得到最终的输出结果z<br> <img src="https://images2.imgbox.com/c7/b5/BBtngleM_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/90/64/IxZdncLU_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="swintransformer_VIT_68"></a>swin-transformer 与VIT的区别</h3> 
<p><img src="https://images2.imgbox.com/1f/dd/BAuSUhgD_o.png" alt="在这里插入图片描述"><br> vit：</p> 
<ol><li>使用低分辨率的输入<br> 计算复杂度与图像大小是二次函数关系</li><li>全局自注意力计算</li><li>特征图大小固定</li></ol> 
<p>将图像划分成16x16的patch，每一层的transformer得到的特征图都是16倍下采样的大小, 没有多尺寸特征，感受野相同；<br> 可以通过全局的自注意力机制达到全局的建模能力，但是对多尺寸特征的提取能力弱，不利于目标检测、分割等下游任务，检测分割时只能从bottleneck上提取特征，无法从主干提取浅层特征；<br> 自注意力始终在整图上进行，计算复杂度是图像尺寸的平方，无法使用大分辨率的图像；</p> 
<p>swin-transformer:</p> 
<ol><li>使用任意尺度的输入<br> 计算复杂度与图像大小是线性关系</li><li>在小窗口内计算自注意力</li><li>特征图大小递减</li></ol> 
<p>在小窗口内计算自注意力，而不是在整图上计算自注意力，当窗口大小固定，自注意力的复杂度就固定，整图的计算复杂度与图像尺度呈线性增长关系，利用了图像的局部性的先验知识；<br> 局部性：同一个物体不不同部位，或语义相近的不同物体，大概率会出现在相邻的地方，所以对视觉任务来说，在小窗口内计算自注意力是合理的，在全局计算会造成计算的浪费；</p> 
<h3><a id="swintransformer_90"></a>swin-transformer整体架构</h3> 
<p><img src="https://images2.imgbox.com/1b/c5/obarR4Zi_o.png" alt="在这里插入图片描述"><br> Stage1</p> 
<ol><li>Patch Partition: 图像分块 patch size=4x4 4<em>4</em>3=48 224<em>224</em>3-&gt;56<em>56</em>48</li><li>Linear Embeding: 将图像patch映射成目标维度C 56<em>56</em>48-&gt;56<em>56</em>96</li><li>swin-transformer block: 自注意力计算</li></ol> 
<p>Stage2</p> 
<ol><li>patch merging: Pixel Shuffle的逆操作，将patch块合并，特征图大小减半，通道数翻倍，增大感受野，形成特征金字塔</li><li>swin-transformer block: 自注意力计算</li></ol> 
<p>Stage3与Stage4重复Stage2，仅是swin-transformer block数量不同</p> 
<h3><a id="Patch_Merging_104"></a>Patch Merging</h3> 
<p><img src="https://images2.imgbox.com/4d/86/VoEtCiAX_o.png" alt="在这里插入图片描述"><br> Pixel Shuffle的逆操作<br> 将patch块合并，特征图大小减半，通道数翻倍<br> 起到类似pooling层的作用，增大感受野，形成特征金字塔</p> 
<p>有利于检测、分割等下游视觉任务</p> 
<h3><a id="Swintransformer_Block_114"></a>Swin-transformer Block</h3> 
<p><img src="https://images2.imgbox.com/63/ed/EOeOT3VT_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/eb/8c/Owa9mBtk_o.png" alt="在这里插入图片描述"><br> Swin Transformer结构中，</p> 
<ol><li>在网络第l层，采用规则的窗口划分图像，并在每个窗口内计算自注意力</li><li>在第l+1层中，窗口分区移位，产生新窗口<br> 新窗口中的自注意力计算跨越了第l层中窗口的边界，提供了它们之间的联系，增强全局建模能力</li></ol> 
<h3><a id="Swintransformer_121"></a>Swin-transformer中的掩码机制</h3> 
<p><img src="https://images2.imgbox.com/25/3e/HF6u7qZF_o.png" alt="在这里插入图片描述"><br> shifted-window存在的问题：</p> 
<ol><li>shift之后窗口的数量增加</li><li>新增窗口大小不一致，无法进行批运算</li></ol> 
<p>常规解决方案：<br> 将小窗口padding后进行批运算，但是会造成计算资源的巨大浪费</p> 
<p>Swin-transformer解决方案：掩码机制</p> 
<ol><li>按照新窗口的划分切割特征图</li><li>特征图循环移位拼接，新窗口的大小相同(AB、AC等之间语义不同，不应该进行自注意力)</li><li>掩码机制计算：先将特征图拉长，在与转置之后的向量相乘，得到自注意力矩阵，mask矩阵内自注意力部分为赋值为0，非自注意力部分赋值为很大的负数，mask矩阵与自注意力矩阵相加</li><li>将计算完自注意力的特征图逆循环位移，还原原特征图位置<br> <img src="https://images2.imgbox.com/2f/17/sCd9jX4e_o.png" alt="在这里插入图片描述"></li></ol> 
<p>优点：没有增加额外的窗口数量，只需要一次计算就可以得到自注意力矩阵，提高了shifted-window的批计算效率；</p> 
<h3><a id="_141"></a>自注意力的复杂度计算</h3> 
<p><img src="https://images2.imgbox.com/9f/8f/a7Ml6TPC_o.png" alt="在这里插入图片描述"><br> 普通多头自注意力：<br> <img src="https://images2.imgbox.com/e8/3e/NJTL62Qu_o.png" alt="在这里插入图片描述"><br> 基于窗口的多头自注意力：<br> <img src="https://images2.imgbox.com/2f/3a/KAC2cr0M_o.png" alt="在这里插入图片描述"><br> 计算复杂度由图像大小的二次函数变为了图像大小的一次函数</p> 
<h3><a id="SwinTransformer_149"></a>Swin-Transformer的详细架构</h3> 
<p><img src="https://images2.imgbox.com/5d/b9/YXJ2v0ib_o.png" alt="在这里插入图片描述"><br> 不同参数量的Swin-Transformer仅是stage3的swin-block</p> 
<h3><a id="_153"></a>效果比较</h3> 
<h4><a id="ImageNet_154"></a>ImageNet数据集</h4> 
<p>Swin相较于ViT有较大的提升，单仅与EffNet相差不多<br> 提升数据集规模，ViT与Swin都有较大提升</p> 
<p>Swin-transformer在分类上提升不多，但在检测与分割等下游任务上有巨大提升。<br> <img src="https://images2.imgbox.com/b3/c3/xkmNBFd9_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="COCO_160"></a>COCO数据集</h4> 
<p>不同算法框架下，Swin与CNN的比较<br> <img src="https://images2.imgbox.com/b0/da/f86hNvqP_o.png" alt="在这里插入图片描述"><br> 结论：Swin可以当作通用的骨干网络使用</p> 
<p>相同同算法框架下，Swin与不同骨干网络的比较<br> <img src="https://images2.imgbox.com/3d/ff/IlpS8Mrv_o.png" alt="在这里插入图片描述"><br> 结论：相似参数量下，Swin的性能更好</p> 
<p>极致性能的比较(各种方法都用上)<br> <img src="https://images2.imgbox.com/77/ee/KXpM3Xyy_o.png" alt="在这里插入图片描述"><br> 结论：Swin-L的性能最优</p> 
<h4><a id="ADE20K_173"></a>语义分割数据集ADE20K</h4> 
<p>各种方法都用上后的极致性能的比较<br> <img src="https://images2.imgbox.com/fb/b9/w6fjHZ8d_o.png" alt="在这里插入图片描述"><br> 结论：Swin-L的性能最优(UperNet框架)</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0a570c77885a9bde0bee316840e3f6ec/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">小程序笔记3</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/368ceb5eebb26030c0c2483632085999/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">supervisor 命令详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>