<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>VQGAN-CLIP: Open Domain Image Generationand Editing with Natural Language Guidance - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="VQGAN-CLIP: Open Domain Image Generationand Editing with Natural Language Guidance" />
<meta property="og:description" content="1.VQGAN-CLIP: 在自然语言指导下开放域图像生成和编辑
机构：EleutherAI
github:GitHub - EleutherAI/vqgan-clip
2.介绍和摘要
摘要：从开放域文本提示生成和编辑图像是一项具有挑战性的任务，迄今为止，它需要昂贵且经过专门训练的模型。我们为这两个任务演示了一种新颖的方法，该方法能够通过使用多模态编码器来指导图像生成，而无需任何训练就可以从具有显着语义复杂性的文本提示中产生高视觉质量的图像。我们在各种任务上演示了如何使用CLIP 来指导VQGAN产生比以前更高的视觉质量输出，尽管没有为提出的任务进行培训。
示例vqgan-clip世代及其文本提示。选择提示以演示vqgan-clip能够产生的一系列视觉样式，包括古典艺术 (g，i)，现代艺术 (l)，图纸 (e)，油 画(a) 以及其他由于空间而未包括在内的样式。
贡献：
1.图像生成和处理的高视觉质量。
2.文本和生成物之间的语义保真度高，特别是当语义上不可能的内容共同出现时。
3.效率，因为我们的方法不需要除了预先训练的模型之外的额外训练，只需要使用少量的每个推理优化。
4.开放开发和研究的价值。这项技术是在公共场合开发的，开放式协作对于其在现实世界中的快速成功至关重要。非作者已经将我们的方法扩展到其他方式 (例如，为音频替换文本) 和商业应用程序
3.方法（method） 为了证明我们方法的有效性，我们使用VQGAN 和CLIP 作为预先训练的模型来应用它，因此将我们的方法称为vqgan-clip。然而，我们强调，我们的方法并不特定于这两个模型，随后的工作已经显示出成功的基础是我们使用其他模型的工作，甚至在其他模式。
我们从文本提示开始，使用 GAN 迭代生成候选图像，在每一步使用 CLIP 改进图像。我们通过处理 VQGAN-CLIP 的嵌入之间的平方球面距离来优化图像：候选图像嵌入和文本提示的嵌入来计算他们之间的损失函数，并通过clip区分图像的GAN潜在矢量表示，我们将其称为Oord，Vinyals和Kavukcuoglu之后的 “z矢量” 。这个过程在图1中概述。
显示如何添加增强以稳定和改进优化的图。应用多种crop，每种crop具有不同的随机增强，以在单个来源产生平均损失。这改善了相对于单个潜在Z矢量的结果。 要生成图像，“初始图像”包含随机像素值。重复优化过程以改变图像，直到输出图像逐渐改善，使其在语义上与目标文本匹配。我们还可以通过将要编辑的图像作为“初始图像”来编辑现有图像。用于描述我们希望用文本提示来更改图像，并且除了如何选择“初始图像”之外，在生成和操作之间不存在架构变化 （模型架构统一） 3.1图像的离散潜在空间
通过使用编码器 E 和解码器 G 构建卷积自动编码器，将其应用于图像。输入图像 x ∈ I 首先嵌入编码器 z = E(x)。然后我们可以将向量量化嵌入 x 计算为：
然后我们可以将其乘以词汇表以执行重建。然后，我们可以在量化步骤上使用直通式估计器，以便让 CNN 和码本进行端到端的联合训练。我们使用流行的 VQGAN [11] 模型进行本文的实验
3.2 Contrastive Text-Image Models
为了指导生成模型，我们需要一种方法来调整候选生成与指导文本的相似度。为实现这一目标，我们使用 CLIP 独立嵌入文本和生成图像，并测量嵌入之间的余弦相似度。然后将这种相似性重新定义为我们可以使用梯度下降来最小化的损失。
3.3 Augmentations
使用 vqgan-clip 的一个挑战是，如果在单个图像上计算，来自 CLIP 损失的梯度更新非常嘈杂。为了克服这个问题，我们采用生成的候选图像并对其进行多次修改，从而产生大量增强图像。我们随机裁剪生成的候选图像，然后应用进一步的增强，例如翻转、颜色抖动、噪声等。图像的大多数高级语义特征对这些变化相对不变，因此平均 CLIP 损失相对于所有增强图像都减少了每个更新步骤的方差。随机裁剪可能会显着改变图像的语义内容（例如裁剪掉一个重要的对象），但我们发现在实践中这不会导致任何问题" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/7a52c59ba4c87952eb3768f0d43b7e25/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-28T16:01:08+08:00" />
<meta property="article:modified_time" content="2022-11-28T16:01:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">VQGAN-CLIP: Open Domain Image Generationand Editing with Natural Language Guidance</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#fe2c24;"><strong>1.VQGAN-CLIP: 在自然语言指导下开放域图像生成和编辑</strong></span></p> 
<p>机构：EleutherAI</p> 
<p>github:<a href="https://github.com/EleutherAI/vqgan-clip" title="GitHub - EleutherAI/vqgan-clip">GitHub - EleutherAI/vqgan-clip</a></p> 
<p><img alt="" height="351" src="https://images2.imgbox.com/c9/a5/Fe3KQ9sR_o.png" width="991"></p> 
<p> </p> 
<p>2.介绍和摘要</p> 
<p>        摘要：从开放域文本提示生成和编辑图像是一项具有挑战性的任务，迄今为止，它需要昂贵且经过专门训练的模型。我们为这两个任务演示了一种新颖的方法，该方法能够通过使用多模态编码器来指导图像生成，而无需任何训练就可以从具有显着语义复杂性的文本提示中产生高视觉质量的图像。我们在各种任务上演示了如何使用CLIP 来指导VQGAN产生比以前更高的视觉质量输出，尽管没有为提出的任务进行培训。</p> 
<p><img alt="" height="692" src="https://images2.imgbox.com/c3/cd/hCI73bmb_o.png" width="543"></p> 
<p>        示例vqgan-clip世代及其文本提示。选择提示以演示vqgan-clip能够产生的一系列视觉样式，包括古典艺术 (g，i)，现代艺术 (l)，图纸 (e)，油 画(a) 以及其他由于空间而未包括在内的样式。</p> 
<p><span style="background-color:#38d8f0;">贡献：</span></p> 
<p>1.图像生成和处理的高视觉质量。</p> 
<p>2.文本和生成物之间的语义保真度高，特别是当语义上不可能的内容共同出现时。</p> 
<p>3.效率，因为我们的方法不需要除了预先训练的模型之外的额外训练，只需要使用少量的每个推理优化。</p> 
<p>4.开放开发和研究的价值。这项技术是在公共场合开发的，开放式协作对于其在现实世界中的快速成功至关重要。非作者已经将我们的方法扩展到其他方式 (例如，为音频替换文本) 和商业应用程序</p> 
<p><span style="color:#fe2c24;"><strong>3.方法（method） </strong></span></p> 
<p>        为了证明我们方法的有效性，我们使用VQGAN 和CLIP 作为预先训练的模型来应用它，因此将我们的方法称为vqgan-clip。然而，我们强调，我们的方法并不特定于这两个模型，随后的工作已经显示出成功的基础是我们使用其他模型的工作，甚至在其他模式。</p> 
<p>        我们从文本提示开始，使用 GAN 迭代生成候选图像，在每一步使用 CLIP 改进图像。我们通过处理 VQGAN-CLIP 的嵌入之间的平方球面距离来优化图像：候选图像嵌入和文本提示的嵌入来计算他们之间的损失函数，并通过clip区分图像的GAN潜在矢量表示，我们将其称为Oord，Vinyals和Kavukcuoglu之后的 “z矢量” 。这个过程在图1中概述。</p> 
<p><img alt="" height="553" src="https://images2.imgbox.com/d5/65/Uk9k5Mvz_o.png" width="1200"></p> 
<p><strong>        显示如何添加增强以稳定和改进优化的图。应用多种crop，每种crop具有不同的随机增强，以在单个来源产生平均损失。这改善了相对于单个潜在Z矢量的结果。 </strong></p> 
<p>        要生成图像，“初始图像”包含随机像素值。重复优化过程以改变图像，直到输出图像逐渐改善，使其在语义上与目标文本匹配。我们还可以通过将要编辑的图像作为“初始图像”来编辑现有图像。用于描述我们希望用文本提示来更改图像，并且除了如何选择“初始图像”之外，在生成和操作之间不存在架构变化 （<span style="color:#956fe7;"><strong>模型架构统一</strong></span>）       </p> 
<p>        <span style="background-color:#38d8f0;">3.1图像的离散潜在空间</span></p> 
<p>通过使用编码器 E 和解码器 G 构建卷积自动编码器，将其应用于图像。输入图像 x ∈ I 首先嵌入编码器 z = E(x)。然后我们可以将向量量化嵌入 x 计算为：</p> 
<p><img alt="" height="146" src="https://images2.imgbox.com/a8/f8/mrihdM90_o.png" width="530"></p> 
<p> 然后我们可以将其乘以词汇表以执行重建。然后，我们可以在量化步骤上使用直通式估计器，以便让 CNN 和码本进行端到端的联合训练。我们使用流行的 VQGAN [11] 模型进行本文的实验</p> 
<p><span style="background-color:#38d8f0;">        3.2 Contrastive Text-Image Models</span></p> 
<p>        为了指导生成模型，我们需要一种方法来调整候选生成与指导文本的相似度。为实现这一目标，我们使用 CLIP 独立嵌入文本和生成图像，并测量嵌入之间的余弦相似度。然后将这种相似性重新定义为我们可以使用梯度下降来最小化的损失。</p> 
<p><span style="background-color:#38d8f0;">        3.3 Augmentations</span></p> 
<p>        使用 vqgan-clip 的一个挑战是，如果在单个图像上计算，来自 CLIP 损失的梯度更新非常嘈杂。为了克服这个问题，我们采用生成的候选图像并对其进行多次修改，从而产生大量增强图像。我们随机裁剪生成的候选图像，然后应用进一步的增强，例如翻转、颜色抖动、噪声等。图像的大多数高级语义特征对这些变化相对不变，因此平均 CLIP 损失相对于所有增强图像都减少了每个更新步骤的方差。随机裁剪可能会显着改变图像的语义内容（例如裁剪掉一个重要的对象），但我们发现在实践中这不会导致任何问题</p> 
<p><span style="background-color:#38d8f0;">        3.4正则化潜在向量</span></p> 
<p>        当使用无约束的VQGAN生成图像时，我们发现输出往往是非结构化的。添加增强功能有助于总体一致性，但最终输出通常仍将包含不需要的纹理补丁。为了解决这个问题，我们将加权L2正则化应用于z向量。</p> 
<p><img alt="" height="191" src="https://images2.imgbox.com/ae/52/dUKiX0Ag_o.png" width="973"></p> 
<p>    <strong>    其中 α 是正则化权重</strong>。这鼓励了表示中的简约性，将 VQGAN 代码本中的低信息代码发送到零。在实践中，我们注意到正则化似乎可以提高输出的连贯性并产生更好的结构化图像。我们在生成过程中将正则化项衰减 0.005 </p> 
<p><span style="color:#fe2c24;"><strong>4语义图像生成</strong></span></p> 
<p>我们的重点是制作与自然语言提示相关的语义上有意义的高视觉质量图像</p> 
<p><img alt="" height="596" src="https://images2.imgbox.com/29/07/w43zbItu_o.png" width="1200"></p> 
<p><span style="background-color:#38d8f0;">4.1 Artistic Impressions </span></p> 
<p><img alt="" height="690" src="https://images2.imgbox.com/b4/4c/LR1m02yl_o.png" width="1200"></p> 
<p><span style="background-color:#38d8f0;">4.2 Comparisons to Other Approaches </span></p> 
<p><img alt="" height="306" src="https://images2.imgbox.com/06/a1/UkxnSbs3_o.png" width="1054"></p> 
<p><img alt="" height="765" src="https://images2.imgbox.com/c5/06/EGd9Orub_o.png" width="1104"> </p> 
<p><span style="color:#fe2c24;"><strong> 5.图像编辑</strong></span></p> 
<p><img alt="" height="454" src="https://images2.imgbox.com/fc/20/czZk3u5X_o.png" width="919"></p> 
<p><span style="color:#fe2c24;"><strong> 6.Conclusion</strong></span></p> 
<p>        我们介绍了 vqgan-clip，这是一种仅基于人类书面文本提示生成和处理图像的方法。我们的模型生成的质量具有高视觉保真度，并且忠实于文本提示，优于 DALL-E 和 GLIDE 等先前的方法。保真度已通过商业成功和多家公司的使用进行了外部验证。与唯一可比的基于文本的图像编辑方法相比，vqgan-clip 继续生成更高质量的视觉图像——尤其是当文本提示和图像内容具有较低的语义相似性时。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/91dbf8bf679197dcf250dff079d06e52/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java重复注解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ef1a36da451d2fe71b1e9eb1edd0f076/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux之SSH、rsync</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>