<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pythonï¼šä½¿ç”¨seleniumçˆ¬å–51jobï¼ˆå‰ç¨‹æ— å¿§ï¼‰å¹¶å°†çˆ¬å–æ•°æ®å­˜å‚¨åˆ°MySqlæ•°æ®åº“ä¸­çš„ä»£ç å®ä¾‹ - ç¼–ç¨‹å¤§ç™½çš„åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pythonï¼šä½¿ç”¨seleniumçˆ¬å–51jobï¼ˆå‰ç¨‹æ— å¿§ï¼‰å¹¶å°†çˆ¬å–æ•°æ®å­˜å‚¨åˆ°MySqlæ•°æ®åº“ä¸­çš„ä»£ç å®ä¾‹" />
<meta property="og:description" content="è‡ªå·±æ£é¼“äº†å‡ å¤©å†™çš„ä»£ç ï¼ŒåŸºæœ¬ä¸ŠæŠŠ51jobçš„å²—ä½ç›¸å…³çš„æ•°æ®éƒ½çˆ¬ä¸‹æ¥äº†ï¼Œå¯ä»¥è§†è¦æ±‚è‡ªè¡Œå¢å‡ï¼Œä»£ç è™½ç„¶æœ‰äº›ç®€é™‹ï¼Œä¸è¿‡æˆ‘çˆ¬å–çš„æ—¶å€™æ²¡æŠ¥ä»€ä¹ˆé”™ã€‚ä»£ç é€‚åˆåˆå­¦è€…å­¦ä¹ ä½¿ç”¨ï¼ŒåºŸè¯ä¸å¤šè¯´ï¼Œä»£ç å¦‚ä¸‹ï¼š
from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import NoSuchElementException from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.common.by import By from selenium import webdriver from time import sleep import pymysql import re class Crawler: def __init__(self): self.wd = webdriver.Chrome() self.wd.implicitly_wait(20) self.DBHOST = &#34;localhost&#34; self.DBUSER = &#34;root&#34; self.DBPASS = &#34;123456&#34; self.DBNAME = &#34;51job&#34; # è·å–å½“å‰é¡µé¢çš„æ•°æ® def getData(self, len_Css): rows = [] for i in range(1, len_Css): # å²—ä½åç§° job_name = self.wd.find_element(By.CSS_SELECTOR, &#39;div." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8239be0de79298c7293e717782b56354/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-27T14:48:43+08:00" />
<meta property="article:modified_time" content="2021-07-27T14:48:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§ç™½çš„åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§ç™½çš„åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pythonï¼šä½¿ç”¨seleniumçˆ¬å–51jobï¼ˆå‰ç¨‹æ— å¿§ï¼‰å¹¶å°†çˆ¬å–æ•°æ®å­˜å‚¨åˆ°MySqlæ•°æ®åº“ä¸­çš„ä»£ç å®ä¾‹</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>Â  Â  Â  Â  Â è‡ªå·±æ£é¼“äº†å‡ å¤©å†™çš„ä»£ç ï¼ŒåŸºæœ¬ä¸ŠæŠŠ51jobçš„å²—ä½ç›¸å…³çš„æ•°æ®éƒ½çˆ¬ä¸‹æ¥äº†ï¼Œå¯ä»¥è§†è¦æ±‚è‡ªè¡Œå¢å‡ï¼Œä»£ç è™½ç„¶æœ‰äº›ç®€é™‹ï¼Œä¸è¿‡æˆ‘çˆ¬å–çš„æ—¶å€™æ²¡æŠ¥ä»€ä¹ˆé”™ã€‚ä»£ç é€‚åˆåˆå­¦è€…å­¦ä¹ ä½¿ç”¨ï¼ŒåºŸè¯ä¸å¤šè¯´ï¼Œä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.common.by import By
from selenium import webdriver
from time import sleep
import pymysql
import re

class Crawler:
    def __init__(self):
        self.wd = webdriver.Chrome()
        self.wd.implicitly_wait(20)
        self.DBHOST = "localhost"
        self.DBUSER = "root"
        self.DBPASS = "123456"
        self.DBNAME = "51job"
    # è·å–å½“å‰é¡µé¢çš„æ•°æ®
    def getData(self, len_Css):
        rows = []
        for i in range(1, len_Css):
            # å²—ä½åç§°
            job_name = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) span.jname.at'.format(i)).text
            # å…¬å¸åç§°
            company_name = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) a.cname.at'.format(i)).text
            # åŸå¸‚ å·¥ä½œç»éªŒ å­¦å† æ‹›è˜äººæ•°
            al = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) span.d.at'.format(i)).text.split('|')
            # åˆ†åˆ«å¯¹åº”ä¸åŒæƒ…å†µï¼Œæœ‰çš„å²—ä½ç¼ºå°‘å­¦å†ï¼Œæœ‰çš„ç¼ºå°‘å·¥ä½œç»éªŒ
            if len(al) == 4:
                city = al[0]
                experience = al[1]
                education = al[2]
                recruits_Number = al[3]
            elif len(al) == 3:
                city = al[0]
                experience = al[1]
                education = None
                recruits_Number = al[2]
            elif len(al) == 2:
                city = al[0]
                experience = None
                education = None
                recruits_Number = al[1]
            else:
                city = None
                experience = None
                education = None
                recruits_Number = None
            # å‘å¸ƒæ—¥æœŸ
            release_Date = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) span.time'.format(i)).text
            # å…¬å¸ç¦åˆ©
            # æœ‰çš„å²—ä½ä¸èƒ½å®šä½åˆ°ç¦åˆ©å…ƒç´ ï¼Œé€šè¿‡è‡ªå®šä¹‰NoExistsæ–¹æ³•åˆ¤æ–­èƒ½å¦å®šä½åˆ°å…ƒç´ 
            # if self.NoExists('div.j_joblist &gt; div:nth-child({0}) p.tags'.format(i)):
            #     welfare = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) p.tags'.format(i)).get_attribute("title")
            # else:
            #     welfare = None
            # è–ªæ°´
            # æœ‰çš„å²—ä½è–ªæ°´èƒ½å®šä½åˆ°å…ƒç´ ï¼Œä½†æ˜¯æ˜¯ç©ºä¸²ï¼Œé˜²æ­¢æŠ¥é”™
            if bool(self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) span.sal'.format(i)).text):
                salary = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) span.sal'.format(i)).text
            else:
                salary = None
            # å…¬å¸ç±»å‹
            company_type = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) p.int.at'.format(i)).text
            # æ‹›è˜è¯¦æƒ…url
            job_ex_url = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) a.el[target=_blank]'.format(i)).get_attribute("href")
            # å…¬å¸url
            company_url = self.wd.find_element(By.CSS_SELECTOR, 'div.j_joblist &gt; div:nth-child({0}) a.cname.at'.format(i)).get_attribute("href")
            rows.append([job_name, company_name, city, experience, education, recruits_Number, release_Date, salary, company_type, job_ex_url, company_url])
        return rows
    # å°†çˆ¬å–çš„æ•°æ®å­˜è¿›æ•°æ®åº“
    def saveData(self, rows):
        db = pymysql.connect(host=self.DBHOST, user=self.DBUSER, password=self.DBPASS, database=self.DBNAME)
        cur = db.cursor()
        sql = "INSERT INTO ods_51job_job(job_name, company_name, job_city, job_experience, job_education, recruits_Number, release_Date, salary, company_type, job_ex_url, company_url) " \
              "VALUE (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"
        try:
            for row in rows:
                cur.execute(sql, row)
                db.commit()
        except pymysql.Error as e:
            print(e)
        finally:
            cur.close()
            db.close()
    # ä¸€æ¬¡çˆ¬å–å­˜å‚¨ä¸€é¡µæ•°æ®ï¼Œè‡ªåŠ¨é€’å¢ç›´åˆ°çˆ¬å®Œ
    def scrapingData(self, City, keyWord, start_Page):
        wait = WebDriverWait(self.wd, 20, 0.5)
        
        # å¾—å‡ºæ€»é¡µæ•°
        isNextpage = self.wd.find_element(By.CSS_SELECTOR,
                                     'body &gt; div:nth-child(4) &gt; div.j_result &gt; div &gt; div.leftbox &gt; div:nth-child(4) &gt; div.j_page &gt; div &gt; div &gt; div &gt; span:nth-child(1)').text
        result = re.findall(r'\d+', isNextpage)
        condition = int(result[0])
        
        sleep(2)
        print('åŸå¸‚ç¼–å·ï¼š%s  å…³é”®è¯ï¼š%s  æ€»é¡µæ•°ï¼š%d' % (City, keyWord, condition))
        
        while start_Page &lt;= condition:
            # å½“å‰é¡µé¢æ€»å…±æœ‰å¤šå°‘æ¡æ‹›è˜å²—ä½ï¼ˆä¸€èˆ¬æ˜¯50æ¡ï¼‰
            pubCss = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,
                                                                  'body &gt; div:nth-child(4) &gt; div.j_result &gt; div &gt; div.leftbox &gt; div:nth-child(4) &gt; div.j_joblist &gt; div.e')))
            # è·å–å½“å‰é¡µé¢æ•°æ®å¹¶å­˜è¿›æ•°æ®åº“
            rows1 = self.getData(len(pubCss)+1)
            self.saveData(rows1)
            print('\tå·²çˆ¬å–ç¬¬%dé¡µ;' % start_Page)
            
            # åˆ¤æ–­æ˜¯å¦æœ€åä¸€é¡µ
            if start_Page &lt; condition:
                nextpage = self.wd.find_element(By.CSS_SELECTOR, 'li.next a[style="cursor: pointer;"]')
                nextpage.click()
                self.wd.refresh()
                start_Page += 1
            else:
                print('å·²çˆ¬å–å®Œå½“å‰åŸå¸‚å…³é”®è¯ï¼')
                break
            sleep(2)

    def NoExists(self, Css):
        try:
            self.wd.find_element(By.CSS_SELECTOR, Css)
            return True
        except NoSuchElementException:
            return False
    # è‡ªåŠ¨å¾ªç¯éå†åŸå¸‚å’Œå…³é”®è¯
    def getUrl(self, workCity, startPage, keywords):
        # çˆ¬å–ä¸­æ–­åéœ€è¦æ›´æ”¹i,jçš„ä¸‹æ ‡åˆå§‹ä½ç½®å’Œstart_pageé‡æ–°ç»§ç»­çˆ¬å–
        for i in range(0, len(workCity)):
            for j in range(0, len(keywords)):
                suffix = str(
                    startPage) + '.html?lang=c&amp;postchannel=0000&amp;workyear=99&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;ord_field=0&amp;dibiaoid=0&amp;line=&amp;welfare='
                url = 'https://search.51job.com/list/' + str(
                    workCity[i]) + ',000000,0000,00,9,99,' + keywords[j] + ',2,' + suffix
                self.wd.get(url)
                self.scrapingData(workCity[i], keywords[j], startPage)
                # æ›´æ”¹start_pageåä¼šä»start_pageé¡µå¼€å§‹çˆ¬ï¼Œçˆ¬åˆ°ä¸‹ä¸€ä¸ªå…³é”®è¯å†æŠŠstart_pageé‡ç½®æˆ1ä»ç¬¬ä¸€é¡µå¼€å§‹çˆ¬
                if startPage &gt; 1:
                    startPage = 1
# çƒ­é—¨åŸå¸‚ç¼–ç 
# {"åŒ—äº¬", "010000"}, {"ä¸Šæµ·", "020000"}, {"å¹¿å·", "030200"}, {"æ·±åœ³", "040000"}, {"æ­¦æ±‰", "180200"},
# {"è¥¿å®‰", "200200"}, {"æ­å·", "080200"}, {"å—äº¬", "070200"}, {"æˆéƒ½", "090200"}, {"é‡åº†", "060000"},
# {"ä¸œè", "030800"}, {"å¤§è¿", "230300"}, {"æ²ˆé˜³", "230200"}, {"è‹å·", "070300"}, {"æ˜†æ˜", "250200"},
# {"é•¿æ²™", "190200"}, {"åˆè‚¥", "150200"}, {"å®æ³¢", "080300"}, {"éƒ‘å·", "170200"}, {"å¤©æ´¥", "050000"},
# {"é’å²›", "120300"}, {"å“ˆå°”æ»¨", "220200"}, {"é•¿æ˜¥", "240200"}, {"ç¦å·", "110200"}, {"ç ä¸‰è§’", "01"};

if __name__ == '__main__':
    # å°†éœ€è¦çˆ¬å–çš„åŸå¸‚ç¼–å·å’Œå…³é”®è¯æ”¾è¿›æ•°ç»„ï¼Œstart_pageä¸ºä»ç¬¬å‡ é¡µå¼€å§‹çˆ¬
    cities = ['040000', '080200', '070200', '190200', '090200', '180200']
    keyword = ['å¤§æ•°æ®', 'python', 'çˆ¬è™«', 'Hadoop', 'æ•°æ®åˆ†æå¸ˆ', 'Hadoop']
    start_page = 1
    
    a = Crawler()
    a.getUrl(cities, start_page, keyword)
</code></pre> 
<p>Â Â Â Â Â Â Â Â Â ä¸Šé¢çš„ä»£ç é‡Œå…¬å¸ç¦åˆ©çš„æ•°æ®æˆ‘æ³¨é‡Šæ‰äº†ï¼Œå› ä¸ºåŸºæœ¬æ¯é¡µéƒ½æœ‰å‡ æ¡æ²¡æœ‰å…¬å¸ç¦åˆ©çš„å²—ä½æ•°æ®ï¼Œå¤„ç†é”™è¯¯è€—æ—¶å¤ªä¹…ï¼Œçˆ¬å–å¤§é‡æ•°æ®çš„æ—¶å€™å¤ªç…ç†¬äº†ï¼Œå¹²è„†ä¸è¦äº†ã€‚è¿˜æœ‰å°±æ˜¯cssè·¯å¾„æˆ‘éƒ½æ˜¯ç›´æ¥å¤åˆ¶çš„ï¼Œå¥½å¤šéƒ½è¿˜å¯ä»¥å†åˆ å‡ä¼˜åŒ–ï¼Œä¸è¿‡æˆ‘æ¯”è¾ƒæ‡’ï¼Œä¹Ÿå¯ä»¥æ¢æˆxpathè·¯å¾„ï¼Œå¯ä»¥æ›´ç²¾ç®€ã€‚æœ€åå°±æ˜¯æ•°æ®åº“éœ€è¦è‡ªå·±å»ºè¡¨ï¼Œè¿æ¥çš„æ—¶å€™æ³¨æ„æ”¹ä¸‹ä»£ç é‡Œçš„å‚æ•°è¿˜æœ‰sqlé‡Œçš„å­—æ®µåç§°å°±è¡Œï¼Œè¿˜æ˜¯æ¯”è¾ƒç®€å•çš„ã€‚</p> 
<p>Â Â Â Â Â Â Â Â æˆ‘è‡ªå·±è¿è¡Œä»£ç çš„æ—¶å€™å‡ºé”™ä¸€èˆ¬éƒ½æ˜¯çˆ¬äº†å¾ˆä¹…åæŠ¥timeouté”™è¯¯ï¼Œå¯ä»¥æŠŠç­‰å¾…æ—¶é—´ç¨å¾®åŠ é•¿ç‚¹ï¼Œä¸è¿‡ä¼°è®¡çˆ¬å¤šäº†ä¹Ÿè¿˜ä¼šæŠ¥é”™ï¼Œæ¯•ç«Ÿ51jobè™½ç„¶å¾ˆéšä¾¿ä½†çˆ¬å¤šäº†ä¹Ÿä¼šåçˆ¬ï¼Œåªæ˜¯ä¸åƒbossç›´è˜çˆ¬äº†å‡ åƒæ¡æ•°æ®å°±å°ipä¸¤å¤©é‚£ä¹ˆç‹ ï¼ˆè¡¨ç¤ºè¢«å°è¿‡å¥½å‡ æ¬¡ğŸ˜¤ï¼‰ï¼Œæœ€åå°±æ˜¯å‡ºé”™äº†éœ€è¦æ‰‹åŠ¨é‡æ–°æ›´æ”¹å‚æ•°ç»§ç»­çˆ¬ï¼Œæœ‰äº›éº»çƒ¦ï¼Œè¿˜èƒ½å†æ”¹è¿›ï¼Œä¸è¿‡æˆ‘æ‡’å¾—æ”¹äº†ï¼Œåæ­£ä¼°è®¡ä¹Ÿæ²¡å¤šå°‘äººçœ‹ï¼Œè‡ªå·±èƒ½ç”¨å°±è¡Œå•¦ã€‚</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9d3280070b244e7a6ebfc2205db77684/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">è®¡ç®—æœºæ²¡ä¿å­˜çš„wordæ–‡ä»¶æ€ä¹ˆæ‰¾,wordæ–‡ä»¶æœªä¿å­˜å…³é—­äº†æ€ä¹ˆæ¢å¤_wordæ–‡ä»¶æœªä¿å­˜è¢«å…³é—­çš„æ¢å¤æ–¹æ³•...</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8be22f737eaecc245ae042e8da5d9697/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">wmic memorychip</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§ç™½çš„åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>