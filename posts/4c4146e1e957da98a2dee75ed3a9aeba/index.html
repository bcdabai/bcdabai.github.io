<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用sparkstreaming将kafka中的主题数据做一些转换再写到kafka中新的主题中 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用sparkstreaming将kafka中的主题数据做一些转换再写到kafka中新的主题中" />
<meta property="og:description" content="项目架构为：
1.新建一个maven工程——MySparkstreaming
2.导入pom.xml依赖。各个依赖版本要匹配哦，不然会报错哦~比如会报错AbstractMethodError
&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;!--这边spark降版本是因为要和下面的spark-streaming-kafka-0-10_2.11依赖匹配，不然会报错AbstractMethodError.我spark版本为2.4.4--&gt; &lt;spark.version&gt;2.1.0&lt;/spark.version&gt; &lt;kafka.version&gt;2.0.0&lt;/kafka.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt; &lt;version&gt;${kafka.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;${kafka.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10 --&gt; &lt;!--创建kafka的DStream数据源需要用到他KafkaUtils--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;${kafka.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 3.创建一个读特质——ReadTrait" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/4c4146e1e957da98a2dee75ed3a9aeba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-08T00:01:27+08:00" />
<meta property="article:modified_time" content="2021-04-08T00:01:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用sparkstreaming将kafka中的主题数据做一些转换再写到kafka中新的主题中</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> 项目架构为：</p> 
<p><img alt="" height="408" src="https://images2.imgbox.com/cf/d0/ywwJxjFs_o.png" width="314"></p> 
<p>1.新建一个maven工程——MySparkstreaming</p> 
<p>2.导入pom.xml依赖。各个依赖版本要匹配哦，不然会报错哦~比如会报错AbstractMethodError</p> 
<pre><code class="language-XML">  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;!--这边spark降版本是因为要和下面的spark-streaming-kafka-0-10_2.11依赖匹配，不然会报错AbstractMethodError.我spark版本为2.4.4--&gt;    
&lt;spark.version&gt;2.1.0&lt;/spark.version&gt;
    &lt;kafka.version&gt;2.0.0&lt;/kafka.version&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.11&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
      &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
      &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
      &lt;version&gt;${spark.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
      &lt;artifactId&gt;kafka_2.12&lt;/artifactId&gt;
      &lt;version&gt;${kafka.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
      &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
      &lt;version&gt;${kafka.version}&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10 --&gt;
&lt;!--创建kafka的DStream数据源需要用到他KafkaUtils--&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
      &lt;version&gt;${kafka.version}&lt;/version&gt;
    &lt;/dependency&gt;

  &lt;/dependencies&gt;</code></pre> 
<p>3.创建一个读特质——ReadTrait</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services

import java.util.Properties

import org.apache.spark.streaming.dstream. InputDStream

trait ReadTrait[T] {
    //java和scala集合之间的隐式转换
  import scala.collection.JavaConversions._
  def reader(prop:Map[String,Object],tableName:String):InputDStream[T]
  def reader(prop:Properties,tableName:String):InputDStream[T]=reader(prop.toMap[String,Object],tableName)
}</code></pre> 
<p>4.创建一个写特质——WriteTrait</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services

import java.util.Properties

import org.apache.spark.streaming.dstream.DStream

trait WriteTrait[T] {
  import scala.collection.JavaConversions._
  def writer(prop:Map[String,Object],tableName:String,ds:DStream[T]):Unit
  def writer(prop:Properties,tableName:String,ds:DStream[T]):Unit=writer(prop.toMap[String,Object],tableName,ds)
}
</code></pre> 
<p>5.创建一个伴生类伴生对象——KafkaReader,用于实现ReadTrait</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services.impl

import cn.alisa.mySparkstreaming.services.ReadTrait
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}

class KafkaReader(ssc:StreamingContext) extends ReadTrait[ConsumerRecord[String,String]]{
  override def reader(prop: Map[String,Object], tableName: String): InputDStream[ConsumerRecord[String,String]] = {
    KafkaUtils.createDirectStream(ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String,String](Set(tableName),prop))
  }
}
//伴生对象，
// 伴生对象通常会使用apply函数定义伴生类的构造方法。 这样在创建伴生类的对象时就可以省略 new 关键字
object KafkaReader{
  def apply(ssc: StreamingContext): KafkaReader = new KafkaReader(ssc)
}
</code></pre> 
<p>6.创建一个伴生类伴生对象——KafkaSink,用于KafkaWriter广播和发送消息的</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services.commons

import java.util.Properties

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}

//kafkasink用来发消息的
//序列化
class KafkaSink[K,V](producer:()=&gt;KafkaProducer[K,V]) extends Serializable {
  //lazy实现延迟加载(懒加载)。只有在被使用时才会执行初始化
  //连接序列化问题通过懒加载的方式解决，此代码不会因为每次发送数据时重新建立连接。
  lazy val prod: KafkaProducer[K, V] = producer()

  def send(topic: String, key: K, value: V) = {
    prod.send(new ProducerRecord[K, V](topic, key, value))
  }

  def send(topic: String, value: V) = {
    prod.send(new ProducerRecord[K, V](topic, value))
  }
}

  object KafkaSink{
    //在java和scala之间进行隐式转换,第二个apply中的properties是java中的
    import scala.collection.JavaConversions._
    def apply[K,V](config:Map[String,Object]) = {
     //构建一个匿名函数
      val createKafkaProducer=()=&gt;{
        val produ = new KafkaProducer[K,V](config)
        //调用钩子函数，关闭JVM
        sys.addShutdownHook{
          produ.close()
        }
        produ
      }
      new KafkaSink[K,V](createKafkaProducer)
    }

    def apply[K,V](config:Properties): KafkaSink[K,V] = apply(config.toMap)
  }
</code></pre> 
<p>7.创建一个伴生类伴生对象——KafkaWriter,用于实现WriteTrait</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services.impl

import cn.alisa.mySparkstreaming.services.WriteTrait
import cn.alisa.mySparkstreaming.services.commons.KafkaSink
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream

class KafkaWriter(ssc:StreamingContext) extends WriteTrait[String]{
  override def writer(prop: Map[String, Object], tableName: String, ds: DStream[String]): Unit = {
    val bc = ssc.sparkContext.broadcast(KafkaSink[String,String](prop))
    ds.foreachRDD(rdd=&gt;rdd.foreachPartition(iter=&gt;{
      iter.map(msg=&gt;{
        bc.value.send(tableName,msg)
      }).foreach(x=&gt;x)
    }))
  }
}
//伴生对象
object KafkaWriter{
  def apply(ssc: StreamingContext): KafkaWriter = new KafkaWriter(ssc)
}
</code></pre> 
<p>8.创建一个转换特质——TransformTrait,用于转换。</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services

import org.apache.spark.streaming.dstream.{DStream, InputDStream}

trait TransformTrait[T,V] {
  def transform(in:InputDStream[T]):DStream[V]

}
</code></pre> 
<p>9.创建一个特质Event_Attendees_Trait,用来实现TransformTrait，做具体的转换</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services.userImpl

import cn.alisa.mySparkstreaming.services.TransformTrait
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.dstream.{DStream, InputDStream}

//进来按照读，出去按照写
trait Event_Attendees_Trait extends TransformTrait[ConsumerRecord[String,String],String]{
  override def transform(in: InputDStream[ConsumerRecord[String, String]]): DStream[String] = {
    in.flatMap(line=&gt;{
      var info = line.value().split(",", -1)
      //[(123,456,yes),(123,456,yes)......]
      var yes = info(1).split(" ").map(us=&gt;(info(0),us,"yes"))
      var maybe =info(2).split(" ").map(us=&gt;(info(0),us,"maybe"))
      var invited = info(3).split(" ").map(us=&gt;(info(0),us,"invited"))
      var no = info(4).split(" ").map(us=&gt;(info(0),us,"no"))
      yes++maybe++invited++no
    }.filter(_._2.trim()!="").map(tp=&gt;tp.productIterator.mkString(",")))
  }
}
</code></pre> 
<p>10.创建一个特质User_Friends_Trait,用来实现TransformTrait，做具体的转换</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services.userImpl

import cn.alisa.mySparkstreaming.services.TransformTrait
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.dstream.{DStream, InputDStream}

trait User_Friends_Trait extends TransformTrait[ConsumerRecord[String,String],String]{
  override def transform(in: InputDStream[ConsumerRecord[String, String]]): DStream[String] = {
    in.filter(line=&gt;{
      val ln=line.value()
      val reg=",$".r
      val iter = reg.findAllMatchIn(ln)
      !iter.hasNext
    }).flatMap(line=&gt;{
      val info = line.value().split(",",-1)
      info(1).filter(_!="").split(" ").map(friendid=&gt;{
        (info(0),friendid)
      })
    }).map(_.productIterator.mkString(","))
  }
}
</code></pre> 
<p>11.创建一个类——KTKExecutor,用于混入特质，就是把之前的几个特质拼起来</p> 
<pre><code class="language-Scala">package cn.alisa.mySparkstreaming.services

import cn.alisa.mySparkstreaming.services.impl.{KafkaReader, KafkaWriter}
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

//要混入特质
class KTKExecutor(readConf:Map[String,Object],writeConf:Map[String,Object]) {
 tran:TransformTrait[ConsumerRecord[String,String],String]=&gt;

  def worker(intopic:String,outtopic:String)={
    //创建一个Streamingcontext
    val sc = new SparkConf().setMaster("local[*]").setAppName("read test")
      .set("spark.serializer","org.apache.spark.serializer.KryoSerializer")
      .set("spark.streaming.kafka.consumer.poll.ms","10000")
    val ssc = new StreamingContext(sc, Seconds(1))
    ssc.checkpoint("e:/ck")
    //调用kafka数据读取
    val kr=new KafkaReader(ssc).reader(readConf,intopic)
    //调用混入特质进行数据处理
    var ds=tran.transform(kr)
    //调用kafka写入topic
    new KafkaWriter(ssc).writer(writeConf,outtopic,ds)

    ssc.start()
    ssc.awaitTermination()
  }
}

</code></pre> 
<p>12.创建测试对象——MyTest</p> 
<pre><code class="language-Scala">object MyTest {
  def main(args: Array[String]): Unit = {
val inParams=Map(
      //建立与kafka集群的连接
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG-&gt;"192.168.21.130:9092",
      //消费组名字
      ConsumerConfig.GROUP_ID_CONFIG-&gt;"alisa",
      //每次最大消费消息数量
      ConsumerConfig.MAX_POLL_RECORDS_CONFIG-&gt;"1000",
      //消费者通过反序列化将kafka收到的字节数组转换成相应的对象
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG-&gt;classOf[StringDeserializer],
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG-&gt;classOf[StringDeserializer],
      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG-&gt;"earliest"
    )
    val outParams=Map(
      ProducerConfig.BOOTSTRAP_SERVERS_CONFIG-&gt;"192.168.21.130:9092",
      ProducerConfig.ACKS_CONFIG-&gt;"1",
      ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG-&gt;classOf[StringSerializer],
      ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG-&gt;classOf[StringSerializer]
    )
    //动态混入
//    (new KTKExecutor(inParams,outParams) with User_Friends_Trait).worker("user_friends_raw","user_friends_ss")
    (new KTKExecutor(inParams,outParams) with Event_Attendees_Trait).worker("event_attendees_raw","event_attendees_ss")
  }
}</code></pre> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/677f9ff6923a52fd928f504dfd2a6ae2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据库面试题：对称加密和非对称加密的区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/66aef41d1ac97619a96110d963305fd4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">GPUTerrain简单实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>