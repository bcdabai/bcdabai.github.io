<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>数据挖掘之聚类分析（Cluster Analysis） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="数据挖掘之聚类分析（Cluster Analysis）" />
<meta property="og:description" content="1.Motivations（目的） Identify grouping structure of data so that objects within the same group are closer (more similar) to each other while farther (less similar) to those in different groups Various distance/proximity functionsintra-cluster distance vs. inter-cluster distance Unsupervised learning: used for data exploration Can also be adapted for supervised learning purpose Types of cluster analysis Partitional vs. Hierarchical: one point can belong to one or multiple clusterskmeans algorithm vs. HAC (Hierarchical Agglomerative Clustering) algorithm 中：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/fc97924014379e1b6bd4f76948d56439/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-07-05T10:00:04+08:00" />
<meta property="article:modified_time" content="2019-07-05T10:00:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">数据挖掘之聚类分析（Cluster Analysis）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="1Motivations_1"></a>1.Motivations（目的）</h3> 
<ul><li>Identify grouping structure of data so that objects within the same group are closer (more similar) to each other while farther (less similar) to those in different groups 
  <ul><li>Various distance/proximity functions</li><li>intra-cluster distance vs. inter-cluster distance</li></ul> </li><li>Unsupervised learning: used for data exploration 
  <ul><li>Can also be adapted for supervised learning purpose</li></ul> </li><li>Types of cluster analysis 
  <ul><li>Partitional vs. Hierarchical: one point can belong to one or multiple clusters</li><li>kmeans algorithm vs. HAC (Hierarchical Agglomerative Clustering) algorithm</li></ul> </li></ul> 
<p>中：</p> 
<ul><li>识别数据的分组结构，以便同一组内的对象彼此更接近（更相似），而与不同组中的对象更远（更不相似） 
  <ul><li>各种距离/接近功能</li><li>簇内距离与簇间距离</li></ul> </li><li>无监督学习：用于数据探索 
  <ul><li>也可以适应监督学习目的</li></ul> </li><li>聚类分析的类型 
  <ul><li>分区与分层：一个点可以属于一个或多个集群</li><li>kmeans算法与HAC（Hierarchical Agglomerative Clustering）算法</li></ul> </li></ul> 
<h3><a id="2Partitional_vs_Hierarchical_Clustering_25"></a>2.Partitional vs. Hierarchical Clustering（分区与分层聚类）</h3> 
<p><img src="https://images2.imgbox.com/e7/61/4EMDF6ob_o.png" alt="[外链图片转存失败(img-jTROiuj3-1562291299486)(C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1562220427447.png)]"></p> 
<h3><a id="3Major_Applications_of_Cluster_Analysis_31"></a>3.Major Applications of Cluster Analysis</h3> 
<ul><li>Data sampling: use centroid as the representative samples 
  <ul><li>Centroid: center of mass which is caculated as the (arithmetic) mean of all the data points in the same cluster (separately for each dimension)</li></ul> </li><li>Marketing segmentation analysis: help marketers segment their customer bases and develop targeted marketing programs</li><li>Insurance: identify groups of motor insurance policy holders with a higher average claim cost</li><li>Microarray analysis for biomedical research: A high throughput technology which allows testing thousand of genes simultaneously in different disease states</li></ul> 
<p>中:</p> 
<p>聚类分析的主要应用</p> 
<ul><li>数据采样：使用质心作为代表性样本 
  <ul><li>质心：质心，计算为同一簇中所有数据点的（算术）平均值（每个维度分别）</li></ul> </li><li>营销细分分析：帮助营销人员细分客户群并制定有针对性的营销计划</li><li>保险：确定具有较高平均索赔成本的汽车保险保单持有人群体</li><li>用于生物医学研究的微阵列分析：一种高通量技术，允许在不同疾病状态下同时测试数千个基因</li></ul> 
<h3><a id="4Distance_Functions_51"></a>4.Distance Functions</h3> 
<ul><li>Minkowski</li><li>Manhattan</li><li>Euclidean distance</li></ul> 
<p><img src="https://images2.imgbox.com/e3/72/0gaNAP6s_o.png" alt="[外链图片转存失败(img-nibEjViE-1562291299488)(C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1562222191249.png)]"></p> 
<h3><a id="5_Cluster_Validity_Evaluation_61"></a>5. Cluster Validity Evaluation</h3> 
<ul><li>SSE (Sum of Square Error): most common measure</li></ul> 
<p><img src="https://images2.imgbox.com/50/ea/GZ6pWkTP_o.png" alt="[外链图片转存失败(img-I6jLCv5v-1562291299489)(C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1562222168396.png)]"></p> 
<ul><li>Cluster cohesion</li><li>SSE is a monotonically decreasing function of number of clusters. Therefore it can be used to compare cluster performance only for similar number of clusters</li></ul> 
<p>中：</p> 
<ul><li>SSE（平方误差之和）：最常见的度量</li><li>集群凝聚力</li><li>SSE是群集数量的单调递减函数。因此，它可用于仅针对相似数量的群集比较群集性能</li></ul> 
<h3><a id="6Objective_Function_for_Kmeans_Clustering_SSEKmeansSSE_78"></a>6.Objective Function for Kmeans Clustering: SSE（Kmeans聚类的目标函数：SSE）</h3> 
<ul><li>Also known as loss/cost function</li><li>Goal of Kmeans method is to 
  <ul><li>Minimize SSE of within-cluster variance</li><li>Same as the sum of Euclidean distance between all data points with their respective cluster centroids</li></ul> </li><li>Therefore no needs to set the distance function for Kmeans method</li></ul> 
<p>中：</p> 
<ul><li>也称为损失/成本函数</li><li>Kmeans方法的目标是 
  <ul><li>最小化群内方差的SSE</li><li>与所有数据点之间的欧几里德距离与其各自的聚类质心的总和相同</li></ul> </li><li>因此无需为Kmeans方法设置距离函数</li></ul> 
<h3><a id="7Model_Preprocessing_98"></a>7.Model Preprocessing（模型预处理）</h3> 
<ul><li>Data standardization, normalization, rescale 
  <ul><li>Applies to all algorithms based on distance: KNN</li></ul> </li><li>Remove or impute missing values</li><li>Detection and removal of noisy data and <em>outliers</em></li><li>Transformation of categorical attributes into numerical attributes (opposite to association rule mining)</li></ul> 
<p>中：</p> 
<ul><li>数据标准化，规范化，重新缩放 
  <ul><li>适用于基于距离的所有算法：KNN</li></ul> </li><li>删除或估算缺失值</li><li>检测和消除噪声数据和<em>异常值</em></li><li>将分类属性转换为数字属性（与关联规则挖掘相反）</li></ul> 
<h3><a id="8Animation_of_How_kmeans_Algorithm_workskmeans_118"></a>8.Animation of How kmeans Algorithm works（kmeans算法如何工作的动画）</h3> 
<p><img src="https://images2.imgbox.com/bf/99/J5d6ZN0q_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="9Summary_of_kmeans_and_HACkmeansHAC_126"></a>9.Summary of kmeans and HAC（kmeans和HAC摘要）</h3> 
<ul><li>Kmeans 
  <ul><li>Randomly assign k centroids</li><li>Assign all data points to their closest centroids</li><li>Update centroid assignments</li><li>Repeat the previous two steps until centroids are stable</li></ul> </li><li>HAC 
  <ul><li>Treat each data point as a cluster</li><li>Compute the pairwise distance matrix for all clusters</li><li>Merge closer clusters into a larger one and update the distance matrix</li><li>keep repeating the previous step until there is only one cluster left</li><li>Opposite: HDC (Hierarchical Divisive Clustering)</li></ul> </li></ul> 
<p>中：</p> 
<ul><li>K均值 
  <ul><li>随机分配k个质心</li><li>将所有数据点分配给最近的质心</li><li>更新质心分配</li><li>重复前两个步骤，直到质心稳定</li></ul> </li><li>HAC 
  <ul><li>将每个数据点视为一个集群</li><li>计算所有聚类的成对距离矩阵</li><li>将更近的聚类合并为更大的聚类并更新距离矩阵</li><li>继续重复上一步，直到只剩下一个簇</li><li>相反：HDC（分层分裂聚类）</li></ul> </li></ul> 
<h3><a id="10Defintion_of_InterCluster_Distance_156"></a>10.Defintion of Inter-Cluster Distance（群间距离的定义）</h3> 
<ul><li>Single linkage: minimal pairwise distance between data points belonging to different clusters</li><li>Complete linkage: maximal pairwise distance between data points belonging to different clusters</li><li>Average linkage method: average pairwise distance between all data points belonging to different clusters</li><li>Centroid linkage method: pairwise distance between centroids of different clusters</li></ul> 
<p>中：</p> 
<ul><li>单链接：属于不同群集的数据点之间的最小成对距离</li><li>完全链接：属于不同簇的数据点之间的最大成对距离</li><li>平均连锁方法：属于不同聚类的所有数据点之间的平均成对距离</li><li>质心联动方法：不同聚类的质心之间的成对距离</li></ul> 
<h3><a id="11Model_Parameters_174"></a>11.Model Parameters（模型参数）</h3> 
<ul><li>Algorithm-specific 
  <ul><li>kmeans 
    <ul><li>Number of clusters: Elbow method to estimate</li><li>Initial choice of cluster centers (centroid) 
      <ul><li>Kmeans method is guaranteed to converge but not guaranteed to converge to global optimial</li></ul> </li><li>Maximal number of repeats</li></ul> </li><li>Hierarchical clustering 
    <ul><li>Distance function between data points</li><li>Definition of intercluster distance: single vs. complete vs. average vs. centroid linkage</li><li>Number of clusters to output (needed after clustering)</li></ul> </li></ul> </li><li>算法的具体 
  <ul><li>k均值 
    <ul><li>簇数：肘法估算</li><li>群集中心的初始选择（质心） 
      <ul><li>Kmeans方法保证收敛但不保证收敛到全局优化</li></ul> </li><li>最大重复次数</li></ul> </li><li>分层聚类 
    <ul><li>数据点之间的距离函数</li><li>集群间距离的定义：单对与完全对比平均与质心联系</li><li>要输出的簇数（在群集后需要）</li></ul> </li></ul> </li></ul> 
<h3><a id="12Comparison_Between_Kmeans_and_HAC_1_199"></a>12.Comparison Between Kmeans and HAC (1)</h3> 
<ul><li> <p>Objective function</p> 
  <ul><li>Kmeans: sum of square difference between data points and their respective centroid (within SS)</li><li>HAC: no objective function</li></ul> </li><li> <p>Parameter setting</p> 
  <ul><li>Kmeans: need to specify number of clusters prior to running aglorithm</li><li>HAC: no need to choose number of clusters a priori; can choose after fact</li></ul> </li><li> <p>Performance</p> 
  <ul><li>Kmeans: Fast with linear time complexity <strong>O(N)</strong> (ñ: number of data points)</li><li>HAC: Slow with quadratic complexity O(N^2)</li><li>Hybrid approach: run Kmeans first to reduce dataset size and then HAC to cluster</li></ul> </li></ul> 
<p>中：</p> 
<ul><li>目标功能 
  <ul><li>Kmeans：数据点与其各自质心之间的平方差的总和（在SS内）</li><li>HAC：没有客观功能</li></ul> </li><li>参数设置 
  <ul><li>Kmeans：需要在运行aglorithm之前指定簇数</li><li>HAC：无需先验地选择集群数量; 事后可以选择</li></ul> </li><li>性能 
  <ul><li>Kmeans：线性时间复杂度快 O （N）(ñ：数据点数）</li><li>HAC：二次复杂性缓慢 O （N^2）</li><li>混合方法：首先运行Kmeans以减少数据集大小，然后运行HAC以进行群集</li></ul> </li></ul> 
<h3><a id="13Comparison_Between_Kmeans_and_HAC_2_236"></a>13.Comparison Between Kmeans and HAC (2)</h3> 
<ul><li>Structure of clusters 
  <ul><li>Kmeans: works best with sphere clusters with similar size</li><li>HAC: works with clusters of different size and shape 
    <ul><li>Depending on inter-cluster distance definition</li><li>Single-linkage method: clusters of different size; prone to outliers</li><li>Complete-linkage method: clusters of similar size; prone to outliers</li><li>Average/Centroid linkage: resist outliers</li></ul> </li><li>Both methods couldn’t deal well with clusters of different densities: DBScan</li></ul> </li><li>Both methods mostly work with numerical attributes only (contrary to association rule mining)</li></ul> 
<p>​</p> 
<h4><a id="14Demo1_254"></a>14.Demo(1)</h4> 
<h4><a id="141_Demo_Dataset_USArrests_258"></a>14.1 Demo Dataset: USArrests</h4> 
<ul><li>Crime dataset: 
  <ul><li>Arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973</li><li>Percent of the population living in urban areas</li></ul> </li></ul> 
<pre><code class="prism language-R">&gt; library(datasets)
&gt; str(USArrests)
'data.frame':	50 obs. of  4 variables:
 $ Murder  : num  13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ...
 $ Assault : int  236 263 294 190 276 204 110 238 335 211 ...
 $ UrbanPop: int  58 48 80 50 91 78 77 72 80 60 ...
 $ Rape    : num  21.2 44.5 31 19.5 40.6 38.7 11.1 15.8 31.9 25.8 ...
&gt; row.names(USArrests)
 [1] "Alabama"        "Alaska"         "Arizona"        "Arkansas"       "California"     "Colorado"      
 [7] "Connecticut"    "Delaware"       "Florida"        "Georgia"        "Hawaii"         "Idaho"         
[13] "Illinois"       "Indiana"        "Iowa"           "Kansas"         "Kentucky"       "Louisiana"     
[19] "Maine"          "Maryland"       "Massachusetts"  "Michigan"       "Minnesota"      "Mississippi"   
[25] "Missouri"       "Montana"        "Nebraska"       "Nevada"         "New Hampshire"  "New Jersey"    
[31] "New Mexico"     "New York"       "North Carolina" "North Dakota"   "Ohio"           "Oklahoma"      
[37] "Oregon"         "Pennsylvania"   "Rhode Island"   "South Carolina" "South Dakota"   "Tennessee"     
[43] "Texas"          "Utah"           "Vermont"        "Virginia"       "Washington"     "West Virginia" 
[49] "Wisconsin"      "Wyoming"       
</code></pre> 
<h4><a id="142_Data_Preprocess_286"></a>14.2 Data Preprocess</h4> 
<pre><code class="prism language-R">&gt; sum(!complete.cases(USArrests))
[1] 0
&gt; summary(USArrests)
     Murder          Assault         UrbanPop          Rape      
 Min.   : 0.800   Min.   : 45.0   Min.   :32.00   Min.   : 7.30  
 1st Qu.: 4.075   1st Qu.:109.0   1st Qu.:54.50   1st Qu.:15.07  
 Median : 7.250   Median :159.0   Median :66.00   Median :20.10  
 Mean   : 7.788   Mean   :170.8   Mean   :65.54   Mean   :21.23  
 3rd Qu.:11.250   3rd Qu.:249.0   3rd Qu.:77.75   3rd Qu.:26.18  
 Max.   :17.400   Max.   :337.0   Max.   :91.00   Max.   :46.00  
&gt; df &lt;- na.omit(USArrests)
&gt; df &lt;- scale(df, center = T, scale = T)
&gt; summary(df)
     Murder           Assault           UrbanPop             Rape        
 Min.   :-1.6044   Min.   :-1.5090   Min.   :-2.31714   Min.   :-1.4874  
 1st Qu.:-0.8525   1st Qu.:-0.7411   1st Qu.:-0.76271   1st Qu.:-0.6574  
 Median :-0.1235   Median :-0.1411   Median : 0.03178   Median :-0.1209  
 Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  
 3rd Qu.: 0.7949   3rd Qu.: 0.9388   3rd Qu.: 0.84354   3rd Qu.: 0.5277  
 Max.   : 2.2069   Max.   : 1.9948   Max.   : 1.75892   Max.   : 2.6444  
</code></pre> 
<h4><a id="143_Distance_function_and_visualization_313"></a>14.3 Distance function and visualization</h4> 
<pre><code class="prism language-R">&gt; library(ggplot2)
&gt; library(factoextra)
&gt; distance &lt;- get_dist(df, method = "euclidean")
&gt; fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
</code></pre> 
<p><img src="https://images2.imgbox.com/8b/84/CBbcw7J4_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="144_kmeans_Function_325"></a>14.4 kmeans Function</h4> 
<pre><code class="prism language-R">&gt; km_output &lt;- kmeans(df, centers = 2, nstart = 25, iter.max = 100, algorithm = "Hartigan-Wong")
&gt; str(km_output)
List of 9
 $ cluster     : Named int [1:50] 1 1 1 2 1 1 2 2 1 1 ...
  ..- attr(*, "names")= chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...
 $ centers     : num [1:2, 1:4] 1.005 -0.67 1.014 -0.676 0.198 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:2] "1" "2"
  .. ..$ : chr [1:4] "Murder" "Assault" "UrbanPop" "Rape"
 $ totss       : num 196
 $ withinss    : num [1:2] 46.7 56.1
 $ tot.withinss: num 103
 $ betweenss   : num 93.1
 $ size        : int [1:2] 20 30
 $ iter        : int 1
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"
</code></pre> 
<h4><a id="145_Loss_Function_Sum_of_Square_Error_349"></a>14.5 Loss Function: Sum of Square Error</h4> 
<pre><code class="prism language-R">&gt; km_output$totss
[1] 196
&gt; km_output$withinss
[1] 46.74796 56.11445
&gt; km_output$betweenss
[1] 93.1376
&gt; sum(c(km_output$withinss, km_output$betweenss))
[1] 196
</code></pre> 
<p><img src="https://images2.imgbox.com/c3/c0/D9rwFveR_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="146_Visualize_Cluster_Assignment_364"></a>14.6 Visualize Cluster Assignment</h4> 
<p><code>fviz_cluster(km_output, data = df)</code></p> 
<h4><a id="147_Visual_Cluster_Assignment_on_Map_1_370"></a>14.7 Visual Cluster Assignment on Map (1)</h4> 
<pre><code class="prism language-R">cluster_df &lt;- data.frame(state = tolower(row.names(USArrests)), 
                         cluster = unname(km_output$cluster))
library(maps)
states &lt;- map_data("state")
states %&gt;%
  left_join(cluster_df, by = c("region" = "state")) %&gt;% 
  ggplot() +
  geom_polygon(
      aes(
          x = long,
          y = lat, 
          fill = as.factor(cluster), 
          group = group
  		), 
         color = "white") +
  coord_fixed(1.3) +
  guides(fill = F) +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank()
       )
</code></pre> 
<p><img src="https://images2.imgbox.com/e3/e6/pFkYspjI_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="148_Elbow_method_to_decide_Optimal_Number_of_Clusters_1_402"></a>14.8 Elbow method to decide Optimal Number of Clusters (1)</h4> 
<pre><code class="prism language-R">set.seed(8)
wss &lt;- function(k){
  return(kmeans(df, k, nstart = 25)$tot.withinss)
}

k_values &lt;- 1:15

wss_values &lt;- purrr::map_dbl(k_values, wss)

plot(x = k_values, 
     y = wss_values, 
     type = "b", frame = F,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of square")
</code></pre> 
<p><img src="https://images2.imgbox.com/f7/6c/UP1Em1Bt_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="149_Hierarchical_Clustering_424"></a>14.9 Hierarchical Clustering</h4> 
<pre><code class="prism language-R">hac_output &lt;- hclust(dist(USArrests, method = "euclidean"), method = "complete")
plot(hac_output)
</code></pre> 
<p><img src="https://images2.imgbox.com/8d/8f/fkyDzltM_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="1410_Ouput_Desirable_Number_of_Clusters_After_Modeling_433"></a>14.10 Ouput Desirable Number of Clusters After Modeling</h4> 
<pre><code class="prism language-R">&gt; for (i in 1:length(hac_cut)){
+     if(hac_cut[i] != km_output$cluster[i]) print(names(hac_cut)[i])
+ }
[1] "Colorado"
[1] "Delaware"
[1] "Georgia"
[1] "Missouri"
[1] "Tennessee"
[1] "Texas"
</code></pre> 
<h3><a id="15Demo2_447"></a>15.Demo(2)</h3> 
<h4><a id="151__449"></a>15.1 数据准备</h4> 
<pre><code class="prism language-R">#install.packages("ggplot2")
#install.packages("factoextra")
#载入包
library(factoextra)
# 载入数据
data("USArrests") 
# 数据进行标准化
df &lt;- scale(USArrests) 
# 查看数据的前五行
head(df, n = 5)
               Murder   Assault   UrbanPop         Rape
Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
Arizona    0.07163341 1.4788032  0.9989801  1.042878388
Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
California 0.27826823 1.2628144  1.7589234  2.067820292

#确定最佳聚类数目
fviz_nbclust(df, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
</code></pre> 
<p><img src="https://images2.imgbox.com/50/30/lHyLsFBm_o.png" alt="æœ€ä½³èšç±»æ•°ç›®"></p> 
<h4><a id="152_Kmeans_477"></a>15.2 K-means聚类</h4> 
<pre><code class="prism language-R">#从指标上看，选择坡度变化不明显的点最为最佳聚类数目。可以初步认为聚为四类最合适。
#设置随机数种子，保证实验的可重复进行
set.seed(123)
#利用k-mean是进行聚类
km_result &lt;- kmeans(df, 4, nstart = 24)
#查看聚类的一些结果
print(km_result)
#提取类标签并且与原始数据进行合并
dd &lt;- cbind(USArrests, cluster = km_result$cluster)
head(dd)
           Murder Assault UrbanPop Rape cluster
Alabama      13.2     236       58 21.2       4
Alaska       10.0     263       48 44.5       3
Arizona       8.1     294       80 31.0       3
Arkansas      8.8     190       50 19.5       4
California    9.0     276       91 40.6       3
Colorado      7.9     204       78 38.7       3

#查看每一类的数目
table(dd$cluster)
 1  2  3  4 
13 16 13  8 
#进行可视化展示
fviz_cluster(km_result, data = df,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type = "euclid",
             star.plot = TRUE, 
             repel = TRUE,
             ggtheme = theme_minimal()
)
</code></pre> 
<p><img src="https://images2.imgbox.com/cd/8a/Ewt17u32_o.png" alt="èšç±»æ•ˆæžœå›¾"></p> 
<h4><a id="153__514"></a>15.3 层次聚类</h4> 
<pre><code class="prism language-R">#先求样本之间两两相似性
result &lt;- dist(df, method = "euclidean")

#产生层次结构
result_hc &lt;- hclust(d = result, method = "ward.D2")

#进行初步展示
fviz_dend(result_hc, cex = 0.6)
</code></pre> 
<p><img src="https://images2.imgbox.com/f0/0f/3Q3Gkhhv_o.png" alt="å±‚æ¬¡ç»“æž„å›¾"></p> 
<pre><code class="prism language-R">fviz_dend(result_hc, k = 4, 
          cex = 0.5, 
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE          
)
</code></pre> 
<p><img src="https://images2.imgbox.com/2b/bd/AlHCVqaN_o.png" alt="å±‚æ¬¡èšç±»æ•ˆæžœå›¾"></p> 
<h4><a id="154_All_code_546"></a>15.4 All code</h4> 
<pre><code class="prism language-R">#load library
library(ggplot2)
library(factoextra)

# load data
data("USArrests") 

# Data standardization
df &lt;- scale(USArrests) 

# Top 10 rows of view data
head(df, n = 10)


# Determining the optimal number of clusters
fviz_nbclust(df, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)



# From the point of view of the index, the best number of 
# clustering is to select the points whose gradient changes are not obvious. 
# It is preliminarily considered that it is most appropriate to divide into four categories.

# Setting up random number seeds to ensure the repeatability of the experiment
set.seed(123)

# Clustering by K-means
km_result &lt;- kmeans(df, 4, nstart = 24)

# Look at some of the results of clustering
print(km_result)

# Extract class labels and merge them with the original data
dd &lt;- cbind(USArrests, cluster = km_result$cluster)
head(dd)


# View the number of each category
table(dd$cluster)

# Visual display
fviz_cluster(km_result, data = df,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type = "euclid",
             star.plot = TRUE, 
             repel = TRUE,
             ggtheme = theme_minimal()
)


# Seek the similarity between two samples
result &lt;- dist(df, method = "euclidean")

# Generating hierarchy
result_hc &lt;- hclust(d = result, method = "ward.D2")

# preliminary dispaly
fviz_dend(result_hc, cex = 0.6)


# According to this graph, it is convenient to determine the suitable
# grouping into several categories, such as we grouped into four categories 
# and displayed visually.

fviz_dend(result_hc, k = 4, 
          cex = 0.5, 
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE          
)
</code></pre> 
<h3><a id="16Homework_623"></a>16.Homework</h3> 
<pre><code>Option 1:

Pick a dataset of your choice, apply both K-means and HAC algorithms to identify the underlying cluster structures and compare the differene between two outputs (if you are using a labeled dataset, you can also evaluate the performance of the cluster assignments by comparing them to the true class labels)
Submit your R codes with the cluster assignment outputs.


Option 2:

Identify a successful real world application of cluster analysis algorithm and discuss how it works. Please include a discussion of your understanding of clustering model, and how clustering model helps discover hidden data patterns for the application. The application could be in any kind of industries: banking, insurance, education, public administration, technology, healthcare management, etc.
Submit your essay which should be no less than 300 words
</code></pre> 
<h3><a id="17Relevant_references_642"></a>17.Relevant references</h3> 
<p>一些相关资料<br> <a href="https://www.cnblogs.com/lliuye/p/9144312.html" rel="nofollow">kmeans算法理解及代码实现</a></p> 
<p><a href="https://blog.csdn.net/glodon_mr_chen/article/details/79867268">【数据挖掘】使用R语言进行聚类分析</a></p> 
<p><a href="https://blog.csdn.net/hfutxiaoguozhi/article/details/78828047">基于R语言的聚类分析（k-means,层次聚类）</a></p> 
<p><a href="https://www.jianshu.com/p/50cb85285af0" rel="nofollow">聚类分析原理及R语言实现过程(简书)</a></p> 
<p><a href="https://www.cnblogs.com/xmeo/p/6543057.html" rel="nofollow">Kmeans聚类与层次聚类</a></p> 
<p><a href="https://blog.csdn.net/zouxy09/article/details/17589329">机器学习算法与Python实践之（五）k均值聚类（k-means）</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/87a515abcdef0083e80325f4947d58d6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何检测资源泄露</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0e1e26618c0f4374f2b01ed6ce6a04b1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">EndNote x9下载、安装及使用详细教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>