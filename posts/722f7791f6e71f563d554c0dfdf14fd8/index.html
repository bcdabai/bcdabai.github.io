<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>PyTorch 内 LibTorch/TorchScript 的使用 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="PyTorch 内 LibTorch/TorchScript 的使用" />
<meta property="og:description" content="PyTorch 内 LibTorch/TorchScript 的使用 1. .pt .pth .bin .onnx 格式1.1 模型的保存与加载到底在做什么？1.2 为什么要约定格式？1.3 格式汇总1.3.1 .pt .pth 格式1.3.2 .bin 格式1.3.3 直接保存完整模型1.3.4 .onnx 格式1.3.5 jit.trace1.3.6 jit.script 1.4 总结 2. TorchScript 的转换2.1 jit trace 注意事项2.2 jit trace 验证技巧2.3 混合使用 trace 和 script2.4 trace 和 script 的性能2.5 总结 3. LibTorch 的使用3.1 LibTorch 的链接3.2 接口和实现 Reference：
[Pytorch].pth转.pt文件Pytorch格式 .pt .pth .bin .onnx 详解pytorch 基于tracing/script方式转ONNX 1. .pt .pth .bin .onnx 格式 1.1 模型的保存与加载到底在做什么？ 我们在使用pytorch构建模型并且训练完成后，下一步要做的就是把这个模型放到实际场景中应用，或者是分享给其他人学习、研究、使用。因此，我们开始思考一个问题，提供哪些模型信息，能够让对方能够完全复现我们的模型？
模型代码： 包含了我们如何定义模型的结构，包括模型有多少层/每层有多少神经元等等信息；包含了我们如何定义的训练过程，包括epoch batch_size等参数；包含了我们如何加载数据和使用；包含了我们如何测试评估模型。 模型参数：提供了模型代码之后，对方确实能够复现模型，但是运行的参数需要重新训练才能得到，而没有办法在我们的模型参数基础上继续训练，因此对方还希望我们能够把模型的参数也保存下来给对方。 包含model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/722f7791f6e71f563d554c0dfdf14fd8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-22T13:48:52+08:00" />
<meta property="article:modified_time" content="2024-01-22T13:48:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch 内 LibTorch/TorchScript 的使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>PyTorch 内 LibTorch/TorchScript 的使用</h4> 
 <ul><li><a href="#1_pt_pth_bin_onnx__10" rel="nofollow">1. .pt .pth .bin .onnx 格式</a></li><li><ul><li><a href="#11__11" rel="nofollow">1.1 模型的保存与加载到底在做什么？</a></li><li><a href="#12__32" rel="nofollow">1.2 为什么要约定格式？</a></li><li><a href="#13__39" rel="nofollow">1.3 格式汇总</a></li><li><ul><li><a href="#131_pt_pth__47" rel="nofollow">1.3.1 .pt .pth 格式</a></li><li><a href="#132_bin__126" rel="nofollow">1.3.2 .bin 格式</a></li><li><a href="#133__175" rel="nofollow">1.3.3 直接保存完整模型</a></li><li><a href="#134_onnx__194" rel="nofollow">1.3.4 .onnx 格式</a></li><li><a href="#135_jittrace_237" rel="nofollow">1.3.5 jit.trace</a></li><li><a href="#136_jitscript_273" rel="nofollow">1.3.6 jit.script</a></li></ul> 
   </li><li><a href="#14__306" rel="nofollow">1.4 总结</a></li></ul> 
  </li><li><a href="#2_TorchScript__317" rel="nofollow">2. TorchScript 的转换</a></li><li><ul><li><a href="#21_jit_trace__330" rel="nofollow">2.1 jit trace 注意事项</a></li><li><a href="#22_jit_trace__565" rel="nofollow">2.2 jit trace 验证技巧</a></li><li><a href="#23__trace__script_590" rel="nofollow">2.3 混合使用 trace 和 script</a></li><li><a href="#24_trace__script__679" rel="nofollow">2.4 trace 和 script 的性能</a></li><li><a href="#25__705" rel="nofollow">2.5 总结</a></li></ul> 
  </li><li><a href="#3_LibTorch__714" rel="nofollow">3. LibTorch 的使用</a></li><li><ul><li><a href="#31_LibTorch__717" rel="nofollow">3.1 LibTorch 的链接</a></li><li><a href="#32__737" rel="nofollow">3.2 接口和实现</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<p><strong>Reference：</strong></p> 
<ol><li><a href="https://blog.csdn.net/yz2zcx/article/details/100609210">[Pytorch].pth转.pt文件</a></li><li><a href="https://zhuanlan.zhihu.com/p/620688513" rel="nofollow">Pytorch格式 .pt .pth .bin .onnx 详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/456770536" rel="nofollow">pytorch 基于tracing/script方式转ONNX</a></li></ol> 
<p><img src="https://images2.imgbox.com/20/30/Aca5zBiW_o.jpg" alt="在这里插入图片描述" width="500"></p> 
<h2><a id="1_pt_pth_bin_onnx__10"></a>1. .pt .pth .bin .onnx 格式</h2> 
<h3><a id="11__11"></a>1.1 模型的保存与加载到底在做什么？</h3> 
<p>我们在使用pytorch构建模型并且训练完成后，下一步要做的就是把这个模型放到实际场景中应用，或者是分享给其他人学习、研究、使用。因此，我们开始思考一个问题，提供哪些模型信息，能够让对方能够完全复现我们的模型？</p> 
<ul><li><strong>模型代码</strong>： 
  <ol><li>包含了我们如何定义模型的结构，包括模型有多少层/每层有多少神经元等等信息；</li><li>包含了我们如何定义的训练过程，包括epoch batch_size等参数；</li><li>包含了我们如何加载数据和使用；</li><li>包含了我们如何测试评估模型。</li></ol> </li><li><strong>模型参数</strong>：提供了模型代码之后，对方确实能够复现模型，但是运行的参数需要重新训练才能得到，而没有办法在我们的模型参数基础上继续训练，因此对方还希望我们能够把模型的参数也保存下来给对方。 
  <ol><li>包含model.state_dict()，这是模型每一层可学习的节点的参数，比如weight/bias；</li><li>包含optimizer.state_dict()，这是模型的优化器中的参数；</li><li>包含我们其他参数信息，如epoch/batch_size/loss等。</li></ol> </li><li><strong>数据集</strong>： 
  <ol><li>包含了我们训练模型使用的所有数据；</li><li>可以提示对方如何去准备同样格式的数据来训练模型。</li></ol> </li><li><strong>使用文档</strong>： 
  <ol><li>根据使用文档的步骤，每个人都可以重现模型；</li><li>包含了模型的使用细节和我们相关参数的设置依据等信息。</li></ol> </li></ul> 
<p>可以看到，根据我们提供的<strong>模型代码/模型参数/数据集/使用文档</strong>，我们就可以有理由相信对方是有手就会了，那么目的就达到了。</p> 
<p><strong>现在我们反转一下思路，我们希望别人给我们提供模型的时候也能够提供这些信息，那么我们就可以拿捏住别人的模型了。</strong></p> 
<h3><a id="12__32"></a>1.2 为什么要约定格式？</h3> 
<p>根据上一段的思路，我们知道模型重现的关键是模型结构/模型参数/数据集，那么我们提供或者希望别人提供这些信息，需要一个交流的规范，这样才不会1000个人给出1000种格式，而 <code>.pt</code> <code>.pth</code> <code>.bin</code> 以及 <code>.onnx</code> 就是约定的格式。</p> 
<blockquote> 
 <p>torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.</p> 
</blockquote> 
<p>不同的后缀只是用于提示我们文件可能包含的内容，但是具体的内容需要看模型提供者编写的 README.md 才知道。而在使用 <code>torch.load()</code> 方法加载模型信息的时候，并不是根据文件的后缀进行的读取，而是根据文件的实际内容自动识别的，因此<strong>对于 torch.load() 方法而言，不管你把后缀改成是什么，只要文件是对的都可以读取</strong>。</p> 
<blockquote> 
 <p>torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into.</p> 
</blockquote> 
<h3><a id="13__39"></a>1.3 格式汇总</h3> 
<table><thead><tr><th>格式</th><th>解释</th><th>适用场景</th><th>可对应的后缀</th></tr></thead><tbody><tr><td>.pt 或 .pth</td><td>PyTorch 的默认模型文件格式，用于保存和加载完整的 PyTorch 模型，包含模型的结构和参数等信息</td><td>需要保存和加载完整的 PyTorch 模型的场景，例如在训练中保存最佳的模型或在部署中加载训练好的模型</td><td>.pt 或 .pth</td></tr><tr><td>.bin</td><td>一种通用的二进制格式，可以用于保存和加载各种类型的模型和数据</td><td>需要将 PyTorch 模型转换为通用的二进制格式的场景</td><td>.bin</td></tr><tr><td>ONNX</td><td>一种通用的模型交换格式，可以用于将模型从一个深度学习框架转换到另一个深度学习框架或硬件平台。在 PyTorch 中，可以使用 torch.onnx.export 函数将 PyTorch 模型转换为 ONNX 格式</td><td>需要将 PyTorch 模型转换为其他深度学习框架或硬件平台可用的格式的场景</td><td>.onnx</td></tr><tr><td>TorchScript</td><td>PyTorch 提供的一种序列化和优化模型的方法，可以将 PyTorch 模型转换为一个序列化的程序，并使用 JIT 编译器对模型进行优化。在 PyTorch 中，<strong>可以使用 torch.git.trace 或 torch.git.script 函数将 PyTorch 模型转换为 TorchScript 格式</strong></td><td>需要将 PyTorch 模型序列化和优化，并<strong>在没有 Python 环境的情况下运行模型的场景</strong></td><td>.pt 或 .pth</td></tr></tbody></table> 
<h4><a id="131_pt_pth__47"></a>1.3.1 .pt .pth 格式</h4> 
<p>一个完整的 PyTorch 模型文件，包含了如下参数：</p> 
<ul><li>model_state_dict：模型参数</li><li>optimizer_state_dict：优化器的状态</li><li>epoch：当前的训练轮数</li><li>loss：当前的损失值</li></ul> 
<p>下面是一个 <code>.pt</code> 文件的保存和加载示例(注意，后缀也可以是 <code>.pth</code>)：</p> 
<ul><li><code>.state_dict()</code>：包含所有的参数和持久化缓存的字典，model 和 optimizer 都有这个方法</li><li><code>torch.save()</code>：将所有的组件保存到文件中</li></ul> 
<p><strong>模型保存</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的模型</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment"># 初始化优化器</span>

loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 初始化损失函数</span>

PATH <span class="token operator">=</span> <span class="token string">"model.pth"</span> <span class="token comment"># 保存路径</span>

<span class="token comment"># 保存模型</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
            <span class="token string">'epoch'</span><span class="token punctuation">:</span> <span class="token number">10</span><span class="token punctuation">,</span>
            <span class="token string">'model_state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'optimizer_state_dict'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">'loss'</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<p>netron 可得：<br> <img src="https://images2.imgbox.com/fa/6c/XaTvP6g4_o.png" alt="在这里插入图片描述" width="400"></p> 
<p><strong>模型加载</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义同样的模型结构</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 加载模型</span>
model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
PATH <span class="token operator">=</span> <span class="token string">"model.pth"</span>
checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'model_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>
loss <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="132_bin__126"></a>1.3.2 .bin 格式</h4> 
<p><code>.bin</code> 文件是一个二进制文件，可以保存 PyTorch 模型的参数和持久化缓存。.bin 文件的大小较小，加载速度较快，因此在生产环境中使用较多。</p> 
<p>下面是一个.bin文件的保存和加载示例(注意：也可以使用 .pt .pth 后缀—后缀无意义)：<br> <strong>保存模型</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的模型</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 保存参数到.bin文件</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<p><strong>加载模型</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义相同的模型结构</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token comment"># 加载.bin文件</span>
model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="133__175"></a>1.3.3 直接保存完整模型</h4> 
<p>可以看出来，我们在之前的保存方式中，都是保存了 <code>.state_dict()</code>，但是没有保存模型的结构，在其他地方使用的时候，必须先重新定义相同结构的模型（或兼容模型），才能够加载模型参数进行使用，如果我们想直接把整个模型都保存下来，避免重新定义模型，可以按如下操作：<br> <strong>保存模型</strong></p> 
<pre><code class="prism language-python">PATH <span class="token operator">=</span> <span class="token string">"entire_model.pt"</span>
<span class="token comment"># PATH = "entire_model.pth"</span>
<span class="token comment"># PATH = "entire_model.bin"</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>
</code></pre> 
<p>netron 可得：<br> <img src="https://images2.imgbox.com/9e/c9/pDPkS8Zw_o.png" alt="在这里插入图片描述" width="400"></p> 
<p>可以看到与上面仅保存参数的方式相比，多了很多信息。</p> 
<p><strong>加载模型</strong></p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"entire_model.pt"</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="134_onnx__194"></a>1.3.4 .onnx 格式</h4> 
<p>上述保存的文件可以通过 PyTorch 提供的 <code>torch.onnx.export</code> 函数<strong>转化为ONNX格式</strong>，这样可以在其他深度学习框架中使用 PyTorch 训练的模型。转化方法如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>onnx

<span class="token comment"># 将模型保存为.bin文件</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"model.bin"</span><span class="token punctuation">)</span>
<span class="token comment"># torch.save(model.state_dict(), "model.pt")</span>
<span class="token comment"># torch.save(model.state_dict(), "model.pth")</span>

<span class="token comment"># 将.bin文件转化为ONNX格式</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"model.bin"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># model.load_state_dict(torch.load("model.pt"))</span>
<span class="token comment"># model.load_state_dict(torch.load("model.pth"))</span>
example_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>onnx<span class="token punctuation">.</span>export<span class="token punctuation">(</span>model<span class="token punctuation">,</span> example_input<span class="token punctuation">,</span> <span class="token string">"model.onnx"</span><span class="token punctuation">,</span> input_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>加载 ONNX 格式</strong>的代码可以参考以下示例代码(<font color="green">注意 ONNX 只能推理不能训练，不包含反向信息的</font>)：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> onnx
<span class="token keyword">import</span> onnxruntime

<span class="token comment"># 加载ONNX文件</span>
onnx_model <span class="token operator">=</span> onnx<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"model.onnx"</span><span class="token punctuation">)</span>

<span class="token comment"># 将ONNX文件转化为ORT格式</span>
ort_session <span class="token operator">=</span> onnxruntime<span class="token punctuation">.</span>InferenceSession<span class="token punctuation">(</span><span class="token string">"model.onnx"</span><span class="token punctuation">)</span>

<span class="token comment"># 输入数据</span>
input_data <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

<span class="token comment"># 运行模型</span>
outputs <span class="token operator">=</span> ort_session<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> input_data<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment"># 输出结果</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
</code></pre> 
<p>注意，需要安装 <code>onnx</code> 和 <code>onnxruntime</code> 两个 Python 包。此外，还需要使用 numpy 等其他常用的科学计算库。</p> 
<h4><a id="135_jittrace_237"></a>1.3.5 jit.trace</h4> 
<p><strong>保存模型</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的模型</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span> <span class="token comment"># 初始化优化器</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 初始化损失函数</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

PATH <span class="token operator">=</span> <span class="token string">"model_trace.pth"</span>

<span class="token comment"># 保存模型</span>
example <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
traced_module <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>model<span class="token punctuation">,</span> example<span class="token punctuation">)</span>
traced_module<span class="token punctuation">.</span>save<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/d9/a2/SiTGq6D5_o.png" alt="在这里插入图片描述" width="400"></p> 
<h4><a id="136_jitscript_273"></a>1.3.6 jit.script</h4> 
<p><strong>保存模型</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义一个简单的模型</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span> <span class="token comment"># 初始化优化器</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 初始化损失函数</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

PATH <span class="token operator">=</span> <span class="token string">"model_script.pth"</span> <span class="token comment"># 保存路径</span>

<span class="token comment"># 保存模型</span>
scripted_module <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
scripted_module<span class="token punctuation">.</span>save<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>
</code></pre> 
<p>netron 可得：<br> <img src="https://images2.imgbox.com/d8/ba/uDlTFuPb_o.png" alt="在这里插入图片描述" width="400"></p> 
<h3><a id="14__306"></a>1.4 总结</h3> 
<p>综上，PyTorch 可以导出的模型的几种后缀格式，但是模型导出的关键并不是后缀，而是到处时候提供的信息到底是什么，只要知道了模型的 <code>model.state_dict()</code> 和 <code>optimizer.state_dict()</code>，以及相应的<code>epoch</code> <code>batch_size</code> <code>loss</code>等信息，我们就能够重建出模型，至于要导出哪些信息，就取决于你了，务必在 readme.md 中写清楚，导出了哪些信息。</p> 
<table><thead><tr><th>保存场景</th><th>保存方法</th><th>文件后缀</th></tr></thead><tbody><tr><td>整个模型(保存模型结构)</td><td>model = Net() <br> torch.save(model, PATH)</td><td>.pt .pth .bin</td></tr><tr><td>仅模型参数(不保存模型结构)</td><td>model = Net() <br> torch.save(model.state_dict(), PATH)</td><td>.pt .pth .bin</td></tr><tr><td>checkpoints使用</td><td>model = Net() <br> torch.save({‘epoch’:10,‘model_state_dict’:model.state_dict(),‘optimizer_state_dict’: optimizer.state_dict(),‘loss’: loss,}, PATH)</td><td>.pt .pth .bin</td></tr><tr><td>ONNX通用保存</td><td>model = Net() <br> model.load_state_dict(torch.load(“model.bin”)) <br> example_input = torch.randn(1, 3) <br> torch.onnx.export(model, example_input, “model.onnx”, input_names=[“input”], output_names=[“output”])</td><td>.onnx</td></tr><tr><td>TorchScript 无 Python 环境使用</td><td>model = Net() <br> model_scripted = torch.jit.script(model) # Export to TorchScript <br> model_scripted.save(‘model_scripted.pt’) <br> model = torch.jit.load(‘model_scripted.pt’) <br> model.eval()</td><td>.pt .pth</td></tr></tbody></table> 
<h2><a id="2_TorchScript__317"></a>2. TorchScript 的转换</h2> 
<p>上文内提到 <code>.pth</code> 与 <code>pt</code> 等价，而且后缀主要用于提示。不过相对来说，PyTorch 的模型文件一般保存为 <code>.pth</code> 文件的更多一点，而 C++ 接口一般读取的是 <code>.pt</code> 文件，因此，C++ 在调用 PyTorch 训练好的模型文件的时候，就需要转换为以 <code>.pt</code> 为代表的 <code>TorchScript</code> 文件，才能够读取。</p> 
<p>Script mode 通过 <code>torch.jit.trace</code> 或者 <code>torch.jit.script</code> 来调用。这两个函数都是将 Python 代码转换为 TorchScript 的两种不同的方法。</p> 
<ul><li> <p><code>torch.jit.trace</code>：将一个特定的输入(通常是一个张量，需要我们提供一个input)传递给一个 PyTorch 模型，torch.jit.trace 会跟踪此 input 在 model 中的计算过程，然后将其转换为 Torch 脚本。<font color="red">这个方法适用于那些在静态图中可以完全定义的模型</font>，例如具有固定输入大小的神经网络。通常用于转换预训练模型。</p> </li><li> <p><code>torch.jit.script</code> 直接将 Python 函数(或者一个 Python 模块)通过 Python 语法规则和编译转换为 Torch 脚本。torch.jit.script <font color="red">更适用于动态图模型，这些模型的结构和输入可以在运行时发生变化</font>。例如，对于 RNN 或者一些具有可变序列长度的模型，使用 torch.jit.script 会更为方便。</p> </li></ul> 
<p>在通常情况下，更应该倾向于使用 <code>torch.jit.trace</code> 而不是 <code>torch.jit.script</code>。</p> 
<p>在模型部署方面，ONNX 被大量使用。而导出 ONNX 的过程，也是 model 进行 torch.jit.trace 的过程，因此这里我们把 torch 的 trace 做稍微详细一点的介绍。</p> 
<h3><a id="21_jit_trace__330"></a>2.1 jit trace 注意事项</h3> 
<p>为了能够把模型编写的更能够被 jit trace，需要对代码做一些妥协，例如：</p> 
<ol><li> <p>如果 model 中有 <code>DataParallel</code> 的子模块，或者 model 中有将 tensors 转换为 numpy arrays，或者调用了 OpenCV 的函数等，这种情况下，model 不是一个正确的在单个设备上、正确连接的 graph，这种情况下，不管是使用 <code>torch.jit.script</code> 还是 <code>torch.jit.trace</code> 都不能 <code>trace</code> 出正确的 TorchScript 来。</p> </li><li> <p>model 的输入输出应该是 <code>Union[Tensor, Tuple[Tensor], Dict[str, Tensor]]</code> 的类型，而且在 dict 中的值，应该是同样的类型。但是对于 model 中间子模块的输入输出，可以是任意类型，例如 dicts of Any, classes, kwargs 以及 Python 支持的都可以。对于 model 输入输出类型的限制是比较容易满足的，在Detectron2中，有类似的例子：</p> <pre><code class="prism language-python">outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>   <span class="token comment"># inputs和outputs是python的类型, 例如dictsor classes</span>
<span class="token comment"># torch.jit.trace(model, inputs)  # 失败！trace只支持Union[Tensor,Tuple[Tensor], Dict[str, Tensor]]类型</span>
adapter <span class="token operator">=</span> TracingAdapter<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>  <span class="token comment"># 使用Adapter，将model inputs包装为trace支持的类型</span>
traced <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>adapter<span class="token punctuation">,</span> adapter<span class="token punctuation">.</span>flattened_inputs<span class="token punctuation">)</span>  <span class="token comment"># 现在以trace成功</span>

<span class="token comment"># Traced model的输出只能是tuple tensors类型:</span>
flattened_outputs <span class="token operator">=</span> traced<span class="token punctuation">(</span><span class="token operator">*</span>adapter<span class="token punctuation">.</span>flattened_inputs<span class="token punctuation">)</span>
<span class="token comment"># 再通过adapter转换为想要的输出类型</span>
new_outputs <span class="token operator">=</span> adapter<span class="token punctuation">.</span>outputs_schema<span class="token punctuation">(</span>flattened_outputs<span class="token punctuation">)</span>
</code></pre> </li><li> <p>一些数值类型的问题。比如下面的代码片段：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch
a<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>在eager mode下，这几个返回值的类型都是int型。上面代码的输出为：</p> <pre><code class="prism language-python"><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'int'</span><span class="token operator">&gt;</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'int'</span><span class="token operator">&gt;</span>
<span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'int'</span><span class="token operator">&gt;</span>
</code></pre> <p>但是在 trace mode 下，这几个表达式的返回值类型都是 <code>Tensor</code> 类型。因此，有些表达式使用不当，如果在 trace 过程中，一些 shape 表达式的返回值类型是 int 型，那么可能造成这块代码没有被 trace。在代码中，可以通过使用 <code>torch.jit.is_tracing</code> 来检查这块代码在 <code>trace mode</code> 下有没有被执行。</p> </li><li> <p>由于动态的 control flow，造成模型没有被完整的 trace。看下面的例子：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">if</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>square<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

m <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span>code<span class="token punctuation">)</span>
</code></pre> <p>输出为：</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tensor<span class="token punctuation">:</span>
  <span class="token keyword">return</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> <p>可以看到 trace 后的 model 只保留了一条分支。因此由于输入造成的 dynamic 的 control flow，trace 后容易出现错误。</p> <p>这种情况下，我们可以使用 <code>torch.jit.script</code> 来进行 TorchScript 的转换。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">if</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>square<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

m <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script<span class="token punctuation">(</span>f<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span>code<span class="token punctuation">)</span>
</code></pre> <p>输出为：</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tensor<span class="token punctuation">:</span>
  <span class="token keyword">if</span> <span class="token builtin">bool</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    _0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    _0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>square<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  <span class="token keyword">return</span> _0
</code></pre> <p><font color="red">在大多数情况下，我们应该使用 <code>torch.jit.trace</code>，但是像上面的这种 <code>dynamic control flow</code> 的情况，我们可以混合使用 <code>torch.jit.trace</code> 和 <code>torch.jit.script</code>，在后面会进行阐述</font>。<br> 另外在一些 Blog 中，对于 dynamic control flow 的定义是有错误的，例如 if x[0] == 4: x += 1 是 dynamic control flow，但是：</p> <pre><code class="prism language-python">model<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Sequential <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token keyword">for</span> m <span class="token keyword">in</span> model<span class="token punctuation">:</span>
  x <span class="token operator">=</span> m<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> <p>以及：</p> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  backbone<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module
  head<span class="token punctuation">:</span> Optiona<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span>
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>backbone<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>head <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> x
</code></pre> <p>都不是 <code>dynamic control flow</code>。<font color="red"><code>dynamic control flow</code> 是由于对输入条件的判断造成的不同分支的执行</font>。</p> </li><li> <p>trace 过程中，将变量 trace 成了常量。看下面一个例子：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch
a<span class="token punctuation">,</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">f1</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">f2</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f1<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 输出: tensor([0, 1])</span>
<span class="token comment"># 可以看到trace后的model是没问题的，这里使用变量a作为torch.jit.trace的example input，然后将转换后的TorchScript用变量b作为输入，正常情况下，b的shape是2维的，因此返回值是tensor([0,1])是正确的</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f2<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 输出：</span>
<span class="token comment"># TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.</span>
<span class="token comment"># tensor([0])</span>
<span class="token comment"># 可以看到这个输出结果是错误的，b的维度是2维，输出应该是tensor([0,1])，这里torch.jit.trace也提示了，使用len可能会造成不正确的trace。</span>

<span class="token comment"># 我们打印一下两者的区别</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f1<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span>code<span class="token punctuation">,</span> <span class="token string">'\n'</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f2<span class="token punctuation">,</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span>code<span class="token punctuation">)</span>
<span class="token comment"># 输出</span>
<span class="token comment"># def f1(x: Tensor) -&gt; Tensor:</span>
<span class="token comment">#   _0 = ops.prim.NumToTensor(torch.size(x, 0))</span>
<span class="token comment">#   _1 = torch.arange(annotate(number, _0), dtype=None, layout=None, device=torch.device("cpu"), pin_memory=False)</span>
<span class="token comment">#   return _1</span>

<span class="token comment">#  def f2(x: Tensor) -&gt; Tensor:</span>
<span class="token comment">#   _0 = torch.arange(1, dtype=None, layout=None, device=torch.device("cpu"), pin_memory=False)</span>
<span class="token comment">#   return _0</span>

<span class="token comment"># TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.</span>

<span class="token comment"># 从trace的code中可以看出，使用x.shape这种方式，在trace后的code里面，是有shape的一个变量值存在的，但是直接使用len这种方式，trace后的code里面，就直接是1</span>
</code></pre> <p>我们导出 ONNX 的过程，也是进行 <code>torch.jit.trace</code> 的过程，在导出 ONNX 的时候，有时候也会遇到</p> 
  <blockquote> 
   <p>TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.</p> 
  </blockquote> <p>这样的提示信息，这时候要检查一下代码中是不是有可能 trace 过程中，变量会被当做常量的情况，有可能会导致导出的 ONNX 精度异常。</p> 
  <ul><li><strong>关于 ONNX</strong>： <br> ONNX 默认基于 trace 的方式，运行一次模型，记录下和 tensor 的相关操作。trace 将不会捕获根据输入数据而改变的行为。比如 if 语句，只会记录执行的那一条分支，同样的，for 循环的次数，导出与跟踪运行完全相同的静态图。如果要使用动态控制流导出模型，则需要使用 <code>torch.jit.script</code>。<br> <code>torch.jit.script</code>：真正的去编译，在 PYTHON 的 AST 语法树做语法分析句法分析。因此可以使用if等动态控制流。返回 ScriptModule。<br> <code>torch.onnx.export</code> 在运行时，先判断是否是 SriptModule，如果不是，则进行 <code>torch.jit.trace</code>，因此 export 需要一个随机生成的输入参数。<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> cv2
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> onnx
<span class="token keyword">import</span> onnxruntime <span class="token keyword">as</span> ort

<span class="token comment">#from torch.onnx import register_custom_op_symbolic # 私有层支持</span>

<span class="token keyword">class</span> <span class="token class-name">test_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>test_net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#self.model = nn.MaxPool3d(kernel_size=(1,3,3), stride=(2,1,2))</span>
        <span class="token comment">#self.model = nn.AvgPool3d(kernel_size=(1,3,3), stride=(2,1,2)) #-&gt; AveragePool</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv3d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu6 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu66 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out1 <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        f_mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>out1<span class="token punctuation">)</span> <span class="token comment"># -&gt; ReduceMean</span>
        <span class="token comment">#f_mean = torch.mean(out1).item() # item()会将f_mean转换为常数 会丢失 mean操作</span>
        <span class="token comment"># script模式转onnx会报错 torch._C._jit_pass_erase_number_types(graph) RuntimeError: Unknown number type: Scalar</span>
        out2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>out1<span class="token punctuation">,</span> f_mean<span class="token punctuation">)</span>
        <span class="token comment">#outlist = list()</span>
        <span class="token comment">#for i in range(3):</span>
        <span class="token comment">#    if i in [0]:</span>
        <span class="token comment">#        #outlist.append(nn.ReLU()(out2))  # script模式下报错 类对象要提前构建</span>
        <span class="token comment">#        outlist.append(self.relu(out2))   # scrip_to_onnx 报错 找不到25 BUG</span>
        <span class="token comment">#    else:</span>
        <span class="token comment">#        #outlist.append(nn.ReLU6()(out2))</span>
        <span class="token comment">#        outlist.append(self.relu6(out2))</span>
        <span class="token comment">#out = torch.cat(outlist)</span>
        <span class="token comment"># 上述 for循环构图在tracing模式下会展开</span>
        <span class="token comment"># script模式下难转换，报错</span>
        <span class="token comment"># 手动平铺</span>
        o1 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out2<span class="token punctuation">)</span>
        o2 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu6<span class="token punctuation">(</span>out2<span class="token punctuation">)</span>
        <span class="token comment">#o3 = self.relu6(out2)   # script模式下被优化掉了 BUG</span>
        o3 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu66<span class="token punctuation">(</span>out2<span class="token punctuation">)</span>   <span class="token comment"># script模式下被优化掉了</span>
        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>o1<span class="token punctuation">,</span>o2<span class="token punctuation">,</span>o3<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> out

<span class="token comment"># 模型构建和运行</span>
imgh<span class="token punctuation">,</span> imgw <span class="token operator">=</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">94</span>
net <span class="token operator">=</span> test_net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 若存在batchnorm、dropout层则一定要eval() 使得BN层参数不更新</span>
dummy_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>imgh<span class="token punctuation">,</span> imgw<span class="token punctuation">)</span><span class="token comment"># n c d h w</span>
torch_out <span class="token operator">=</span> net<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>dummy_input<span class="token punctuation">)</span><span class="token comment"># net(dummy_input)</span>


<span class="token comment"># export onnx</span>
dynamic_axes <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">'input'</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span><span class="token number">3</span><span class="token punctuation">:</span> <span class="token string">'height'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">'width'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token string">'output'</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span><span class="token number">3</span><span class="token punctuation">:</span> <span class="token string">'height'</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">'width'</span><span class="token punctuation">}</span><span class="token punctuation">}</span> <span class="token comment"># 配置动态分辨率</span>
onnx_pth <span class="token operator">=</span> <span class="token string">"test-conv-relu.onnx"</span>

<span class="token comment"># 传入原model，采用默认trace方式捕获模型，需要运行模型</span>
torch<span class="token punctuation">.</span>onnx<span class="token punctuation">.</span>export<span class="token punctuation">(</span>net<span class="token punctuation">,</span> dummy_input<span class="token punctuation">,</span> onnx_pth<span class="token punctuation">,</span> input_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'input'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dynamic_axes<span class="token operator">=</span>dynamic_axes<span class="token punctuation">)</span>
<span class="token comment"># 也可传入 scriptModule</span>
<span class="token comment">#net_script= torch.jit.script(test_net())</span>
<span class="token comment"># 需要外加配置 example_outputs，用来获取输出的shape和dtype，无需运行模型</span>
<span class="token comment">#torch.onnx.export(net_script, dummy_input, onnx_pth, input_names=['input'], output_names=['output'], dynamic_axes=dynamic_axes, example_outputs=[torch_out])</span>

<span class="token comment"># ort run</span>
oxx_m <span class="token operator">=</span> ort<span class="token punctuation">.</span>InferenceSession<span class="token punctuation">(</span>onnx_pth<span class="token punctuation">)</span>
onnx_blob <span class="token operator">=</span> dummy_input<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
onnx_out <span class="token operator">=</span> oxx_m<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'input'</span><span class="token punctuation">:</span>onnx_blob<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

dummy_input2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>imgh<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> imgw<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span>
onnx_blob2 <span class="token operator">=</span> dummy_input2<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
onnx_out2 <span class="token operator">=</span> oxx_m<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'input'</span><span class="token punctuation">:</span>onnx_blob2<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

<span class="token comment"># opencv run</span>
<span class="token comment">#cv_m = cv2.dnn.readNet(onnx_pth)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'mean diff = '</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>onnx_out <span class="token operator">-</span> torch_out<span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li></ul> <p>除了 <code>len</code> 会导致 trace 错误，其他几个也会导致 trace 出现问题：</p> 
  <ul><li> <p><code>.item()</code> 会在 trace 过程中将 tensors 转为 int/float</p> </li><li> <p>任何将 torch 类型转为 numpy/python 类型的代码</p> </li><li> <p>一些有问题的算子，例如 advanced indexing</p> </li><li> <p><code>torch.jit.trace</code> 不会对传入的 device 生效</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
m <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>f<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span>code<span class="token punctuation">)</span>
<span class="token comment"># 输出</span>
<span class="token comment"># def f(x: Tensor) -&gt; Tensor:</span>
<span class="token comment">#   _0 = ops.prim.NumToTensor(torch.size(x, 0))</span>
<span class="token comment">#   _1 = torch.arange(annotate(number, _0), dtype=None, layout=None, device=torch.device("cpu"), pin_memory=False)</span>
<span class="token comment">#   return _1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>m<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span>
<span class="token comment"># 输出：device(type='cpu')</span>
</code></pre> <p>trace 不会对传入的 cuda device 生效。</p> </li></ul> </li></ol> 
<h3><a id="22_jit_trace__565"></a>2.2 jit trace 验证技巧</h3> 
<p>为了保证trace的正确，我们可以通过一下的一些方法来尽量保证 trace 后的模型不会出错：<br> 1.注意 warnings 信息。类似这样的：</p> 
<blockquote> 
 <p>TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.</p> 
</blockquote> 
<p>TraceWarnings信息，它会造成模型的结果有可能不正确，但是它只是个 warning 等级。<br> 2. 做单元测试。需要验证一下 eager mode 的模型输出与 trace 后的模型输出是否一致。</p> 
<pre><code class="prism language-python"><span class="token keyword">assert</span> allclose<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>model<span class="token punctuation">,</span> input1<span class="token punctuation">)</span><span class="token punctuation">(</span>input2<span class="token punctuation">)</span><span class="token punctuation">,</span> model<span class="token punctuation">(</span>input2<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<ol start="3"><li>避免一些特殊的情况。例如下面的代码：</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">if</span> x<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
  output <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
  output <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 会创建一个空的输出</span>
</code></pre> 
<p>避免一些特殊情况比如空的输入输出之类的。</p> 
<ol start="4"><li>注意shape的使用。前面提到，tensor.size()在trace过程中会返回Tensor类型的数据，Tensor类型会在计算过程中被添加到计算图中，应该避免将Tensor类型的shape转为了常量。主要注意以下两点：</li></ol> 
<ul><li>使用 <code>torch.size(0)</code> 来代替 <code>len(tensor)</code>，因为 <code>torch.size(0)</code> 返回的是 Tensor，<code>len(tensor)</code> 返回的是 int。对于自定义类，实现一个 <code>.size</code> 方法或者使用 <code>.__len__()</code> 方法来代替 <code>len()</code> ，例如这个例子</li><li>不要使用 <code>int()</code> 或者 <code>torch.as_tensor</code> 来转换 size 的类型，因为这些操作也会被视为常量。</li></ul> 
<ol start="5"><li>混合 tracing 和 scripting 方法。可以使用 <code>torch.jit.script</code> 来转换一些 <code>torch.jit.trace</code> 不能搞定的小的代码片段，混合使用 tracing 和 scripting，基本可以解决所有的问题。</li></ol> 
<h3><a id="23__trace__script_590"></a>2.3 混合使用 trace 和 script</h3> 
<p>trace 和 script 都有他们的问题，混合使用可以解决大部分问题。但是为了尽可能减小对于代码质量的负面影响，大部分情况下，都应该使用 <code>torch.jit.trace</code>，必要时才使用 <code>torch.jit.script</code>。</p> 
<ol><li> <p><strong>在使用 <code>torch.jit.trace</code> 时，使用 <code>@script_if_tracing</code> 装饰器可以让被装饰的函数使用 script 方式进行编译</strong></p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># ... some forward logic</span>
  <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script_if_tracing</span>
  <span class="token keyword">def</span> <span class="token function">_inner_impl</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> flag<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token comment"># use control flow, etc.</span>
      <span class="token keyword">return</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
  output <span class="token operator">=</span> _inner_impl<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">,</span> flag<span class="token punctuation">)</span>
  <span class="token comment"># ... other forward logic</span>
</code></pre> <p>但是使用 <code>@script_if_tracing</code> 时，需要保证函数中没有 PyTorch 的 modules，如果有的话，需要做一些修改，例如下面的：</p> <pre><code class="prism language-python"><span class="token comment"># 因为代码中有self.layers()，是一个pytorch的module，因此不能使用@script_if_tracing</span>
<span class="token keyword">if</span> x<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
  x <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
  output <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
  <span class="token comment"># Create empty outputs</span>
  output <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
</code></pre> <p>这里需要做如下修改：</p> <pre><code class="prism language-python"><span class="token comment"># 需要将self.layers移出if判断，这时候可以用@script_if_tracing</span>
<span class="token keyword">if</span> x<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
  x <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
  <span class="token comment"># Create empty inputs</span>
  x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
<span class="token comment"># 需要将self.layers()修改为支持empty的输入，或者将原先的条件判断加入到self.layers中</span>
output <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>合并多次 trace 的结果</strong><br> 使用 <code>torch.jit.script</code> 生成的模型相比使用 <code>torch.jit.trace</code> 有两个好处：</p> 
  <ul><li>可以使用条件控制流，例如模型中使用一个 bool 值来控制 forward 的 flow，在 traced modules 里面是不支持的</li><li>使用 traced module，只能有一个 forward() 函数，但是使用 scripted module，可以有多个前向计算的函数</li></ul> <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Detector</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  do_keypoint<span class="token punctuation">:</span> <span class="token builtin">bool</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>
      box <span class="token operator">=</span> self<span class="token punctuation">.</span>predict_boxes<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
      <span class="token keyword">if</span> self<span class="token punctuation">.</span>do_keypoint<span class="token punctuation">:</span>
          kpts <span class="token operator">=</span> self<span class="token punctuation">.</span>predict_keypoint<span class="token punctuation">(</span>img<span class="token punctuation">,</span> box<span class="token punctuation">)</span>

  <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>export</span>
  <span class="token keyword">def</span> <span class="token function">predict_boxes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">pass</span>

  <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>export</span>
  <span class="token keyword">def</span> <span class="token function">predict_keypoint</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> box<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">pass</span>
</code></pre> <p>对于这种有 bool 值的控制流，除了使用 script，还可以多次进行 trace，然后将结果合并。</p> <pre><code class="prism language-python">det1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>Detector<span class="token punctuation">(</span>do_keypoint<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
det2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>Detector<span class="token punctuation">(</span>do_keypoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
</code></pre> <p>然后将他们的 weight 复制一遍，并合并两次 trace 的结果：</p> <pre><code class="prism language-python">det2<span class="token punctuation">.</span>submodule<span class="token punctuation">.</span>weight <span class="token operator">=</span> det1<span class="token punctuation">.</span>submodule<span class="token punctuation">.</span>weight
<span class="token keyword">class</span> <span class="token class-name">Wrapper</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> do_keypoint<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> do_keypoint<span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
exported <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script<span class="token punctuation">(</span>Wrapper<span class="token punctuation">(</span><span class="token punctuation">[</span>det1<span class="token punctuation">,</span> det2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>对于这种有 bool 值的控制流，除了使用 script，还可以多次进行 trace，然后将结果合并。</p> <pre><code class="prism language-python">det1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>Detector<span class="token punctuation">(</span>do_keypoint<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
det2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>Detector<span class="token punctuation">(</span>do_keypoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
</code></pre> <p>然后将他们的 weight 复制一遍，并合并两次 trace 的结果：</p> <pre><code class="prism language-python">det2<span class="token punctuation">.</span>submodule<span class="token punctuation">.</span>weight <span class="token operator">=</span> det1<span class="token punctuation">.</span>submodule<span class="token punctuation">.</span>weight
<span class="token keyword">class</span> <span class="token class-name">Wrapper</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> do_keypoint<span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> do_keypoint<span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>img<span class="token punctuation">)</span>
exported <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script<span class="token punctuation">(</span>Wrapper<span class="token punctuation">(</span><span class="token punctuation">[</span>det1<span class="token punctuation">,</span> det2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li></ol> 
<h3><a id="24_trace__script__679"></a>2.4 trace 和 script 的性能</h3> 
<p>trace 总是会比 script 生成一样或者更简单的计算图，因此性能会更好一些。因为 script 会完整的表达 Python 代码的逻辑，甚至一些不必要的代码也会如实表达。例如下面的例子：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x3<span class="token punctuation">)</span><span class="token punctuation">:</span>
    z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>
    xs <span class="token operator">=</span> <span class="token punctuation">[</span>x1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x3<span class="token punctuation">]</span>
    <span class="token keyword">for</span> k <span class="token keyword">in</span> z<span class="token punctuation">:</span> x1 <span class="token operator">+=</span> xs<span class="token punctuation">[</span>k<span class="token punctuation">]</span>
    <span class="token keyword">return</span> x1
model <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>script<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">.</span>code<span class="token punctuation">)</span>
<span class="token comment"># def forward(self, x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor:</span>
<span class="token comment">#   z = [0, 1, 2]</span>
<span class="token comment">#   xs = [x1, x2, x3]</span>
<span class="token comment">#   x10 = x1</span>
<span class="token comment">#   for _0 in range(torch.len(z)):</span>
<span class="token comment">#     k = z[_0]</span>
<span class="token comment">#     x10 = torch.add_(x10, xs[k])</span>
<span class="token comment">#   return x10</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>code<span class="token punctuation">)</span>
<span class="token comment"># def forward(self, x1: Tensor, x2: Tensor, x3: Tensor) -&gt; Tensor:</span>
<span class="token comment">#   x10 = torch.add_(x1, x1)</span>
<span class="token comment">#   x11 = torch.add_(x10, x2)</span>
<span class="token comment">#   return torch.add_(x11, x3)</span>
</code></pre> 
<h3><a id="25__705"></a>2.5 总结</h3> 
<p>trace 具有明显的局限性：这篇文章的大部分篇幅都在谈论 trace 的局限性以及如何解决这些问题。实际上，这正是 trace 的优势所在：它有明确的局限性（和解决方案），因此你可以推理它是否有效。</p> 
<p>相反，script 更像是一个黑盒子：在尝试之前，没有人知道它是否有效。文章中没有提到如何修复 script 的任何诀窍：有很多诀窍，但不值得你花时间去探究和修复一个黑盒子。</p> 
<p>trace 和 script 都会影响代码的编写方式，但 trace 因为我们明确它的要求，对我们原始的代码造成的一些修改也不会太严重：</p> 
<ul><li>它限制了输入/输出格式，但仅限于最外层的模块。(如上所述，这个问题可以通过一个wrapper解决）。</li><li>它需要修改一些代码才能通用（例如在 trace 时添加一些 script），但这些修改只涉及受影响模块的内部实现，而不是它们的接口。</li></ul> 
<h2><a id="3_LibTorch__714"></a>3. LibTorch 的使用</h2> 
<p>在得到所需模型后，可以尝试在 C++ 环境下使用得到的模型，这里就用到了 LibTorch。</p> 
<h3><a id="31_LibTorch__717"></a>3.1 LibTorch 的链接</h3> 
<p>结合自己环境的 CUDA 版本，去官网下载对应版本的 libTorch。例如 CUDA 版本为 11.1，则需要在<a href="#https://download.pytorch.org/libtorch/cu111/" rel="nofollow">下载地址</a>中找到 <code>libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu111.zip</code> 进行下载。</p> 
<p>链接进需要再 cmake 内加上这几行即可：</p> 
<pre><code class="prism language-cpp"><span class="token function">set</span><span class="token punctuation">(</span>TORCH_PATH <span class="token string">"/home/yj/libtorch/share/cmake/Torch"</span><span class="token punctuation">)</span>
<span class="token function">message</span><span class="token punctuation">(</span><span class="token string">"TORCH_PATH set to: ${TORCH_PATH}"</span><span class="token punctuation">)</span>
<span class="token function">set</span><span class="token punctuation">(</span>Torch_DIR $<span class="token punctuation">{<!-- --></span>TORCH_PATH<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token function">find_package</span><span class="token punctuation">(</span>Torch REQUIRED<span class="token punctuation">)</span>
<span class="token function">message</span><span class="token punctuation">(</span>STATUS <span class="token string">"Torch version is: ${Torch_VERSION}"</span><span class="token punctuation">)</span>

# <span class="token operator">&lt;</span>target<span class="token operator">&gt;</span> is your target<span class="token number">'</span>s name
<span class="token function">target_link_libraries</span><span class="token punctuation">(</span><span class="token operator">&lt;</span>target<span class="token operator">&gt;</span> 
  $<span class="token punctuation">{<!-- --></span>TORCH_LIBRARIES<span class="token punctuation">}</span>
<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="32__737"></a>3.2 接口和实现</h3> 
<ol><li> <p>头文件引入 ：</p> <pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;torch/script.h&gt;</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;torch/torch.h&gt;</span></span>
</code></pre> </li><li> <p>加载模型</p> <pre><code class="prism language-cpp"><span class="token keyword">module</span> <span class="token operator">=</span> torch<span class="token double-colon punctuation">::</span>jit<span class="token double-colon punctuation">::</span><span class="token function">load</span><span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> </li><li> <p>函数实现</p> </li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5be32e350f20ac75906b773bb2e572bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Pycharm连接远程服务器遇到的问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/da4ce81ccdc3d1e8207b89d8e58d5252/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【算法系列】一文彻底讲懂隐马尔可夫模型</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>