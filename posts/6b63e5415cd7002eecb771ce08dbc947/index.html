<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DStream实战之Spark Streaming整合Kafka, 通过 KafkaUtils.createDstream方式整合Kafka 37 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DStream实战之Spark Streaming整合Kafka, 通过 KafkaUtils.createDstream方式整合Kafka 37" />
<meta property="og:description" content="前言 kafka作为一个实时的分布式消息队列，实时的生产和消费消息，这里我们可以利用SparkStreaming实时地读取kafka中的数据，然后进行相关计算。在Spark1.3版本后，KafkaUtils里面提供了两个创建dstream的方法，一种为KafkaUtils.createDstream，另一种为KafkaUtils.createDirectStream。 1. KafkaUtils.createDstream方式整合Kafka 此种方式现在已被淘汰,大家可作为了解学习~
KafkaUtils.createDstream(ssc, [zk], [group id], [per-topic,partitions] ) 使用了receivers接收器来接收数据，利用的是Kafka高层次的消费者api.对于所有的receivers接收到的数据将会保存在Spark executors中.然后通过Spark Streaming启动job来处理这些数据，默认会丢失.可启用WAL日志，它同步将接受到数据保存到分布式文件系统上比如HDFS。 所以数据在出错的情况下可以恢复出来 。
Spark集群中的某个executor中有一个receiver线程, 这个线程负责从kafka中获取数据.
当receiver线程接收到数据后会做备份处理,即把数据备份到其他的executor中,也可能会备份到这个receiver线程所以在节点的executor中.当备份完毕后该现场会把每个partition的消费偏移量在zookeeper中修改,(新版本的kafka的offset保存在kafka集群中)修改完offset后,该receiver线程会把消费的数据告诉Driver.Driver分发任务时会根据每个executor上的数据,根据数据本地性发送. 2. 代码实现 pom文件中添加kafka依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka_0-8_2.11&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt; &lt;/dependency&gt; 编写Spark Streaming程序 package cn.acece.sparkStreamingKafkaTest import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream} import org.apache.spark.streaming.kafka.KafkaUtils import org.apache.spark.streaming.{Seconds, StreamingContext} import org.apache.spark.{SparkConf, SparkContext} /** * 从kafka中拉取数据 * 读取数据时, consumer记录的offset发送回kafka中, 保存在zk中 * 需要开始WAL日志保存模式防止数据丢失, 需要设置检查点 */ object SparkStreamingPollKafkaHighAPI { def main(args: Array[String]): Unit = { //1. 初始化参数,conf, sc, ssc val conf: SparkConf = new SparkConf() ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6b63e5415cd7002eecb771ce08dbc947/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-03T16:09:41+08:00" />
<meta property="article:modified_time" content="2019-11-03T16:09:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DStream实战之Spark Streaming整合Kafka, 通过 KafkaUtils.createDstream方式整合Kafka 37</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>前言</h2> 
<ul><li>kafka作为一个实时的分布式消息队列，实时的生产和消费消息，这里我们可以利用<strong>SparkStreaming实时地读取kafka中的数据</strong>，然后进行相关计算。</li><li>在Spark1.3版本后，<strong>KafkaUtils里面提供了两个创建dstream的方法，一种为KafkaUtils.createDstream，另一种为KafkaUtils.createDirectStream</strong>。</li></ul> 
<h2><a id="1_KafkaUtilscreateDstreamKafka_3"></a>1. KafkaUtils.createDstream方式整合Kafka</h2> 
<p><strong>此种方式现在已被淘汰,大家可作为了解学习~</strong></p> 
<ul><li>KafkaUtils.createDstream(ssc, [zk], [group id], [per-topic,partitions] ) 使用了<strong>receivers</strong>接收器来接收数据，利用的是<strong>Kafka高层次的消费者api</strong>.</li><li>对于所有的<strong>receivers接收到的数据将会保存在Spark executors</strong>中.</li><li>然后通过Spark Streaming启动job来处理这些数据，默认会丢失.</li><li>可启用WAL日志，它同步将接受到数据保存到分布式文件系统上比如HDFS。 所以数据在出错的情况下可以恢复出来 。<br> <img src="https://images2.imgbox.com/0b/2d/fEPeArOI_o.png" alt="在这里插入图片描述"></li><li>Spark集群中的某个executor中有一个<strong>receiver线程</strong>, 这个线程负责从<strong>kafka中获取数据</strong>.<br> <img src="https://images2.imgbox.com/06/75/SRQz0LJy_o.png" alt="在这里插入图片描述"></li><li>当<strong>receiver线程</strong>接收到数据后会做备份处理,即把数据备份到其他的executor中,也可能会备份到这个receiver线程所以在节点的executor中.</li><li>当备份完毕后该现场会把每个<strong>partition</strong>的<strong>消费偏移量</strong>在<strong>zookeeper</strong>中修改,(新版本的kafka的offset保存在kafka集群中)</li><li>修改完<strong>offset</strong>后,该receiver线程会把<strong>消费的数据</strong>告诉<strong>Driver</strong>.</li><li><strong>Driver</strong>分发任务时会根据每个<strong>executor</strong>上的数据,根据数据本地性发送.</li></ul> 
<h2><a id="2__16"></a>2. 代码实现</h2> 
<ul><li>pom文件中添加kafka依赖</li></ul> 
<pre><code class="prism language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>spark-streaming-kafka_0-8_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.0.2<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span>
</code></pre> 
<ul><li>编写Spark Streaming程序</li></ul> 
<pre><code class="prism language-java"><span class="token keyword">package</span> cn<span class="token punctuation">.</span>acece<span class="token punctuation">.</span>sparkStreamingKafkaTest

<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>dstream<span class="token punctuation">.</span><span class="token punctuation">{<!-- --></span>DStream<span class="token punctuation">,</span> ReceiverInputDStream<span class="token punctuation">}</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span>kafka<span class="token punctuation">.</span>KafkaUtils
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>streaming<span class="token punctuation">.</span><span class="token punctuation">{<!-- --></span>Seconds<span class="token punctuation">,</span> StreamingContext<span class="token punctuation">}</span>
<span class="token keyword">import</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span><span class="token punctuation">{<!-- --></span>SparkConf<span class="token punctuation">,</span> SparkContext<span class="token punctuation">}</span>
<span class="token comment">/**
  * 从kafka中拉取数据
  *   读取数据时, consumer记录的offset发送回kafka中, 保存在zk中
  *   需要开始WAL日志保存模式防止数据丢失, 需要设置检查点
  */</span>
object SparkStreamingPollKafkaHighAPI <span class="token punctuation">{<!-- --></span>
  def <span class="token function">main</span><span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span>String<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> Unit <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//1. 初始化参数,conf, sc, ssc</span>
    val conf<span class="token operator">:</span> SparkConf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkConf</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">setAppName</span><span class="token punctuation">(</span><span class="token string">"SparkStreamingPollKafkaHighAPI"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">setMaster</span><span class="token punctuation">(</span><span class="token string">"local[2]"</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">set</span><span class="token punctuation">(</span><span class="token string">"spark.streaming.receiver.writeAheadLog.enable"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span>
    val sc<span class="token operator">:</span> SparkContext <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SparkContext</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span>
    sc<span class="token punctuation">.</span><span class="token function">setLogLevel</span><span class="token punctuation">(</span><span class="token string">"WARN"</span><span class="token punctuation">)</span>

    val ssc<span class="token operator">:</span> StreamingContext <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">StreamingContext</span><span class="token punctuation">(</span>sc<span class="token punctuation">,</span><span class="token function">Seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">//设置检查点, 开启WLA日志保存机制就要设置检查点</span>
    ssc<span class="token punctuation">.</span><span class="token function">checkpoint</span><span class="token punctuation">(</span><span class="token string">"./check"</span><span class="token punctuation">)</span>

    <span class="token comment">//2. 从kafka中拉取数据, KafKaUtil</span>
    val zkQuorum <span class="token operator">=</span> <span class="token string">"node01:2181,node02:2181,node03:2181"</span>
    val groupId <span class="token operator">=</span> <span class="token string">"group"</span>
    <span class="token comment">//这里的1, 代表每一个分区被N个消费者消费</span>
    val topics <span class="token operator">=</span> Map<span class="token punctuation">[</span>String<span class="token punctuation">,</span>Int<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token string">"itcast"</span><span class="token operator">-</span><span class="token operator">&gt;</span><span class="token number">1</span><span class="token punctuation">)</span>
    val receiver<span class="token operator">:</span> ReceiverInputDStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> String<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> KafkaUtils<span class="token punctuation">.</span><span class="token function">createStream</span><span class="token punctuation">(</span>ssc<span class="token punctuation">,</span>zkQuorum<span class="token punctuation">,</span>groupId<span class="token punctuation">,</span>topics<span class="token punctuation">)</span>

    <span class="token comment">//3. 从主体中获取具体的数据, 也就是value值, key是offect</span>
    val lines<span class="token operator">:</span> DStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> receiver<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span>_2<span class="token punctuation">)</span>

    <span class="token comment">//4. 单词计数</span>
    val words<span class="token operator">:</span> DStream<span class="token punctuation">[</span>String<span class="token punctuation">]</span> <span class="token operator">=</span> lines<span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>_<span class="token punctuation">.</span><span class="token function">split</span><span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    val wordAnd1<span class="token operator">:</span> DStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> words<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    val result<span class="token operator">:</span> DStream<span class="token punctuation">[</span><span class="token punctuation">(</span>String<span class="token punctuation">,</span> Int<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> wordAnd1<span class="token punctuation">.</span><span class="token function">reduceByKeyAndWindow</span><span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token operator">:</span>Int<span class="token punctuation">,</span>y<span class="token operator">:</span>Int<span class="token punctuation">)</span> <span class="token operator">=</span><span class="token operator">&gt;</span> x<span class="token operator">+</span>y<span class="token punctuation">,</span> <span class="token function">Seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token function">Seconds</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment">//5. 打印</span>
    result<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment">//6. 开启流模式</span>
    ssc<span class="token punctuation">.</span><span class="token function">start</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    ssc<span class="token punctuation">.</span><span class="token function">awaitTermination</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h2><a id="3_KafkaSpark_Streaming_77"></a>3. 先启动Kafka再启动Spark Streaming程序,试试采集</h2> 
<ul><li>启动zookeeper集群</li></ul> 
<pre><code class="prism language-shell">zkServer.sh start
</code></pre> 
<ul><li>先启动kafka集群</li></ul> 
<pre><code class="prism language-shell">kafka-server-start.sh /export/servers/kafka/config/server.properties
</code></pre> 
<ul><li>创建topic</li></ul> 
<pre><code class="prism language-shell">kafka-topic.sh --create --zookeeper node01:2181 --replication-factor 1 --partitions 3 --topic kafka_spark
</code></pre> 
<ul><li>向topic中生产数据, 通过shell命令向topic发送消息</li></ul> 
<pre><code class="prism language-shell">kafka-console-producer.sh --broker-list node01:9092 --topic kafka-spark
</code></pre> 
<p><img src="https://images2.imgbox.com/07/e2/L6aBhTz7_o.png" alt="在这里插入图片描述"></p> 
<ul><li>在IDEA中执行2中编写的Spark Streaming程序</li><li>运行代码, 查看控制台结果<br> <img src="https://images2.imgbox.com/6e/04/GyBeh8PM_o.png" alt="在这里插入图片描述"></li></ul> 
<h2><a id="4__98"></a>4. 总结</h2> 
<ul><li>通过这种方式实现，刚开始的时候系统正常运行，没有发现问题，但是如果系统异常重新启动sparkstreaming程序后，<strong>发现程序会重复处理已经处理过的数据</strong>.</li><li>这种基于receiver的方式，是使用<strong>Kafka的高级API</strong>，<strong>topic的offset偏移量在ZooKeeper中</strong>。</li><li>这是消费Kafka数据的传统方式。这种方式<strong>配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据只被处理一次，可能会处理两次</strong>。</li><li>因为<strong>Spark和ZooKeeper之间可能是不同步的</strong>。官方现在也已经不推荐这种整合方式，我们使用官网推荐的第二种方式kafkaUtils的createDirectStream()方式。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a216fccf4f0f034d582e710e165e8754/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">节流、防抖及使用场景</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bb81d17bcf921d5afcff8f02cd456511/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">puppeteer离线安装不同版本的chrome</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>