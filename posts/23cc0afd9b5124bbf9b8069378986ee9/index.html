<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>面试常问的深度学习(DNN、CNN、RNN)的相关问题 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="面试常问的深度学习(DNN、CNN、RNN)的相关问题" />
<meta property="og:description" content="神经网络 的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。
对卡在局部极小值的处理方法： 1.调节步伐：调节学习速率，使每一次的更新“步伐”不同；2.优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）。
浅层VS深层： 浅层神经网络可以模拟任何函数，但数据量的代价是无法接受的。深层解决了这个问题。相比浅层神经网络，深层神经网络可以用更少的数据量来学到更好的拟合。深层的前提是：空间中的元素可以由迭代发展而来的。
防止过拟合： L2正则化，Dropout(若规律不是在所有样本中都存在,则dropout会删除这样的规律)，每个epoch之后shuffle训练数据，设置early-stopping。加Batch Normalization(BN首先是把所有的samples的统计分布标准化，降低了batch内不同样本的差异性，然后又允许batch内的各个samples有各自的统计分布)，BN最大的优点为允许网络使用较大的学习速率进行训练，加快网络的训练速度（减少epoch次数），提升效果。
为何使用Batch Normalization： 若用多个梯度的均值来更新权重的批量梯度下降法可以用相对少的训练次数遍历完整个训练集，其次可以使更新的方向更加贴合整个训练集，避免单个噪音样本使网络更新到错误方向。然而也正是因为平均了多个样本的梯度，许多样本对神经网络的贡献就被其他样本平均掉了，相当于在每个epoch中，训练集的样本数被缩小了。batch中每个样本的差异性越大，这种弊端就越严重。一般的解决方法就是在每次训练完一个epoch后，将训练集中样本的顺序打乱再训练另一个epoch，不断反复。这样重新组成的batch中的样本梯度的平均值就会与上一个epoch的不同。而这显然增加了训练的时间。同时因为没办法保证每次更新的方向都贴合整个训练集的大方向，只能使用较小的学习速率。这意味着训练过程中，一部分steps对网络最终的更新起到了促进，一部分steps对网络最终的更新造成了干扰，这样“磕磕碰碰”无数个epoch后才能达到较为满意的结果。
为了解决这种“不效率”的训练，BN首先是把所有的samples的统计分布标准化，降低了batch内不同样本的差异性，然后又允许batch内的各个samples有各自的统计分布。
1. 为什么神经网络高效：并行的先验知识使得模型可用线性级数量的样本学习指数级数量的变体
2. 学习的本质是什么：将变体拆分成因素和知识（Disentangle Factors of Variation）
i. 为什么深层神经网络比浅层神经网络更高效：迭代组成的先验知识使得样本可用于帮助训练其他共用同样底层结构的样本。
ii. 神经网络在什么问题上不具备优势：不满足并行与迭代先验的任务
3. 非迭代：该层状态不是由上层状态构成的任务（如：很深的CNN因为有max pooling，信息会逐渐丢失。而residual network再次使得迭代的先验满足）
CNN: 1）卷积：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。
2）池化：降维的方法，按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。
3) 全连接：softmax分类
训练过程：
卷积核中的因子(×1或×0)其实就是需要学习的参数，也就是卷积核矩阵元素的值就是参数值。一个特征如果有9个值，1000个特征就有900个值，再加上多个层，需要学习的参数还是比较多的。
CNN的三个优点： sparse interaction(稀疏的交互)，parameter sharing(参数共享)，equivalent respresentation(等价表示)。适合于自动问答系统中的答案选择模型的训练。
CNN与DNN的区别： DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），CNN的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。
filter尺寸计算：Feature Map的尺寸等于(input_size &#43; 2 * padding_size − filter_size)/stride&#43;1
RNN: 1. 为什么具有记忆功能？
这个是在RNN就解决的问题，就是因为有递归效应，上一时刻隐层的状态参与到了这个时刻的计算过程中，直白一点呢的表述也就是选择和决策参考了上一次的状态。
2. 为什么LSTM记的时间长？
因为特意设计的结构中具有CEC的特点，误差向上一个状态传递时几乎没有衰减，所以权值调整的时候，对于很长时间之前的状态带来的影响和结尾状态带来的影响可以同时发挥作用，最后训练出来的模型就具有较长时间范围内的记忆功能。
误差回传的主力还是通过了Memory Cell而保持了下来。所以我们现在用的LSTM模型，依然有比较好的效果。
最后整个梳理一下误差回传的过程，误差通过输出层，分类器，隐层等进入某个时刻的Block之后，先将误差传递给了Output Gate和Memory Cell两个地方。
到达输出门的误差，用来更新了输出门的参数w，到达Memory Cell之后，误差经过两个路径，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/23cc0afd9b5124bbf9b8069378986ee9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-07-24T16:46:40+08:00" />
<meta property="article:modified_time" content="2017-07-24T16:46:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">面试常问的深度学习(DNN、CNN、RNN)的相关问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="font-family:Times New Roman; font-size:18px"></span></p> 
<h3><strong><span style="color:rgb(51,51,51)">神经网络</span></strong></h3> 
<p><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span>的学习就是</span><span style="color:#333333">学习如何利用<u>矩阵的线性变换</u>加<u>激活函数的非线性变换</u>，将原始输入空间投向线性可分</span><span style="color:#333333">/</span><span style="color:#333333">稀疏的空间去分类</span><span style="color:#333333">/</span><span style="color:#333333">回归。</span><strong><span style="color:rgb(133,133,133)">增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。</span></strong></p> 
<h4><strong><span style="color:#333333">对卡在局部极小值的处理方法：</span></strong></h4> 
<p><u><span style="color:#333333"><span style="white-space:pre"></span>1.</span></u><strong><u><span style="color:rgb(51,51,51)">调节步伐</span></u><span style="color:rgb(51,51,51)">：</span></strong><span style="color:rgb(51,51,51)">调节学习速率，使每一次的更新</span>“步伐”不同<span style="color:rgb(51,51,51)">；</span><u><span style="color:rgb(51,51,51)">2.</span></u><strong><u><span style="color:rgb(51,51,51)">优化起点</span></u></strong><span style="color:rgb(51,51,51)">：合理初始化权重（</span>weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）。</p> 
<h4><strong><span style="color:rgb(51,51,51)">浅层</span><span style="color:rgb(51,51,51)">VS</span><span style="color:rgb(51,51,51)">深层</span></strong><span style="color:rgb(51,51,51)">：</span></h4> 
<p><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span>浅层神经网络可以模拟任何函数，但数据量的代价是无法接受的。深层解决了这个问题。相比浅层神经网络，深层神经网络可以用</span><strong>更少的数据量</strong>来学到更好的拟合。深层的前提是：<u>空间中的元素可以由迭代发展而来的。</u></p> 
<h4><strong><span style="color:rgb(51,51,51)">防止过拟合</span></strong><span style="color:rgb(51,51,51)">：</span></h4> 
<p><u><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span>L2</span><span style="color:rgb(51,51,51)">正则化</span></u><span style="color:rgb(51,51,51)">，</span><u><span style="color:rgb(51,51,51)">Dropout</span></u><span style="color:rgb(51,51,51)">(</span><span style="color:rgb(51,51,51)">若规律不是在所有样本中都存在</span><span style="color:rgb(51,51,51)">,</span><span style="color:rgb(51,51,51)">则</span><span style="color:rgb(51,51,51)">dropout</span><span style="color:rgb(51,51,51)">会删除这样的规律</span><span style="color:rgb(51,51,51)">)</span><span style="color:rgb(51,51,51)">，每个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">之后</span><span style="color:rgb(51,51,51)">shuffle</span><span style="color:rgb(51,51,51)">训练数据，设置</span><u><span style="color:rgb(51,51,51)">early-stopping</span></u><span style="color:rgb(51,51,51)">。加</span><u><span style="color:rgb(51,51,51)">Batch Normalization</span></u><span style="color:rgb(51,51,51)">(BN</span><span style="color:rgb(51,51,51)">首先是把所有的</span><span style="color:rgb(51,51,51)">samples</span><span style="color:rgb(51,51,51)">的统计分布标准化，降低了</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">内不同样本的差异性，然后又允许</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">内的各个</span><span style="color:rgb(51,51,51)">samples</span><span style="color:rgb(51,51,51)">有各自的统计分布</span><span style="color:rgb(51,51,51)">)</span><span style="color:rgb(51,51,51)">，</span><span style="color:rgb(51,51,51)">BN</span><span style="color:rgb(51,51,51)">最大的优点为允许网络使用较大的学习速率进行训练，加快网络的训练速度（减少</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">次数），提升效果。</span></p> 
<h4><strong><span style="color:rgb(51,51,51)">为何使用</span><span style="color:rgb(51,51,51)">Batch Normalization</span></strong><span style="color:rgb(51,51,51)">：</span></h4> 
<p><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span>若用</span><strong>多个梯度的均值</strong>来更新权重的批量梯度下降法可以用相对少的训练次数遍历完整个训练集，其次可以使更新的方向更加贴合整个训练集，避免单个噪音样<span style="color:rgb(51,51,51)">本使网络更新到错误方向</span>。然而也正是因为平均了多个样本的梯度<span style="color:rgb(51,51,51)">，许多样本对神经网络的贡献就被其他样本平均掉了，相当于在每个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">中，训练集的样本数被缩小了。</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">中每个样本的差异性越大，这种弊端就越严重。一般的解决方法就是在每次训练完一个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">后，将训练集中样本的顺序打乱再训练另一个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">，不断反复。这样重新组成的</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">中的样本梯度的平均值就会与上一个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">的不同。而这显然增加了训练的时间。同时因为没办法保证每次更新的方向都贴合整个训练集的大方向，只能使用较小的学习速率。这意味着训练过程中，一部分</span><span style="color:rgb(51,51,51)">steps</span><span style="color:rgb(51,51,51)">对网络最终的更新起到了促进，一部分</span><span style="color:rgb(51,51,51)">steps</span><span style="color:rgb(51,51,51)">对网络最终的更新造成了干扰，这样</span><span style="color:rgb(51,51,51)">“</span><span style="color:rgb(51,51,51)">磕磕碰碰</span><span style="color:rgb(51,51,51)">”</span><span style="color:rgb(51,51,51)">无数个</span><span style="color:rgb(51,51,51)">epoch</span><span style="color:rgb(51,51,51)">后才能达到较为满意的结果。</span></p> 
<p><u><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span>为了解决这种</span><span style="color:rgb(51,51,51)">“</span><span style="color:rgb(51,51,51)">不效率</span><span style="color:rgb(51,51,51)">”</span><span style="color:rgb(51,51,51)">的训练，</span><span style="color:rgb(51,51,51)">BN</span><span style="color:rgb(51,51,51)">首先是把所有的</span><span style="color:rgb(51,51,51)">samples</span><span style="color:rgb(51,51,51)">的统计分布标准化，降低了</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">内不同样本的差异性，然后又允许</span><span style="color:rgb(51,51,51)">batch</span><span style="color:rgb(51,51,51)">内的各个</span><span style="color:rgb(51,51,51)">samples</span><span style="color:rgb(51,51,51)">有各自的统计分布</span></u><span style="color:rgb(51,51,51)">。</span></p> 
<p align="left"><span style="color:#333333">1.     </span><strong><span style="color:#333333">为什么神经网络高效</span></strong><span style="color:#333333">：并行的先验知识使得模型可用线性级数量的样本学习指数级数量的变体</span></p> 
<p align="left"><span style="color:#333333">2.     </span><strong><span style="color:#333333">学习的本质是什么</span></strong><span style="color:#333333">：将变体拆分成因素和知识（</span><span style="color:#333333">Disentangle Factors of Variation</span><span style="color:#333333">）</span></p> 
<p align="left"><span style="color:#333333">i.           </span><strong><span style="color:#333333">为什么深层神经网络比浅层神经网络更高效</span></strong><span style="color:#333333">：迭代组成的先验知识使得样本可用于帮助训练其他共用同样底层结构的样本。</span></p> 
<p align="left"><span style="color:#333333">ii.           </span><strong><span style="color:#333333">神经网络在什么问题上不具备优势</span></strong><span style="color:#333333">：不满足并行与迭代先验的任务</span></p> 
<p align="left"><span style="color:#333333">3.     </span><span style="color:#333333">非迭代：该层状态不是由上层状态构成的任务（如：很深的</span><span style="color:#333333">CNN</span><span style="color:#333333">因为有</span><span style="color:#333333">max pooling</span><span style="color:#333333">，信息会逐渐丢失。而</span><span style="color:#333333">residual network</span><span style="color:#333333">再次使得迭代的先验满足）</span></p> 
<br> 
<h3><span style="font-family:Times New Roman; font-size:18px">CNN: </span></h3> 
<p><span style="font-family:Times New Roman; font-size:18px">1）卷积：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算 ，这也就是多层卷积。</span></p> 
<span style="font-family:Times New Roman; font-size:18px">2）池化：降维的方法，按照卷积计算得出的特征向量维度大的惊人，不但会带来非常大的计算量，而且容易出现过拟合，解决过拟合的办法就是让模型尽量“泛化”，也就是再“模糊”一点，那么一种方法就是把图像中局部区域的特征做一个平滑压缩处理，这源于局部图像一些特征的相似性(即局部相关性原理)。<br> 3) 全连接：softmax分类<br> 训练过程：<br> 卷积核中的因子(×1或×0)其实就是需要学习的参数，也就是卷积核矩阵元素的值就是参数值。一个特征如果有9个值，1000个特征就有900个值，再加上多个层，需要学习的参数还是比较多的。<br> </span> 
<h4><span style="font-family:Times New Roman; font-size:18px">CNN的三个优点：</span></h4> 
<p><span style="font-family:Times New Roman; font-size:18px">sparse interaction(稀疏的交互)，parameter sharing(参数共享)，equivalent respresentation(等价表示)。适合于自动问答系统中的答案选择模型的训练。</span></p> 
<p><span style="font-family:Times New Roman; font-size:18px"></span></p> 
<h4><strong><span style="color:rgb(51,51,51)">CNN</span><span style="color:rgb(51,51,51)">与</span><span style="color:rgb(51,51,51)">DNN</span><span style="color:rgb(51,51,51)">的区别</span></strong><span style="color:rgb(51,51,51)">：</span></h4> 
<p><span style="color:rgb(51,51,51)"><span style="white-space:pre"></span><span style="font-size:18px">DNN</span></span><span style="font-size:18px"><span style="color:rgb(51,51,51)">的输入是向量形式，并未考虑到平面的结构信息，在图像和</span><span style="color:rgb(51,51,51)">NLP</span><span style="color:rgb(51,51,51)">领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），</span><span style="color:rgb(51,51,51)">CNN</span><span style="color:rgb(51,51,51)">的输入可以是</span><span style="color:rgb(51,51,51)">tensor</span><span style="color:rgb(51,51,51)">，例如二维矩阵，通过</span><span style="color:rgb(51,51,51)">filter</span><span style="color:rgb(51,51,51)">获得局部特征，较好的保留了平面结构信息。</span></span></p> 
<p><span style="font-size:18px">filter尺寸计算<strong><span style="color:rgb(51,51,51)">：</span></strong><span style="color:rgb(51,51,51)">Feature Map</span>的尺寸等于(input_size + 2 * padding_size − filter_size)/stride+1</span></p> 
<br> 
<h3><span style="font-family:Times New Roman; font-size:18px">RNN:</span></h3> 
<span style="font-family:Times New Roman; font-size:18px">1.      为什么具有记忆功能？<br> 这个是在RNN就解决的问题，就是因为有递归效应，上一时刻隐层的状态参与到了这个时刻的计算过程中，直白一点呢的表述也就是选择和决策参考了上一次的状态。<br> 2.      为什么LSTM记的时间长？<br> 因为特意设计的结构中具有CEC的特点，误差向上一个状态传递时几乎没有衰减，所以权值调整的时候，对于很长时间之前的状态带来的影响和结尾状态带来的影响可以同时发挥作用，最后训练出来的模型就具有较长时间范围内的记忆功能。<br> 误差回传的主力还是通过了Memory Cell而保持了下来。所以我们现在用的LSTM模型，依然有比较好的效果。<br> 最后整个梳理一下误差回传的过程，误差通过输出层，分类器，隐层等进入某个时刻的Block之后，先将误差传递给了Output Gate和Memory Cell两个地方。<br> 到达输出门的误差，用来更新了输出门的参数w，到达Memory Cell之后，误差经过两个路径，<br> 1是通过这个cell向前一个时刻传递或者更前的时刻传递，<br> 2是用来传递到input gate和block的输入，用来更新了相应的权值（注意！不会经过这里向前一个时刻传递误差）。<br> 最关键的问题就是，这个回传的算法，只通过中间的Memory Cell向更前的时刻传递误差。<br> <img src="https://images2.imgbox.com/e8/19/ixR7D1Ki_o.png" alt=""><br> <img src="https://images2.imgbox.com/ce/5a/qazMGtm9_o.png" alt=""><br> </span> 
<p><span style="font-family:Times New Roman; font-size:18px">在RNN中U、V、W的参数都是共享的，也就是只需要关注每一步都在做相同的事情，只是输入不同，这样来降低参数个数和计算量。</span></p> 
<p><span style="font-family:Times New Roman; font-size:18px"><img src="https://images2.imgbox.com/53/25/FyksjTJ0_o.png" alt=""><br> </span></p> 
<p><span style="font-family:Times New Roman; font-size:18px"></span></p> 
<h4><span style="font-size:18px"><strong>RNN特点</strong>：</span></h4> 
<p align="left"><span style="font-size:24px"><u><span style="color:rgb(51,51,51)">时序长短可变</span></u><span style="color:rgb(51,51,51)">（只要知道上一时刻的隐藏状态</span>ht−1ht−1<span style="color:rgb(51,51,51)">与当前时刻的输入</span>xtxt<span style="color:rgb(51,51,51)">，就可以计算当前时刻的隐藏状态</span>htht<span style="color:rgb(51,51,51)">。并且由于计算所用到的</span>WxhWxh<span style="color:rgb(51,51,51)">与</span>WhhWhh<span style="color:rgb(51,51,51)">在任意时刻都是共享的。递归网络可以处理任意长度的时间序列）<u>顾及时间依赖，未来信息依赖</u>（双向递归）</span></span></p> 
<p align="left"><span style="font-size:24px"><span style="color:rgb(51,51,51)">RNN主要包括LSTM,GRU</span></span></p> 
<p align="left" style="background:rgb(249,249,245)"><span style="font-size:18px"><strong><span style="color:red; background:white">GRU</span><span style="color:red; background:white">对</span><span style="color:red">LSTM</span><span style="color:red">做了两个大改动</span></strong><span style="color:#2C3E50">：</span></span></p> 
<p align="left" style="background:rgb(249,249,245)"><span style="font-size:18px"><span style="color:#2C3E50">1.     </span><span style="color:#2C3E50">将输入门、遗忘门、输出门变为两个门：更新门（</span><span style="color:#2C3E50">Update Gate</span><span style="color:#2C3E50">）和重置门（</span><span style="color:#2C3E50">Reset Gate</span><span style="color:#2C3E50">）。</span></span></p> 
<p align="left" style="background:rgb(249,249,245)"><span style="font-size:18px"><span style="color:#2C3E50">2.     </span><span style="color:#2C3E50">将单元状态与输出合并为一个状态：。</span></span></p> 
<p align="left"><span style="font-size:18px"><span style="color:rgb(51,51,51)">GRU</span><span style="color:rgb(51,51,51)">只用了两个</span><span style="color:rgb(51,51,51)">gates</span><span style="color:rgb(51,51,51)">，将</span><span style="color:rgb(51,51,51)">LSTM</span><span style="color:rgb(51,51,51)">中的输入门和遗忘门合并成了更新门。并且并不把线性自更新建立在额外的</span><span style="color:rgb(51,51,51)">memory cell</span><span style="color:rgb(51,51,51)">上，而是直接线性累积建立在隐藏状态上，并靠</span><span style="color:rgb(51,51,51)">gates</span><span style="color:rgb(51,51,51)">来调控。</span></span></p> 
<br>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/80ceb8fdc1a7bef7f5860b1aa779d55c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">文本挖掘：手把手教你分析携程网评论数据</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/936634152c7b140b482be9d052482503/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">NOIP 2013 普及组 小朋友的数字</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>