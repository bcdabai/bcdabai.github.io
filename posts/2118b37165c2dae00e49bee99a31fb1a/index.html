<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于K8S的hadoop集群初探 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于K8S的hadoop集群初探" />
<meta property="og:description" content="个人博客服务器没在续，在这里开始第一遍 1、hadoop集群1.1、基于yarn的hdfs安装1.2、使用hive 2、spark集群2.1 基于yarn的spark集群2.2 yarn资源调度流程 总结： 1、hadoop集群 1.1、基于yarn的hdfs安装 前言：yarn服务需要每个节点之间通过host进行通信，hdfs的name node也需要知道data node的host
1、基于docker-compose启动可以使用link来获取其他容器host
2、基于k8s的搭建，难点则在host的同步
本次搭建基于四台机器的集群测试搭建，基于yarn的hdfs部署，由于本次记录是为了k8s集群搭建，主要记录搭建步骤，文件的详细配置、其他安装方式如local、standalone可以参考其他文章，环境如下：
master：192.168.81.232
slave1： 192.168.81.231
slave2： 192.168.81.227
slave3： 192.168.81.228
masterslave1slave2slave3hdfsname node, data nodedata nodedata node，secondarynamenodedata nodeyarnnode managerresource manager,node managernode managernode managerhivehivesparkspark filespark filespark filespark file 1、hadoop文件配置（括号中配置主要是地址配置）：
每台机器部署jdk安装（1.8版本）core-site.xml文件配置（存储目录、name node地址）hdfs-site.xml文件配置（副本数量、secondarynamenode地址）yarn-env.sh文件配置jdk路径yarn-site.xml文件配置resourcemanager地址mapred-env.sh配置jdk路径mapred-site.xml配置mr运行在yarn 上 2、配置好的hadoop文件夹分发到4台机器上
3、在namenode上执行hdfs namenode -format
4、启动集群（没有用yarn的启动文件，所以不需要设置免密登陆，为了用k8s的启动命令）:
(nohup 命令 &amp;) ：以子进程后台启动
# master: (nohup hdfs namenode &amp;) (nohup hdfs datanode -regular &amp;) (nohup yarn nodemanager -regular &amp;) # slave1: (nohup hdfs datanode -regular &amp;) (nohup yarn nodemanager &amp;) (nohup yarn resourcemanager &amp;) # slave2: (nohup hdfs datanode -regular &amp;) (nohup yarn nodemanager &amp;) (nohup hdfs secondarynamenode &amp;) # slave3: (nohup hdfs datanode -regular &amp;) (nohup yarn nodemanager &amp;) 至此hdfs集群就已经安装成功了，如需启动name node的HA，可以启动zookeeper的k8s集群，并在hadoop的：hdfs-site." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2118b37165c2dae00e49bee99a31fb1a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-08T23:23:54+08:00" />
<meta property="article:modified_time" content="2020-12-08T23:23:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于K8S的hadoop集群初探</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>个人博客服务器没在续，在这里开始第一遍</h4> 
 <ul><li><a href="#1hadoop_2" rel="nofollow">1、hadoop集群</a></li><li><ul><li><a href="#11yarnhdfs_3" rel="nofollow">1.1、基于yarn的hdfs安装</a></li><li><a href="#12hive_50" rel="nofollow">1.2、使用hive</a></li></ul> 
  </li><li><a href="#2spark_71" rel="nofollow">2、spark集群</a></li><li><ul><li><a href="#21_yarnspark_72" rel="nofollow">2.1 基于yarn的spark集群</a></li><li><a href="#22_yarn_93" rel="nofollow">2.2 yarn资源调度流程</a></li></ul> 
  </li><li><a href="#_102" rel="nofollow">总结：</a></li></ul> 
</div> 
<p></p> 
<h2><a id="1hadoop_2"></a>1、hadoop集群</h2> 
<h3><a id="11yarnhdfs_3"></a>1.1、基于yarn的hdfs安装</h3> 
<p>前言：yarn服务需要每个节点之间通过host进行通信，hdfs的name node也需要知道data node的host</p> 
<blockquote> 
 <p>1、基于docker-compose启动可以使用link来获取其他容器host<br> 2、基于k8s的搭建，难点则在host的同步</p> 
</blockquote> 
<p>本次搭建基于四台机器的集群测试搭建，基于yarn的hdfs部署，由于本次记录是为了k8s集群搭建，主要记录搭建步骤，文件的详细配置、其他安装方式如local、standalone可以参考其他文章，环境如下：<br> master：192.168.81.232<br> slave1： 192.168.81.231<br> slave2： 192.168.81.227<br> slave3： 192.168.81.228</p> 
<table><thead><tr><th align="center"></th><th align="center">master</th><th align="center">slave1</th><th align="center">slave2</th><th align="center">slave3</th></tr></thead><tbody><tr><td align="center">hdfs</td><td align="center">name node, data node</td><td align="center">data node</td><td align="center">data node，secondarynamenode</td><td align="center">data node</td></tr><tr><td align="center">yarn</td><td align="center">node manager</td><td align="center">resource manager,node manager</td><td align="center">node manager</td><td align="center">node manager</td></tr><tr><td align="center">hive</td><td align="center">hive</td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">spark</td><td align="center">spark file</td><td align="center">spark file</td><td align="center">spark file</td><td align="center">spark file</td></tr></tbody></table> 
<p>1、hadoop文件配置（括号中配置主要是地址配置）：</p> 
<ul><li>每台机器部署jdk安装（1.8版本）</li><li>core-site.xml文件配置（存储目录、name node地址）</li><li>hdfs-site.xml文件配置（副本数量、secondarynamenode地址）</li><li>yarn-env.sh文件配置jdk路径</li><li>yarn-site.xml文件配置resourcemanager地址</li><li>mapred-env.sh配置jdk路径</li><li>mapred-site.xml配置mr运行在yarn 上</li></ul> 
<p>2、配置好的hadoop文件夹分发到4台机器上<br> 3、在namenode上执行hdfs namenode -format<br> 4、启动集群（没有用yarn的启动文件，所以不需要设置免密登陆，为了用k8s的启动命令）:<br> (nohup 命令 &amp;) ：以子进程后台启动</p> 
<pre><code class="prism language-powershell"><span class="token comment"># master:</span>
<span class="token punctuation">(</span>nohup hdfs namenode &amp;<span class="token punctuation">)</span>  
<span class="token punctuation">(</span>nohup hdfs datanode <span class="token operator">-</span>regular &amp;<span class="token punctuation">)</span> 
<span class="token punctuation">(</span>nohup yarn nodemanager <span class="token operator">-</span>regular &amp;<span class="token punctuation">)</span>
<span class="token comment"># slave1: </span>
<span class="token punctuation">(</span>nohup hdfs datanode <span class="token operator">-</span>regular &amp;<span class="token punctuation">)</span> 
<span class="token punctuation">(</span>nohup yarn nodemanager &amp;<span class="token punctuation">)</span>
<span class="token punctuation">(</span>nohup yarn resourcemanager &amp;<span class="token punctuation">)</span> 
<span class="token comment"># slave2: </span>
<span class="token punctuation">(</span>nohup hdfs datanode <span class="token operator">-</span>regular &amp;<span class="token punctuation">)</span> 
<span class="token punctuation">(</span>nohup yarn nodemanager &amp;<span class="token punctuation">)</span>
<span class="token punctuation">(</span>nohup hdfs secondarynamenode &amp;<span class="token punctuation">)</span> 
<span class="token comment"># slave3: </span>
<span class="token punctuation">(</span>nohup hdfs datanode <span class="token operator">-</span>regular &amp;<span class="token punctuation">)</span> 
<span class="token punctuation">(</span>nohup yarn nodemanager &amp;<span class="token punctuation">)</span> 
</code></pre> 
<p>至此hdfs集群就已经安装成功了，如需启动name node的HA，可以启动zookeeper的k8s集群，并在hadoop的：hdfs-site.xml配置ha的name node地址及数据一致性的方式等，core-site.xml配置zookeeper的地址</p> 
<h3><a id="12hive_50"></a>1.2、使用hive</h3> 
<p>hive只需要启动一台机器，并且并不需要一定在刚刚的集群上安装，只需要机器上有刚刚配置好的hadoop文件夹就行<br> 1、hive文件配置</p> 
<ul><li>hive-env.sh配置hadoop的目录地址，jdk地址</li><li>hive-site.xml配置存储目录、hdfs地址、mysql地址、metastore、hiveserver2配置</li><li>lib目录上传mysql的jdbc连接包</li></ul> 
<pre><code class="prism language-powershell"><span class="token comment"># hive</span>
<span class="token comment"># 第一次启动进行初始化</span>
<span class="token punctuation">(</span>nohup schematool <span class="token operator">-</span>initSchema <span class="token operator">-</span>dbType mysql &amp;<span class="token punctuation">)</span>

<span class="token punctuation">(</span>nohup hive <span class="token operator">--</span>service metastore &amp;<span class="token punctuation">)</span>  

hive <span class="token operator">--</span>service hiveserver2
<span class="token comment"># debug信息测试用</span>
<span class="token comment">#hive --service hiveserver2 -hiveconf hive.root.logger=debug,console</span>
</code></pre> 
<p>2、启动服务</p> 
<h2><a id="2spark_71"></a>2、spark集群</h2> 
<h3><a id="21_yarnspark_72"></a>2.1 基于yarn的spark集群</h3> 
<p>spark基于yarn的集群部署，只需要在yarn集群每台机器上配置spark的文件（运行节点），spark在任意一台机器上都可以spark submit到yarn上（不在yarn的集群上submit也可以，只要下面对应配置目录做好）<br> 1、安装jdk<br> 2、安装scala<br> 3、文件配置</p> 
<ul><li>spark-env.sh配置jdk、scala、yarn(hadoop)目录、spark运行参数、spark配置目录</li><li>基于yarn的spark，不需要配置slaves文件（没有主从之分，stanalone需要配置）</li></ul> 
<p>4、任务运行</p> 
<pre><code class="prism language-powershell"><span class="token comment"># 会根据配置的yarn目录，寻找到yarn 的rm节点提交，rm分配置资源运行</span>
spark<span class="token operator">-</span>submit <span class="token operator">--</span>master yarn <span class="token operator">--</span><span class="token keyword">class</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>examples<span class="token punctuation">.</span>SparkPi <span class="token variable">$SPARK_HOME</span><span class="token operator">/</span>examples<span class="token operator">/</span>jars<span class="token operator">/</span>spark<span class="token operator">-</span>examples<span class="token operator">*</span><span class="token punctuation">.</span>jar

<span class="token comment"># mster woker方式提交，指定master节点（）stannalone模式</span>
spark<span class="token operator">-</span>submit <span class="token operator">--</span>master spark:<span class="token operator">/</span><span class="token operator">/</span>192<span class="token punctuation">.</span>168<span class="token punctuation">.</span>80<span class="token punctuation">.</span>235:7077 <span class="token operator">--</span><span class="token keyword">class</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>examples<span class="token punctuation">.</span>SparkPi <span class="token variable">$SPARK_HOME</span><span class="token operator">/</span>examples<span class="token operator">/</span>jars<span class="token operator">/</span>spark<span class="token operator">-</span>examples<span class="token operator">*</span><span class="token punctuation">.</span>jar

<span class="token comment">#Local 调试</span>
spark<span class="token operator">-</span>submit <span class="token operator">--</span>master local <span class="token operator">--</span><span class="token keyword">class</span> demo <span class="token operator">/</span>Users<span class="token operator">/</span>gumkk<span class="token operator">/</span>IdeaProjects<span class="token operator">/</span>myFirstProject<span class="token operator">/</span>out<span class="token operator">/</span>artifacts<span class="token operator">/</span>myFirstProject_jar<span class="token operator">/</span>myFirstProject<span class="token punctuation">.</span>jar
</code></pre> 
<h3><a id="22_yarn_93"></a>2.2 yarn资源调度流程</h3> 
<p>这边主要是自己的一个记录，Yarn主要由四个重要角色组成：</p> 
<ol><li> <p>ResourceManager：资源管理器</p> </li><li> <p>NodeManager：节点管理器</p> </li><li> <p>ApplicaitonMaster：用户提交的每个program都会对应一个ApplicationMaster，ApplicationMaster向ResourceManager申请资源，请求NodeManager启动task</p> </li><li> <p>Container：容器是资源调度的单位，Application Master会给task分配Container，分配流程为Resource Manager -&gt;Application Master -&gt; task</p> </li></ol> 
<h2><a id="_102"></a>总结：</h2> 
<p>基于k8s的hadoop集群部署，难点在于各个组件的master和node通过hosts进行通信，阅读其他技术文章，有如下两种思路：1、固定每个pod的ip和hosts，然后通过公司的dns做一个解析，这样只要在配置文件里配置好各组件的地址、hosts的文件，启动方式和宿主机一样；2、通过zookeeper或etcd的一些服务发现组件，在各服务节点启动时进行注册、并更新本地hosts文件。</p> 
<p>后面会进行具体实施的结果对比，不过CDH其实挺香，考虑基于CDH的容器化部署。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ff81078ae4560df21a7bc4f476a1e630/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">springboot 获取application参数_Spring Boot 配置加载顺序详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7ba831e844634556219bdaf905072c4d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">佳能6780清零软件_佳能G2810 3800系列出现5B00如何解决</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>