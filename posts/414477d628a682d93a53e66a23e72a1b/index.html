<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>爬虫---scrapy爬虫框架（详细&#43;实战） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="爬虫---scrapy爬虫框架（详细&#43;实战）" />
<meta property="og:description" content="​
活动地址：CSDN21天学习挑战赛
爬虫---scrapy爬虫框架 爬虫---scrapy爬虫框架一、简介1、基本功能2、架构3、scrapy项目的结构 二、scrapy环境搭建三、如何开始1、新建项目 ：新建一个新的爬虫项目2、明确目标 （items.py）：明确你想要抓取的目标3、制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页4、存储内容 （pipelines.py）：设计管道存储爬取内容5、运行爬虫 四、项目实战 爬虫—scrapy爬虫框架 一、简介 1、基本功能 Scrapy是一个适用爬取网站数据、提取结构性数据的应用程序框架，它可以应用在广泛领域：Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。
2、架构 Scrapy Engine(引擎)：负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。Scheduler(调度器)：它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理。Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)。Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。Downloader Middlewares（下载中间件）：一个可以自定义扩展下载功能的组件。Spider Middlewares（Spider中间件）：一个可以自定扩展和操作引擎和Spider中间通信的功能组件。
3、scrapy项目的结构 项目名字 项目的名字 spiders文件夹（存储的是爬虫文件） init 自定义的爬虫文件 核心功能文件 init items 定义数据结构的地方 爬虫的数据都包含哪些 middleware 中间件 代理 pipelines 管道 用来处理下载的数据 settings 配置文件 robots协议 ua定义等 二、scrapy环境搭建 三、如何开始 1、新建项目 ：新建一个新的爬虫项目 打开cmd，输入scrapy startproject 项目的名字 (默认是在C:\Users\...这个目录下，你可以自行切换到对应的 文件下） 注意：项目的名字不允许使用数字开头 也不能包含中文 2、明确目标 （items.py）：明确你想要抓取的目标 选择你需要爬取的内容，例如作者名字、小说名、封面图片等 在items.py文件中定义 import scrapy class AdicrawlerItem(scrapy.Item): author = scrapy.Field() theme = scrapy." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/414477d628a682d93a53e66a23e72a1b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-01T16:50:24+08:00" />
<meta property="article:modified_time" content="2022-08-01T16:50:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">爬虫---scrapy爬虫框架（详细&#43;实战）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>​</p> 
<blockquote> 
 <p>活动地址：<a href="https://marketing.csdn.net/p/bdabfb52c5d56532133df2adc1a728fd">CSDN21天学习挑战赛</a></p> 
</blockquote> 
<p></p> 
<div class="toc"> 
 <h4>爬虫---scrapy爬虫框架</h4> 
 <ul><li><a href="#scrapy_4" rel="nofollow">爬虫---scrapy爬虫框架</a></li><li><ul><li><a href="#_5" rel="nofollow">一、简介</a></li><li><ul><li><a href="#1_6" rel="nofollow">1、基本功能</a></li><li><a href="#2_9" rel="nofollow">2、架构</a></li><li><a href="#3scrapy_22" rel="nofollow">3、scrapy项目的结构</a></li></ul> 
   </li><li><a href="#scrapy_35" rel="nofollow">二、scrapy环境搭建</a></li><li><a href="#_39" rel="nofollow">三、如何开始</a></li><li><ul><li><a href="#1__40" rel="nofollow">1、新建项目 ：新建一个新的爬虫项目</a></li><li><a href="#2_itemspy_45" rel="nofollow">2、明确目标 （items.py）：明确你想要抓取的目标</a></li><li><a href="#3_spidersxxspiderpy_56" rel="nofollow">3、制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</a></li><li><a href="#4_pipelinespy_124" rel="nofollow">4、存储内容 （pipelines.py）：设计管道存储爬取内容</a></li><li><a href="#5_165" rel="nofollow">5、运行爬虫</a></li></ul> 
   </li><li><a href="#_174" rel="nofollow">四、项目实战</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="scrapy_4"></a>爬虫—scrapy爬虫框架</h2> 
<h3><a id="_5"></a>一、简介</h3> 
<h4><a id="1_6"></a>1、基本功能</h4> 
<p><em><strong>Scrapy</strong></em>是一个适用爬取网站数据、提取结构性数据的应用程序框架，它可以应用在广泛领域：Scrapy 常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。</p> 
<h4><a id="2_9"></a>2、架构</h4> 
<ul><li><em><strong>Scrapy Engine(引擎)</strong></em>：负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。</li><li><em><strong>Scheduler(调度器)</strong></em>：它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。</li><li><em><strong>Downloader（下载器）</strong></em>：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理。</li><li><em><strong>Spider（爬虫）</strong></em>：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)。</li><li><em><strong>Item Pipeline(管道)</strong></em>：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。</li><li><em><strong>Downloader Middlewares（下载中间件）</strong></em>：一个可以自定义扩展下载功能的组件。</li><li><em><strong>Spider Middlewares（Spider中间件）</strong></em>：一个可以自定扩展和操作引擎和Spider中间通信的功能组件。<br> <img src="https://images2.imgbox.com/9a/3d/KXQ7D4ct_o.png" alt="scrapy8"></li></ul> 
<p><img src="https://images2.imgbox.com/14/3a/Btj8hjMs_o.png" alt="scrapy7"></p> 
<h4><a id="3scrapy_22"></a>3、scrapy项目的结构</h4> 
<pre><code>项目名字
    项目的名字
        spiders文件夹（存储的是爬虫文件）
            init
            自定义的爬虫文件        核心功能文件
        init
        items           定义数据结构的地方 爬虫的数据都包含哪些
        middleware      中间件 代理
        pipelines       管道  用来处理下载的数据
        settings        配置文件 robots协议 ua定义等
</code></pre> 
<h3><a id="scrapy_35"></a>二、scrapy环境搭建</h3> 
<p><img src="https://images2.imgbox.com/b6/c4/Fdrrj5qN_o.png" alt="scrapy1"><br> <img src="https://images2.imgbox.com/01/da/Mygem0av_o.png" alt="scrapy2"></p> 
<h3><a id="_39"></a>三、如何开始</h3> 
<h4><a id="1__40"></a>1、新建项目 ：新建一个新的爬虫项目</h4> 
<pre><code> 打开cmd，输入scrapy startproject 项目的名字
 (默认是在C:\Users\...这个目录下，你可以自行切换到对应的 文件下）
 注意：项目的名字不允许使用数字开头 也不能包含中文
</code></pre> 
<h4><a id="2_itemspy_45"></a>2、明确目标 （items.py）：明确你想要抓取的目标</h4> 
<pre><code>选择你需要爬取的内容，例如作者名字、小说名、封面图片等
在items.py文件中定义
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">class</span> <span class="token class-name">AdicrawlerItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    author <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    theme <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 以上定义了两个变量 分别是作者名、主题。</span>
</code></pre> 
<h4><a id="3_spidersxxspiderpy_56"></a>3、制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页</h4> 
<p><em><strong>创建爬虫文件</strong></em></p> 
<pre><code>	要在spiders文件在去创建爬虫文件
	     cd 项目的名字\项目的名字\spiders
	     eg : cd scrapy_baidu\scrapy_baidu\spiders

    创建爬虫文件
     	scrapy genspider 爬虫文件的名字 要爬的网页
     	eg : scrapy genspider baidu www.baidu.com
      	一般情况下不需要添加http协议
      	因为start_urls的值是根据allowed_domains修改的
</code></pre> 
<p><em><strong>爬虫文件的解释：</strong></em></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">class</span> <span class="token class-name">BaiduSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 爬虫的名字 一般运行爬虫的时候 使用的值</span>
    name <span class="token operator">=</span> <span class="token string">'baidu'</span>
    <span class="token comment"># 允许访问的域名</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'www.baidu.com'</span><span class="token punctuation">]</span>
    <span class="token comment"># 起始的url地址  指的是第一次要访问的域名</span>
    <span class="token comment"># start_urls   是在allowed_domains的前面添加一个http：//</span>
    <span class="token comment">#              是在allowed_domains的后面添加一个/</span>
    <span class="token comment"># 如果以html结尾 就不用加/ 否则网站进不去  报错</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://www.baidu.com/'</span><span class="token punctuation">]</span>

    <span class="token comment"># 是执行了start_urls之后  执行的方法</span>
    <span class="token comment"># 方法中的response  就是返回的那个对象</span>
    <span class="token comment"># 相当于 response = urllib.request.urlopen()</span>
    <span class="token comment">#       response = requests.get()</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>
</code></pre> 
<p><em><strong>response的属性和方法</strong></em></p> 
<ul><li>response.text <br> 获取的是响应的字符串</li><li>response.body <br> 获取的是二进制数据</li><li>response.xpath<br> 可以直接使用xpath方法来解析response中的内容</li><li>response.extract()<br> 提取seletor对象的data属性值</li><li>response.extract_first()<br> 提取的seletor列表的第一个数据</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> AdiCrawler<span class="token punctuation">.</span>items <span class="token keyword">import</span> AdicrawlerItem


<span class="token keyword">class</span> <span class="token class-name">ThousandpicSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'thousandpic'</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'www.58pic.com'</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://www.58pic.com/c/'</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        author <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="wrap-list fl"]//span[@class="fl info-h1"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        theme <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//div[@class="wrap-list fl"]//span[@class="usernameColor"]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        item <span class="token operator">=</span> AdicrawlerItem<span class="token punctuation">(</span>author<span class="token operator">=</span>author<span class="token punctuation">,</span>theme<span class="token operator">=</span>theme<span class="token punctuation">)</span>
        <span class="token keyword">yield</span> item
</code></pre> 
<h4><a id="4_pipelinespy_124"></a>4、存储内容 （pipelines.py）：设计管道存储爬取内容</h4> 
<p><em><strong>如果想使用管道的话 那么就必须在settings中开启管道</strong></em></p> 
<pre><code class="prism language-python">ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
	    <span class="token comment"># 管道可以有很多个 那么管道是有优先级 优先级的范围是1到1000 值越小优先级越高</span>
	   <span class="token string">'scrapy_dangdang.pipelines.ScrapyDangdangPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
    	<span class="token comment"># 将在settings.py中这段话取消注释，则打开了通道。</span>
</code></pre> 
<p><em><strong>然后去pippelines.py中设计管道：</strong></em></p> 
<p><strong>方法一：</strong></p> 
<pre><code>class ScrapyDangdangPipeline:
	def process_item(self, item, spider):
        # 以下这种模式不推荐 因为每传递一个对象 那么就打开一次文件对文件的操作过于频繁
        # write方法必须要写一个字符串 而不能是其他的对象
        # w模式 会每一个对象都打开一次文件 覆盖之前的内容
        # 文件存储就不多讲啦
		with open('book.json','a',encoding='utf-8') as fp:
		fp.write(str(item))
		return item
</code></pre> 
<p><strong>方法二：（推荐）</strong></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ScrapyDangdangPipeline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'book.json'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>

    <span class="token comment"># item就是yield后面的对象</span>
    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>   
        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="5_165"></a>5、运行爬虫</h4> 
<blockquote> 
 <p>一般在运行爬虫的时候仍然没有内容查询，则需要考虑将settings.py文件中的ROBOTSTXT_OBEY = True注释掉<br> robots协议 注释之后就不遵守协议了 他是君子协议 一般情况下我们不遵守 # BOTSTXT_OBEY = True</p> 
</blockquote> 
<pre><code>在cmd中输入：scrapy crawl 爬虫的名字
  eg:scrapy crawl baidu
</code></pre> 
<h3><a id="_174"></a>四、项目实战</h3> 
<ul><li>打开cmd，创建项目</li></ul> 
<blockquote> 
 <p>scrapy startproject scrapy_dangdang</p> 
</blockquote> 
<ul><li>创建爬虫文件</li></ul> 
<blockquote> 
 <p>先到spiders文件下 : <br> cd scrapy_dangdang\scrapy_dangdang\spiders<br> 然后创建爬虫文件 :<br> scrapy genspider dang category.dangdang.com</p> 
</blockquote> 
<ul><li> <p>项目目录<br> <img src="https://images2.imgbox.com/66/bd/tE5Q3PkG_o.png" alt="scrapy3"></p> </li><li> <p>确定需要下载的数据，去items.py文件中添加。这里我们准备存储图片、名字和价格</p> </li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">ScrapyDangdangItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># define the fields for your item here like:</span>
    <span class="token comment"># name = scrapy.Field()</span>
    <span class="token comment"># 通俗的说就是你要下载的数据都有什么</span>
    <span class="token comment"># 图片</span>
    src <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 名字</span>
    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 价格</span>
    price <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>接下来我们就可以去爬虫文件中去爬取我们需要的内容了（这里是在dang.py文件中）</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy
<span class="token keyword">from</span> scrapy_dangdang<span class="token punctuation">.</span>items <span class="token keyword">import</span> ScrapyDangdangItem


<span class="token keyword">class</span> <span class="token class-name">DangSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 爬虫的名字 一般运行爬虫的时候 使用的值</span>
    name <span class="token operator">=</span> <span class="token string">'dang'</span>

    <span class="token comment"># 允许访问的域名</span>
    <span class="token comment"># 如果是多页下载的话 那么必须要调整的是allowed_domains的范围 一般情况下只写域名</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'category.dangdang.com'</span><span class="token punctuation">]</span>

    <span class="token comment"># 起始的url地址  指的是第一次要访问的域名</span>
    <span class="token comment"># start_urls   是在allowed_domains的前面添加一个http：//</span>
    <span class="token comment">#              是在allowed_domains的后面添加一个/</span>
    <span class="token comment"># 如果以html结尾 就不用加/</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://category.dangdang.com/cp01.01.02.00.00.00.html'</span><span class="token punctuation">]</span>
    base_url <span class="token operator">=</span> <span class="token string">'http://category.dangdang.com/pg'</span>
    page <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token comment"># 是执行了start_urls之后  执行的方法</span>
    <span class="token comment"># 方法中的response  就是返回的那个对象</span>
    <span class="token comment"># 相当于 response = urllib.request.urlopen()</span>
    <span class="token comment">#       response = requests.get()</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># pipelines     下载数据</span>
        <span class="token comment"># items         定义数据结构的</span>
        <span class="token comment"># src = //ul[@id="component_59"]/li//img/@src</span>
        <span class="token comment"># alt = //ul[@id="component_59"]/li//img/@alt</span>
        <span class="token comment"># price = //ul[@id="component_59"]/li//p[@class="price"]/span[1]/text()</span>
        <span class="token comment"># 所有的seletor的对象 都可以再次调用xpath方法</span>
        li_list <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//ul[@id="component_59"]/li'</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> li <span class="token keyword">in</span> li_list<span class="token punctuation">:</span>
        <span class="token comment">#  第一张图片和其他的图片的标签是属性是不一样的</span>
        <span class="token comment">#  第一张图片src是可以使用的 其他图片的地址data-original</span>
            src <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//img/@data-original'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> src<span class="token punctuation">:</span>
               src <span class="token operator">=</span> src
            <span class="token keyword">else</span><span class="token punctuation">:</span>
               src <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//img/@src'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>

            name <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//img/@alt'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            price <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'.//p[@class="price"]/span[1]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>

            book <span class="token operator">=</span> ScrapyDangdangItem<span class="token punctuation">(</span>src<span class="token operator">=</span>src<span class="token punctuation">,</span>name<span class="token operator">=</span>name<span class="token punctuation">,</span>price<span class="token operator">=</span>price<span class="token punctuation">)</span>
            <span class="token comment"># 获取一个book就交给pipelines</span>
            <span class="token keyword">yield</span> book

        <span class="token comment"># 每一页爬取的业务逻辑都是一样的</span>
        <span class="token comment"># 所以我们只需要将执行的那个页的请求再次调用parse方法就可以了</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>page <span class="token operator">&lt;</span> <span class="token number">100</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>page <span class="token operator">=</span> self<span class="token punctuation">.</span>page <span class="token operator">+</span> <span class="token number">1</span>
            url <span class="token operator">=</span> self<span class="token punctuation">.</span>base_url <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>page<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'-cp01.01.02.00.00.00.html'</span>
            <span class="token comment"># 怎么去调用parse方法</span>
            <span class="token comment"># scrapy.Request就是scrpay的get方法</span>
            <span class="token comment"># url就是请求地址</span>
            <span class="token comment"># callback是你要执行的那个函数 注意不需要加圆括号</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>
</code></pre> 
<ul><li>通过解析拿到数据之后，我们就可以去通道中添加保存的方法了（pippelines.py）</li><li>首先我们要去settings.py在打开通道和添加通道，完成之后进行下一步</li></ul> 
<pre><code class="prism language-python">ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment"># 管道可以有很多个 那么管道是有优先级 优先级的范围是1到1000 值越小优先级越高</span>
   <span class="token string">'scrapy_dangdang.pipelines.ScrapyDangdangPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
   <span class="token string">'scrapy_dangdang.pipelines.DangDangDownloadPiepline'</span><span class="token punctuation">:</span> <span class="token number">301</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre> 
<ul><li>通道打开后，在pippelines.py完成下列操作</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token comment"># 如果想使用管道的话 那么就必须在settings中开启管道</span>
<span class="token keyword">class</span> <span class="token class-name">ScrapyDangdangPipeline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'book.json'</span><span class="token punctuation">,</span><span class="token string">'w'</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>

    <span class="token comment"># item就是yield后面的book对象</span>
    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 一下这种模式不推荐 因为每传递一个对象 那么就打开一次文件对文件的操作过于频繁</span>
        <span class="token comment"># # write方法必须要写一个字符串 而不能是其他的对象</span>
        <span class="token comment"># # w模式 会每一个对象都打开一次文件 覆盖之前的内容</span>
        <span class="token comment"># with open('book.json','a',encoding='utf-8') as fp:</span>
        <span class="token comment">#     fp.write(str(item))</span>

        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 多条管道开启</span>
    <span class="token comment"># 定义管道类</span>
    <span class="token comment"># 在settings中开启管道</span>
    <span class="token comment"># 'scrapy_dangdang.pipelines.DangDangDownloadPiepline': 301,</span>
<span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request


<span class="token keyword">class</span> <span class="token class-name">DangDangDownloadPiepline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>item<span class="token punctuation">,</span>spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        url <span class="token operator">=</span> <span class="token string">'http:'</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'src'</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token string">'./books/'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span><span class="token string">'./books/'</span><span class="token punctuation">)</span>
        filename <span class="token operator">=</span> <span class="token string">'./books/'</span> <span class="token operator">+</span> item<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'name'</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.jpg'</span>
        urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span>filename<span class="token operator">=</span>filename<span class="token punctuation">)</span>
        <span class="token keyword">return</span> item
</code></pre> 
<ul><li>最后在cmd中输入：scrapy crawl dang</li><li>完成之后就开始下载了，全部完成之后你就会看到多了book.json文件和books文件夹在自己的项目中。里面有数据，则表示项目成功了。</li></ul> 
<p>​</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/bcc38368671abeca56f2fab069a0d641/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Vue 项目 js 文件中使用 vue-i18n</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/40be4c550b7b8a8f26c741223cde90f3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">MyBatis Plus处理Mysql JSON Data Type类型字段</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>