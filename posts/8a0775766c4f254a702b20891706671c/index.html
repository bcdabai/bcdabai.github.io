<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Epoch、Batch、Batch size - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Epoch、Batch、Batch size" />
<meta property="og:description" content=" 知识来源知乎专栏作者风度、
作者眼睛里面进砖头了https://www.zhihu.com/question/456600260
epoch：
一个完整的数据集通过了神经网络一次并返回一次，称为一个epoch。所有的训练样本在神经网络中都进行了一次正向传播和反向传播，就是一个epoch将所有的训练集训练一次的过程。
Batch：
当一个epoch的样本数量太庞大，需要把它分成多个小块，也就是分成多个batch来进行训练。batch(批/一批样本），将整个训练样本分成若干个batch。
batch_size:
每批样本的大小
iteration:
一次迭代，训练一个Batch就是一次iteration
神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。使用有限的数据集，使用一个迭代过程即梯度下降优化学习过程。随着epoch数量增加，神经网络中的权重的更新次数也在增加，但是具体多少个epoch case by case
Batch_size:它的大小影响模型的优化程度和速度，直接影响到GPU内存的使用情况，GPU内存不大，数值最好设置小一点。
Batch_size增大，梯度变得准确，当非常准确的时候，在增加batch_size也没用了。
batch_size增大要到达相同的准确度，必须增大epoch：
batch size的大小影响的是训练过程中的完成每个epoch所需的时间和每次迭代(iteration)之间梯度的平滑程度。其实看完batch_size的解释，基本就应该知道epoch在发挥个什么作用了。说简单点，epoch影响的就是迭代计算的次数。
在不限内存的情况，应当用大点的batch_size, 进行模型优化，稳且快。在内存有限的情况下，减小batch_size的情况下应该相应减小的是learning_rate, 至于epoch怎么调，你就需要综合看你的loss下降情况与验证集的指标情况了。
若是loss还能降，指标还在升，那说明欠拟合，还没收敛，应该继续train，增大epoch。若是loss还能再降，指标也在降，说明过拟合了，那就得采用提前终止（减少epoch）或采用weight_decay等防过拟合措施。
3.若是设置epoch=16，到第8个epoch，loss也不降了，指标也不动了，说明8个epoch就够了，剩下的白算了。当然以上说的都是预设的一些理想情况，现实中往往没有这么明确，就如第三种情况，它可能只是到了局部最优点，并没有最优，你可能换个大点的batch_size，模型就调了个方向继续下坡，指标又能往上走点。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8a0775766c4f254a702b20891706671c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-24T16:01:31+08:00" />
<meta property="article:modified_time" content="2023-04-24T16:01:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Epoch、Batch、Batch size</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><a href="https://zhuanlan.zhihu.com/p/390341772" rel="nofollow">知识来源知乎专栏作者风度、</a><br> <a href="https://www.zhihu.com/question/456600260" rel="nofollow">作者眼睛里面进砖头了https://www.zhihu.com/question/456600260</a><br> epoch：<br> 一个完整的数据集通过了神经网络一次并返回一次，称为一个epoch。所有的训练样本在神经网络中都进行了一次正向传播和反向传播，就是一个epoch将所有的训练集训练一次的过程。<br> Batch：<br> 当一个epoch的样本数量太庞大，需要把它分成多个小块，也就是分成多个batch来进行训练。batch(批/一批样本），将整个训练样本分成若干个batch。<br> batch_size:<br> 每批样本的大小<br> iteration:<br> 一次迭代，训练一个Batch就是一次iteration</p> 
<p><em><strong>神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。使用有限的数据集，使用一个迭代过程即梯度下降优化学习过程。随着epoch数量增加，神经网络中的权重的更新次数也在增加，但是具体多少个epoch case by case</strong></em><br> Batch_size:它的大小影响模型的优化程度和速度，直接影响到GPU内存的使用情况，GPU内存不大，数值最好设置小一点。<br> Batch_size增大，梯度变得准确，当非常准确的时候，在增加batch_size也没用了。</p> 
<p>batch_size增大要到达相同的准确度，必须增大epoch：<br> batch size的大小影响的是训练过程中的完成每个epoch所需的时间和每次迭代(iteration)之间梯度的平滑程度。其实看完batch_size的解释，基本就应该知道epoch在发挥个什么作用了。说简单点，epoch影响的就是迭代计算的次数。<br> 在不限内存的情况，应当用大点的batch_size, 进行模型优化，稳且快。在内存有限的情况下，减小batch_size的情况下应该相应减小的是learning_rate, 至于epoch怎么调，你就需要综合看你的loss下降情况与验证集的指标情况了。</p> 
<ol><li>若是loss还能降，指标还在升，那说明欠拟合，还没收敛，应该继续train，增大epoch。</li><li>若是loss还能再降，指标也在降，说明过拟合了，那就得采用提前终止（减少epoch）或采用weight_decay等防过拟合措施。<br> 3.若是设置epoch=16，到第8个epoch，loss也不降了，指标也不动了，说明8个epoch就够了，剩下的白算了。当然以上说的都是预设的一些理想情况，现实中往往没有这么明确，就如第三种情况，它可能只是到了局部最优点，并没有最优，你可能换个大点的batch_size，模型就调了个方向继续下坡，指标又能往上走点。</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8229849740415a863fc462ee7959c686/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Assimp的安装编译及使用过程全纪录</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/799c2a65520f9324b4e7c84d13115d59/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">package.json的scripts节点下的脚本可以用npm run执行</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>