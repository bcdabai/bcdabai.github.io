<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>（四）tensorflow2.0 - 实战稀疏自动编码器SAE - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="（四）tensorflow2.0 - 实战稀疏自动编码器SAE" />
<meta property="og:description" content="前文已经介绍完了tensorflow2.0自定义layer、model、loss function，本文将结合前述知识，搭建一个稀疏自动编码器SAE。
（一）tensorflow2.0 - 自定义layer
（二）tensorflow2.0 - 自定义Model
（三）tensorflow2.0 - 自定义loss function（损失函数）
（四）tensorflow2.0 - 实战稀疏自动编码器SAE
先简单介绍稀疏自动编码器SAE，其架构如下图所示（图源网络，侵删），三层结构，输出层应尽量和输入层接近，其重点在于中间的隐藏层，隐藏层将数据进行了重新编码，这样做的目的是获得输入数据更好的数据表示。在普通自动编码器中，往往要求隐藏层元素个数要比输入层元素个数少，但是稀疏自动编码器的要求不同，它可以允许隐藏层元素比输入层元素多，但是要保证稀疏性，即大多数隐藏层节点的输出值为0。
而SAE的损失函数，则为：
第一部分为预测值与真实值的SAE，第二部分为对权重的惩罚项，第三部分ρ和ρ帽分别代表期望的稀疏度和实际的稀疏度。
下面放上代码：
import tensorflow as tf import numpy as np import pandas as pd from tensorflow.keras import * import tensorflow.keras.backend as kb import sys import matplotlib.pyplot as plt # 输入输出为16 × 1的列表 # inputList为输入列表 def SAEFC(inputList): inputList = np.array(inputList) # 输入特征个数 inputFeatureNum = len(inputList[0]) # 隐藏层参数个数：输入特征3倍 hiddenNum = 3 * inputFeatureNum # 稀疏度(密度) density = 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3554fd7e9afcb4e3e75ffdf23e17d829/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-02-03T13:13:55+08:00" />
<meta property="article:modified_time" content="2020-02-03T13:13:55+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">（四）tensorflow2.0 - 实战稀疏自动编码器SAE</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>前文已经介绍完了tensorflow2.0自定义layer、model、loss function，本文将结合前述知识，搭建一个稀疏自动编码器SAE。<br> <a href="https://blog.csdn.net/qq_32623363/article/details/104152718">（一）tensorflow2.0 - 自定义layer</a><br> <a href="https://blog.csdn.net/qq_32623363/article/details/104153148">（二）tensorflow2.0 - 自定义Model</a><br> <a href="https://blog.csdn.net/qq_32623363/article/details/104154418">（三）tensorflow2.0 - 自定义loss function（损失函数）</a><br> <a href="https://blog.csdn.net/qq_32623363/article/details/104154883">（四）tensorflow2.0 - 实战稀疏自动编码器SAE</a></p> 
<hr> 
<p>先简单介绍稀疏自动编码器SAE，其架构如下图所示（图源网络，侵删），三层结构，输出层应尽量和输入层接近，其重点在于中间的隐藏层，隐藏层将数据进行了重新编码，这样做的目的是获得输入数据更好的数据表示。在普通自动编码器中，往往要求隐藏层元素个数要比输入层元素个数少，但是稀疏自动编码器的要求不同，它可以允许隐藏层元素比输入层元素多，但是要保证稀疏性，即大多数隐藏层节点的输出值为0。<br> <img src="https://images2.imgbox.com/8c/f3/3VdTJwgS_o.png" alt="在这里插入图片描述"><br> 而SAE的损失函数，则为：<br> <img src="https://images2.imgbox.com/ed/02/DeoWL2qi_o.png" alt="在这里插入图片描述"><br> 第一部分为预测值与真实值的SAE，第二部分为对权重的惩罚项，第三部分ρ和ρ帽分别代表期望的稀疏度和实际的稀疏度。</p> 
<p>下面放上代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">import</span> tensorflow<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>backend <span class="token keyword">as</span> kb
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt


<span class="token comment"># 输入输出为16 × 1的列表</span>
<span class="token comment"># inputList为输入列表</span>
<span class="token keyword">def</span> <span class="token function">SAEFC</span><span class="token punctuation">(</span>inputList<span class="token punctuation">)</span><span class="token punctuation">:</span>
    inputList <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>inputList<span class="token punctuation">)</span>
    <span class="token comment"># 输入特征个数</span>
    inputFeatureNum <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>inputList<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 隐藏层参数个数：输入特征3倍</span>
    hiddenNum <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> inputFeatureNum
    <span class="token comment"># 稀疏度(密度)</span>
    density <span class="token operator">=</span> <span class="token number">0.1</span>

    lossList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    saeModel <span class="token operator">=</span> SAEModel<span class="token punctuation">(</span>inputList<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hiddenNum<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> saeModel<span class="token punctuation">.</span>network_learn<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span>inputList<span class="token punctuation">)</span><span class="token punctuation">)</span>
        lossList<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>

    <span class="token comment"># 绘制损失值图像</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>lossList<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> lossList<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> saeModel



<span class="token comment"># 自定义隐藏层</span>
<span class="token keyword">class</span> <span class="token class-name">SAELayer</span><span class="token punctuation">(</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SAELayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 该层最后一个节点，其值固定为1,</span>
        <span class="token comment"># 前期可以按照同样的手段让该节点和其他节点一样进行计算，</span>
        <span class="token comment"># 最后在传递给下一层前，将其设置为1即可（即其值固定为1）</span>
        self<span class="token punctuation">.</span>num_outputs <span class="token operator">=</span> num_outputs

    <span class="token keyword">def</span> <span class="token function">build</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>kernel <span class="token operator">=</span> self<span class="token punctuation">.</span>add_variable<span class="token punctuation">(</span><span class="token string">"kernel"</span><span class="token punctuation">,</span>
                                        shape<span class="token operator">=</span><span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">(</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                               self<span class="token punctuation">.</span>num_outputs <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> self<span class="token punctuation">.</span>add_variable<span class="token punctuation">(</span><span class="token string">"bias"</span><span class="token punctuation">,</span>
                                      shape<span class="token operator">=</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_outputs <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>kernel<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token comment"># sigmoid函数</span>
        output <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        bias_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>output<span class="token punctuation">,</span> bias_list<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>result <span class="token operator">=</span> output
        <span class="token keyword">return</span> output


<span class="token comment"># 自定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">SAEModel</span><span class="token punctuation">(</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 可以传入一些超参数，用以动态构建模型</span>
    <span class="token comment"># __init_——()方法在创建模型对象时被调用</span>
    <span class="token comment"># input_shape: 输入层和输出层的节点个数（输入层实际要比这多1，因为有个bias）</span>
    <span class="token comment"># hidden_shape: 隐藏层节点个数，隐藏层节点的最后一个节点值固定为1，也是bias</span>
    <span class="token comment"># 使用方法：直接传入实际的input_shape即可，在call中也直接传入原始Input_tensor即可</span>
    <span class="token comment"># 一切关于数据适配模型的处理都在模型中实现</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_shape<span class="token punctuation">,</span> hidden_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># print("init")</span>
        <span class="token comment"># 隐藏层节点个数默认为输入层的3倍</span>
        <span class="token keyword">if</span> hidden_shape <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            hidden_shape <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> input_shape
        <span class="token comment"># 调用父类__init__()方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SAEModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>train_loss <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>layer_2 <span class="token operator">=</span> SAELayer<span class="token punctuation">(</span>hidden_shape<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer_3 <span class="token operator">=</span> layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>input_shape<span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">,</span> training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 将input_tensor最后加一列1</span>
        bias_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        input_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_tensor<span class="token punctuation">,</span> bias_list<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 输入数据</span>
        <span class="token comment"># x = self.layer_1(input_tensor)</span>
        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_2<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_3<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
    
    <span class="token keyword">def</span> <span class="token function">get_loss</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># print("get_loss")</span>
        bias_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        new_input <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_tensor<span class="token punctuation">,</span> bias_list<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_2<span class="token punctuation">(</span>new_input<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_3<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
        
        <span class="token comment"># 计算loss</span>
        <span class="token comment"># 计算MSE</span>
        mse <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kb<span class="token punctuation">.</span>square<span class="token punctuation">(</span>input_tensor <span class="token operator">-</span> output<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 计算权重乘法项</span>
        alpha <span class="token operator">=</span> <span class="token number">0.1</span>
        W1 <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_2<span class="token punctuation">.</span>kernel
        W2 <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_3<span class="token punctuation">.</span>kernel
        weightPunish <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kb<span class="token punctuation">.</span>square<span class="token punctuation">(</span>W1<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>kb<span class="token punctuation">.</span>square<span class="token punctuation">(</span>W2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># 计算KL散度</span>
        beita <span class="token operator">=</span> <span class="token number">0.1</span>
        desired_density <span class="token operator">=</span> <span class="token number">0.1</span>
        layer2_output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_2<span class="token punctuation">.</span>result
        <span class="token comment"># 实际密度是所有输入数据的密度的平均值</span>
        actual_density <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>layer2_output<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> layer2_output<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        actual_density <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>actual_density<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        <span class="token keyword">if</span> actual_density <span class="token operator">==</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">:</span>
            actual_density <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">0.999</span><span class="token punctuation">)</span>
        actual_density <span class="token operator">=</span> actual_density<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

		
        KL <span class="token operator">=</span> desired_density <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>desired_density <span class="token operator">/</span> actual_density<span class="token punctuation">)</span>
        KL <span class="token operator">+=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> desired_density<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> desired_density<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> actual_density<span class="token punctuation">)</span><span class="token punctuation">)</span>
        KL <span class="token operator">*=</span> beita
        ans <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span>mse <span class="token operator">+</span> weightPunish <span class="token operator">+</span> KL<span class="token punctuation">)</span>
        <span class="token keyword">return</span> ans
    
    <span class="token keyword">def</span> <span class="token function">get_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
            tape<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>self<span class="token punctuation">.</span>variables<span class="token punctuation">)</span>
            L <span class="token operator">=</span> self<span class="token punctuation">.</span>get_loss<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>
            <span class="token comment"># 保存一下loss，用于输出</span>
            self<span class="token punctuation">.</span>train_loss <span class="token operator">=</span> L
            g <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>L<span class="token punctuation">,</span> self<span class="token punctuation">.</span>variables<span class="token punctuation">)</span>
        <span class="token keyword">return</span> g

    <span class="token keyword">def</span> <span class="token function">network_learn</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        g <span class="token operator">=</span> self<span class="token punctuation">.</span>get_grad<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>
        optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>g<span class="token punctuation">,</span> self<span class="token punctuation">.</span>variables<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>train_loss

    <span class="token comment"># 如果模型训练好了，需要获得隐藏层的输出，直接获取麻烦，则直接运行一遍</span>
    <span class="token keyword">def</span> <span class="token function">getReprestation</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        bias_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        new_input <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>input_tensor<span class="token punctuation">,</span> bias_list<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_2<span class="token punctuation">(</span>new_input<span class="token punctuation">)</span>
        <span class="token keyword">return</span> hidden
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c00b93c9ff340239b4570191f4d35f3c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">12小时上线“新冠肺炎同程查询工具”，开发者这样狙击疫情</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/41cb633d9a5eaa1224ab6108702dba88/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">服务器安装jupyter notebook</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>