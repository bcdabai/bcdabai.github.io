<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CUDA Sample中的reduce实现 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CUDA Sample中的reduce实现" />
<meta property="og:description" content="我们知道，GPU擅长做并行计算，像element-wise操作。GEMM, Conv这种不仅结果张量中元素的计算相互不依赖，而且输入数据还会被反复利用的更能体现GPU的优势。但AI模型计算或者HPC中还有一类操作由于元素间有数据依赖，会给并行化带来挑战，那就是reduce操作。它代表一类操作，即将多个元素通过某种特定的运算进行归约。其应用很广泛，很多其它算法也以它为基础，如scan, histogram等操作。
最naive的计算方式是序列化地挨个累加。虽然序列化的实现很简单，但无法获得并行处理器带来的好处。为了得到并行计算的好处，需要将计算分布到多个核上。但就像前面说的，序列化的方式由于会导致每个元素计算时都依赖前面的累加结果，难以完全并行。对于reduce，一个基本思路是尽可能地将部分计算并行，以一种树形的结构来分阶段计算最终结果。
CUDA SDK Sample中的关于reduce的例子展示了如何用CUDA高效地进行计算。其中包含了CUDA中的一些特性的使用及技巧。相关代码在cuda-samples项目的Samples/2_Concepts_and_Techniques/reduction目录。Reduce相关的sample主要散落在下面几个子目录中：
reductionreductionMultiBlockCGthreadFenceReduction Reduction 对于reduction这个sample，文件reduction/reduction.cpp中的runTest为程序的主入口函数。如果运行时命令行指定--shmoo参数，则会对于1到32M的数据，执行7种不同的kernel。然后生成csv格式的报告。
./reduction --shmoo 这里采用的是two pass reduction的方法，即分两次pass做全部数据的计算。第一个pass是做CTA内的reduce，然后做CTA的local sum做reduce。这几个kernel主要用于第一个pass的。下面看看这几个reduce kernel的实现：
reduce0 这是最基础的版本。它对于n（2的幂）个元素的输入数组，使用n个线程，在log(n)步内完成计算。
首先将输入元素从global memory g_idata移到shared memory sdata，每个线程移一个元素。
// load shared mem unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x * blockDim.x &#43; threadIdx.x; sdata[tid] = (i &lt; n) ? g_idata[i] : 0; cg::sync(cta); 然后用log-step reduction在CTA内部做reduction。
// do reduction in shared mem for (unsigned int s = 1; s &lt; blockDim.x; s *= 2) { // modulo arithmetic is slow!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/84068170176b2771797923be41a50c10/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-10T21:52:30+08:00" />
<meta property="article:modified_time" content="2023-05-10T21:52:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CUDA Sample中的reduce实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我们知道，GPU擅长做并行计算，像element-wise操作。GEMM, Conv这种不仅结果张量中元素的计算相互不依赖，而且输入数据还会被反复利用的更能体现GPU的优势。但AI模型计算或者HPC中还有一类操作由于元素间有数据依赖，会给并行化带来挑战，那就是reduce操作。它代表一类操作，即将多个元素通过某种特定的运算进行归约。其应用很广泛，很多其它算法也以它为基础，如scan, histogram等操作。</p> 
<p>最naive的计算方式是序列化地挨个累加。虽然序列化的实现很简单，但无法获得并行处理器带来的好处。为了得到并行计算的好处，需要将计算分布到多个核上。但就像前面说的，序列化的方式由于会导致每个元素计算时都依赖前面的累加结果，难以完全并行。对于reduce，一个基本思路是尽可能地将部分计算并行，以一种树形的结构来分阶段计算最终结果。</p> 
<p>CUDA SDK Sample中的关于reduce的例子展示了如何用CUDA高效地进行计算。其中包含了CUDA中的一些特性的使用及技巧。相关代码在cuda-samples项目的<code>Samples/2_Concepts_and_Techniques/reduction</code>目录。Reduce相关的sample主要散落在下面几个子目录中：</p> 
<ul><li><code>reduction</code></li><li><code>reductionMultiBlockCG</code></li><li><code>threadFenceReduction</code></li></ul> 
<h3><a id="Reduction_9"></a>Reduction</h3> 
<p>对于<code>reduction</code>这个sample，文件<code>reduction/reduction.cpp</code>中的<code>runTest</code>为程序的主入口函数。如果运行时命令行指定<code>--shmoo</code>参数，则会对于1到32M的数据，执行7种不同的kernel。然后生成csv格式的报告。</p> 
<pre><code class="prism language-bash">./reduction --shmoo
</code></pre> 
<p>这里采用的是two pass reduction的方法，即分两次pass做全部数据的计算。第一个pass是做CTA内的reduce，然后做CTA的local sum做reduce。这几个kernel主要用于第一个pass的。下面看看这几个reduce kernel的实现：</p> 
<ul><li><strong>reduce0</strong></li></ul> 
<p>这是最基础的版本。它对于n（2的幂）个元素的输入数组，使用n个线程，在log(n)步内完成计算。</p> 
<p>首先将输入元素从global memory <code>g_idata</code>移到shared memory <code>sdata</code>，每个线程移一个元素。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// load shared mem                                              </span>
<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>                                 
<span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>         
                                                                
sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token operator">?</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">;</span>                          
                                                                
cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                                  
</code></pre> 
<p>然后用log-step reduction在CTA内部做reduction。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// do reduction in shared mem                               </span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> s <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> s <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> s <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>          
  <span class="token comment">// modulo arithmetic is slow!                             </span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>tid <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> s<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                               
    sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> s<span class="token punctuation">]</span><span class="token punctuation">;</span>                           
  <span class="token punctuation">}</span>                                                         
                                                            
  cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                            
<span class="token punctuation">}</span>                                                           
</code></pre> 
<p>最后由CTA的第一个thread将该CTA的partial sum放到输出元素的对应的位置：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// write result for this block to global mem     </span>
<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> sdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>    
</code></pre> 
<p>它的问题是使用modulo运算确定哪些thread是active的，而这个操作在GPU上很耗时。另一方面，每个warp都不是100%的thread在工作，效率较低。</p> 
<ul><li><strong>reduce1</strong></li></ul> 
<p>该kernel通过更改数据元素与线程的对应关系，去除了modulo运算。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// do reduction in shared mem                                </span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> s <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> s <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> s <span class="token operator">*=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>           
  <span class="token keyword">int</span> index <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> s <span class="token operator">*</span> tid<span class="token punctuation">;</span>                                   
                                                             
  <span class="token keyword">if</span> <span class="token punctuation">(</span>index <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                  
    sdata<span class="token punctuation">[</span>index<span class="token punctuation">]</span> <span class="token operator">+=</span> sdata<span class="token punctuation">[</span>index <span class="token operator">+</span> s<span class="token punctuation">]</span><span class="token punctuation">;</span>                        
  <span class="token punctuation">}</span>                                                          
                                                             
  cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                             
<span class="token punctuation">}</span>                                                            
</code></pre> 
<p>它与上一个kernel的区别是其中active的thread是连续的。这样一些warp中的thread是完全利用了，但其缺点是数据访问方式会导致shared memory的bank conflicts。</p> 
<p>避免bank conflict是GPU的最重要的性能优化点之一。我们知道，为了提升访存的bandwidth，GPU中将shared memory均匀切成相同大小的bank，不同bank是可以同时访问的，这就使得数据访问可以并行起来。但是，一旦发生了bank conflict，意味着同一bank被同时访问，则只能串行访问。有一种例外是warp中所有线程访问同一元素，则会以广播的方式将数据给到所有线程。在compute capability 5.x后，每个bank在每个clock cycle的bandwidth为32 bit。连续的32 bit对应连续的bank。内存地址与bank index的对应公式为<code>bank index = (byte address ÷ 4 bytes/bank) % 32 banks</code>。</p> 
<ul><li><strong>reduce2</strong><br> 该kernel将interleaved addressing改为sequential addressing：</li></ul> 
<pre><code class="prism language-cpp"><span class="token comment">// do reduction in shared mem                           </span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> s <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> s <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">;</span> s <span class="token operator">&gt;&gt;=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> 
  <span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> s<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                        
    sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">+=</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> s<span class="token punctuation">]</span><span class="token punctuation">;</span>                       
  <span class="token punctuation">}</span>                                                     
                                                        
  cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                        
<span class="token punctuation">}</span>                                                       
</code></pre> 
<p>这样相邻的thread访问的是相邻的数据，从而避免了bank conflict。这个kernel有个问题是在第一次迭代中有一半的线程是空闲的。空闲的硬件就是浪费啊。</p> 
<ul><li><strong>reduce3</strong></li></ul> 
<p>前面提到的实现中每个CTA处理的数据量与CTA的线程数一致，而这个kernel中线程数减半，因此每个线程处理的数据会增大一倍。核心部分代码如下：</p> 
<pre><code class="prism language-cpp"><span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token punctuation">(</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>  

T mySum <span class="token operator">=</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token operator">?</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">;</span> 

<span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> mySum <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i <span class="token operator">+</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">]</span><span class="token punctuation">;</span>           
                                                                    
sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum<span class="token punctuation">;</span>                                                 
cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                                      
                                                                    
<span class="token comment">// do reduction in shared mem                                       </span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> s <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> s <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">;</span> s <span class="token operator">&gt;&gt;=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>             
  <span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> s<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                    
    sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum <span class="token operator">=</span> mySum <span class="token operator">+</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> s<span class="token punctuation">]</span><span class="token punctuation">;</span>                    
  <span class="token punctuation">}</span>                                                                 
                                                                    
  cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                                    
<span class="token punctuation">}</span>                                                                   
</code></pre> 
<ul><li><strong>reduce4</strong></li></ul> 
<p>这个kernel使用了warp级的优化。Warp是CUDA上调度和运行的基本单元。在warp范围内的计算有一些特有的优化机会，如由于warp内部线程是同步的，因此可以免去<code>__syncthreads()</code>，还有关于tid的一些判断所带来的开销。</p> 
<p>Warp内优化主要手段之一就是warp shuffle操作（Warp shuffle介绍可参考<a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/" rel="nofollow">Using CUDA Warp-Level Primitives</a>）。该kernel主要逻辑如下：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// do reduction in shared mem                                                 </span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> s <span class="token operator">=</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> s <span class="token operator">&gt;</span> <span class="token number">32</span><span class="token punctuation">;</span> s <span class="token operator">&gt;&gt;=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                      
  <span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> s<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                              
    sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum <span class="token operator">=</span> mySum <span class="token operator">+</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> s<span class="token punctuation">]</span><span class="token punctuation">;</span>                              
  <span class="token punctuation">}</span>                                                                           
                                                                              
  cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                                              
<span class="token punctuation">}</span>                                                                             
                                                                              
cg<span class="token double-colon punctuation">::</span>thread_block_tile<span class="token operator">&lt;</span><span class="token number">32</span><span class="token operator">&gt;</span> tile32 <span class="token operator">=</span> cg<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">tiled_partition</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token number">32</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>              
                                                                              
<span class="token keyword">if</span> <span class="token punctuation">(</span>cta<span class="token punctuation">.</span><span class="token function">thread_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">32</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                 
  <span class="token comment">// Fetch final intermediate sum from 2nd warp                               </span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>blockSize <span class="token operator">&gt;=</span> <span class="token number">64</span><span class="token punctuation">)</span> mySum <span class="token operator">+=</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">;</span>                              
  <span class="token comment">// Reduce final warp using shuffle                                          </span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> offset <span class="token operator">=</span> tile32<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> offset <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">;</span> offset <span class="token operator">/=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>             
    mySum <span class="token operator">+=</span> tile32<span class="token punctuation">.</span><span class="token function">shfl_down</span><span class="token punctuation">(</span>mySum<span class="token punctuation">,</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>                                 
  <span class="token punctuation">}</span>                                                                           
<span class="token punctuation">}</span>                                                                             
</code></pre> 
<p>注意前面tree-based reduction到32为止，warp size为32，也就是说剩下的是warp级的reduction计算。Warp级计算中使用<code>shfl_down</code>操作在warp内进行tree-based reduction。</p> 
<ul><li><strong>reduce5</strong></li></ul> 
<p>这个kernel主要减少地址计算和循环开销。要达到这个目的，一个常用的方法就是loop unrolling。实现上比较直观，不作展开了。</p> 
<pre><code class="prism language-cpp"><span class="token comment">// do reduction in shared mem                            </span>
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>blockSize <span class="token operator">&gt;=</span> <span class="token number">512</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                 
  sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum <span class="token operator">=</span> mySum <span class="token operator">+</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> <span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">;</span>         
<span class="token punctuation">}</span>                                                        
                                                         
cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                           
                                                         
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>blockSize <span class="token operator">&gt;=</span> <span class="token number">256</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                 
  sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum <span class="token operator">=</span> mySum <span class="token operator">+</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">;</span>         
<span class="token punctuation">}</span>                                                        
                                                         
cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>                                           
                                                         
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>blockSize <span class="token operator">&gt;=</span> <span class="token number">128</span><span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                  
  sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span> <span class="token operator">=</span> mySum <span class="token operator">=</span> mySum <span class="token operator">+</span> sdata<span class="token punctuation">[</span>tid <span class="token operator">+</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">;</span>          
<span class="token punctuation">}</span>                                                        
</code></pre> 
<ul><li><strong>reduce6</strong></li></ul> 
<p>前面的kernel中元素数量与线程数相等，或者一倍关系。这个kernel中在每个线程中串行累加多个元素。 我们知道，要让GPU达到更好的性能，有两个维度的考量。一个是occupancy，即尽可能多的warp可以让SM有东西可调度，避免SM空闲。这提升了warp间的并行程度。另一个是让线程内有足够多工作，这样可以有更多指令级并行的机会。每个线程干更多活，意味着更少的线程，也意味着更少的warp和CTA。这相当于以occupancy为代价，提升了指令级并行（Instruction-level parallelism）。遗憾的是两者通常是矛盾的，需要我们在实践中找到一个平衡点。具体一个线程干多少活合适，是需要case-by-case分析的。</p> 
<p>这部分逻辑主要体现在下面这部分代码中：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// perform first level of reduction,                                         </span>
<span class="token comment">// reading from global memory, writing to shared memory                      </span>
<span class="token keyword">unsigned</span> <span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>                                              
<span class="token keyword">unsigned</span> <span class="token keyword">int</span> gridSize <span class="token operator">=</span> blockSize <span class="token operator">*</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span>                               
                                                                             
T mySum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                                                                 
                                                                             
<span class="token comment">// we reduce multiple elements per thread.  The number is determined by the  </span>
<span class="token comment">// number of active thread blocks (via gridDim).  More blocks will result    </span>
<span class="token comment">// in a larger gridSize and therefore fewer elements per thread              </span>
<span class="token keyword">if</span> <span class="token punctuation">(</span>nIsPow2<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                               
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockSize <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>                 
  gridSize <span class="token operator">=</span> gridSize <span class="token operator">&lt;&lt;</span> <span class="token number">1</span><span class="token punctuation">;</span>                                                  
                                                                             
  <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                            
    mySum <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                                                     
    <span class="token comment">// ensure we don't read out of bounds -- this is optimized away for      </span>
    <span class="token comment">// powerOf2 sized arrays                                                 </span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>i <span class="token operator">+</span> blockSize<span class="token punctuation">)</span> <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                               
      mySum <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i <span class="token operator">+</span> blockSize<span class="token punctuation">]</span><span class="token punctuation">;</span>                                       
    <span class="token punctuation">}</span>                                                                        
    i <span class="token operator">+=</span> gridSize<span class="token punctuation">;</span>                                                           
  <span class="token punctuation">}</span>                                                                          
<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>                                                                     
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockSize <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>                     
  <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                            
    mySum <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                                                     
    i <span class="token operator">+=</span> gridSize<span class="token punctuation">;</span>                                                           
  <span class="token punctuation">}</span>                                                                          
<span class="token punctuation">}</span>                                                                            
</code></pre> 
<p>其中的while循环用于在线程中累加多个元素。这里有个小优化，如果元素个数是2的幂，每个迭代可以累加两个元素。</p> 
<ul><li><strong>reduce7</strong></li></ul> 
<p>该kernel的前半部分，即first level reduction与前一kernel是类似的。后面的部分利用了warp级别的优化：</p> 
<pre><code class="prism language-cpp"><span class="token comment">// unsigned int maskLength = (blockSize &amp; 31);  // 31 = warpSize-1    </span>
<span class="token comment">// maskLength = (maskLength &gt; 0) ? (32 - maskLength) : maskLength;    </span>
<span class="token comment">// const unsigned int mask = (0xffffffff) &gt;&gt; maskLength;              </span>
                                                       
<span class="token comment">// Reduce within warp using shuffle or reduce_add if T==int &amp; CUDA_ARCH ==          </span>
<span class="token comment">// SM 8.0                                                                           </span>
mySum <span class="token operator">=</span> <span class="token generic-function"><span class="token function">warpReduceSum</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> mySum<span class="token punctuation">)</span><span class="token punctuation">;</span>                                              
                                                                                    
<span class="token comment">// each thread puts its local sum into shared memory                                </span>
<span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>tid <span class="token operator">%</span> warpSize<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                        
  sdata<span class="token punctuation">[</span>tid <span class="token operator">/</span> warpSize<span class="token punctuation">]</span> <span class="token operator">=</span> mySum<span class="token punctuation">;</span>                                                    
<span class="token punctuation">}</span>                                                                                   
                                                                                    
<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                                                    
                                                                                    
<span class="token keyword">const</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> shmem_extent <span class="token operator">=</span>                                                   
    <span class="token punctuation">(</span>blockSize <span class="token operator">/</span> warpSize<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token operator">?</span> <span class="token punctuation">(</span>blockSize <span class="token operator">/</span> warpSize<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">;</span>                        
<span class="token keyword">const</span> <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ballot_result <span class="token operator">=</span> <span class="token function">__ballot_sync</span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> tid <span class="token operator">&lt;</span> shmem_extent<span class="token punctuation">)</span><span class="token punctuation">;</span>         
<span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">&lt;</span> shmem_extent<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                           
  mySum <span class="token operator">=</span> sdata<span class="token punctuation">[</span>tid<span class="token punctuation">]</span><span class="token punctuation">;</span>                                                               
  <span class="token comment">// Reduce final warp using shuffle or reduce_add if T==int &amp; CUDA_ARCH ==         </span>
  <span class="token comment">// SM 8.0                                                                         </span>
  mySum <span class="token operator">=</span> <span class="token generic-function"><span class="token function">warpReduceSum</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>ballot_result<span class="token punctuation">,</span> mySum<span class="token punctuation">)</span><span class="token punctuation">;</span>                                   
<span class="token punctuation">}</span>                                                                                   

</code></pre> 
<p>其中关键的是<code>warpReduceSum()</code>函数，其作用是做warp内的reduction。在SM 8.0前是用的warp shuffle，SM 8.0后可以用<code>__reduce_add_sync</code>。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token class-name">T</span><span class="token operator">&gt;</span>                                                              
__device__ __forceinline__ T <span class="token function">warpReduceSum</span><span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> mask<span class="token punctuation">,</span> T mySum<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>        
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> offset <span class="token operator">=</span> warpSize <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">;</span> offset <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">;</span> offset <span class="token operator">/=</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                    
    mySum <span class="token operator">+=</span> <span class="token function">__shfl_down_sync</span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> mySum<span class="token punctuation">,</span> offset<span class="token punctuation">)</span><span class="token punctuation">;</span>                             
  <span class="token punctuation">}</span>                                                                             
  <span class="token keyword">return</span> mySum<span class="token punctuation">;</span>                                                                 
<span class="token punctuation">}</span>                                                                               
                                                                                
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">if</span> <span class="token expression">__CUDA_ARCH__ <span class="token operator">&gt;=</span> <span class="token number">800</span>                                                        </span></span>
<span class="token comment">// Specialize warpReduceFunc for int inputs to use __reduce_add_sync intrinsic  </span>
<span class="token comment">// when on SM 8.0 or higher                                                     </span>
<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token operator">&gt;</span>                                                                     
__device__ __forceinline__ <span class="token keyword">int</span> <span class="token generic-function"><span class="token function">warpReduceSum</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> mask<span class="token punctuation">,</span>            
                                                  <span class="token keyword">int</span> mySum<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                  
  mySum <span class="token operator">=</span> <span class="token function">__reduce_add_sync</span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> mySum<span class="token punctuation">)</span><span class="token punctuation">;</span>                                       
  <span class="token keyword">return</span> mySum<span class="token punctuation">;</span>                                                                 
<span class="token punctuation">}</span>                                                                               
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span>                                                                          </span>
</code></pre> 
<ul><li><strong>cg_reduce</strong></li></ul> 
<p>Cooperative Groups是CUDA 9引入的特性，用于更灵活地组织与同步线程组（可参考<a href="https://developer.nvidia.com/blog/cooperative-groups/" rel="nofollow">Cooperative Groups: Flexible CUDA Thread Programming</a>）。这个kernel实现利用了<code>cooperative group</code>特性中的<code>thread_block_tile</code>，将CTA分成warp。先在CTA范围作reduction，直到warp级别，最后调用<code>cg_reduce_n</code>做warp级别的reduction。</p> 
<pre><code class="prism language-cpp">T threadVal <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                                            
<span class="token punctuation">{<!-- --></span>                                                           
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> i <span class="token operator">=</span> threadIndex<span class="token punctuation">;</span>                             
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> indexStride <span class="token operator">=</span> <span class="token punctuation">(</span>numCtas <span class="token operator">*</span> ctaSize<span class="token punctuation">)</span><span class="token punctuation">;</span>           
  <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                           
    threadVal <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                                
    i <span class="token operator">+=</span> indexStride<span class="token punctuation">;</span>                                       
  <span class="token punctuation">}</span>                                                         
  sdata<span class="token punctuation">[</span>threadRank<span class="token punctuation">]</span> <span class="token operator">=</span> threadVal<span class="token punctuation">;</span>                            
<span class="token punctuation">}</span>                                                           
                                                            
<span class="token comment">// Wait for all tiles to finish and reduce within CTA       </span>
<span class="token punctuation">{<!-- --></span>                                                           
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ctaSteps <span class="token operator">=</span> tile<span class="token punctuation">.</span><span class="token function">meta_group_size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>           
  <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ctaIndex <span class="token operator">=</span> ctaSize <span class="token operator">&gt;&gt;</span> <span class="token number">1</span><span class="token punctuation">;</span>                     
  <span class="token keyword">while</span> <span class="token punctuation">(</span>ctaIndex <span class="token operator">&gt;=</span> <span class="token number">32</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                  
    cta<span class="token punctuation">.</span><span class="token function">sync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                             
    <span class="token keyword">if</span> <span class="token punctuation">(</span>threadRank <span class="token operator">&lt;</span> ctaIndex<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                            
      threadVal <span class="token operator">+=</span> sdata<span class="token punctuation">[</span>threadRank <span class="token operator">+</span> ctaIndex<span class="token punctuation">]</span><span class="token punctuation">;</span>            
      sdata<span class="token punctuation">[</span>threadRank<span class="token punctuation">]</span> <span class="token operator">=</span> threadVal<span class="token punctuation">;</span>                        
    <span class="token punctuation">}</span>                                                       
    ctaSteps <span class="token operator">&gt;&gt;=</span> <span class="token number">1</span><span class="token punctuation">;</span>                                         
    ctaIndex <span class="token operator">&gt;&gt;=</span> <span class="token number">1</span><span class="token punctuation">;</span>                                         
  <span class="token punctuation">}</span>                                                         
<span class="token punctuation">}</span>                                                           
                                                            
<span class="token comment">// Shuffle redux instead of smem redux                      </span>
<span class="token punctuation">{<!-- --></span>                                                           
  cta<span class="token punctuation">.</span><span class="token function">sync</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                               
  <span class="token keyword">if</span> <span class="token punctuation">(</span>tile<span class="token punctuation">.</span><span class="token function">meta_group_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                        
    threadVal <span class="token operator">=</span> <span class="token function">cg_reduce_n</span><span class="token punctuation">(</span>threadVal<span class="token punctuation">,</span> tile<span class="token punctuation">)</span><span class="token punctuation">;</span>               
  <span class="token punctuation">}</span>                                                         
<span class="token punctuation">}</span>                                                           
</code></pre> 
<p>这里的处理可以大体分为三段：</p> 
<ol><li>处理超过grid的数据。将要处理的数据减少到一个grid。</li><li>在CTA内做reduction，直到warp级别。</li><li>在warp内做reduction。</li></ol> 
<ul><li><strong>multi_warp_cg_reduce</strong></li></ul> 
<p>该kernel实现将CTA与warp间通过cooperative group又分了一层，称为multi warp group。它主要流程分几段：</p> 
<ol><li>处理超过grid的数据。这部分和reduce6基本一致。</li><li>通过<code>cg_reduce_n</code>函数，做multi warp group内的reduction。</li><li>让CTA的每一个thread做在多个multi warp group做reduction。</li></ol> 
<h3><a id="reductionMultiBlockCG_296"></a>reductionMultiBlockCG</h3> 
<p>该kernel实现在<code>Samples/2_Concepts_and_Techniques/reductionMultiBlockCG/reductionMultiBlockCG.cu</code>文件中。函数<code>call_reduceSinglePassMultiBlockCG</code>是kernel的wrapper函数。它通过<code>cudaLaunchCooperativeKernel</code>函数启动kernel <code>reduceSinglePassMultiBlockCG</code>。其中thread与CTA数量由<code>getNumBlocksAndThreads()</code>函数调用<code>cudaOccupancyMaxPotentialBlockSize()</code>函数确定。</p> 
<p>该kernel可在一个kernel调用中对任意size的数据做reduction。它是一个single pass的kernel。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> grid<span class="token punctuation">.</span><span class="token function">thread_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i <span class="token operator">+=</span> grid<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  sdata<span class="token punctuation">[</span>block<span class="token punctuation">.</span><span class="token function">thread_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">+=</span> g_idata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>block<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">// Reduce each block (called once per block)</span>
<span class="token function">reduceBlock</span><span class="token punctuation">(</span>sdata<span class="token punctuation">,</span> block<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// Write out the result to global memory</span>
<span class="token keyword">if</span> <span class="token punctuation">(</span>block<span class="token punctuation">.</span><span class="token function">thread_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  g_odata<span class="token punctuation">[</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> sdata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>grid<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token keyword">if</span> <span class="token punctuation">(</span>grid<span class="token punctuation">.</span><span class="token function">thread_rank</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> block <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> block <span class="token operator">&lt;</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> block<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    g_odata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> g_odata<span class="token punctuation">[</span>block<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>整体流程大体分几个阶段：</p> 
<ol><li>以grid为stride对全部数据做reduction。结果放在shared memory中。</li><li><code>reduceBlock()</code>函数在每个block内做reduction。计算结果（即每个block的partial sum）放在global memory中。</li><li>整个grid的第一个线程，将每个block的partial sum累加起来，得到最终的值。</li></ol> 
<h3><a id="threadFenceReduction_327"></a>threadFenceReduction</h3> 
<p>该kernel实现在<code>Samples/2_Concepts_and_Techniques/threadFenceReduction_kernel.cu</code>文件中。其中<code>reduce()</code>函数是一个wrapper，启动真正的kernel。这里有multi pass与single pass两个实现：</p> 
<pre><code class="prism language-cpp"><span class="token comment"></span>
<span class="token comment">// This function performs a reduction of the input data multiple times and</span>
<span class="token comment">// measures the average reduction time.</span>
<span class="token comment"></span>
<span class="token keyword">float</span> <span class="token function">benchmarkReduce</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span> numThreads<span class="token punctuation">,</span> <span class="token keyword">int</span> numBlocks<span class="token punctuation">,</span> <span class="token keyword">int</span> maxThreads<span class="token punctuation">,</span>
                      <span class="token keyword">int</span> maxBlocks<span class="token punctuation">,</span> <span class="token keyword">int</span> testIterations<span class="token punctuation">,</span> <span class="token keyword">bool</span> multiPass<span class="token punctuation">,</span>
                      <span class="token keyword">bool</span> cpuFinalReduction<span class="token punctuation">,</span> <span class="token keyword">int</span> cpuFinalThreshold<span class="token punctuation">,</span>
                      StopWatchInterface <span class="token operator">*</span>timer<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>h_odata<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_idata<span class="token punctuation">,</span>
                      <span class="token keyword">float</span> <span class="token operator">*</span>d_odata<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>multiPass<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// execute the kernel</span>
        <span class="token function">reduce</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> numThreads<span class="token punctuation">,</span> numBlocks<span class="token punctuation">,</span> d_idata<span class="token punctuation">,</span> d_odata<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>cpuFinalReduction<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// sum partial sums from each block on CPU</span>
            <span class="token comment">// copy result from device to host</span>
            <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> numBlocks<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                gpu_result <span class="token operator">+=</span> h_odata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
            <span class="token comment">// sum partial block sums on GPU</span>
            <span class="token keyword">int</span> s <span class="token operator">=</span> numBlocks<span class="token punctuation">;</span>
            
            <span class="token keyword">while</span> <span class="token punctuation">(</span>s <span class="token operator">&gt;</span> cpuFinalThreshold<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
                <span class="token keyword">int</span> threads <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> blocks <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
                <span class="token function">getNumBlocksAndThreads</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> maxBlocks<span class="token punctuation">,</span> maxThreads<span class="token punctuation">,</span> blocks<span class="token punctuation">,</span> threads<span class="token punctuation">)</span><span class="token punctuation">;</span>
                <span class="token function">reduce</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> threads<span class="token punctuation">,</span> blocks<span class="token punctuation">,</span> d_odata<span class="token punctuation">,</span> d_odata<span class="token punctuation">)</span><span class="token punctuation">;</span>
                s <span class="token operator">=</span> s <span class="token operator">/</span> <span class="token punctuation">(</span>threads <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// execute the kernel </span>
        <span class="token function">reduceSinglePass</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> numThreads<span class="token punctuation">,</span> numBlocks<span class="token punctuation">,</span> d_idata<span class="token punctuation">,</span> d_odata<span class="token punctuation">)</span><span class="token punctuation">;</span>
        
    <span class="token punctuation">}</span>
    <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>对于multi pass的情况，首先调用<code>reduce()</code>函数计算CTA中的partial sum。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> blockSize<span class="token punctuation">,</span> <span class="token keyword">bool</span> nIsPow2<span class="token operator">&gt;</span>
__global__ <span class="token keyword">void</span> <span class="token function">reduceMultiPass</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span>g_idata<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>g_odata<span class="token punctuation">,</span>
                                <span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token comment">// Handle to thread block group</span>
  cg<span class="token double-colon punctuation">::</span>thread_block cta <span class="token operator">=</span> cg<span class="token double-colon punctuation">::</span><span class="token function">this_thread_block</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token generic-function"><span class="token function">reduceBlocks</span><span class="token generic class-name"><span class="token operator">&lt;</span>blockSize<span class="token punctuation">,</span> nIsPow2<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>g_idata<span class="token punctuation">,</span> g_odata<span class="token punctuation">,</span> n<span class="token punctuation">,</span> cta<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token comment"></span>
<span class="token comment">// Wrapper function for kernel launch</span>
<span class="token comment"></span>
<span class="token keyword">extern</span> <span class="token string">"C"</span> <span class="token keyword">void</span> <span class="token function">reduce</span><span class="token punctuation">(</span><span class="token keyword">int</span> size<span class="token punctuation">,</span> <span class="token keyword">int</span> threads<span class="token punctuation">,</span> <span class="token keyword">int</span> blocks<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_idata<span class="token punctuation">,</span>
                       <span class="token keyword">float</span> <span class="token operator">*</span>d_odata<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>threads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>blocks<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">int</span> smemSize <span class="token operator">=</span>
      <span class="token punctuation">(</span>threads <span class="token operator">&lt;=</span> <span class="token number">32</span><span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token number">2</span> <span class="token operator">*</span> threads <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span> <span class="token operator">:</span> threads <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// choose which of the optimized versions of reduction to launch</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">isPow2</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">switch</span> <span class="token punctuation">(</span>threads<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
      <span class="token keyword">case</span> <span class="token number">512</span><span class="token operator">:</span>
        reduceMultiPass<span class="token operator">&lt;</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token operator">&gt;</span>
            <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dimGrid<span class="token punctuation">,</span> dimBlock<span class="token punctuation">,</span> smemSize<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>d_idata<span class="token punctuation">,</span> d_odata<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>
</code></pre> 
<p>然后计将partial sum累加成最终的结果。这步如果在CPU上算，比较方便，将前面计算的partial sum从GPU移到CPU，然后累加就完了。如果是GPU上算，则需要再调用<code>reduce()</code>函数做reduction。如果一把做不完，还得继续，从而构成循环。</p> 
<p>对于single pass的情况，它通过atomic operation与shared memory实现single pass。即只需一次kernel调用。在device memory中记录哪个CTA已写入partial sum，当所有CTA完成后，由一个CTA执行最后的log-step reduction。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> blockSize<span class="token punctuation">,</span> <span class="token keyword">bool</span> nIsPow2<span class="token operator">&gt;</span>
__global__ <span class="token keyword">void</span> <span class="token function">reduceSinglePass</span><span class="token punctuation">(</span><span class="token keyword">const</span> <span class="token keyword">float</span> <span class="token operator">*</span>g_idata<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>g_odata<span class="token punctuation">,</span>
                                 <span class="token keyword">unsigned</span> <span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">//</span>
    <span class="token comment">// PHASE 1: Process all inputs assigned to this block</span>
    <span class="token comment">//</span>
    <span class="token generic-function"><span class="token function">reduceBlocks</span><span class="token generic class-name"><span class="token operator">&lt;</span>blockSize<span class="token punctuation">,</span> nIsPow2<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>g_idata<span class="token punctuation">,</span> g_odata<span class="token punctuation">,</span> n<span class="token punctuation">,</span> cta<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token comment">//</span>
    <span class="token comment">// PHASE 2: Last block finished will process all partial sums</span>
    <span class="token comment">//</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>gridDim<span class="token punctuation">.</span>x <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token comment">// wait until all outstanding memory instructions in this thread are</span>
        <span class="token comment">// finished</span>
        <span class="token function">__threadfence</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        
        <span class="token comment">// Thread 0 takes a ticket                                             </span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                                        
        <span class="token keyword">unsigned</span> <span class="token keyword">int</span> ticket <span class="token operator">=</span> <span class="token function">atomicInc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>retirementCount<span class="token punctuation">,</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>        
        <span class="token comment">// If the ticket ID is equal to the number of blocks, we are the last</span>
        <span class="token comment">// block!                                                            </span>
        amLast <span class="token operator">=</span> <span class="token punctuation">(</span>ticket <span class="token operator">==</span> gridDim<span class="token punctuation">.</span>x <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                                  
        
        cg<span class="token double-colon punctuation">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span><span class="token punctuation">;</span>     
        <span class="token comment">// The last block sums the results of all other blocks </span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>amLast<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;</span> gridDim<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                
                mySum <span class="token operator">+=</span> g_odata<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>                                 
                i <span class="token operator">+=</span> blockSize<span class="token punctuation">;</span>                                      
            <span class="token punctuation">}</span>                                                      
                                                             
            <span class="token generic-function"><span class="token function">reduceBlock</span><span class="token generic class-name"><span class="token operator">&lt;</span>blockSize<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>smem<span class="token punctuation">,</span> mySum<span class="token punctuation">,</span> tid<span class="token punctuation">,</span> cta<span class="token punctuation">)</span><span class="token punctuation">;</span>         
                                                             
            <span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>                                        
                g_odata<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> smem<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">;</span>                                
                                                             
                <span class="token comment">// reset retirement count so that next run succeeds  </span>
                retirementCount <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>                                 
            <span class="token punctuation">}</span>                                                      
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>          

<span class="token keyword">extern</span> <span class="token string">"C"</span> <span class="token keyword">void</span> <span class="token function">reduceSinglePass</span><span class="token punctuation">(</span><span class="token keyword">int</span> size<span class="token punctuation">,</span> <span class="token keyword">int</span> threads<span class="token punctuation">,</span> <span class="token keyword">int</span> blocks<span class="token punctuation">,</span>
                                 <span class="token keyword">float</span> <span class="token operator">*</span>d_idata<span class="token punctuation">,</span> <span class="token keyword">float</span> <span class="token operator">*</span>d_odata<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  dim3 <span class="token function">dimBlock</span><span class="token punctuation">(</span>threads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  dim3 <span class="token function">dimGrid</span><span class="token punctuation">(</span>blocks<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">int</span> smemSize <span class="token operator">=</span> threads <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

  <span class="token comment">// choose which of the optimized versions of reduction to launch</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">isPow2</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">switch</span> <span class="token punctuation">(</span>threads<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
      <span class="token keyword">case</span> <span class="token number">512</span><span class="token operator">:</span>
        reduceSinglePass<span class="token operator">&lt;</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token operator">&gt;</span>
            <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>dimGrid<span class="token punctuation">,</span> dimBlock<span class="token punctuation">,</span> smemSize<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>d_idata<span class="token punctuation">,</span> d_odata<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">break</span><span class="token punctuation">;</span>
</code></pre> 
<p>在第一个阶段中，在CTA范围内算partial sum。第二个阶段，先判断是否CTA大于1个。如果总共就一个CTA，那partial sum就是total sum，到这就完事了。否则的话，先用<code>__threadfence()</code>保证到这时grid中前面的内存事务本线程可见。假设现在有n个CTA。我们只需要一个CTA就行，因此让每个CTA的0号线程拿ticket（通过<code>atomicInc()</code>函数实现），拿到第n张ticket的就是最后一个到这的CTA的0号线程。它会将shared memory中的变量<code>amLast</code>置为true，这样通过判断这个变量就可以只让被选中的CTA干活。这个被选中的CTA先将超过thread block size的数据累加，这样将要处理的数据个数缩小到thread block size以内，再调用<code>reduceBlock</code>这个kernel，对shared memory中的数据进行求和。这里的kernel <code>reduceBlock</code>先将CTA切成warp，在warp内部做累加，最后内CTA的0号线程做CTA范围的累加。该kernel执行完成后，最后由0号线程将最终结果写入到global memory中。</p> 
<p>尽管现实中很多时候要计算reduce时，为了易用性，可维护性等因素我们会直接使用一些库，如cub，thrust。但是，学习cuda-sample中的相关实现仍可以帮助我们了解一些CUDA特性的用法，以及理解kernel优化中的一些注意点。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b50912d722dfccf7e6aeb562e2e358c0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">利用VASP进行弹性模量的计算笔记</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1d51f24ce124ac1a97f8d2ddd8525342/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Python基础(三)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>