<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Open X-Embodiment 超大规模开源真实机器人数据集分享 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Open X-Embodiment 超大规模开源真实机器人数据集分享" />
<meta property="og:description" content="近期，Google旗下的前沿人工智能企业DeepMind汇集了来自 22 种不同机器人类型的数据，创建了 Open X-Embodiment 数据集并开源了出来。该数据集让他们研发的RT-2 机器人在制造和编程方式上有了重大飞跃。
有分析称，在上述数据集上训练的 RT-2-X 在现实世界机器人技能上的表现提高了 2 倍，而且通过学习新数据RT-2-X 掌握了很多新技能。英伟达高级人工智能科学家Jim Fan甚至公开表示，这个数据集可能是机器人的ImageNet时刻。
谷歌开放了X-具身存储库（robotics-transformer-X.github.io）来存储Open X-Embodiment数据集。这是一个开源存储库，包括用于X-具身机器人学习研究的大规模数据以及预训练模型的检查点。
为助力具身机器人技术研究，提高数据准备效率，OpenDataLab（opendatalab.com）整理并上架了DeepMind公开的Open X-Embodiment数据集，欢迎大家下载与探索。
另外寻星计划正在火热进行中，上传原创数据集领好礼，点击参加→寻找最闪亮的 OpenDataLab 数据之星， We want you !
数据集概述 Open X-Embodiment子数据集信息列表：
https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?pli=1#gid=0
关键词：
● 21个科研机构
● 22个机器人
● 60个已有数据集的融合
● 527个技能
● 160,266个任务
● 1,402,930条数据（共约3600G）
数据处理：
所有源数据集统一转化为RLDS格式。
对于源数据的不同格式和内容，做了以下处理：
1. 对于存在多视角的数据集，仅选择其中“canonical”的一个视角图像（猜测为比较接近top-down第一人称视角/Proprioception的那一个）。
2. 将图像resize到320×256(width×height)。
3. 将原有的动作（比如joint position）都转换为EE的动作，但是该动作量可能为相对值，也可能为绝对值。在模型输出action tokens∈ [0, 255]\in [0, 255]后根据不同的机器人做不同的de-normalization后再下达具体的控制指令。
数据集特征：
1. 60个数据集中涉及到的机器人有单臂、双臂和四足，Franka占多数。
2. 数据量上，xArm占最大头，主要是language table的数据集体量很大，有44万条；Kuka iiwa主要来自于QT-Opt的贡献；另外就是原来在Everday Robot（现在论文中称为Google Robot）上采集的RT1的数据。
3. 技能上主要还是集中在pick-place上，整体仍呈现长尾分布，尾部有许多如wiping、assembling等难度更高的技能。
4. 主要的场景和被操作物体集中在家庭、厨房场景和家具、食物、餐具等物品。[1]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0c979f218a9e7f68c7340e3515e2dc00/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-14T15:15:56+08:00" />
<meta property="article:modified_time" content="2023-11-14T15:15:56+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Open X-Embodiment 超大规模开源真实机器人数据集分享</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>近期，Google旗下的前沿人工智能企业DeepMind汇集了来自 22 种不同机器人类型的数据，创建了 Open X-Embodiment 数据集并开源了出来。该数据集让他们研发的RT-2 机器人在制造和编程方式上有了重大飞跃。</p> 
<p>有分析称，在上述数据集上训练的 RT-2-X 在现实世界机器人技能上的表现提高了 2 倍，而且通过学习新数据RT-2-X 掌握了很多新技能。英伟达高级人工智能科学家Jim Fan甚至公开表示，这个数据集可能是机器人的ImageNet时刻。</p> 
<p><img src="https://images2.imgbox.com/15/7b/Jzoowm8y_o.png" alt="图片"></p> 
<p>谷歌开放了X-具身存储库（robotics-transformer-X.github.io）来存储Open X-Embodiment数据集。这是一个开源存储库，包括用于X-具身机器人学习研究的大规模数据以及预训练模型的检查点。</p> 
<p>为助力具身机器人技术研究，提高数据准备效率，OpenDataLab（<a href="https://openxlab.org.cn/datasets?source=Q1NETg" rel="nofollow">opendatalab.com</a>）整理并上架了DeepMind公开的Open X-Embodiment数据集，欢迎大家下载与探索。</p> 
<p>另外寻星计划正在火热进行中，上传原创数据集领好礼，点击参加→<a href="https://mp.weixin.qq.com/s?__biz=MzkxMjMxMjIwNQ==&amp;mid=2247524116&amp;idx=1&amp;sn=51ac0acca9f8e5834ab24ecff0f0c25c&amp;chksm=c10c0df6f67b84e000840d9b0208b6d379a18bece0c193748b6a3fa6be61355666a59e3aa5bb&amp;token=2004545382&amp;lang=zh_CN#rd" rel="nofollow">寻找最闪亮的 OpenDataLab 数据之星， We want you !</a></p> 
<h2><a id="_13"></a><strong>数据集概述</strong></h2> 
<p><strong>Open X-Embodiment子数据集信息列表：</strong></p> 
<p><a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3%5C_t2A9g/edit?pli=1#gid=0" rel="nofollow">https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?pli=1#gid=0</a></p> 
<p><strong>关键词：</strong></p> 
<p>● 21个科研机构</p> 
<p>● 22个机器人</p> 
<p>● 60个已有数据集的融合</p> 
<p>● 527个技能</p> 
<p>● 160,266个任务</p> 
<p>● 1,402,930条数据（共约3600G）</p> 
<p><strong>数据处理：</strong></p> 
<p><strong>所有源数据集统一转化为RLDS格式。</strong><br> 对于源数据的不同格式和内容，做了以下处理：</p> 
<p>1. 对于存在多视角的数据集，仅选择其中“canonical”的一个视角图像（猜测为比较接近top-down第一人称视角/Proprioception的那一个）。</p> 
<p>2. 将图像resize到320×256(width×height)。</p> 
<p>3. 将原有的动作（比如joint position）都转换为EE的动作，但是该动作量可能为相对值，也可能为绝对值。在模型输出action tokens∈ [0, 255]\in [0, 255]后根据不同的机器人做不同的de-normalization后再下达具体的控制指令。</p> 
<p><strong>数据集特征：</strong></p> 
<p>1. 60个数据集中涉及到的机器人有单臂、双臂和四足，Franka占多数。</p> 
<p>2. 数据量上，xArm占最大头，主要是language table的数据集体量很大，有44万条；Kuka iiwa主要来自于QT-Opt的贡献；另外就是原来在Everday Robot（现在论文中称为Google Robot）上采集的RT1的数据。</p> 
<p>3. 技能上主要还是集中在pick-place上，整体仍呈现长尾分布，尾部有许多如wiping、assembling等难度更高的技能。</p> 
<p>4. 主要的场景和被操作物体集中在家庭、厨房场景和家具、食物、餐具等物品。[1]</p> 
<p><img src="https://images2.imgbox.com/39/21/fZEuAncg_o.png" alt="图片"></p> 
<h2><a id="_60"></a><strong>子数据集介绍</strong></h2> 
<h3><a id="No1_RoboVQAhttpsopendatalabcomOpenDataLabRoboVQAsourceQ1NETg_62"></a>No.1 <a href="https://opendatalab.com/OpenDataLab/RoboVQA?source=Q1NETg" rel="nofollow">RoboVQA</a></h3> 
<p><strong>● 发布方</strong>：不来梅大学</p> 
<p>● <strong>发布时间</strong>：2019</p> 
<p>● <strong>简介</strong>：</p> 
<p>RobotVQA 以场景RGB（D）图像作为输入并输出相应的场景图。RobotVQA 代表机器人视觉问答。作者展示了 RobotVQA 知识从虚拟世界到现实世界的可转移性以及对机器人控制程序的适用性。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/RoboVQA?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/RoboVQA</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/1709.10489.pdf</em></p> 
<h3><a id="No2_RoboNethttpsopendatalabcomOpenDataLabRoboNetsourceQ1NETg_80"></a>No.2 <a href="https://opendatalab.com/OpenDataLab/RoboNet?source=Q1NETg" rel="nofollow">RoboNet</a></h3> 
<p><strong>● 发布方</strong>：卡内基梅隆大学·宾夕法尼亚大学·斯坦福大学</p> 
<p>● <strong>发布时间</strong>：2020</p> 
<p>● <strong>简介</strong>：</p> 
<p>一个用于共享机器人经验的开放数据库，它提供了来自 7 个不同机器人平台的 1500 万个视频帧的初始池，并研究了如何使用它来学习基于视觉的机器人操作的通用模型。</p> 
<p><strong>● 下载地址</strong>：<br> <a href="https://opendatalab.com/OpenDataLab/RoboNet?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/RoboNet</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/1910.11215v2.pdf</em></p> 
<h3><a id="No3_BridgeData_V2httpsopendatalabcomOpenDataLabBridgeData_V2sourceQ1NETg_97"></a>No.3 <a href="https://opendatalab.com/OpenDataLab/BridgeData_V2?source=Q1NETg" rel="nofollow">BridgeData V2</a></h3> 
<p><strong>● 发布方</strong>：Google·加州大学伯克利分校·斯坦福大学</p> 
<p>● <strong>发布时间</strong>：2023</p> 
<p>● <strong>简介</strong>：</p> 
<p>BridgeData V2 是一个庞大而多样化的机器人操作行为数据集，旨在 促进可扩展机器人学习的研究。数据集兼容开放词汇、多任务 以目标图像或自然语言指令为条件的学习方法。从数据中学到的技能 推广到新的对象和环境，以及跨机构。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/BridgeData_V2?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/BridgeData_V2</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/2308.12952.pdf</em></p> 
<h3><a id="No4_Language_TablehttpsopendatalabcomOpenDataLabLanguage_TablesourceQ1NETg_117"></a>No.4 <a href="https://opendatalab.com/OpenDataLab/Language_Table?source=Q1NETg" rel="nofollow">Language Table</a></h3> 
<p><strong>● 发布方</strong>：Google</p> 
<p>● <strong>发布时间</strong>：2022</p> 
<p>● <strong>简介</strong>：</p> 
<p>Language-Table 是一套人类收集的数据集，也是开放词汇视觉语言运动学习的多任务连续控制基准。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/Language_Table?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/Language_Table</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/2210.06407.pdf</em></p> 
<h3><a id="No5_BCZhttpsopendatalabcomOpenDataLabBC_ZsourceQ1NETg_137"></a>No.5 <a href="https://opendatalab.com/OpenDataLab/BC_Z?source=Q1NETg" rel="nofollow">BC-Z</a></h3> 
<p><strong>● 发布方</strong>：卡内基梅隆大学·宾夕法尼亚大学·斯坦福大学</p> 
<p>● <strong>发布时间</strong>：2020</p> 
<p>● <strong>简介</strong>：</p> 
<p>作者收集了 100 个操作任务的大规模 VR 远程操作演示数据集，并训练卷积神经网络来模仿 RGB 像素观察的闭环动作。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/BC_Z?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/BC_Z</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/1910.11215v2.pdf</em></p> 
<h3><a id="No6_CMU_Food_ManipulationFood_Playing_DatasethttpsopendatalabcomOpenDataLabCMU_Food_ManipulationsourceQ1NETg_155"></a>No.6 <a href="https://opendatalab.com/OpenDataLab/CMU_Food_Manipulation?source=Q1NETg" rel="nofollow">CMU Food Manipulation（Food Playing Dataset）</a></h3> 
<p><strong>● 发布方</strong>：卡内基梅隆大学机器人研究所</p> 
<p>● <strong>发布时间</strong>：2021</p> 
<p>● <strong>简介</strong>：</p> 
<p>使用机械臂和一系列传感器（使用 ROS 进行同步）收集的多样化的数据集，其中包含 21 种具有不同切片和特性的独特食品。通过视觉嵌入网络，该网络利用本体感受、音频和视觉数据的组合，使用三元组损失公式对食物之间的相似性进行了编码。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/CMU_Food_Manipulation?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/CMU_Food_Manipulation</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/2309.14320.pdf</em></p> 
<h3><a id="No7_TOTO_BenchmarkhttpsopendatalabcomOpenDataLabTOTO_BenchmarksourceQ1NETg_174"></a>No.7 <a href="https://opendatalab.com/OpenDataLab/TOTO_Benchmark?source=Q1NETg" rel="nofollow">TOTO Benchmark</a></h3> 
<p><strong>● 发布方</strong>：纽约大学·Meta AI·卡内基梅隆大学</p> 
<p>● <strong>发布时间</strong>：2022</p> 
<p>● <strong>简介</strong>：</p> 
<p>在线训练离线测试 （TOTO） 是一个在线基准测试，提供：开源操作数据集,访问共享机器人进行评估。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/TOTO_Benchmark?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/TOTO_Benchmark</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/2306.00942.pdf</em></p> 
<h3><a id="No8_QUT_Dynamic_GraspinghttpsopendatalabcomOpenDataLabQUT_Dynamic_GraspingsourceQ1NETg_194"></a>No.8 <a href="https://opendatalab.com/OpenDataLab/QUT_Dynamic_Grasping?source=Q1NETg" rel="nofollow">QUT Dynamic Grasping</a></h3> 
<p><strong>● 发布方</strong>：昆士兰科技大学</p> 
<p>● <strong>发布时间</strong>：2022</p> 
<p>● <strong>简介</strong>：</p> 
<p>该数据集包含 812 个成功的轨迹，这些轨迹与使用 Franka Panda 机器人机械臂自上而下的动态抓取移动物体有关。物体随机放置在XY运动平台上，该平台可以以不同的速度在任意轨迹中移动物体。该系统使用此处描述的 CoreXY 运动平台设计。设计中的所有部件都可以3D打印或轻松采购。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/QUT_Dynamic_Grasping?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/QUT_Dynamic_Grasping</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/2309.02754.pdf</em></p> 
<h3><a id="No9_TaskAgnostic_Real_World_Robot_PlayhttpsopendatalabcomOpenDataLabTask_Agnostic_Real_World_Robot_PlaysourceQ1NETg_214"></a>No.9 <a href="https://opendatalab.com/OpenDataLab/Task_Agnostic_Real_World_Robot_Play?source=Q1NETg" rel="nofollow">Task-Agnostic Real World Robot Play</a></h3> 
<p><strong>● 发布方</strong>：弗赖堡大学·埃尔朗根-纽伦堡大学</p> 
<p>● <strong>发布时间</strong>：2023</p> 
<p>● <strong>简介</strong>：</p> 
<p>7-DoF 机械臂和平行钳口抓手执行各种无定向操作任务的情节，大约 1% 的数据使用自然语言嵌入进行注释。通过VR控制器使用远程操作收集情节，告诉用户在没有特定任务的情况下远程操作机器人。每个状态-动作对都编码在 Numpy npz 文件中，由静态和抓手相机、本体感受状态以及与该状态对应的机器人未来动作的 RGB-D 图像组成。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/Task_Agnostic_Real_World_Robot_Play?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/Task_Agnostic_Real_World_Robot_Play</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>http://tacorl.cs.uni-freiburg.de/paper/taco-rl.pdf</em></p> 
<h3><a id="No10_RoboturkhttpsopendatalabcomOpenDataLabRoboturksourceQ1NETg_233"></a>No.10 <a href="https://opendatalab.com/OpenDataLab/Roboturk?source=Q1NETg" rel="nofollow">Roboturk</a></h3> 
<p><strong>● 发布方</strong>：斯坦福大学</p> 
<p>● <strong>发布时间</strong>：2019</p> 
<p>● <strong>简介</strong>：</p> 
<p>RoboTurk 真实机器人数据集收集了有关三个不同现实世界任务的大型数据集：洗衣房布局、塔楼创建和对象搜索。所有三个数据集都是使用 RoboTurk 平台收集的，由众包工作人员远程收集。我们的数据集包含来自 54 个不同用户的 2144 个不同演示。我们提供用于训练的完整数据集和用于探索的数据集的较小子样本。</p> 
<p><strong>● 下载地址</strong>：</p> 
<p><a href="https://opendatalab.com/OpenDataLab/Roboturk?source=Q1NETg" rel="nofollow">https://opendatalab.com/OpenDataLab/Roboturk</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><em>https://arxiv.org/pdf/1911.04052.pdf</em></p> 
<p>因篇幅有限，更多机器人学习开源数据集，请访问OpenDataLab：</p> 
<p><a href="https://openxlab.org.cn/datasets?source=Q1NETg" rel="nofollow">https://opendatalab.org.cn/</a></p> 
<p>参考：[1]https://www.zhihu.com/question/624716226</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b956b212bfeaba886b237f3f6f8892e2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">centos7防火墙关闭以及清除防火墙规则</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e815540d6b8852c1275e94312b24a5d3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VuePress介绍及使用指南</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>