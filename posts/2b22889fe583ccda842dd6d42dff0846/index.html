<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python requests请求爬虫 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python requests请求爬虫" />
<meta property="og:description" content="Python 疫情__数据【包括国内外】 更多关于机器学习的知识请加关注哟~~。若需联系请私信或者加博主联系方式:
新浪微博:Jetuser
微信公众号：不一样的数据分析
抖音号：1858359940
CSDN : Jetuser-data
前言 这几天上班忙的不可开交，纵观新型冠状病毒近日在国外大势猖獗，随国内控制的不错，但仍需努力，不可懈怠，武汉加油！中国加油！全世界人民加油！刚好赶上周末，就闲下来写一下python requests请求方式爬取国内外疫情数据，我的方法是最易懂、最高效、爬取是最全的，我以人头担保【惊恐/滑稽】，没有谁的会比我的全。这次我将以该百度链接进行抓取。
1、如何开始？ 首先我们看看该上网页的数据
可以看到数据在网页上显示为这个鬼样子，那么我们如何将其抓取下来呢？
2、思路解析 其实爬虫就如同我们做事情一样的，按部就班，一步一个脚印，解决途中一步步困难。包括读取网页、数据读取、数据清洗以及后期的可视化展现…
我的思路就是先将网页request出来，text读取里面的内容，然后使用强大的正则表达式匹配出我们想要的内容再写入到excel 中，注意这样是得到一天的数据，那么我们要写一个while循环，让其固定在每天晚上11点过进行自动爬取，每天的数据依次累加到excel里面，最终得到完整的表格 [ps:我这里用Google浏览器]。
3、爬虫解析 进入页面后按键盘上的F12，依次点击 Network–&gt;XHR–&gt; 并按下ctrl&#43;R，此时点击下方出现的XHR，找到
Request Headers下的Referer，你会看到这是一个链接，这个链接就是我们要请求的网页，将其复制下来。
4、爬虫步骤 4.1 请求网页响应且返回数据 url = &#39;https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_3&#39;
response = requests.get(url=url)
response.text
这时候往下滑动你会得到一串带有&#34;\u&#34;的字符串
&#34;subList&#34;:[{&#34;city&#34;:&#34;\\u897f\\u5b81&#34;,&#34;confirmed&#34;:&#34;15&#34;,&#34;died&#34;:&#34;&#34;,&#34;crued&#34;:&#34;1
5&#34;,&#34;confirmedRelative&#34;:&#34;0&#34;,&#34;curConfirm&#34;:&#34;0&#34;,&#34;cityCode&#34;:&#34;66&#34;},{&#34;city&#34;:&#34;\\u6d77\\u5317\\u5dde&#34;,&#34;confirmed&#34;:&#34;3&#34;,&#34;died&#34;:&#34;&#34;,&#34;crue
d&#34;:&#34;3&#34;,&#34;confirmedRelative&#34;:&#34;0&#34;,&#34;curConfirm&#34;:&#34;0&#34;,&#34;cityCode&#34;:&#34;67&#34;
}]},
{&#34;confirmed&#34;:&#34;298&#34;,&#34;died&#34;:&#34;2&#34;,&#34;crued&#34;:&#34;30&#34;,&#34;relativeTime&#34;:&#34;1585
324800&#34;,&#34;confirmedRelative&#34;:&#34;16&#34;,&#34;diedRelative&#34;:&#34;0&#34;,&#34;curedRelat
ive&#34;:&#34;0&#34;,&#34;curConfirm&#34;:&#34;266&#34;,&#34;curConfirmRelative&#34;:&#34;16&#34;,&#34;icuDisab
le&#34;:&#34;1&#34;,&#34;area&#34;:&#34;\\u53f0\\u6e7e&#34;,&#34;subList&#34;:[]},
{&#34;confirmed&#34;:&#34;641&#34;,&#34;died&#34;:&#34;4&#34;,&#34;crued&#34;:&#34;118&#34;,&#34;relativeTime&#34;:&#34;158
5324800&#34;,&#34;confirmedRelative&#34;:&#34;64&#34;,&#34;diedRelative&#34;:&#34;0&#34;,&#34;curedRelative&#34;:&#34;1&#34;,&#34;curConfirm&#34;:&#34;519&#34;,&#34;curConfirmRelative&#34;:&#34;63&#34;,&#34;cityCode&#34;:&#34;2912&#34;,&#34;icuDisable&#34;:&#34;1&#34;,&#34;area&#34;:&#34;\\u9999\\u6e2f&#34;,&#34;subList&#34;:[]}
这TM是什么鬼，好！既然是鬼就收它，我猜这个Unicode就是城市名称，打开站长解码输入带有&#34;\u&#34;的任意字符比如&#34;\u897f\u5b81&#34;你会得到“\西\宁”二字,果然就是，那么其实我们已经找到了，只是需要将其解码出来即可。
此时我们又有新思路，先将中国疫情数据找到并且匹配出来，再将国外疫情数据找到又匹配出来，最终得到每个字段并且存入excel即可。
4.2 找到数据列表 其实将带有&#34;\u&#34;的字符串进行解码可知，网页上的图表数据在返回的&#34;captain-config&#34;到&#34;bundle&#34;之间，于是有
datas=re.findall(r&#39;id=&#34;captain-config&#34;&gt;(.*?)&#34;bundle&#34;&#39;, response.text)# 得到初步 来源方式
data=re.findall(r&#39;(.*),&#34;version&#34;&#39;, datas[0])# 得到初步 来源方式 加？是贪婪模式 最短匹配
这样就会得到我们所需的数据集，但是我后来发现，老是这样一次次解码太麻烦，干脆一次性将其转为中文，便于观察，但是带有&#34;\u&#34;的翻译出来又多一个&#34;&#34;，所以先将其替换掉再转为中文，于是有
data0 = &#39;u&#39;&#43;&#39;\&#39;&#39;&#43;data[0].replace(&#39;\\\\&#39;, &#39;\\&#39;)&#43;&#39;\&#39;&#39;
data3=eval(data0) # 替换斜杠并转编码 神奇的eval函数" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2b22889fe583ccda842dd6d42dff0846/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-04-14T20:07:33+08:00" />
<meta property="article:modified_time" content="2020-04-14T20:07:33+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python requests请求爬虫</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Python____1"></a>Python 疫情__数据【包括国内外】</h3> 
<p>更多关于机器学习的知识请加关注哟~~。若需联系请私信或者加博主联系方式:<br>     新浪微博:Jetuser<br>     微信公众号：不一样的数据分析</p> 
<p>    抖音号：1858359940<br></p> 
<p>    CSDN : Jetuser-data<br></p> 
<h3>前言</h3> 
<p>       这几天上班忙的不可开交，纵观新型冠状病毒近日在国外大势猖獗，随国内控制的不错，但仍需努力，不可懈怠，武汉加油！中国加油！全世界人民加油！刚好赶上周末，就闲下来写一下python requests请求方式爬取国内外疫情数据，我的方法是最易懂、最高效、爬取是最全的，我以人头担保【惊恐/滑稽】，没有谁的会比我的全。这次我将以该百度链接进行抓取。</p> 
<h3>1、如何开始？</h3> 
<p>       首先我们看看该上网页的数据<br><img class="" src="https://images2.imgbox.com/2b/e6/Ww6lrmAq_o.jpg"><img class="" src="https://images2.imgbox.com/cb/96/JtiQiPe1_o.jpg"><br>       可以看到数据在网页上显示为这个鬼样子，那么我们如何将其抓取下来呢？</p> 
<h3>2、思路解析</h3> 
<p>       其实爬虫就如同我们做事情一样的，按部就班，一步一个脚印，解决途中一步步困难。包括读取网页、数据读取、数据清洗以及后期的可视化展现…<br>       我的思路就是先将网页request出来，text读取里面的内容，然后使用强大的正则表达式匹配出我们想要的内容再写入到excel 中，注意这样是得到一天的数据，那么我们要写一个while循环，让其固定在每天晚上11点过进行自动爬取，每天的数据依次累加到excel里面，最终得到完整的表格 [ps:我这里用Google浏览器]。</p> 
<h3>3、爬虫解析</h3> 
<p>       进入页面后按键盘上的<kbd>F12</kbd>，依次点击 Network–&gt;XHR–&gt; 并按下<kbd>ctrl</kbd>+<kbd>R</kbd>，此时点击下方出现的XHR，找到<br>Request Headers下的Referer，你会看到这是一个链接，这个链接就是我们要请求的网页，将其复制下来。<br><img class="" src="https://images2.imgbox.com/95/c8/wnCGuX51_o.jpg"></p> 
<h3>4、爬虫步骤</h3> 
<h4>4.1 请求网页响应且返回数据</h4> 
<pre><code>url = 'https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_3'<br>response = requests.get(url=url)<br>response.text<br></code></pre> 
<p>       这时候往下滑动你会得到一串带有"\u"的字符串</p> 
<pre><code>"subList":[{<!-- -->"city":"\\u897f\\u5b81","confirmed":"15","died":"","crued":"1<br>5","confirmedRelative":"0","curConfirm":"0","cityCode":"66"},{"city":"\\u6d77\\u5317\\u5dde","confirmed":"3","died":"","crue<br>d":"3","confirmedRelative":"0","curConfirm":"0","cityCode":"67"<br>}]},<br>{<!-- -->"confirmed":"298","died":"2","crued":"30","relativeTime":"1585<br>324800","confirmedRelative":"16","diedRelative":"0","curedRelat<br>ive":"0","curConfirm":"266","curConfirmRelative":"16","icuDisab<br>le":"1","area":"\\u53f0\\u6e7e","subList":[]},<br>{<!-- -->"confirmed":"641","died":"4","crued":"118","relativeTime":"158<br>5324800","confirmedRelative":"64","diedRelative":"0","curedRelative":"1","curConfirm":"519","curConfirmRelative":"63","cityCode":"2912","icuDisable":"1","area":"\\u9999\\u6e2f","subList":[]}<br></code></pre> 
<p>       这TM是什么鬼，好！既然是鬼就收它，我猜这个Unicode就是城市名称，打开站长解码输入带有"\u"的任意字符比如"\u897f\u5b81"你会得到“\西\宁”二字,果然就是，那么其实我们已经找到了，只是需要将其解码出来即可。<br>       此时我们又有新思路，先将中国疫情数据找到并且匹配出来，再将国外疫情数据找到又匹配出来，最终得到每个字段并且存入excel即可。</p> 
<h4>4.2 找到数据列表</h4> 
<p>       其实将带有"\u"的字符串进行解码可知，网页上的图表数据在返回的"captain-config"到"bundle"之间，于是有</p> 
<pre><code>datas=re.findall(r'id="captain-config"&gt;(.*?)"bundle"', response.text)# 得到初步  来源方式<br>data=re.findall(r'(.*),"version"', datas[0])# 得到初步  来源方式  加？是贪婪模式 最短匹配<br></code></pre> 
<p>       这样就会得到我们所需的数据集，但是我后来发现，老是这样一次次解码太麻烦，干脆一次性将其转为中文，便于观察，但是带有"\u"的翻译出来又多一个""，所以先将其替换掉再转为中文，于是有</p> 
<pre><code>data0 = 'u'+'\''+data[0].replace('\\\\', '\\')+'\''<br>data3=eval(data0) # 替换斜杠并转编码 神奇的eval函数<br># 打印部分结果<br><br>"area":"贵州","subList":[{<!-- -->"city":"六盘水","confirmed":"10","died":"1","crued":"9","confirmedRelative":"0","curConfirm":"0","cityCode":"147"},<br>{<!-- -->"city":"毕节地区","confirmed":"23","died":"","crued":"23","confirmedRelative":"0","curConfirm":"0","cityCode":"206"},<br>{<!-- -->"city":"黔西南州","confirmed":"4","died":"","crued":"4","confirmedRelative":"0","curConfirm":"0","cityCode":"343"},<br></code></pre> 
<h4>4.3 找到国内疫情数据</h4> 
<p>       同样的正则表达式匹配方法，先找出国内的疫情数据，再将其按照字段封为列表，最后将列表挨个存到datefream里最后写为excel文件。</p> 
<pre><code>Home_List=re.findall(r'"caseList":(.*),"caseOutsideList"',data3)  # 得到国内疫情<br>Home_Lis=eval(Home_List[0])<br>Home_Lis[6]  # 得到吉林省[你的可能不是吉林]以及其各城市疫情数据<br></code></pre> 
<pre><code>{<!-- -->'confirmed': '98',<br> 'died': '1',<br> 'crued': '92',<br> 'relativeTime': '1585324800',<br> 'confirmedRelative': '1',<br> 'diedRelative': '0',<br> 'curedRelative': '0',<br> 'curConfirm': '5',<br> 'curConfirmRelative': '1',<br> 'icuDisable': '1',<br> 'area': '吉林',<br> 'subList': [{<!-- -->'city': '延边',<br>   'confirmed': '5',<br>   'died': '',<br>   'crued': '5',<br>   'confirmedRelative': '0',<br>   'curConfirm': '0',<br>   'cityCode': '54'},<br>  {<!-- -->'city': '四平',<br>   'confirmed': '15',<br>   'died': '1',<br>   'crued': '14',<br>   'confirmedRelative': '0',<br>   'curConfirm': '0',<br>   'cityCode': '56'},<br>  {<!-- -->'city': '通化',<br>   'confirmed': '6',<br>   'died': '',<br>   'crued': '6',<br>   'confirmedRelative': '0',<br>   'curConfirm': '0',<br>   'cityCode': '165'},...]}<br></code></pre> 
<p>       到这里可以发现Home_Lis下的前11个键值就是该省份数据，而该省各城市数据就在’subList’里面，对应的建就有：“累计确诊”，“新增确诊”，“累计治愈”，“新增治愈”，“累计死亡”，“新增死亡”，“累计确诊”，"新增确诊"等数据。此时对于一个省份我想生成这样的数据集：</p> 
<pre><code>   报道时间   国家  省分  城市 累计确诊 新增确诊 累计治愈 ...<br> 2020-03-29  中国  吉林   -      98     0     92    ...<br> 2020-03-29  中国  吉林  四平     5     0     14    ...<br> 2020-03-29  中国  吉林  通化     15    0     6    ...<br> ....<br></code></pre> 
<p>       也就是要生成时间列、国家列、省份，而且这些列的长度刚好等于每个’subList’下城市个数+1（因为Home_Lis下的前11个键值就是列表的第一行，这行就是该省的汇总行，也是该省的第一行，所以长度得多出一个），这样得到每个省份数据然后再拼接起来就是国内数据啦！！代码为：</p> 
<pre><code>date=datetime.datetime.now()  # 获取当地时间<br>start_date = '%s-%02d-%02d'%(date.year,date.month,date.day)<br>province=[Home_Lis[6]['area']]*(len(Home_Lis[6]['subList'])+1)   # 得到省份<br>dataloct=[start_date]*(len(Home_Lis[6]['subList'])+1)  # 为了扩展长度一致<br>country=["中国"]*(len(Home_Lis[6]['subList'])+1)<br>countren=["China"]*(len(Home_Lis[6]['subList'])+1)<br>city=re.findall(r"city: (.*?),", str(Home_Lis[6]['subList']).replace("'",''))  # 得到城市<br>city.insert(0,'-')<br>confirmed=re.findall(r"confirmed: (.*?),", str(Home_Lis[6]).replace("'",''))#得到累积确诊<br>died=re.findall(r"died: (.*?),", str(Home_Lis[6]).replace("'",'')) # 得到累积死亡<br>confirmedRelative=re.findall(r"confirmedRelative: (.*?),", str(Home_Lis[6]).replace("'",'')) # 得到新增确诊<br>Cure=re.findall(r"crued: (.*?),", str(Home_Lis[6]).replace("'",'')) # 得到累积治愈<br>curedRelative=re.findall(r"curedRelative: (.*?),", str(Home_Lis[6]).replace("'",'')) # 得到新增治愈<br>curedRelative.extend(['']*len(Home_Lis[6]['subList']))<br>diedRelative=re.findall(r"diedRelative: (.*?),", str(Home_Lis[6]).replace("'",'')) # 得到新增死亡<br>diedRelative.extend(['']*len(Home_Lis[6]['subList']))<br>requ=pd.DataFrame({<!-- -->'报道时间':dataloct,'国家':country,'国家EN':countren,'省份/国家(国外)':province,<br>                       '城市':city,'累积确诊':confirmed,'新增确诊':confirmedRelative,<br>                       '累积治愈':Cure,'新增治愈':curedRelative,'累积死亡':died,<br>                       '新增死亡':diedRelative})<br></code></pre> 
<h4>4.3 匹配综述</h4> 
<p>       到这里其实你想要的数据都可以获取到，只要正则表达式匹配的正确，按照同样的思路即可找出国内外的疫情数据，以及国内外疫情数据汇总，方法和思路都是一样的，当然这仅仅是我的方法和思路，还有很多种做法供大家发散思维，欢迎评论区探讨。</p> 
<h3>5、主函数[你可以直接运行]</h3> 
<p>       运行说明：1、当你打开<code>if __name__ == '__main__':</code>里面的“控制链1”和“控制链2”并且调整好缩进后，程序就会在固定每天晚上23:55:59运行该脚本下载当天的疫情数据，如果挂机30天将会下载一个整月的数据！如果不打卡这两个控制链，程序仅会运行一次且下载当天的疫情数据。结果会生成两个excel文档，“全球疫情汇总数据”和“全球疫情数据”，前者是每日都有的，后者是当天的，生成的路径是在指定的“D:\Data\epidemic_situation\疫情数据”下。</p> 
<pre><code>import pandas as pd<br>import requests<br>import urllib.request<br>import time<br>import datetime<br>import http.cookiejar as cj<br>import time<br>import os<br>import json<br>import ast<br>import xlwt<br>import re<br>import warnings<br><br>def catch_daily():<br>    """抓取每日确诊和死亡数据"""<br>    url = 'https://voice.baidu.com/act/newpneumonia/newpneumonia/?from=osari_pc_3'<br>    response = requests.get(url=url)<br>    datas=re.findall(r'id="captain-config"&gt;(.*?)"bundle"', response.text)# 得到初步  来源方式<br>    data=re.findall(r'(.*),"version"', datas[0])# 得到初步  来源方式  加？是贪婪模式 最短匹配<br>    data0 = 'u'+'\''+data[0].replace('\\\\', '\\')+'\''<br>    data2=eval(data0)<br>    return data2<br><br>def China_epid_situ(data3,start_date</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0054463b0d0cf73c86c7718c466528d9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">1103 Integer Factorization (30分)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/085f58e3700724c9b225b3ccb0c20895/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">洛谷P5703 【深基2.例5】苹果采购</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>