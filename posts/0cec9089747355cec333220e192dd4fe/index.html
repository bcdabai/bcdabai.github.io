<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>25FPS！全网首发 | 英伟达开放BEVFusion部署源代码，边缘端实时运行！！！ - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="25FPS！全网首发 | 英伟达开放BEVFusion部署源代码，边缘端实时运行！！！" />
<meta property="og:description" content="作者：手写ai书写未来，来源：集智书童，编辑：3D视觉工坊
英伟达开源BEVFusion边缘部署高达25FPS！
引言： 众所周知，雷达与相机的融合方案由于稀疏卷积的原因导致一直难以落地推广。而没有使用稀疏卷积的方案精度通常会差10-30%左右，并且目前的融合方案在Orin上的性能非常差（例如BEVFusion在3090Ti上只有8.6FPS）,这严重地制约了自动驾驶的落地表现。英伟达刚刚发布了部署BEVFusion的方案，以惊人的25FPS同时保持67.66的高精度mAP运行在Orin上，这对于雷达相机融合的感知落地来说，这是一项十分令人兴奋的工作！这意味着雷达感知的精度和速度将会得到大幅度的提升，并且轻易的部署！
代码发布在 https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution
一、仓库介绍 该仓库主要提供了3个模型（BEVFusion、CenterPoint、PointPillars）和5个libraries（sparse conv, quantization solution, cuPCL, cuOSD, YUVToRGB）
这些模块，通过下面的方式组织起来
下面是一个BEVFusion的执行效果
模型的精度和速度如下表（nuScenes验证集的平均速度）
可以看到，模型在PTQ量化后只有0.3%mAP的损失，是相当不错的
在保持高精度的同时还保持了高的推理速度，这对自动驾驶来说非常友好
仓库提供了模块化的实现，对于按照要求修改会比较友好
二、稀疏卷积(Sparse Convolution) 仓库还提供了稀疏卷积的解决方案，他的实现是独立于tensorRT之外的，纯cuda核函数调用TensorCore实现的一个库
它通过onnx文件加载模型并进行推理加速，轻量且高效
提供了FP16和INT8推理的支持。支持子流形和空间稀疏卷积
同时还提供了CenterPoint和BEVFusion的lidar导出以及PTQ/QAT保证速度和精度的解决方案
详细的精度指标如下
下面是在nuScenes上的耗时统计，稳定性非常好
三、BEVfusion介绍 BEVfusion框架图BEVFusion是一种用于自动驾驶系统的先进技术。它代表着鸟瞰融合（Bird&#39;s Eye View Fusion），是一个多任务、多传感器融合框架，极大地提高了自动驾驶汽车的性能[2]。
四、落地部署的挑战 1. ONNX导出复杂 BEVFusion是一种涉及到SparseConv（稀疏卷积）层和自定义CUDA操作（如BEV池化）等复杂特性的模型[4]。这些特性对于转换到 ONNX，以及后续在TensorRT上部署都构成了难题。
我们简单地总结了一下难以转ONNX的主要原因：
（1）自定义CUDA操作：BEVFusion使用的自定义CUDA操作在ONNX模型导出中并不被原生支持[5]。这意味着这些操作需要手动在ONNX中注册为自定义操作，这是一项具有挑战性的任务。
（2）SparseConv层：在BEVFusion中使用的SparseConv层并不被ONNX原生支持[6]。这使得转换过程变得复杂，因为需要手动处理这些层。
2.Plugin和BEVpooling效率低下 制约BEVfusion广泛落地的另一个原因就是其Plugin和BEVpooling的效率不够高。BEVFusion中的BEVPool使用CUDA来加速，但它仍然需要计算、存储和预处理视锥特征(frustum feature)，这在内存和计算上都十分密集[7]。
尽管BEVfusion在框架设计上已经进行了优化，相比于以前的研究，以更高的精度和降低1.9倍[8]的计算成本在nuscenes上设立了新的基准。但是，对于自动驾驶这种高速场景来说，我们应该追求更高的性能，以确保安全驾驶。因此，对某些操作的计算强度进行进一步优化是非常有价值的。
3.项目工程相对复杂 比起普通的单传感器感知，BEVfusion工程也相对复杂，主要体现在下面几个方面：
（1）多任务多传感器融合框架：BEVFusion被设计为一个高效且通用的多任务多传感器融合框架。它在共享的鸟瞰视图（BEV）表示空间中统一多模态特征，这既保留了几何信息也保留了语义信息[9]。这种任务和方法的复杂性直接反映在仓库的结构和内容上。
（2）坐标系统的修改和体素化（Coordinate system modification and voxelization）：BEVFusion项目对其实现进行了重大更改，例如从zyx体素化切换到xyz体素化。在一些头部，xy BEV坐标与先前的实现相比进行了转置。这可能会增加仓库的复杂性，尤其是对熟悉先前版本的人来说[10]。
4. 总结 尽管BEVFusion确实提供了一种新颖且有效的方式来处理多传感器融合任务，但是在实际部署过程中仍存在以上的挑战。包括ONNX导出复杂、插件和BEVpooling效率低下以及项目工程相对复杂等问题。这些问题需要我们在实践中寻找解决方案。而这，正是英伟达提供的方案致力于解决的，新的部署方案在保持67.66的高精度mAP的同时，实现了在Orin上以惊人的25FPS运行，这对雷达和相机的融合感知落地来说无疑是一次重大突破。这项工作不仅证明了BEVFusion的实用性，还展示了其潜力，即在高精度和高效率之间找到平衡，以满足自动驾驶等高速场景的需求。
reference [1] &#34;Aug 16, 2022 · Multi-sensor fusion is essential for an accurate and reliable autonomous driving system URL: https://github." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0cec9089747355cec333220e192dd4fe/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-24T07:02:46+08:00" />
<meta property="article:modified_time" content="2023-07-24T07:02:46+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">25FPS！全网首发 | 英伟达开放BEVFusion部署源代码，边缘端实时运行！！！</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>作者：手写ai书写未来，来源：集智书童，编辑：3D视觉工坊</p> 
 <p>英伟达开源BEVFusion边缘部署高达25FPS！</p> 
 <h3>引言：</h3> 
 <p>众所周知，雷达与相机的融合方案由于稀疏卷积的原因导致一直难以落地推广。而没有使用稀疏卷积的方案精度通常会差10-30%左右，并且目前的融合方案在Orin上的性能非常差（例如BEVFusion在3090Ti上只有8.6FPS）,这严重地制约了自动驾驶的落地表现。英伟达刚刚发布了部署BEVFusion的方案，以惊人的25FPS同时保持67.66的高精度mAP运行在Orin上，这对于雷达相机融合的感知落地来说，这是一项十分令人兴奋的工作！这意味着雷达感知的精度和速度将会得到大幅度的提升，并且轻易的部署！</p> 
 <p>代码发布在 <strong>https://github.com/NVIDIA-AI-IOT/Lidar_AI_Solution</strong></p> 
 <h3>一、仓库介绍</h3> 
 <img src="https://images2.imgbox.com/3b/7d/kUfCxvCU_o.png" alt="3a8a67cd769dc9a74fa69847ccf67c8a.png"> 
 <ul><li><p>该仓库主要提供了3个模型（BEVFusion、CenterPoint、PointPillars）和5个libraries（sparse conv, quantization solution, cuPCL, cuOSD, YUVToRGB）<img src="https://images2.imgbox.com/af/64/tdStSd5t_o.png" alt="0dded1c8cd74bab5b7ccbfcd1606559b.png"></p></li><li><p>这些模块，通过下面的方式组织起来<img src="https://images2.imgbox.com/66/54/5Ck2Da6t_o.png" alt="49ba0edc3f05b98119e20490a7bcbce6.png"></p></li><li><p>下面是一个BEVFusion的执行效果</p></li><li><p style="text-align:center;"><img src="https://images2.imgbox.com/5d/3c/tug5fr0h_o.gif" alt="4bd269d5ecbfd5444f4433f1c116f459.gif"></p></li><li><p>模型的精度和速度如下表（nuScenes验证集的平均速度）<img src="https://images2.imgbox.com/3e/8d/JPBaHwev_o.png" alt="715d52ef712f09dfa6cb460a4ebc1f5d.png"></p></li><li><p>可以看到，模型在PTQ量化后只有0.3%mAP的损失，是相当不错的</p></li><li><p>在保持高精度的同时还保持了高的推理速度，这对自动驾驶来说非常友好</p></li><li><p>仓库提供了模块化的实现，对于按照要求修改会比较友好<img src="https://images2.imgbox.com/c0/d9/UQFjhcX0_o.png" alt="b9320fe32c12828fb6f99cae29cc4939.png"></p></li></ul> 
 <h3>二、稀疏卷积(Sparse Convolution)</h3> 
 <ul><li><p>仓库还提供了稀疏卷积的解决方案，他的实现是独立于tensorRT之外的，纯cuda核函数调用TensorCore实现的一个库</p></li><li><p>它通过onnx文件加载模型并进行推理加速，轻量且高效</p></li><li><p>提供了FP16和INT8推理的支持。支持子流形和空间稀疏卷积</p></li><li><p>同时还提供了CenterPoint和BEVFusion的lidar导出以及PTQ/QAT保证速度和精度的解决方案<img src="https://images2.imgbox.com/37/df/u7xaLQqH_o.png" alt="29f8a0cfde9cacd7e5d543e018bdf9e4.png"></p></li><li><p>详细的精度指标如下<img src="https://images2.imgbox.com/46/2e/R6A9Fmin_o.png" alt="6626e8d476f2a1efc518f1c2bc83e644.png"></p></li><li><p>下面是在nuScenes上的耗时统计，稳定性非常好<img src="https://images2.imgbox.com/22/bd/YPbj8xJ6_o.png" alt="b7cbade2a259220b12c2b0a6de615477.png"></p></li></ul> 
 <h3>三、BEVfusion介绍</h3> 
 <img src="https://images2.imgbox.com/51/9b/z3fng9Jq_o.png" alt="800efcec21e60efb63eea110c072920d.png"> 
  
 <p>BEVfusion框架图BEVFusion是一种用于自动驾驶系统的先进技术。它代表着鸟瞰融合（Bird's Eye View Fusion），是一个多任务、多传感器融合框架，极大地提高了自动驾驶汽车的性能[2]。</p> 
 <h3>四、落地部署的挑战</h3> 
 <h5>1. ONNX导出复杂</h5> 
 <p>BEVFusion是一种涉及到SparseConv（稀疏卷积）层和自定义CUDA操作（如BEV池化）等复杂特性的模型[4]。这些特性对于转换到 ONNX，以及后续在TensorRT上部署都构成了难题。</p> 
 <p>我们简单地总结了一下难以转ONNX的主要原因：</p> 
 <ul><li><p>（1）自定义CUDA操作：BEVFusion使用的自定义CUDA操作在ONNX模型导出中并不被原生支持[5]。这意味着这些操作需要手动在ONNX中注册为自定义操作，这是一项具有挑战性的任务。</p></li><li><p>（2）SparseConv层：在BEVFusion中使用的SparseConv层并不被ONNX原生支持[6]。这使得转换过程变得复杂，因为需要手动处理这些层。</p></li></ul> 
 <h5>2.Plugin和BEVpooling效率低下</h5> 
 <p>制约BEVfusion广泛落地的另一个原因就是其Plugin和BEVpooling的效率不够高。BEVFusion中的BEVPool使用CUDA来加速，但它仍然需要计算、存储和预处理视锥特征(frustum feature)，这在内存和计算上都十分密集[7]。</p> 
 <p>尽管BEVfusion在框架设计上已经进行了优化，相比于以前的研究，以更高的精度和降低1.9倍[8]的计算成本在nuscenes上设立了新的基准。但是，对于自动驾驶这种高速场景来说，我们应该追求更高的性能，以确保安全驾驶。因此，对某些操作的计算强度进行进一步优化是非常有价值的。</p> 
 <h5>3.项目工程相对复杂</h5> 
 <p>比起普通的单传感器感知，BEVfusion工程也相对复杂，主要体现在下面几个方面：</p> 
 <ul><li><p>（1）多任务多传感器融合框架：BEVFusion被设计为一个高效且通用的多任务多传感器融合框架。它在共享的鸟瞰视图（BEV）表示空间中统一多模态特征，这既保留了几何信息也保留了语义信息[9]。这种任务和方法的复杂性直接反映在仓库的结构和内容上。</p></li><li><p>（2）坐标系统的修改和体素化（Coordinate system modification and voxelization）：BEVFusion项目对其实现进行了重大更改，例如从zyx体素化切换到xyz体素化。在一些头部，xy BEV坐标与先前的实现相比进行了转置。这可能会增加仓库的复杂性，尤其是对熟悉先前版本的人来说[10]。</p></li></ul> 
 <h5>4. 总结</h5> 
 <p>尽管BEVFusion确实提供了一种新颖且有效的方式来处理多传感器融合任务，但是在实际部署过程中仍存在以上的挑战。包括ONNX导出复杂、插件和BEVpooling效率低下以及项目工程相对复杂等问题。这些问题需要我们在实践中寻找解决方案。而这，正是英伟达提供的方案致力于解决的，新的部署方案在保持67.66的高精度mAP的同时，实现了在Orin上以惊人的25FPS运行，这对雷达和相机的融合感知落地来说无疑是一次重大突破。这项工作不仅证明了BEVFusion的实用性，还展示了其潜力，即在高精度和高效率之间找到平衡，以满足自动驾驶等高速场景的需求。</p> 
 <h3>reference</h3> 
 <p>[1] "Aug 16, 2022 ·  Multi-sensor fusion is essential for an accurate and reliable autonomous driving system URL: https://github.com/mit-han-lab/bevfusion</p> 
 <p>[2] "In this paper, we break this deeply-rooted convention with BEVFusion URL: https://bevfusion.mit.edu/</p> 
 <p>[3] "IEI-BEVFusion++. This method is built based on the BEVFusion URL: https://github.com/IEI-AP/IEI-BEVFusion_plus_plus</p> 
 <p>[4] "Jul 7, 2022 ·  It might be hard to export BEVFusion to onnx currently" URL: https://github.com/mit-han-lab/bevfusion/issues/60</p> 
 <p>[5] "Jun 4, 2022 ·  It will be a bit hard to export the current model to ONNX because of the existence of custom CUDA ops URL: https://github.com/mit-han-lab/bevfusion/issues/18</p> 
 <p>[6] "Nov 29, 2022 ·  Onnx export might be very challenging since sparse convolution operators are not natively supported URL: https://github.com/mit-han-lab/bevfusion/issues/26</p> 
 <p>[7] "of cumulative sum, BEVPool in BEVFusion URL: https://arxiv.org/pdf/2211.17111v1.pdf</p> 
 <p>[8] "May 26, 2022 ·  BEVFusion is fundamentally URL: https://paperswithcode.com/paper/bevfusion-multi-task-multi-sensor-fusion-with</p> 
 <p>[9] "Aug 16, 2022 ·  we break this deeply-rooted convention with BEVFusion URL: https://github.com/mit-han-lab/bevfusion</p> 
 <p>[10] "Oct 4, 2022 ·  The most important change other than the coordinate system URL: https://github.com/mit-han-lab/bevfusion/issues/163</p> 
 <p>标签：#英伟达 #稀疏卷积 #BEVFusion #自动驾驶 #技术突破</p> 
 <p>注：以上内容为英伟达最新的研究成果，具体技术细节和应用可能会有更深入的探索和改进。欢迎关注我们的微信公众号，获取更多的科技资讯和深度解析！</p>—END— 
 <h4>高效学习3D视觉三部曲</h4> 
 <h5>第一步 加入行业交流群，保持技术的先进性</h5> 
 <p style="text-align:left;">目前工坊已经建立了3D视觉方向多个社群，包括SLAM、工业3D视觉、自动驾驶方向，细分群包括：[<strong>工业方向</strong>]三维点云、结构光、机械臂、缺陷检测、三维测量、TOF、相机标定、综合群；[<strong>SLAM方向</strong>]多传感器融合、ORB-SLAM、激光SLAM、机器人导航、RTK|GPS|UWB等传感器交流群、SLAM综合讨论群；[<strong>自动驾驶方向</strong>]深度估计、Transformer、毫米波|激光雷达|视觉摄像头传感器讨论群、多传感器标定、自动驾驶综合群等。[<strong>三维重建方向</strong>]NeRF、colmap、OpenMVS等。除了这些，还有求职、硬件选型、视觉产品落地等交流群。大家可以添加小助理微信: dddvisiona，备注：加群+方向+学校|公司, 小助理会拉你入群。</p> 
 <img src="https://images2.imgbox.com/20/28/0N9T6hPr_o.png" alt="2503acb432be9e0511e88903e46f424b.jpeg"> 
 <figcaption></figcaption> 
 <figcaption>
   添加小助理微信： 
   dddvisiona 
  ，拉你入群 
   
 </figcaption> 
 <h5>第二步 加入知识星球，问题及时得到解答</h5> 
 <p style="text-align:left;">针对3D视觉领域的视频课程（三维重建、三维点云、结构光、手眼标定、相机标定、激光/视觉SLAM、自动驾驶等）、源码分享、知识点汇总、入门进阶学习路线、最新paper分享、<strong>疑问解答</strong>等进行深耕，更有各类大厂的算法工程人员进行技术指导。与此同时，星球将联合知名企业发布3D视觉相关算法开发岗位以及项目对接信息，打造成集技术与就业、项目对接为一体的铁杆粉丝聚集区，6000+星球成员为创造更好的AI世界共同进步，知识星球入口：<a href="" rel="nofollow">「3D视觉从入门到精通」</a></p>学习3D视觉核心技术，扫描查看，3天内无条件退款 
 <img src="https://images2.imgbox.com/ed/16/blxAxqNO_o.png" alt="ceb0d3ab36cb90a21f8f07cdb0a31820.jpeg"> 
 <figcaption>
   高质量教程资料、答疑解惑、助你高效解决问题 
   
 </figcaption> 
 <h5>第三步 系统学习3D视觉，对模块知识体系，深刻理解并运行</h5> 
 <p style="text-align:left;">如果大家对3D视觉某一个细分方向想系统学习[从理论、代码到实战],推荐3D视觉精品课程学习网址：www.3dcver.com</p> 
 <p style="text-align:left;">基础课程：</p> 
 <p style="text-align:left;">[1]<a href="" rel="nofollow">面向三维视觉算法的C++重要模块精讲：从零基础入门到进阶</a></p> 
 <p style="text-align:left;">[2]<a href="" rel="nofollow">如何学习相机模型与标定？（代码+实战）</a></p> 
 <p style="text-align:left;">[3]<a href="" rel="nofollow">ROS2从入门到精通：理论与实战</a></p> 
 <p style="text-align:left;">工业3D视觉方向课程：</p> 
 <p style="text-align:left;">[1]<a href="" rel="nofollow">（第二期）从零搭建一套结构光3D重建系统[理论+源码+实践]</a></p> 
 <p style="text-align:left;">[2] <a href="" rel="nofollow">保姆级线结构光（单目&amp;双目）三维重建系统教程</a></p> 
 <p style="text-align:left;">[3]<a href="" rel="nofollow">机械臂抓取从入门到实战课程（理论+源码）</a></p> 
 <p style="text-align:left;">[4]<a href="" rel="nofollow">三维点云处理：算法与实战汇总</a></p> 
 <p style="text-align:left;">[5]<a href="" rel="nofollow">彻底搞懂基于Open3D的点云处理教程！</a></p> 
 <p style="text-align:left;">[6]<a href="" rel="nofollow">3D视觉缺陷检测教程：理论与实战！</a></p> 
 <p style="text-align:left;">SLAM方向课程:</p> 
 <p style="text-align:left;">[1]<a href="" rel="nofollow">如何高效学习基于LeGo-LOAM框架的激光SLAM?</a></p> 
 <p style="text-align:left;">[1]<a href="" rel="nofollow">彻底剖析激光-视觉-IMU-GPS融合SLAM算法：理论推导、代码讲解和实战</a></p> 
 <p style="text-align:left;">[2]<a href="" rel="nofollow">（第二期）彻底搞懂基于LOAM框架的3D激光SLAM：源码剖析到算法优化</a></p> 
 <p style="text-align:left;">[3]<a href="" rel="nofollow">彻底搞懂视觉-惯性SLAM：VINS-Fusion原理精讲与源码剖析</a></p> 
 <p style="text-align:left;">[4]<a href="" rel="nofollow">彻底剖析室内、室外激光SLAM关键算法和实战(cartographer+LOAM+LIO-SAM)</a></p> 
 <p style="text-align:left;">[5]<a href="" rel="nofollow">ORB-SLAM3理论讲解与代码精析（第2期）</a></p> 
 <p style="text-align:left;">视觉三维重建</p> 
 <p style="text-align:left;">[1]<a href="" rel="nofollow">彻底搞透视觉三维重建：原理剖析、代码讲解、及优化改进</a>)</p> 
 <p style="text-align:left;">自动驾驶方向课程：</p> 
 <p style="text-align:left;">[1] <a href="" rel="nofollow">深度剖析面向自动驾驶领域的车载传感器空间同步（标定）</a></p> 
 <p style="text-align:left;">[2]<a href="" rel="nofollow">面向自动驾驶领域目标检测中的视觉Transformer</a></p> 
 <p style="text-align:left;">[3]<a href="" rel="nofollow">单目深度估计方法：算法梳理与代码实现</a></p> 
 <p style="text-align:left;">[4]<a href="" rel="nofollow">面向自动驾驶领域的3D点云目标检测全栈学习路线！(单模态+多模态/数据+代码)</a></p> 
 <p style="text-align:left;">[5]<a href="" rel="nofollow">如何将深度学习模型部署到实际工程中？（分类+检测+分割）</a></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/96e8ce7de03e9e7b5d94023e87aa3b44/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">TypeScript中Class Interface Type的定义和区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6b1dafac192b7a58f0f470dacd420bd4/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">液相色谱柱柱压升高原因及解决办法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>