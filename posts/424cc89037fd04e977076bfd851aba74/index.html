<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>flink-addSource和addSink分别是kafka、自定义数据、mysql、hbase的java实现 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="flink-addSource和addSink分别是kafka、自定义数据、mysql、hbase的java实现" />
<meta property="og:description" content="flink主程序 public class FinkTest { public static void main(String[] args) throws Exception{ StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);//摄入时间 //env.enableCheckpointing(5000); //创建kafka-topic Properties p = LoadResourcesUtils.getProperties(&#34;kafka.properties&#34;); String inputTopic = p.getProperty(&#34;source.inputTopic&#34;); String outputTopic = p.getProperty(&#34;source.outputTopic&#34;); //kafka addSource DataStream&lt;String&gt; kafkaStream = env.addSource(KafkaStreamBuilder.kafkaConsumer(inputTopic)); //kafka addSink kafkaStream.addSink(KafkaSink.KafkaProducer(driversTopicPattern)); //mysql addSink kafkaStream.addSink(new OrderMySqlSink()); //hbase addSink kafkaStream..addSink(new HbaseSink(configs.topicOut)); //自定义 addSource DataStream&lt;String&gt; myStream = env.addSource(new MySource()); //mysql addSource DataStream&lt;String&gt; driverStream = env.addSource(new MySqlSource()); env.execute(&#34;my flink job&#34;); } } addSource(kafka) import org." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/424cc89037fd04e977076bfd851aba74/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-25T10:07:49+08:00" />
<meta property="article:modified_time" content="2021-05-25T10:07:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">flink-addSource和addSink分别是kafka、自定义数据、mysql、hbase的java实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="flink_0"></a>flink主程序</h2> 
<pre><code>public class FinkTest {

    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);//摄入时间
        //env.enableCheckpointing(5000);
        //创建kafka-topic
        Properties p = LoadResourcesUtils.getProperties("kafka.properties");
        String inputTopic = p.getProperty("source.inputTopic");
        String outputTopic = p.getProperty("source.outputTopic");

        //kafka addSource
        DataStream&lt;String&gt; kafkaStream = env.addSource(KafkaStreamBuilder.kafkaConsumer(inputTopic));
        
        //kafka addSink
        kafkaStream.addSink(KafkaSink.KafkaProducer(driversTopicPattern));
        
        //mysql addSink
        kafkaStream.addSink(new OrderMySqlSink());
        
        //hbase addSink
       kafkaStream..addSink(new HbaseSink(configs.topicOut));
       
		//自定义 addSource
        DataStream&lt;String&gt; myStream = env.addSource(new MySource());

		//mysql addSource
        DataStream&lt;String&gt; driverStream = env.addSource(new MySqlSource());
        
        env.execute("my flink job");

    }
}
</code></pre> 
<h2><a id="addSourcekafka_37"></a>addSource(kafka)</h2> 
<pre><code>import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Properties;

/**
 * @author liquan
 *
 *	构建KafkaStream
 *
 */
public class KafkaStreamBuilder {

	public static FlinkKafkaConsumer&lt;String&gt; kafkaConsumer(String topics) {
		Properties p = LoadResourcesUtils.getProperties("application.properties");
		Properties properties = new Properties();
		properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringDeserializer");
		properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringDeserializer");
		properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, p.getProperty("spring.kafka.bootstrap-servers"));
		properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, p.getProperty("spring.kafka.consumer.group-id"));
		properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, p.getProperty("spring.kafka.consumer.auto-offset-reset"));
		properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, p.getProperty("spring.kafka.consumer.enable-auto-commit"));
		properties.setProperty(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, "org.apache.kafka.clients.consumer.RangeAssignor");
//		String topics = consumerConfig.getTopics();
		List&lt;String&gt; topicsSet = new ArrayList&lt;String&gt;(Arrays.asList(topics.split(",")));
		FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;String&gt;(topicsSet, new SimpleStringSchema(),
				properties);//test0是kafka中开启的topic
//		myConsumer.assignTimestampsAndWatermarks(new CustomWatermarkEmitter());
		return myConsumer;
	}
}
</code></pre> 
<h2><a id="addSinkkafka_76"></a>addSink(kafka)</h2> 
<pre><code>import com.shengekeji.simulator.serialization.OutSerializationSchema;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.kafka.clients.consumer.ConsumerConfig;

import java.util.Properties;

public class KafkaSink {

    public static FlinkKafkaProducer&lt;String&gt; KafkaProducer(String topics) {
        Properties p = LoadResourcesUtils.getProperties("application.properties");
        Properties properties = new Properties();
        properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,"org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, p.getProperty("spring.kafka.bootstrap-servers"));
        properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, p.getProperty("spring.kafka.consumer.group-id"));
        properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, p.getProperty("spring.kafka.consumer.auto-offset-reset"));
        properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, p.getProperty("spring.kafka.consumer.enable-auto-commit"));
        properties.setProperty(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, "org.apache.kafka.clients.consumer.RangeAssignor");

        return new FlinkKafkaProducer&lt;&gt;(topics, new OutSerializationSchema(), properties);
    }
}
</code></pre> 
<h2><a id="addSinkmysql_103"></a>addSink(mysql)</h2> 
<pre><code>import com.alibaba.fastjson.JSONObject;
import com.shengekeji.simulator.dao.OrderDao;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import org.apache.ibatis.session.SqlSession;
import com.shengekeji.simulator.model.OrderModel;

public class OrderMySqlSink extends RichSinkFunction&lt;String&gt; {

    @Override
    public void invoke(String value, Context context) throws Exception {
        SqlSession sqlSession = null;
        try {
            OrderModel order= JSONObject.parseObject(value, OrderModel.class);
            sqlSession = MyBatisUtil.openSqlSession();
            // 通过SqlSession对象得到Mapper接口的一个代理对象
            // 需要传递的参数是Mapper接口的类型
            OrderDao dao = sqlSession.getMapper(OrderDao.class);
            System.err.println(order);
            dao.insert(order);
            sqlSession.commit();

        }catch (Exception e){
            e.printStackTrace();
            System.err.println(e.getMessage());
            sqlSession.rollback();

        }finally {

            if (sqlSession != null){
                sqlSession.close();
            }
        }
    }
}
</code></pre> 
<p>注，数据入库时用的mybatis方式，MyBatisUtil,OrderDao,OrderModel根据自己环境自己定义</p> 
<h2><a id="addSource_143"></a>addSource(自定义)</h2> 
<pre><code>import org.apache.flink.streaming.api.functions.source.SourceFunction;

import java.util.Properties;

public class MySource implements SourceFunction&lt;String&gt; {

    private static final long serialVersionUID = 1L;

    private volatile boolean isRunning = true;


    @Override
    public void run(SourceContext&lt;String&gt; sourceContext) throws Exception {
        while(this.isRunning) {
            Thread.sleep(6000);
            String order = getDriverData();
            sourceContext.collect(order);
        }
    }

    @Override
    public void cancel() {
        this.isRunning = false;
    }

    //随机产生订单数据
    public String getDriverData() {
        Properties p = LoadResourcesUtils.getProperties("content.properties");

        String driverJson = p.getProperty("source.driverJson");

        String value = driverJson ;
        if(value.indexOf("%orderId") &gt;= 0){
            value = value.replaceAll("%orderId",RandomUtil.getOrderId());
        }
        if(value.indexOf("%appId") &gt;= 0){
            value = value.replaceAll("%appId",RandomUtil.getAppId());
        }
        if(value.indexOf("%serviceId") &gt;= 0){
            value = value.replaceAll("%serviceId",RandomUtil.getServiceId());
        }
        if(value.indexOf("%passageId") &gt;= 0){
            value = value.replaceAll("%passageId",RandomUtil.getPassageId());
        }
        if(value.indexOf("%driverId") &gt;= 0){
            value = value.replaceAll("%driverId",RandomUtil.getDriverId());
        }
        if(value.indexOf("%startLoclatitude") &gt;= 0){
            LngLat startLoc=RandomUtil.getCoordinate();
            value = value.replaceAll("%startLoclatitude",Double.toString(startLoc.latitude));
            value = value.replaceAll("%startLoclongitude",Double.toString(startLoc.longitude));
        }
        if(value.indexOf("%endLoclatitude") &gt;= 0){
            LngLat endLoc=RandomUtil.getCoordinate();
            value = value.replaceAll("%endLoclatitude",Double.toString(endLoc.latitude));
            value = value.replaceAll("%endLoclongitude",Double.toString(endLoc.longitude));
        }
        if(value.indexOf("%loclatitude") &gt;= 0){
            LngLat loc=RandomUtil.getCoordinate();
            value = value.replaceAll("%loclatitude",Double.toString(loc.latitude));
            value = value.replaceAll("%loclongitude",Double.toString(loc.longitude));
        }
        if(value.indexOf("%flag") &gt;= 0){
            value = value.replaceAll("%flag",Integer.toString(RandomUtil.getFlag()));
        }
        if(value.indexOf("%pushFlag") &gt;= 0){
            value = value.replaceAll("%pushFlag",Integer.toString(RandomUtil.getPushFlag()));
        }
        if(value.indexOf("%state") &gt;= 0){
            value = value.replaceAll("%state",Integer.toString(RandomUtil.getState()));
        }
        if(value.indexOf("%d") &gt;= 0){
            value = value.replaceAll("%d", RandomUtil.getNum().toString());
        }
        if(value.indexOf("%s") &gt;= 0){
            value = value.replaceAll("%s", RandomUtil.getStr());
        }
        if(value.indexOf("%f") &gt;= 0){
            value = value.replaceAll("%f",RandomUtil.getDoubleStr());
        }
        if(value.indexOf("%ts") &gt;= 0){
            value = value.replaceAll("%ts",RandomUtil.getTimeStr());
        }
        if(value.indexOf("%tl") &gt;= 0){
            value = value.replaceAll("%tl",RandomUtil.getTimeLongStr());
        }

        System.out.println(value);

        return value;
    }
}
</code></pre> 
<h2><a id="addSourcemysql_239"></a>addSource(mysql)</h2> 
<pre><code>import com.alibaba.fastjson.JSONObject;
import com.alibaba.fastjson.serializer.SerializerFeature;
import com.shengekeji.simulator.dao.BestDispatchDao;
import com.shengekeji.simulator.dao.DriverDao;
import com.shengekeji.simulator.model.DispatchModel;
import com.shengekeji.simulator.model.DriverModel;
import com.shengekeji.simulator.model.GeographyOrder;
import com.shengekeji.simulator.model.PassagesModel;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.ibatis.session.SqlSession;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;


public class MySqlDriverSource implements SourceFunction&lt;String&gt; {

    private static final long serialVersionUID = 1L;

    private volatile boolean isRunning = true;

    /**
     * 此处是代码的关键，要从mysql表中，把数据读取出来
     * @param sourceContext
     * @throws Exception
     */

    @Override
    public void run(SourceContext&lt;String&gt; sourceContext) throws Exception {
        while(this.isRunning) {
            Thread.sleep(5000);
            System.out.println("--------------------------");
            SqlSession sqlSession = null;
            Map&lt;String,Object&gt; map = new HashMap&lt;String, Object&gt;();
            map.put("appId","SGKJ");
            try {
                sqlSession = MyBatisUtil.openSqlSession();
                // 通过SqlSession对象得到Mapper接口的一个代理对象
                // 需要传递的参数是Mapper接口的类型
                //司机信息数据
                DriverDao driverdao = sqlSession.getMapper(DriverDao.class);
                List&lt;DriverModel&gt; drivers = driverdao.selectAllActiveDriver(map);
                //处理每个司机
                for (DriverModel driver:drivers){
                    driver.setLoc(new LngLat(locLongitude,locLatitude));
                    driver.setSendTime(RandomUtil.getTimeStr());
                    String dr = JSONObject.toJSONString(driver, SerializerFeature.DisableCircularReferenceDetect);
                    System.out.println(dr);
                    sourceContext.collect(dr);
                }

            }catch (Exception e){
                e.printStackTrace();
                System.err.println(e.getMessage());
                sqlSession.rollback();
            }finally {

                if (sqlSession != null){
                    sqlSession.close();
                }

            }
        }
    }

    @Override
    public void cancel() {
        this.isRunning = false;
    }

}
</code></pre> 
<p>注，数据读取用的mybatis方式，MyBatisUtil,DriverDao,DriverModel根据自己环境自己定义</p> 
<h2><a id="addSinkhbase_318"></a>addSink(hbase)</h2> 
<pre><code>import com.shengekeji.owl.constant.Constants;
import com.shengekeji.owl.pojo.Message;
import com.shengekeji.owl.util.HBaseUtil;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;

import java.util.ArrayList;
import java.util.List;

public class HbaseSink extends RichSinkFunction&lt;Message&gt; {
    private Integer maxSize = 1000;
    private Long delayTime = 5000L;
    private String tableName;

    public HbaseSink(String tableName) {
        this.tableName = tableName;
    }

    public HbaseSink(Integer maxSize, Long delayTime) {
        this.maxSize = maxSize;
        this.delayTime = delayTime;
    }

    private Connection connection;
    private Long lastInvokeTime;
    private List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;();

    // 创建连接
    @Override
    public void open(Configuration parameters) throws Exception {
        super.open(parameters);

        connection = HBaseUtil.getConnection(Constants.ZOOKEEPER_QUORUM,Constants.ZOOKEEPER_PORT);
        // 获取系统当前时间
        lastInvokeTime = System.currentTimeMillis();
    }

    @Override
    public void invoke(Message value, Context context) throws Exception {

        System.out.println(value);
        String rk = value.id+"-"+value.ts;
        //创建put对象，并赋rk值
        Put put = new Put(rk.getBytes());

        // 添加值：f1-&gt;列族, order-&gt;属性名 如age， 第三个-&gt;属性值 如25
        put.addColumn("cf1".getBytes(), "id".getBytes(), value.id.getBytes());
        put.addColumn("cf1".getBytes(), "vals".getBytes(), value.vals.getBytes());
        put.addColumn("cf1".getBytes(), "p".getBytes(), (value.p+"").getBytes());
        put.addColumn("cf1".getBytes(), "ts".getBytes(), (value.ts+"").getBytes());
        System.out.println("----------");
        System.out.println(put);
        puts.add(put);// 添加put对象到list集合

        //使用ProcessingTime
        long currentTime = System.currentTimeMillis();

        System.out.println(currentTime - lastInvokeTime);
        //开始批次提交数据
        if (puts.size() == maxSize || currentTime - lastInvokeTime &gt;= delayTime) {

            //获取一个Hbase表
            Table table = connection.getTable(TableName.valueOf(tableName));
            table.put(puts);//批次提交

            puts.clear();

            lastInvokeTime = currentTime;
            table.close();
        }
    }

    @Override
    public void close() throws Exception {
        connection.close();
    }

}
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9b31a4673da14e3d30d34194b2413dec/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java封装OkHttp3工具类</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/878f79e752cb95cf674c68bf078a49a0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">c语言二级最后一道题运行是灰色的,2003年4月全国计算机等级考试二级C语言笔试试题及答案...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>