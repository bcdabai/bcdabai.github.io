<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ceph的一些学习过程 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ceph的一些学习过程" />
<meta property="og:description" content="内容比较杂，个人学习ceph的一些理解。
集群运行图 Ceph 依赖于 Ceph 客户端和 OSD ，因为它们知道集群的拓扑，这个拓扑由 5 张图共同描述，统称为“集群运行图”：
Montior Map： 包含集群的 fsid 、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监视器图，用 ceph mon dump 命令。OSD Map： 包含集群 fsid 、创建时间、最近修改时间、存储池列表、副本数量、归置组数量、 OSD 列表及其状态（如 up 、 in ）。要查看OSD运行图，用 ceph osd dump 命令。PG Map：：** 包含归置组版本、其时间戳、最新的 OSD 运行图版本、占满率、以及各归置组详情，像归置组 ID 、 up set 、 acting set 、 PG 状态（如 active&#43;clean ），和各存储池的数据使用情况统计。CRUSH Map：：** 包含存储设备列表、故障域树状结构（如设备、主机、机架、行、房间、等等）、和存储数据时如何利用此树状结构的规则。要查看 CRUSH 规则，执行 ceph osd getcrushmap -o {filename} 命令；然后用 crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename} 反编译；然后就可以用 cat 或编辑器查看了。MDS Map： 包含当前 MDS 图的版本、创建时间、最近修改时间，还包含了存储元数据的存储池、元数据服务器列表、还有哪些元数据服务器是 up 且 in 的。要查看 MDS 图，执行 ceph mds dump 。 Ceph 监视器维护着集群运行图的主副本。一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向 Ceph 监视器索取集群运行图的最新副本。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e2d95db597f64faed1186c73c04f84a9/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-18T13:12:23+08:00" />
<meta property="article:modified_time" content="2020-06-18T13:12:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ceph的一些学习过程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>内容比较杂，个人学习ceph的一些理解。</p> 
<h4><a id="_2"></a>集群运行图</h4> 
<p>Ceph 依赖于 Ceph 客户端和 OSD ，因为它们知道集群的拓扑，这个拓扑由 5 张图共同描述，统称为“集群运行图”：</p> 
<ol><li>Montior Map： 包含集群的 fsid 、位置、名字、地址和端口，也包括当前版本、创建时间、最近修改时间。要查看监视器图，用 ceph mon dump 命令。</li><li>OSD Map： 包含集群 fsid 、创建时间、最近修改时间、存储池列表、副本数量、归置组数量、 OSD 列表及其状态（如 up 、 in ）。要查看OSD运行图，用 ceph osd dump 命令。</li><li>PG Map：：** 包含归置组版本、其时间戳、最新的 OSD 运行图版本、占满率、以及各归置组详情，像归置组 ID 、 up set 、 acting set 、 PG 状态（如 active+clean ），和各存储池的数据使用情况统计。</li><li>CRUSH Map：：** 包含存储设备列表、故障域树状结构（如设备、主机、机架、行、房间、等等）、和存储数据时如何利用此树状结构的规则。要查看 CRUSH 规则，执行 ceph osd getcrushmap -o {filename} 命令；然后用 crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename} 反编译；然后就可以用 cat 或编辑器查看了。</li><li>MDS Map： 包含当前 MDS 图的版本、创建时间、最近修改时间，还包含了存储元数据的存储池、元数据服务器列表、还有哪些元数据服务器是 up 且 in 的。要查看 MDS 图，执行 ceph mds dump 。</li></ol> 
<p>Ceph 监视器维护着集群运行图的主副本。一个监视器集群确保了当某个监视器失效时的高可用性。存储集群客户端向 Ceph 监视器索取集群运行图的最新副本。</p> 
<p>Ceph OSD 守护进程检查自身状态、以及其它 OSD 的状态，并报告给监视器们。</p> 
<p>存储集群的客户端和各个 Ceph OSD 守护进程使用 CRUSH 算法高效地计算数据位置，而不是依赖于一个中心化的查询表。它的高级功能包括：基于 librados的原生存储接口、和多种基于 librados 的服务接口。</p> 
<p>也就是说一个OSD守护进程 创建完成后还要加入 集群运行图，才算是加入集群。监视器才能监听OSD的状态。</p> 
<h4><a id="_20"></a>一、公共网络和集群网络</h4> 
<p>官方建议用两个网络运营 Ceph 存储集群：一个公共网（前端）和一个集群网（后端）。<br> 上面部署 只用了一个网卡 公共网和集群网使用的同一个网络。</p> 
<p>测试使用双网络。。。在虚拟机上测试。<br> <img src="https://images2.imgbox.com/76/0f/vZrZ43S6_o.png" alt="虚拟机设置两个网络适配器"><br> 三个 ceph 集群节点设置两个就够了，客户端和集群通信还是使用的公共网络。<br> 假设桥接的是公共网络 NAT的是 集群网络，公共网络要不能访问集群网络。<br> <img src="https://images2.imgbox.com/82/64/PQve3RCr_o.png" alt="ip addr"><br> 这时 ceph的配置文件应该是类似于</p> 
<pre><code>mon_initial_members = ceph-1
mon_host = 192.168.199.81
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

public_network = 192.168.199.0/24
cluster_network = 192.168.136.0/24
</code></pre> 
<p>public_network 表示公共网络网段<br> cluster_network 表示集群网络网段<br> mon_host 表示 监视器 所在主机。客户端与集群通信也是通过这个监视器完成的（应该吧）。</p> 
<p><strong>mon_host 主机IP应该在public_network 网段</strong></p> 
<p>完成后查看，发现状态正常（我这个是重新把ceph 集群安装了一遍。中间修改cluster_network和public_network 还需要更改 map 这个还不太明白）<br> <img src="https://images2.imgbox.com/b9/69/YY0lH51S_o.png" alt="集群状态正常"></p> 
<p>现在把其中一个节点的 NAT网络关掉，也就是集群网络。 测试ceph-2<br> <img src="https://images2.imgbox.com/5c/0f/Yr72NQda_o.png" alt="网络正常"><br> 修改 ens34 网卡</p> 
<pre><code>vi /etc/sysconfig/network-scripts/ifcfg-ens34
</code></pre> 
<p>修改ONBOOT 为 no</p> 
<pre><code>ONBOOT=no
</code></pre> 
<p>重启网络</p> 
<pre><code>systemctl restart network
</code></pre> 
<p><img src="https://images2.imgbox.com/77/32/m5ReNL52_o.png" alt="NAT网络失效"></p> 
<p>查看ceph集群状态<br> <img src="https://images2.imgbox.com/fe/43/GEu16nPZ_o.png" alt="ceph 集群"></p> 
<p>发现集群状态为 WARN 少了一个 osd。看来集群节点之间通信确实使用的集群网。cluster_network 设置是生效的。</p> 
<p>这时候再将NAT网络开启。。等了一段时间发现 ceph状态还是没有恢复正常<br> <img src="https://images2.imgbox.com/de/5a/kWcSTnRQ_o.png" alt="状态WARN"><br> 查看osd 树<br> <img src="https://images2.imgbox.com/5a/2e/JtMC1giH_o.png" alt="ceph osd tree"><br> 可以看到编号为1的osd 状态 是down 主机时 ceph-2 也是就刚停止又重启集群网的主机。。。<br> 这时候需要重新启动该节点的osd 服务 在ceph-2主机执行</p> 
<pre><code> systemctl start ceph-osd@1.service
</code></pre> 
<p><img src="https://images2.imgbox.com/ac/ac/Xu6YcyTk_o.png" alt="重新启动osd服务"><br> 发现状态正常了。。。</p> 
<h4><a id="monmgr_83"></a>二、mon和mgr</h4> 
<p><strong>mon</strong> 维护集群状态的映射，包括监视器映射、管理器映射、OSD映射、MDS映射和拥挤映射。这些映射是Cephdaemons相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。冗余和高可用性通常需要至少三个监视器。<br> <strong>mgr</strong> 负责跟踪运行时指标和ceph集群的当前状态，包括存储利用率、当前性能指标和系统负载。cephManager守护进程还宿主基于python的模块来管理和公开集群信息，包括基于web的集群信息。Ceph Dashboard和RESTAPI。高可用性通常至少需要两名manager。一般会在部署mon的节点都部署一个mgr(尽管并不强制要求必须部署在一起)。这个CephManager是版本 luminous版本之后 才推出的，在这个版本后成为必备组件。<br> **部署额外的管理器守护进程可以确保，如果一个守护进程或主机失败，另一个守护进程可以在不中断服务的情况下接管。**这个意思就是说一个集群中部署了多个mgr 实时起作用的只有一个。<br> <img src="https://images2.imgbox.com/46/eb/GkbDNbsP_o.png" alt="只有一个状态位active"><br> 两个准备接手，一个正在活跃。</p> 
<p>ceph建议的是mon 节点最好不要少于三个 然后保证在ceph集群节点数的半数以上。对于只有三个节点的。测试环境。可以三个节点都安装mon。（另，看到有些文档建议模拟节点的个数应该是单数）</p> 
<p><strong>ceph-mon， ceph-mgr 也是服务（守护进程）</strong>，不同于一个ceph集群的主机上可能有多个osd服务，一个主机最多只有一个mon或mgr服务。因此集群的osd 服务id都是唯一的数字，mon/mgr服务后缀 是主机名。</p> 
<pre><code class="prism language-bash">systemctl status ceph-mon@ceph-1
systemctl status ceph-mgr@ceph-1
</code></pre> 
<h5><a id="mon_99"></a>创建集群时设置mon节点</h5> 
<p>mon的创建，使用ceph-deploy部署ceph集群时。<br> 设置</p> 
<pre><code>ceph-deploy new host1,... 
</code></pre> 
<p>可将mon节点写入ceph.conf配置文件。<br> 然后再使用 命令</p> 
<pre><code>ceph-deploy admin host1 ...
</code></pre> 
<p>可将配置文件 放入到ceph集群主机的对应位置。在ceph集群未启动时，这样设置的mon就加入了ceph集群。但是，当ceph集群已经启动，更改配置文件是没用的。最重要的是要将新的mon 加入 ceph集群的映射 map。</p> 
<h5><a id="mon_114"></a>集群创建完成后新增mon节点</h5> 
<p>新增mon需要设置 配置文件public_network<br> 修改配置文件</p> 
<pre><code>vi ceph.conf
</code></pre> 
<pre><code>public_network = 192.168.199.0/24
</code></pre> 
<p>将配置文件设置到所有集群主机</p> 
<pre><code>ceph-deploy --overwrite-conf admin ceph-1 ceph-2 ceph-3
</code></pre> 
<p><strong>使用ceph-deploy工具新增mon节点的方法</strong></p> 
<pre><code>ceph-deploy mon add host1
</code></pre> 
<p>使用add 命令方法一次只能添加一个mon</p> 
<pre><code>ceph-deploy mon create host1 [host2 host3]
</code></pre> 
<p>create可以一次创建多个mon节点</p> 
<p>将ceph-2,ceph-3也作为mon节点</p> 
<pre><code>ceph-deploy mon create ceph-2 ceph-3
</code></pre> 
<p><strong>Ceph 用 Paxos 算法，要求法定人数里的大多数达成一致。可以只用一个监视器形成法定人数，然而你不能用两个监视器确定大多数。大多数监视器的比例必须像： 1:1 、 2:3 、 3:4 、 3:5 、 4:6 等等。</strong><br> 这个过程我们大多是情况下是不能控制的。还有，并不是说有四个mon监视器的话，就只有三个监视器是一致的，大多情况下，四个监视器都是一致的，3:4 只是说 有一个监视器意外情况坏掉了，那么mon依靠3个还是可以形成法定人数。。但是当有2个mon坏掉了。比如手动停止了服务，2个mon是无法形成法定人数的。<br> 我之前一直理解构成ceph节点中设置mon的节点数:总节点数，并不是这样，上面的比例意指最低一致mon节点数:总mon节点数，一半以上即可。。。偏差的有点大。<br> 另，一般建议 mon 节点是是奇数。</p> 
<p>查看法定人数状态</p> 
<pre><code>ceph quorum_status --format json-pretty
</code></pre> 
<p><strong>添加mon节点后 可以将新建的mon节点写到配置文件中。mon_host中IP用 , 隔开</strong></p> 
<h5><a id="mgr_156"></a>新增mgr</h5> 
<pre><code>ceph-deploy mgr create ceph-2 ceph-3
</code></pre> 
<p><img src="https://images2.imgbox.com/2a/72/HSH0KE0t_o.png" alt="3mon3mgr"></p> 
<h5><a id="Ceph_Dashboard_161"></a>设置Ceph Dashboard</h5> 
<p>luminous</p> 
<p>Ceph 的监控可视化界面方案。原生的Dashboard功能需要mgr组件。<br> luminous版本之后才有原生的Dashboard功能。<br> 在mon节点设置安装mgr</p> 
<pre><code class="prism language-bash">ceph-deploy mgr create ceph-1
</code></pre> 
<p>可在ceph-1 节点查看当前mgr 启用的模块。</p> 
<pre><code class="prism language-bash">ceph mgr module <span class="token function">ls</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/5b/26/eszs35QW_o.png" alt="未启用dashboard模块"><br> 1.设置启用dashboard</p> 
<pre><code class="prism language-bash">ceph mgr module <span class="token function">enable</span> dashboard
</code></pre> 
<p><img src="https://images2.imgbox.com/74/1e/s4ln9kyB_o.png" alt="启用dashboard模块"><br> 2.设置dashboard的ip和端口。IP需要设置为mgr所在IP（且luminous版本的mgr 功能模块可能还存在选举问题，如果多mgr 节点都开启，可能会出现web页面取不到数据，建议只开启一个mgr节点服务，然后关闭其他节点mgr服务）。端口可不用设置，默认为7000。</p> 
<pre><code class="prism language-bash">ceph config-key <span class="token keyword">set</span> mgr/dashboard/server_addr 192.168.199.81
ceph config-key <span class="token keyword">set</span> mgr/dashboard/server_port 7000 
</code></pre> 
<p>3.设置完成重启mgr服务并查看mgr服务。</p> 
<pre><code class="prism language-bash">systemctl restart ceph-mgr@ceph-1
ceph mgr services
</code></pre> 
<p><img src="https://images2.imgbox.com/c5/ea/j0zqRjgT_o.png" alt="开启mgr服务"></p> 
<p>4.网页查看<br> <img src="https://images2.imgbox.com/69/cf/lPkQ8jDf_o.png" alt="内置可视"></p> 
<p><strong>在更高版本中（如nautilus版）使用原生dashboard 需要设置证书等。</strong><br> 需要安装ceph-mgr-dashboard</p> 
<pre><code class="prism language-bash">yum <span class="token function">install</span> -y ceph-mgr-dashboard
</code></pre> 
<p>1、添加mgr 功能</p> 
<pre><code class="prism language-bash">ceph-deploy mgr create ceph-1 ceph-2 ceph-3
</code></pre> 
<p>2、启用dashboard</p> 
<pre><code class="prism language-bash">ceph mgr module <span class="token function">enable</span> dashboard
</code></pre> 
<p>3、生成并安装一个 自签名证书</p> 
<pre><code class="prism language-bash">ceph dashboard create-self-signed-cert
</code></pre> 
<p>4、生成密钥，会生成两个文件----dashboard.crt dashboard.key （不确定这一步是不是必须的），可能并不是必需的，只是登录使用密钥方便。</p> 
<pre><code class="prism language-bash"><span class="token function">mkdir</span> mgr-dashboard
 
<span class="token function">cd</span> mgr-dashboard
 
openssl req -new -nodes -x509   -subj <span class="token string">"/O=IT/CN=ceph-mgr-dashboard"</span> -days 3650   -keyout dashboard.key -out dashboard.crt -extensions v3_ca
</code></pre> 
<p>5、配置服务地址、端口，nautilus版默认的端口是8443<br> 这一步可省，默认端口8443，而IP则会由 ceph 自动选出。</p> 
<pre><code class="prism language-bash">ceph config <span class="token keyword">set</span> mgr mgr/dashboard/server_addr 192.168.199.81
ceph config <span class="token keyword">set</span> mgr mgr/dashboard/server_port 8443
</code></pre> 
<p>6、创建一个web登录用的用户密码</p> 
<pre><code class="prism language-bash">ceph dashboard set-login-credentials user-name password
</code></pre> 
<p>7、查看mgr服务</p> 
<pre><code class="prism language-bash">ceph mgr services
</code></pre> 
<h5><a id="monmgr_248"></a>mon和mgr的移除</h5> 
<p>在移除mon之前要确保移除后ceph集群可以形成新的法定人数。<br> <strong>mon移除</strong></p> 
<pre><code>ceph-deploy mon destroy host1 [host2]
</code></pre> 
<pre><code>ceph-deploy mon destroy ceph-1
</code></pre> 
<p>移除一个后 2个mon似乎难以形成法定人数。。再重新将ceph-1的mon加上去。然后移除两个。。。重点是ceph.conf 配置文件中mon只有 ceph-1。所以在添加ceph-2和ceph-3的mon后，最好也写入配置文件。 host和host之间要使用,隔开。 在移除mon 之前或之后 也要将配置稳健的mon去除。</p> 
<p>如果移除了"quorum_leader_name" 再重新选定法定人的过程比较艰难。感觉很难成功。。可能是因为集群太小 mon 太少的缘故。</p> 
<p><strong>mgr移除</strong><br> 官方文档并没有找到移除mgr 的方式。但是只用将mgr对应守护进程关闭，该mgr就不会再被集群使用了。再启动时仍可以重新加入集群。</p> 
<pre><code class="prism language-bash">systemctl stop ceph-mgr@ceph-node1
</code></pre> 
<h5><a id="mon_269"></a>查看mon状态的特别指令</h5> 
<p>可以查看监视器</p> 
<pre><code>ceph quorum_status --format json-pretty
</code></pre> 
<p>查看详细状态</p> 
<pre><code>ceph mon_status --format json-pretty
</code></pre> 
<p>运行状态</p> 
<pre><code class="prism language-bash">systemctl status ceph-mon@ceph-1
</code></pre> 
<pre><code class="prism language-bash">systemctl status ceph-mgr@ceph-1
</code></pre> 
<h4><a id="OSD_288"></a>三、OSD</h4> 
<p>OSD也是一个守护进程/服务，可以使用systemctl 查看控制运行状态。一个主机上可能有多个osd服务</p> 
<pre><code class="prism language-bash">systemctl status ceph-osd@osd-num
</code></pre> 
<p>osd-num 就是ceph集群中 osd 的唯一ID，当然该OSD需要运行在该主机上时才可查看。使用命令</p> 
<pre><code class="prism language-bash">ceph osd tree 
</code></pre> 
<p>查看OSD 分布状况。</p> 
<h5><a id="OSD__300"></a>OSD 日志文件作用</h5> 
<p>Ceph 的 OSD 使用日志的原因有二：速度和一致性。</p> 
<ul><li>速度： 日志使得 OSD 可以快速地提交小块数据的写入， Ceph 把小片、随机 IO 依次写入日志，这样，后端文件系统就有可能归并写入动作，并最终提升并发承载力。因此，使用 OSD 日志能展现出优秀的突发写性能，实际上数据还没有写入 OSD ，因为文件系统把它们捕捉到了日志。</li><li>一致性： Ceph 的 OSD 守护进程需要一个能保证原子操作的文件系统接口。 OSD 把一个操作的描述写入日志，然后把操作应用到文件系统，这需要原子更新一个对象（例如归置组元数据）。每隔一段 filestore max sync interval 和 <code>filestore min sync interval</code> 之间的时间， OSD 停止写入、把日志同步到文件系统，这样允许 OSD 修整日志里的操作并重用空间。若失败， OSD 从上个同步点开始重放日志。</li></ul> 
<h5><a id="osd_305"></a>osd的移除</h5> 
<p><a href="http://docs.ceph.org.cn/rados/operations/add-or-rm-osds/" rel="nofollow">http://docs.ceph.org.cn/rados/operations/add-or-rm-osds/</a><br> <a href="https://ceph.readthedocs.io/en/latest/rados/operations/add-or-rm-osds/#removing-osds-manual" rel="nofollow">https://ceph.readthedocs.io/en/latest/rados/operations/add-or-rm-osds/#removing-osds-manual</a></p> 
<p>Luminous 版本之前和之后 移除不太一样。只简述Luminous 之后移除方式。</p> 
<p>1.将osd移出集群</p> 
<pre><code class="prism language-bash">ceph osd out <span class="token punctuation">{<!-- --></span>osd-num<span class="token punctuation">}</span>
</code></pre> 
<p>osd-num 就是 集群中osd的唯一编号。</p> 
<p>移除集群时会发生数据迁移，要保证数据迁移成功</p> 
<pre><code class="prism language-bash">ceph -w
</code></pre> 
<p>观察迁移过程，归置组状态从 active+clean 变为 active, some degraded objects 、迁移完成后最终回到 active+clean 状态。</p> 
<p>附： 有时候，（通常是只有几台主机的“小”集群，比如小型测试集群）移除（ out ）某个 OSD 可能会使 CRUSH 进入临界状态，这时某些 PG 一直卡在 active+remapped 状态。<br> 如果遇到了这种情况，你应该重新把此 OSD 标记为 in</p> 
<pre><code class="prism language-bash">ceph osd <span class="token keyword">in</span> <span class="token punctuation">{<!-- --></span>osd-num<span class="token punctuation">}</span>
</code></pre> 
<p>回到最初的状态后，把它的权重设置为 0 ，而不是标记为 out</p> 
<pre><code class="prism language-bash">ceph osd crush reweight osd.<span class="token punctuation">{<!-- --></span>osd-num<span class="token punctuation">}</span> 0
</code></pre> 
<p>把某一 OSD 标记为 out 和权重改为 0 的区别在于，前者，包含此 OSD 的桶、其权重没变；而后一种情况下，桶的权重变了（降低了此 OSD 的权重）。某些情况下， reweight 命令更适合“小”集群。</p> 
<p>2.停止OSD 服务。<br> 将OSD状态设置为out后，对应的OSD服务仍在运行。OSD其状态为 up 且 out。 需要先停止服务才能移除该OSD。<br> 在该OSD对应主机上执行</p> 
<pre><code class="prism language-bash">systemctl stop ceph-osd@<span class="token punctuation">{<!-- --></span>osd-num<span class="token punctuation">}</span>
</code></pre> 
<p>停止服务后 OSD状态变为 down</p> 
<p>3.删除OSD</p> 
<pre><code class="prism language-bash">ceph osd purge <span class="token punctuation">{<!-- --></span>id<span class="token punctuation">}</span> --yes-i-really-mean-it
</code></pre> 
<p>这一步会删除CRUSH 图的对应 OSD 条目（ OSD map、CRUSH map）、 OSD 认证密钥。</p> 
<p>如果在配置文件设置了该OSD ，将ceph管理节点配置文件对应OSD配置删除 然后覆盖ceph集群所有主机配置文件。</p> 
<h5><a id="osd_352"></a>osd的创建</h5> 
<p>有filestore 和bluestore 的区别。这个概念。。。filestore存在写放大的问题，要写到journal一次，data一次且存储通过xfs等文件系统，维护文件系统多了开销，，，bluestore直接使用裸盘存储，上面没有文件系统。。。相比较在使用ssd 等新存储设备bluestore更占优势。<br> 转自<a href="http://www.itworld123.com/2019/06/04/storage/ceph/Ceph%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore%E7%AE%80%E6%9E%90/" rel="nofollow">http://www.itworld123.com/2019/06/04/storage/ceph/Ceph%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore%E7%AE%80%E6%9E%90/</a></p> 
<p>OSD日志文件</p> 
<p><strong>创建osd，bluestore是默认格式</strong></p> 
<pre><code>ceph-deploy osd create --data /dev/sdb ceph-3
</code></pre> 
<p>等同于</p> 
<pre><code>ceph-deploy osd create --data /dev/sdb --bluestore ceph-3
</code></pre> 
<p><strong>如果osd 格式为 filestore 要指定–journal，日志文件的位置。并不是说bluestore不需要日志文件，而是bluestore格式的话会有默认位置。</strong></p> 
<pre><code>[root@ceph-admin my-cluster]# ceph-deploy osd create -h
usage: ceph-deploy osd create [-h] [--data DATA] [--journal JOURNAL]
                              [--zap-disk] [--fs-type FS_TYPE] [--dmcrypt]
                              [--dmcrypt-key-dir KEYDIR] [--filestore]
                              [--bluestore] [--block-db BLOCK_DB]
                              [--block-wal BLOCK_WAL] [--debug]
                              [HOST]

positional arguments:
  HOST                  Remote host to connect

optional arguments:
  -h, --help            show this help message and exit
  --data DATA           The OSD data logical volume (vg/lv) or absolute path
                        to device
  --journal JOURNAL     Logical Volume (vg/lv) or path to GPT partition
  --zap-disk            DEPRECATED - cannot zap when creating an OSD
  --fs-type FS_TYPE     filesystem to use to format DEVICE (xfs, btrfs)
  --dmcrypt             use dm-crypt on DEVICE
  --dmcrypt-key-dir KEYDIR
                        directory where dm-crypt keys are stored
  --filestore           filestore objectstore
  --bluestore           bluestore objectstore
  --block-db BLOCK_DB   bluestore block.db path
  --block-wal BLOCK_WAL
                        bluestore block.wal path
  --debug               Enable debug mode on remote ceph-volume calls
[root@ceph-admin my-cluster]# 
</code></pre> 
<pre><code class="prism language-bash">  --journal JOURNAL     Logical Volume <span class="token punctuation">(</span>vg/lv<span class="token punctuation">)</span> or path to GPT partition
</code></pre> 
<p>日志文件位置一定是一个gpt 分区或者 逻辑卷 lvm卷管理。<br> 官方建议data 数据盘和 journal日志最好不要放在一个盘中。另，日志放在ssd硬盘上更有效率。<br> 使用/dev/sdb 作为osd data /dev/sdc1 分区作为osd journal<br> 使用fdisk 工具创建一个 gpt 分区，大小2G。</p> 
<pre><code>fdisk /dev/sdc
</code></pre> 
<p>设置gpt 格式分区</p> 
<pre><code>g
</code></pre> 
<p>设置分区</p> 
<pre><code>n
</code></pre> 
<p>设置完成。大致如下</p> 
<pre><code>Command (m for help): p

Disk /dev/sdc: 10.7 GB, 10737418240 bytes, 20971520 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: gpt
Disk identifier: A8B68C02-E822-4A2A-89FE-3DC512113DE5


#         Start          End    Size  Type            Name
 1         2048      4196351      2G  Linux filesyste 
</code></pre> 
<p>创建osd指令，再安装ceph-deploy 的节点设置</p> 
<pre><code>ceph-deploy osd create --data /dev/sdb --journal /dev/sdc1 --filestore ceph-1
</code></pre> 
<p>/dev/sdb 是一个裸硬盘。ceph-deploy 自动部署工具创建osd时会将其先设置成一个 lvm 卷组，再创建一个逻辑卷作为osd存储点。<br> <img src="https://images2.imgbox.com/59/b9/p5o53yBp_o.png" alt="osd的逻辑卷"><br> 无非就是卷组名和逻辑卷的名字长了一点。。。</p> 
<p>我们也可以直接 设置一个 逻辑卷 。当作osd存储。不过这是–data 应该是<br> –data 卷组名/逻辑卷名。</p> 
<p><strong>比如 在 ceph-2 节点设置。</strong><br> 设置一个卷组 使用 /dev/sdb 卷组名为 vg_osd1(这里省略了将/dev/sdb设置成lvm物理卷的步骤，因为在创建lvm卷组时会自动设置/dev/sdb)</p> 
<pre><code>vgcreate vg_osd1 /dev/sdb
</code></pre> 
<p>创建一个逻辑卷 使用卷组全部空间 逻辑卷卷名 lv_osd1</p> 
<pre><code>lvcreate -l 100%FREE vg_osd1 -n lv_osd1
</code></pre> 
<p><img src="https://images2.imgbox.com/de/58/kntsXW2f_o.png" alt="lvs"><br> 接着设置 /dev/sdc 设置一个GPT格式的分区。（只是为了统一osd的类型，bluestore 类型同样可以这样设置作为osd存储）</p> 
<pre><code>fdisk /dev/sdc
</code></pre> 
<pre><code>g
n

</code></pre> 
<pre><code>w
</code></pre> 
<p>不再多说。。。<br> 测试 创建osd</p> 
<pre><code>ceph-deploy osd create --data vg_osd1/lv_osd1 --journal /dev/sdc1 --filestore ceph-2
</code></pre> 
<p>创建完成 查看 ceph-2 节点的硬盘和分区<br> <img src="https://images2.imgbox.com/70/13/2EpEk1Sh_o.png" alt="lsblk"><br> 和ceph-1 差不多只不过vg卷组和lv卷名是自己设置的。<br> <strong>前面 说 不建议把数据和日志放在一个硬盘，会影响速率。</strong><br> 这说明是可以放在一个硬盘上的。<br> 可以测试在一个硬盘生成的卷组中创建两个逻辑卷，一个作为data 一个作为journal<br> 在ceph-3 节点测试<br> 创建卷组vg_osd1</p> 
<pre><code>vgcreate vg_osd1 /dev/sdb
</code></pre> 
<p>创建作为data 的逻辑卷lv_osd1</p> 
<pre><code>lvcreate -L 8G vg_osd1 -n lv_osd1
</code></pre> 
<p>剩余空间创建作为journal的逻辑卷lv_osd1_journal</p> 
<pre><code>lvcreate -l 100%FREE vg_osd1 -n lv_osd1_journal
</code></pre> 
<p>创建osd</p> 
<pre><code>ceph-deploy osd create --data vg_osd1/lv_osd1 --journal vg_osd1/lv_osd1_journal --filestore ceph-3 
</code></pre> 
<p>journal 这次也是一个 vg/lv 。</p> 
<p><img src="https://images2.imgbox.com/80/84/Sn18hbnx_o.png" alt="ceph-3的硬盘情况"><br> sdb 下有两个lvm 逻辑卷。一个作为data 一个作为journal。</p> 
<h4><a id="_506"></a>四、存储池</h4> 
<p>存储池是用户存储数据的逻辑分区。在 Ceph 部署中，经常创建存储池作为逻辑分区、用以归类相似的数据。例如，用 Ceph 作为 OpenStack 的后端时，典型的部署通常会创建多个存储池，分别用于存储卷宗、映像、备份和虚拟机，以及用户（如 client.glance 、 client.cinder 等）。</p> 
<p>创建pool，pool是ceph存储数据时的逻辑分区,它起到namespace的作用。<br> 创建两个存储池。存储池设置涉及到了归置组的设置。<br> 用此命令创建存储池时：</p> 
<pre><code class="prism language-bash">ceph osd pool create <span class="token punctuation">{<!-- --></span>pool-name<span class="token punctuation">}</span> pg_num
</code></pre> 
<p>pg_num 取值是强制性的，因为不能自动计算。下面是几个常用的值：</p> 
<ul><li>少于 5 个 OSD 时可把 pg_num 设置为 128</li><li>OSD 数量在 5 到 10 个时，可把 pg_num 设置为 512</li><li>OSD 数量在 10 到 50 个时，可把 pg_num 设置为 4096</li><li>OSD 数量大于 50 时，你得理解权衡方法、以及如何自己计算 pg_num 取值</li></ul> 
<h5><a id="_522"></a>归置组</h5> 
<p><strong>PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略；简单点说就是相同PG内的对象都会放到相同的硬盘上； PG是 ceph的核心概念， 服务端数据均衡和恢复的最小粒度就是PG；</strong></p> 
<p>每个存储池都有很多归置组， CRUSH 动态的把它们映射到 OSD 。 Ceph 客户端要存对象时， CRUSH 将把各对象映射到某个归置组。</p> 
<p>要设置某存储池的归置组数量，你必须在创建它时就指定好，详情见创建存储池。一存储池的归置组数量设置好之后，还可以增加（但不可以减少），下列命令可增加归置组数量：</p> 
<pre><code class="prism language-bash">ceph osd pool <span class="token keyword">set</span> <span class="token punctuation">{<!-- --></span>pool-name<span class="token punctuation">}</span> pg_num <span class="token punctuation">{<!-- --></span>pg_num<span class="token punctuation">}</span>
</code></pre> 
<p>你增加归置组数量后、还必须增加用于归置的归置组（ pgp_num ）数量，这样才会开始重均衡。 pgp_num 数值才是 CRUSH 算法采用的用于归置的归置组数量。虽然 pg_num 的增加引起了归置组的分割，但是只有当用于归置的归置组（即 pgp_num ）增加以后，数据才会被迁移到新归置组里。 pgp_num 的数值应等于 pg_num 。可用下列命令增加用于归置的归置组数量：</p> 
<pre><code class="prism language-bash">ceph osd pool <span class="token keyword">set</span> <span class="token punctuation">{<!-- --></span>pool-name<span class="token punctuation">}</span> pgp_num <span class="token punctuation">{<!-- --></span>pgp_num<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="_537"></a>归置组数量计算</h5> 
<p>其结果汇总后应该接近 2 的幂。汇总并非强制的，如果你想确保所有归置组内的对象数大致相等，最好检查下。<br> 比如，一个配置了 200 个 OSD 且副本数为 3 的集群，你可以这样估算归置组数量：</p> 
<p>(200 * 100)<br> ----------- = 6667. Nearest power of 2: 8192<br> 3</p> 
<p>注，这是单个存储池的计算方法。。。<br> 当用了多个数据存储池来存储数据时，你得确保均衡每个存储池的归置组数量、且归置组数量分摊到每个 OSD ，这样才能达到较合理的归置组总量，并因此使得每个 OSD 无需耗费过多系统资源或拖慢连接进程就能实现较小变迁。</p> 
<p>这个不是很懂。也就只把官方文档内容抄下来了。有一点注意。可以根据设置的归置组数量和池数量计算出每个osd上归置组的大约数量。。。比如三个存储池，存储池默认副本数为2（osd_pool_default_size = 2）。设置存储池是每个存储池pg_num 都是 128。这时每个osd 上归置组数量大约是 128 * 3 * 2 / 3 = 256 。。。<br> 官方建议 每个OSD上大约100个归置组。。。这个建议可以帮助我们计算。默认的每个osd上最大归置组数量是250 当然是可以修改的。</p> 
<pre><code class="prism language-bash">mon_max_pg_per_osd 250
</code></pre> 
<p>这样 设置多个存储池时，我们可以做一下数学题。把归置组数量作为要求的值?。尽量保证每个osd上归置组数量在50-250 之间。尽量往100靠近（最好大于100）。。。最后选择一个接近2的幂的值。<br> <strong>比如。三个osds 副本数量是3（默认为3），准备设置三个存储池（每个存储池设置的归置组数量一致）。<br> 50 * 3(3个osd) &lt;? * 3 * 3(三个存储池，每个存储池还有两个副本)&lt;250 * 3(三个osd)<br> 约 27&lt; ? &lt; 84<br> 选择一个 2的幂。。。那就是32 或者64。然后算一下哪个值 下osd上归置组数量更接近 100<br> 32 的话 32 * 3 * 3 / 3 = 96<br> 64 的话 64 * 3 * 3 / 3 = 192<br> 尽管 96 更接近 100 但是 感觉应该 归置组数量是32和64 都可以。。。另，单个节点osd数量少的情况下mon_max_pg_per_osd 建议可以调大一点。</strong></p> 
<p>附，如果一个节点上有许多osd 那么每个osd上的归置组数量尽量少一点。</p> 
<p>所以设置ceph集群前，最好先确定准备设置多少存储池，然后确定设置每个存储池的归置组的数量。不同存储池的归置组数量可以不一致，但是这些归置组数量加起来 假设平均分布到每个 osd 最好保证每个osd上归置组数量为100。</p> 
<h5><a id="_568"></a>存储池删除</h5> 
<p>默认设置存储池是无法删除的。需要修改配置文件。</p> 
<pre><code class="prism language-bash"><span class="token function">vi</span> ceph.conf 
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>mon<span class="token punctuation">]</span>
mon_allow_pool_delete <span class="token operator">=</span> <span class="token boolean">true</span>
</code></pre> 
<p>设置完成 分发到各个ceph主机</p> 
<pre><code class="prism language-bash">ceph-deploy --overwrite-conf config push <span class="token punctuation">{<!-- --></span>host-name <span class="token punctuation">[</span>host-name<span class="token punctuation">]</span><span class="token punctuation">..</span>.<span class="token punctuation">}</span>
</code></pre> 
<p>需要重启mon服务才可删除pool</p> 
<pre><code class="prism language-bash">systemctl restart ceph-mon@ceph-1
</code></pre> 
<p>删除存储池 rbd。需要写两次 存储池名</p> 
<pre><code class="prism language-bash">ceph osd pool <span class="token function">rm</span> rbd rbd --yes-i-really-really-mean-it
</code></pre> 
<h4><a id="auth___596"></a>五、用户auth添加 与 删除</h4> 
<p><a href="http://docs.ceph.org.cn/rados/operations/user-management/" rel="nofollow">http://docs.ceph.org.cn/rados/operations/user-management/</a></p> 
<p>无论Ceph客户端的类型（例如，块设备、对象存储、文件系统、本机API等），Ceph都将所有数据存储为池中的对象。Ceph用户必须有权访问池才能读取和写入数据。此外，Ceph用户必须具有使用Ceph管理命令的执行权限。</p> 
<h5><a id="__600"></a>赋予用户能力 语法</h5> 
<p>Ceph 用能力（ capabilities, caps ）这个术语来描述给认证用户的授权，这样才能使用监视器、 OSD 、和元数据服务器的功能。能力也用于限制对一存储池内的数据或某个名字空间的访问。 Ceph 的管理用户可在创建或更新某用户时赋予他能力。<br> 能力的语法符合下面的形式：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">{<!-- --></span>daemon-type<span class="token punctuation">}</span> <span class="token string">'allow {capability}'</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>daemon-type<span class="token punctuation">}</span> <span class="token string">'allow {capability}'</span><span class="token punctuation">]</span>
</code></pre> 
<p>同一个 daemon-type 的设置 allow之间可以用,隔开。。。</p> 
<ul><li>监视器（mon）能力： 监视器能力包括 r 、 w 、 x 和 allow profile {cap} ，例如：</li></ul> 
<pre><code class="prism language-bash">mon <span class="token string">'allow rwx'</span>
mon <span class="token string">'allow profile osd'</span>
</code></pre> 
<p>可以设置成</p> 
<pre><code class="prism language-bash">mon <span class="token string">'allow rwx, allow profile osd'</span>
</code></pre> 
<ul><li>OSD 能力： OSD 能力包括 r 、 w 、 x 、 class-read 、 class-write 和 profile osd 。另外， OSD 能力还支持存储池和命名空间的配置。</li></ul> 
<pre><code class="prism language-bash">osd <span class="token string">'allow {capability} [pool={poolname}] [namespace={namespace-name}]'</span>
</code></pre> 
<ul><li>元数据服务器能力： 元数据服务器能力比较简单，只需要 allow 或者空白，也不会解析更多选项。</li></ul> 
<pre><code class="prism language-bash">mds <span class="token string">'allow'</span>
</code></pre> 
<h5><a id="_633"></a>各个能力的含义</h5> 
<ul><li> <p>allow<br> 描述:<br> 在守护进程的访问设置之前，仅对 MDS 隐含 rw 。</p> </li><li> <p>r<br> 描述:<br> 授予用户读权限，监视器需要它才能搜刮 CRUSH 图。</p> </li><li> <p>w<br> 描述:<br> 授予用户写对象的权限。</p> </li><li> <p>x<br> 描述:<br> 授予用户调用类方法的能力，即同时有读和写，且能在监视器上执行 auth 操作。</p> </li><li> <p>class-read<br> 描述:<br> 授予用户调用类读取方法的能力， x 的子集。</p> </li><li> <p>class-write<br> 描述:<br> 授予用户调用类写入方法的能力， x 的子集。</p> </li><li> <p>*<br> 描述:<br> 授权此用户读、写和执行某守护进程/存储池，且允许执行管理命令。</p> </li><li> <p>profile osd<br> 描述:<br> 授权一个用户以 OSD 身份连接其它 OSD 或监视器。授予 OSD 们允许其它 OSD 处理复制、心跳流量和状态报告。</p> </li><li> <p>profile mds<br> 描述:<br> 授权一个用户以 MDS 身份连接其它 MDS 或监视器。</p> </li><li> <p>profile bootstrap-osd<br> 描述:<br> 授权一用户自举引导一 OSD 。授予部署工具，像 ceph-disk 、 ceph-deploy 等等，这样它们在自举引导 OSD 时就有权限增加密钥了。</p> </li><li> <p>profile bootstrap-mds<br> 描述:<br> 授权一用户自举引导一元数据服务器。授予像 ceph-deploy 一样的部署工具，这样它们在自举引导元数据服务器时就有权限增加密钥了。</p> </li></ul> 
<h5><a id="1_679"></a>1.查看用户列表和权限</h5> 
<p>罗列用户</p> 
<pre><code class="prism language-bash">ceph auth list
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># ceph auth list</span>
installed auth entries:

osd.0
	key: AQASyaBfA3GMERAAm0wuaNfzEq1ZkB+2yX6+MQ<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mgr<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> allow *
osd.1
	key: AQBiyaBfEKNNCBAA9Lr2kbiZC8Zb9Pg7bXFpww<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mgr<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> allow *
osd.2
	key: AQC3yaBfLNDHABAAvAxf6tAyJ9J9kFzzYbuN3g<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mgr<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile osd
	caps: <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> allow *
client.admin
	key: AQDCyKBf4EaOJBAA30totF0nzidt0VsafbTfMQ<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mds<span class="token punctuation">]</span> allow *
	caps: <span class="token punctuation">[</span>mgr<span class="token punctuation">]</span> allow *
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow *
	caps: <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> allow *
client.bootstrap-mds
	key: AQDDyKBfBNFCGhAAeKnV7SIAS4v/IrZZSx/ESQ<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile bootstrap-mds
client.bootstrap-mgr
	key: AQDEyKBf/SpjERAAon4r+ffUXQe8EyysNkpbDA<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile bootstrap-mgr
client.bootstrap-osd
	key: AQDFyKBfNBDXCRAA7cLlh3cUqyg8b66u7qQxwg<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile bootstrap-osd
client.bootstrap-rgw
	key: AQDGyKBfukJDABAAizCg47klS1TxdbURLUG9GQ<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile bootstrap-rgw
mgr.ceph-1
	key: AQADyaBfsfcxLhAASZ8wkOXAZsDlj+Bg/SMj3g<span class="token operator">==</span>
	caps: <span class="token punctuation">[</span>mds<span class="token punctuation">]</span> allow *
	caps: <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> allow profile mgr
	caps: <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> allow *
<span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># </span>
</code></pre> 
<p>获取某用户信息</p> 
<pre><code class="prism language-bash">ceph auth get <span class="token punctuation">{<!-- --></span>TYPE.ID<span class="token punctuation">}</span>
</code></pre> 
<p>如</p> 
<pre><code class="prism language-bash">ceph auth get client.admin
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># ceph auth get client.admin</span>
exported keyring <span class="token keyword">for</span> client.admin
<span class="token punctuation">[</span>client.admin<span class="token punctuation">]</span>
	key <span class="token operator">=</span> AQDCyKBf4EaOJBAA30totF0nzidt0VsafbTfMQ<span class="token operator">==</span>
	caps mds <span class="token operator">=</span> <span class="token string">"allow *"</span>
	caps mgr <span class="token operator">=</span> <span class="token string">"allow *"</span>
	caps mon <span class="token operator">=</span> <span class="token string">"allow *"</span>
	caps osd <span class="token operator">=</span> <span class="token string">"allow *"</span>
</code></pre> 
<h5><a id="2_750"></a>2.创建用户和设置权限</h5> 
<p>有多种方法，一般会使用</p> 
<pre><code class="prism language-bash">ceph auth get-or-create 
</code></pre> 
<p>如果用户不存在就创建用户 如果用户已存在就显示该用户的密钥<br> 如</p> 
<pre><code class="prism language-bash">ceph auth get-or-create client.cloudstack mon <span class="token string">'allow r'</span> osd <span class="token string">'allow class-read object_prefix rbd_children, allow rwx pool=cloudstack'</span>
</code></pre> 
<p>甚至设置的存储池 命名空间等可以不存在<br> <img src="https://images2.imgbox.com/a5/6b/8D06oI6k_o.png" alt="创建用户"></p> 
<h5><a id="3_764"></a>3.删除用户</h5> 
<pre><code class="prism language-bash">ceph auth del <span class="token punctuation">{<!-- --></span>TYPE<span class="token punctuation">}</span>.<span class="token punctuation">{<!-- --></span>ID<span class="token punctuation">}</span>
</code></pre> 
<p>其中 {TYPE} 是 client 、 osd 、 mon 或 mds 之一， {ID} 是用户名或守护进程的 ID 。</p> 
<pre><code class="prism language-bash">ceph auth del client.cloudstack
</code></pre> 
<h5><a id="4_774"></a>4.其它</h5> 
<p>查看用户密钥</p> 
<pre><code class="prism language-bash">ceph auth print-key <span class="token punctuation">{<!-- --></span>TYPE<span class="token punctuation">}</span>.<span class="token punctuation">{<!-- --></span>ID<span class="token punctuation">}</span>
</code></pre> 
<pre><code class="prism language-bash">ceph auth print-key client.cloudstack
</code></pre> 
<p>把用户加入密钥环。好像只是把用户相关设置写到一个文件中而已</p> 
<pre><code class="prism language-bash">ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># ceph auth get client.cloudstack -o /etc/ceph/ceph.client.cloudstack.keyring</span>
exported keyring <span class="token keyword">for</span> client.cloudstack
<span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># cat /etc/ceph/client.cloudstack.keyring </span>
<span class="token punctuation">[</span>client.cloudstack<span class="token punctuation">]</span>
	key <span class="token operator">=</span> AQDs+KBfSjJ/OxAAdCUcTr255feHb6/ebYLnzg<span class="token operator">==</span>
	caps mon <span class="token operator">=</span> <span class="token string">"allow r"</span>
	caps osd <span class="token operator">=</span> <span class="token string">"allow class-read object_prefix rbd_children, allow rwx pool=cloudstack"</span>
<span class="token punctuation">[</span>root@ceph-1 ~<span class="token punctuation">]</span><span class="token comment"># </span>
</code></pre> 
<p>从密钥环中导入用户。。。</p> 
<pre><code class="prism language-bash">ceph auth <span class="token function">import</span> -i /etc/ceph/ceph.keyring
</code></pre> 
<p>如</p> 
<pre><code class="prism language-bash">ceph auth <span class="token function">import</span> -i /etc/ceph/ceph.client.cloudstack.keyring 
</code></pre> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>root@ceph-1 ceph<span class="token punctuation">]</span><span class="token comment"># ceph auth get client.cloudstack</span>
Error ENOENT: failed to <span class="token function">find</span> client.cloudstack <span class="token keyword">in</span> keyring
<span class="token punctuation">[</span>root@ceph-1 ceph<span class="token punctuation">]</span><span class="token comment"># ceph auth import -i /etc/ceph/ceph.client.cloudstack.keyring </span>
imported keyring
<span class="token punctuation">[</span>root@ceph-1 ceph<span class="token punctuation">]</span><span class="token comment"># ceph auth get client.cloudstack</span>
exported keyring <span class="token keyword">for</span> client.cloudstack
<span class="token punctuation">[</span>client.cloudstack<span class="token punctuation">]</span>
	key <span class="token operator">=</span> AQDs+KBfSjJ/OxAAdCUcTr255feHb6/ebYLnzg<span class="token operator">==</span>
	caps mon <span class="token operator">=</span> <span class="token string">"allow r"</span>
	caps osd <span class="token operator">=</span> <span class="token string">"allow class-read object_prefix rbd_children, allow rwx pool=cloudstack"</span>
<span class="token punctuation">[</span>root@ceph-1 ceph<span class="token punctuation">]</span><span class="token comment"># </span>
</code></pre> 
<h5><a id="Ceph__824"></a>Ceph 支持用户名和密钥的下列用法：</h5> 
<ul><li>–id | --user<br> 描述:<br> Ceph 用一个类型和 ID（ 如 TYPE.ID 或 client.admin 、 client.user1 ）来标识用户， id 、 name 、和 -n 选项可用于指定用户名（如 admin 、 user1 、 foo 等）的 ID 部分，你可以用 --id 指定用户并忽略类型，例如可用下列命令指定 client.foo 用户：</li></ul> 
<pre><code class="prism language-bash">ceph --id foo --keyring /path/to/keyring health
ceph --user foo --keyring /path/to/keyring health
</code></pre> 
<pre><code class="prism language-bash">ceph --id cloudstack --keyring /etc/ceph/ceph.client.cloudstack.keyring health
</code></pre> 
<ul><li>–name | -n<br> 描述:<br> Ceph 用一个类型和 ID （如 TYPE.ID 或 client.admin 、 client.user1 ）来标识用户， --name 和 -n 选项可用于指定完整的用户名，但必须指定用户类型（一般是 client ）和用户 ID ，例如：</li></ul> 
<pre><code class="prism language-bash">ceph --name client.foo --keyring /path/to/keyring health
ceph -n client.foo --keyring /path/to/keyring health
</code></pre> 
<pre><code class="prism language-bash">ceph --id client.cloudstack --keyring /etc/ceph/ceph.client.cloudstack.keyring health
</code></pre> 
<ul><li>–keyring<br> 描述:<br> 包含一或多个用户名、密钥的密钥环路径。 --secret 选项提供了相同功能，但它不能用于 RADOS 网关，其 --secret 另有用途。你可以用 ceph auth get-or-create 获取密钥环并保存在本地，然后您就可以改用其他用户而无需重指定密钥环路径了。</li></ul> 
<pre><code class="prism language-bash"><span class="token function">sudo</span> rbd map --id foo --keyring /path/to/keyring mypool/myimage
</code></pre> 
<p>如</p> 
<pre><code class="prism language-bash">ceph health --keyring /etc/ceph/ceph.client.admin.keyring 
</code></pre> 
<p>如果指定用户的权限不足。。。那么命令也是会执行失败的。</p> 
<h4><a id="ceph_868"></a>六、ceph的数据清除与卸载</h4> 
<p>如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置：</p> 
<pre><code class="prism language-bash">ceph-deploy purgedata <span class="token punctuation">{<!-- --></span>ceph-node<span class="token punctuation">}</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>ceph-node<span class="token punctuation">}</span><span class="token punctuation">]</span>
ceph-deploy forgetkeys
</code></pre> 
<p>用下列命令可以连 Ceph 安装包一起清除：</p> 
<pre><code class="prism language-bash">ceph-deploy purge <span class="token punctuation">{<!-- --></span>ceph-node<span class="token punctuation">}</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>ceph-node<span class="token punctuation">}</span><span class="token punctuation">]</span>
</code></pre> 
<p>如果执行了 purge ，你必须重新安装 Ceph 。</p> 
<h4><a id="ceph__884"></a>七、ceph配置 文件相关</h4> 
<p>上面已经对配置文件做出各种修改并设置应用了。这里把翻译的官方文档复制过来做总结。<br> ceph节点的配置文件位置是</p> 
<pre><code class="prism language-bash">/etc/ceph/ceph.conf
</code></pre> 
<p>使用ceph-deploy 不是ceph时，一般会在部署节点设置ceph配置文件后再分发到所有节点。</p> 
<h5><a id="ceph_892"></a>ceph配置文件基本关系</h5> 
<p>以下内容就是把官网的复制了一遍。。。</p> 
<p>启动 Ceph 存储集群时，各守护进程都从同一个配置文件（即默认的 ceph.conf ）里查找它自己的配置。</p> 
<p>配置文件定义了（没有定义的就是设置了默认值）：</p> 
<ul><li>集群身份</li><li>认证配置</li><li>集群成员</li><li>主机名</li><li>主机 IP 地址</li><li>密钥环路径</li><li>日志路径</li><li>数据路径</li><li>其它运行时选项</li></ul> 
<h6><a id="1_Ceph__908"></a>1. Ceph 配置文件可用于配置存储集群内的所有守护进程、或者某一类型的所有守护进程。要配置一系列守护进程，这些配置必须位于能收到配置的段落之下，比如：</h6> 
<ul><li> <p>[global]<br> 描述:<br> [global] 下的配置影响 Ceph 集群里的所有守护进程。<br> 实例:<br> auth supported = cephx</p> </li><li> <p>[osd]<br> 描述:<br> [osd] 下的配置影响存储集群里的所有 ceph-osd 进程，并且会覆盖 [global] 下的同一选项。<br> 实例:<br> osd journal size = 1000</p> </li><li> <p>[mon]<br> 描述:<br> [mon] 下的配置影响集群里的所有 ceph-mon 进程，并且会覆盖 [global] 下的同一选项。<br> 实例:<br> mon addr = 10.0.0.101:6789</p> </li><li> <p>[mds]<br> 描述:<br> [mds] 下的配置影响集群里的所有 ceph-mds 进程，并且会覆盖 [global] 下的同一选项。<br> 实例:<br> host = myserver01</p> </li><li> <p>[client]<br> 描述:<br> [client] 下的配置影响所有客户端（如挂载的 Ceph 文件系统、挂载的块设备等等）。<br> 实例:<br> log file = /var/log/ceph/radosgw.log</p> </li></ul> 
<h6><a id="2__global__global__940"></a>2. 全局设置影响集群内所有守护进程的例程，所以 [global] 可用于设置适用所有守护进程的选项。但可以用这些覆盖 [global] 设置：</h6> 
<pre><code class="prism language-bash">1.在 <span class="token punctuation">[</span>osd<span class="token punctuation">]</span> 、 <span class="token punctuation">[</span>mon<span class="token punctuation">]</span> 、 <span class="token punctuation">[</span>mds<span class="token punctuation">]</span> 下更改某一类进程的配置。
2.更改特定进程的设置，如 <span class="token punctuation">[</span>osd.1<span class="token punctuation">]</span> 。
</code></pre> 
<h6><a id="3__osd__mon__mds__OSD__947"></a>3. 可以统一配置一类守护进程。配置写到 [osd] 、 [mon] 、 [mds] 下时，无须再指定某个特定例程，即可分别影响所有 OSD 、监视器、元数据进程。</h6> 
<p>典型的类范畴配置包括日志尺寸、 filestore 选项等，如：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>osd<span class="token punctuation">]</span>
osd journal size <span class="token operator">=</span> 1000
</code></pre> 
<h6><a id="4_ID__OSD__ID__ID__954"></a>4.也可以配置某个特定例程。一个例程由类型和及其例程 ID 确定， OSD 的例程 ID 只能是数字，但监视器和元数据服务器的 ID 可包含字母和数字。</h6> 
<pre><code class="prism language-bash"><span class="token punctuation">[</span>osd.1<span class="token punctuation">]</span>
<span class="token comment"># 设置只影响osd.1</span>

<span class="token punctuation">[</span>mon.a<span class="token punctuation">]</span>
<span class="token comment"># 设置只影响mon.a</span>

<span class="token punctuation">[</span>mds.b<span class="token punctuation">]</span>
<span class="token comment"># 设置只影响mds.b</span>
</code></pre> 
<h6><a id="5_967"></a>5.元变量</h6> 
<p>元变量大大简化了集群配置。 Ceph 会把配置的元变量展开为具体值；元变量功能很强大，可以用在配置文件的 [global] 、 [osd] 、 [mon] 、 [mds] 段里，类似于 Bash 的 shell 扩展。</p> 
<p>Ceph 支持下列元变量：</p> 
<ul><li> <p>$cluster<br> 描述:<br> 展开为存储集群名字，在同一套硬件上运行多个集群时有用。<br> 实例:<br> /etc/ceph/$cluster.keyring<br> 默认值:<br> ceph</p> </li><li> <p>$type<br> 描述:<br> 可展开为 mds 、 osd 、 mon 中的一个，有赖于当前守护进程的类型。<br> 实例:<br> /var/lib/ceph/$type</p> </li><li> <p>$id<br> 描述:<br> 展开为守护进程标识符； osd.0 应为 0 ， mds.a 是 a 。<br> 实例:<br> /var/lib/ceph/$type/$cluster-$id</p> </li><li> <p>$host<br> 描述:<br> 展开为当前守护进程的主机名。</p> </li><li> <p>$name<br> 描述:<br> 展开为 $type.$id 。<br> 实例:<br> /var/run/ceph/$cluster-$name.asok</p> </li></ul> 
<p><a href="http://docs.ceph.org.cn/rados/configuration/" rel="nofollow">http://docs.ceph.org.cn/rados/configuration/</a></p> 
<h5><a id="__1007"></a>配置文件修改后 设置到所有集群主机</h5> 
<p>初次安装ceph后，将配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点。会保存在ceph节点的/etc/ceph/目录</p> 
<pre><code class="prism language-bash">ceph-deploy admin <span class="token punctuation">{<!-- --></span>admin-node<span class="token punctuation">}</span> <span class="token punctuation">{<!-- --></span>ceph-node<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="_1013"></a>分发配置文件</h5> 
<p>要把改过的配置文件分发给集群内各主机，可用 config push 命令。</p> 
<pre><code class="prism language-bash">ceph-deploy config push <span class="token punctuation">{<!-- --></span>host-name <span class="token punctuation">[</span>host-name<span class="token punctuation">]</span><span class="token punctuation">..</span>.<span class="token punctuation">}</span>
</code></pre> 
<p>配置文件不一致 需要加上 --overwrite-conf 参数 覆盖源主机配置</p> 
<pre><code class="prism language-bash">ceph-deploy --overwrite-conf config push <span class="token punctuation">{<!-- --></span>host-name <span class="token punctuation">[</span>host-name<span class="token punctuation">]</span><span class="token punctuation">..</span>.<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="_1024"></a>检索配置文件</h5> 
<p>要从集群内的一主机检索配置文件，用 config pull 命令。</p> 
<pre><code class="prism language-bash">ceph-deploy config pull host-name
</code></pre> 
<p>就是将 主机上的配置文件传到本地。设置多个主机也只会把第一个主机的配置文件传到本地。</p> 
<h4><a id="ceph_1032"></a>附，一些ceph的理解。属于转载</h4> 
<p>转自<a href="https://www.cnblogs.com/me115/p/6366374.html" rel="nofollow">https://www.cnblogs.com/me115/p/6366374.html</a></p> 
<p>Pool 、PG和OSD<br> Pool是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本分布策略；支持两种类型：副本（replicated）和 纠删码（ Erasure Code）。</p> 
<p>PG（ placement group）是一个放置策略组，它是对象的集合，该集合里的所有对象都具有相同的放置策略；简单点说就是相同PG内的对象都会放到相同的硬盘上； PG是 ceph的核心概念， 服务端数据均衡和恢复的最小粒度就是PG；</p> 
<p>OSD是负责物理存储的进程，一般配置成和磁盘一一对应，一块磁盘启动一个OSD进程；</p> 
<p>下面这张图形象的描绘了它们之间的关系：</p> 
<ul><li>一个Pool里有很多PG，</li><li>一个PG里包含一堆对象；一个对象只能属于一个PG；</li><li>PG有主从之分，一个PG会有主副本和从副本，主从PG分布在不同的osd上。</li></ul> 
<p><img src="https://images2.imgbox.com/6c/ed/1ibr45O5_o.jpg" alt="传说中的经典图"><br> 这个图其实我是觉得不是特别形象的。。。<br> 首先搞清楚了一个概念，存储池算一个逻辑分区，和osd算是同一级。（不能这么说，只是类比一下）一个存储池有许多的PG 构成，这些PG会分布在不同的OSD上，且由于一般存储池都是副本池，所以一个PG可能有副本。由主副本构成的就是一个完整的存储池。</p> 
<p>网上找来的一个关系图，我感觉更清晰明了一点。<br> <img src="https://images2.imgbox.com/5e/ef/P5Lzuatn_o.png" alt="三副本存储池"><br> 一个对象只能存储在一个PG里，一个PG里可以许多不同的对象。一个PG属于一个存储池，一个存储池中很多个PG，一个存储池中的PG可能分布在不同的OSD上，且在三副本存储池中，一个PG会有两个副本，分布在不同的OSD上。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/317f8ea0d81575058ae11c32ec16802a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mapnik 3.0.23 和python-mapnik-3.0.16  安装部署</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/db206fa64acfb8beab73a4c79898aa90/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">单端信号和差分信号的区别</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>