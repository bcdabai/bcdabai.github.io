<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用 EmbeddingBag 和 Embedding 完成词嵌入 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用 EmbeddingBag 和 Embedding 完成词嵌入" />
<meta property="og:description" content="🍨 本文为[🔗365天深度学习训练营学习记录博客\n🍦 参考文章：365天深度学习训练营\n🍖 原作者：[K同学啊 | 接辅导、项目定制]\n🚀 文章来源：[K同学的学习圈子](https://www.yuque.com/mingtian-fkmxf/zxwb45)
使用 EmbeddingBag 和 Embedding 完成词嵌入，首先需要处理文档中的文本，将其转换为适合进行词嵌入的格式，涉及到以下步骤：
文本清洗：移除文档中的特殊字符和标点符号，将文本统一为小写（如果适用）。分词：将文本分割成单词或标记（tokens）。建立词汇表：从分词后的文本中创建一个词汇表，每个唯一的单词对应一个索引。文本向量化：将文本转换为数字形式，以便进行嵌入处理。 第二步，使用 EmbeddingBag 和 Embedding 层进行词嵌入。EmbeddingBag 层适用于处理变长的文本，它会计算所有嵌入向量的平均值或和。而 Embedding 层适用于单个单词或固定长度的序列。
目标文件： 实现代码： from collections import Counter import torch import torch.nn as nn import re # 清洗文本并进行分词 def tokenize(text): # 移除特殊字符和标点，并转换为小写 text = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, text).lower() # 分词 return text.split() # 创建词汇表 def create_vocab(text_tokens): vocab = Counter(text_tokens) vocab = sorted(vocab, key=vocab.get, reverse=True) vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # 索引从1开始 return vocab_to_int # 将文本转换为数字形式 def text_to_int(tokens, vocab_to_int): return [vocab_to_int[word] for word in tokens if word in vocab_to_int] # 定义Embedding和EmbeddingBag层 def define_embedding_layers(vocab_size, embedding_dim=100): embedding = nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f2611a3c556e4331c898274475eb4ddd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-12T18:47:12+08:00" />
<meta property="article:modified_time" content="2024-01-12T18:47:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用 EmbeddingBag 和 Embedding 完成词嵌入</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p>🍨 本文为[🔗365天深度学习训练营学习记录博客\n🍦 参考文章：365天深度学习训练营\n🍖 原作者：[K同学啊 | 接辅导、项目定制]\n🚀 文章来源：[K同学的学习圈子](https://www.yuque.com/mingtian-fkmxf/zxwb45)</p> 
</blockquote> 
<p>使用 <code>EmbeddingBag</code> 和 <code>Embedding</code> 完成词嵌入，首先需要处理文档中的文本，将其转换为适合进行词嵌入的格式，涉及到以下步骤：</p> 
<ol><li><strong>文本清洗</strong>：移除文档中的特殊字符和标点符号，将文本统一为小写（如果适用）。</li><li><strong>分词</strong>：将文本分割成单词或标记（tokens）。</li><li><strong>建立词汇表</strong>：从分词后的文本中创建一个词汇表，每个唯一的单词对应一个索引。</li><li><strong>文本向量化</strong>：将文本转换为数字形式，以便进行嵌入处理。</li></ol> 
<p>第二步，使用 <code>EmbeddingBag</code> 和 <code>Embedding</code> 层进行词嵌入。<code>EmbeddingBag</code> 层适用于处理变长的文本，它会计算所有嵌入向量的平均值或和。而 <code>Embedding</code> 层适用于单个单词或固定长度的序列。</p> 
<h4><strong>目标文件：</strong></h4> 
<p><img alt="" height="273" src="https://images2.imgbox.com/9a/de/8o2lZljE_o.png" width="1200"></p> 
<h4><strong>实现代码： </strong></h4> 
<pre><code class="language-python">from collections import Counter
import torch
import torch.nn as nn
import re

# 清洗文本并进行分词
def tokenize(text):
    # 移除特殊字符和标点，并转换为小写
    text = re.sub(r'[^\w\s]', '', text).lower()
    # 分词
    return text.split()

# 创建词汇表
def create_vocab(text_tokens):
    vocab = Counter(text_tokens)
    vocab = sorted(vocab, key=vocab.get, reverse=True)
    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)} # 索引从1开始
    return vocab_to_int

# 将文本转换为数字形式
def text_to_int(tokens, vocab_to_int):
    return [vocab_to_int[word] for word in tokens if word in vocab_to_int]

# 定义Embedding和EmbeddingBag层
def define_embedding_layers(vocab_size, embedding_dim=100):
    embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)
    embedding_bag = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embedding_dim, mode='mean')
    return embedding, embedding_bag

# 读取文件内容
file_path = 'D:/任务文件 (1).txt'
with open(file_path, 'r', encoding='utf-8') as file:
    file_content = file.read()

# 文本清洗和分词
tokens = tokenize(file_content)

# 创建词汇表
vocab_to_int = create_vocab(tokens)

# 将文本转换为数字形式
int_text = text_to_int(tokens, vocab_to_int)

# 定义嵌入层参数
embedding_dim = 100
vocab_size = len(vocab_to_int) + 1

# 定义Embedding和EmbeddingBag层
embedding, embedding_bag = define_embedding_layers(vocab_size, embedding_dim)

# 转换为tensor以供嵌入层使用
input_tensor = torch.tensor([int_text], dtype=torch.long)

# 使用Embedding和EmbeddingBag进行词嵌入
embedded = embedding(input_tensor)
embedded_bag = embedding_bag(input_tensor)

# 打印结果
print("Embedding shape:", embedded.shape)
print("EmbeddingBag shape:", embedded_bag.shape)
</code></pre> 
<p><img alt="" height="86" src="https://images2.imgbox.com/53/af/4MSq6XR6_o.png" width="901"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/79607b975c3267af95f1192b9c1948c6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">最佳解决方案：如何在网络爬虫中解决验证码</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/81fa872675c9ec713863006e2d178404/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于LVGL编写的windows串口工具： LCOM</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>