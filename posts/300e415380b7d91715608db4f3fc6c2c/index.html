<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>RNN、LSTM、GRU基础原理梳理 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="RNN、LSTM、GRU基础原理梳理" />
<meta property="og:description" content="文章目录 前言一、传统RNN双向RNN深层双向RNN 二、LSTM第一层第二层第三层第四层 三、GRU四、 LSTM和GRU区别参考 前言 为了复习NLP自己的相关知识，整理一个博客
一、传统RNN 传统的RNN也即BasicRNNcell单元。内部的运算过程为，(t-1)时刻的隐层输出与w矩阵相乘，与t时刻的输入乘以u之后的值进行相加，然后经过一个非线性变化（tanh或Relu），然后以此方式传递给下一个时刻。
传统的RNN每一步的隐藏单元只是执行一个简单的tanh或者Relu操作。对于激活函数有遗忘的话，可以参考下面文章复习一下。（博主在前几天的面试就被问到了最基本的激活函数问题）
神经网络激活函数汇总（Sigmoid、tanh、ReLU、LeakyReLU、pReLU、ELU、maxout）
常用激活函数优缺点及性能对比
比如一个句子中有5个词，要给这5个词标注词性，那相应的RNN就是个5层的神经网络，每一层的输入是一个词，每一层的输出是这个词的词性。
聊聊文本的分布式表示： Distributional Representation和Distributed Representation的区别
RNN什么时候网络层次比较深呢？
RNN如果有多个时刻输入的时候，网络层次比较深，此时反向传播的路径比较长。反向传播是根据链式法则，如果开始的梯度小于1的话，则最后时刻的梯度几乎为0，则可理解为梯度消失；反之，若开始的梯度大于1的话，则最后时刻的梯度则非常大，可理解为梯度爆炸。这种情况下，开始应用Relu激活函数。
为什么应用Relu函数呢？
Relu函数在小于0时的梯度为0，大于0的时候梯度为1。使用Relu的好处：1 梯度容易求解；2 不会产生梯度消失或梯度爆炸。
双向RNN 双向RNN认为otot不仅依赖于序列之前的元素，也跟tt之后的元素有关，这在序列挖掘中也是很常见的事实。
深层双向RNN 在双向RNN的基础上，每一步由原来的一个隐藏层变成了多个隐藏层。
二、LSTM 优点：
可以在解决梯度消失和梯度爆炸的问题，还可以从语料中学习到长期依赖关系。
图中可以看出，t时刻有两个输出，上面代表长期输出，下面代表短期输出，用短期的输出与矩阵v运算得到t时刻的输出。
Tanh和sigmoid激活函数的优点？
首先tanh激活函数的范围是（-1,1），负的部分可以减一些信息，也即来控制忘记还是记忆信息，而sigmoid激活函数是用来表示记住多少的信息。
第一层 第一层是个忘记层，决定细胞状态中丢弃什么信息。把 ht-1 和 xt 拼接起来，传给一个Ct-1函数，该函数输出0到1之间的值，这个值乘到细胞状态 [公式] 上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。
第二层 上一步的细胞状态 Ct-1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项 Ct ，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。
第三层 现在可以让旧的细胞状态 Ct-1 与 ft （f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分 it*Ct （i是input输入门的意思），这就生成了新的细胞状态Ct
第四层 最后该决定输出什么了。输出值跟细胞状态有关，把 [公式] 输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。
三、GRU GRU（Gated Recurrent Unit）是LSTM最流行的一个变体，比LSTM模型要简单。
GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。使用门控机制学习长期依赖关系的基本思想和 LSTM 一致，但还是有一些关键区别：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/300e415380b7d91715608db4f3fc6c2c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-31T11:09:18+08:00" />
<meta property="article:modified_time" content="2020-08-31T11:09:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RNN、LSTM、GRU基础原理梳理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_8" rel="nofollow">前言</a></li><li><a href="#RNN_14" rel="nofollow">一、传统RNN</a></li><li><ul><li><a href="#RNN_38" rel="nofollow">双向RNN</a></li><li><a href="#RNN_41" rel="nofollow">深层双向RNN</a></li></ul> 
  </li><li><a href="#LSTM_44" rel="nofollow">二、LSTM</a></li><li><ul><li><a href="#_54" rel="nofollow">第一层</a></li><li><a href="#_58" rel="nofollow">第二层</a></li><li><a href="#_62" rel="nofollow">第三层</a></li><li><a href="#_66" rel="nofollow">第四层</a></li></ul> 
  </li><li><a href="#GRU_71" rel="nofollow">三、GRU</a></li><li><a href="#_LSTMGRU_83" rel="nofollow">四、 LSTM和GRU区别</a></li><li><a href="#_109" rel="nofollow">参考</a></li></ul> 
</div> 
<p></p> 
<hr color="#000000" size='1"'> 
<h2><a id="_8"></a>前言</h2> 
<p>为了复习NLP自己的相关知识，整理一个博客</p> 
<h2><a id="RNN_14"></a>一、传统RNN</h2> 
<p><img src="https://images2.imgbox.com/07/51/zHD9zwVx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/91/1c/5DZtEfwo_o.png" alt="在这里插入图片描述"></p> 
<p>传统的RNN也即BasicRNNcell单元。内部的运算过程为，(t-1)时刻的隐层输出与w矩阵相乘，与t时刻的输入乘以u之后的值进行相加，然后经过一个非线性变化（tanh或Relu），然后以此方式传递给下一个时刻。</p> 
<p>传统的RNN每一步的隐藏单元只是执行一个简单的tanh或者Relu操作。对于激活函数有遗忘的话，可以参考下面文章复习一下。（博主在前几天的面试就被问到了最基本的激活函数问题）<br> <a href="https://blog.csdn.net/edogawachia/article/details/80043673">神经网络激活函数汇总（Sigmoid、tanh、ReLU、LeakyReLU、pReLU、ELU、maxout）</a><br> <a href="https://blog.csdn.net/EngineerHe/article/details/100126694?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.edu_weight&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.edu_weight">常用激活函数优缺点及性能对比</a></p> 
<p>比如一个句子中有5个词，要给这5个词标注词性，那相应的RNN就是个5层的神经网络，每一层的输入是一个词，每一层的输出是这个词的词性。</p> 
<p><img src="https://images2.imgbox.com/0e/1f/TaTud68z_o.png" alt="在这里插入图片描述"><br> <a href="https://zhuanlan.zhihu.com/p/22386230" rel="nofollow">聊聊文本的分布式表示： Distributional Representation和Distributed Representation的区别</a></p> 
<p>RNN什么时候网络层次比较深呢？</p> 
<p>RNN如果有多个时刻输入的时候，网络层次比较深，此时反向传播的路径比较长。反向传播是根据链式法则，如果开始的梯度小于1的话，则最后时刻的梯度几乎为0，则可理解为梯度消失；反之，若开始的梯度大于1的话，则最后时刻的梯度则非常大，可理解为梯度爆炸。这种情况下，开始应用Relu激活函数。</p> 
<p>为什么应用Relu函数呢？</p> 
<p>Relu函数在小于0时的梯度为0，大于0的时候梯度为1。使用Relu的好处：1 梯度容易求解；2 不会产生梯度消失或梯度爆炸。</p> 
<h3><a id="RNN_38"></a>双向RNN</h3> 
<p>双向RNN认为otot不仅依赖于序列之前的元素，也跟tt之后的元素有关，这在序列挖掘中也是很常见的事实。<br> <img src="https://images2.imgbox.com/7e/c0/XucffRcG_o.png" alt="在Bidirectional RNNs网络结构"></p> 
<h3><a id="RNN_41"></a>深层双向RNN</h3> 
<p>在双向RNN的基础上，每一步由原来的一个隐藏层变成了多个隐藏层。<img src="https://images2.imgbox.com/16/c4/TMum70wB_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="LSTM_44"></a>二、LSTM</h2> 
<p>优点：</p> 
<p>可以在解决梯度消失和梯度爆炸的问题，还可以从语料中学习到长期依赖关系。</p> 
<p>图中可以看出，t时刻有两个输出，上面代表长期输出，下面代表短期输出，用短期的输出与矩阵v运算得到t时刻的输出。</p> 
<p>Tanh和sigmoid激活函数的优点？</p> 
<p>首先tanh激活函数的范围是（-1,1），负的部分可以减一些信息，也即来控制忘记还是记忆信息，而sigmoid激活函数是用来表示记住多少的信息。</p> 
<h3><a id="_54"></a>第一层</h3> 
<p><img src="https://images2.imgbox.com/51/9e/UWo50048_o.png" alt="在这里插入图片描述"><br> 第一层是个忘记层，决定细胞状态中丢弃什么信息。把 ht-1 和 xt 拼接起来，传给一个Ct-1函数，该函数输出0到1之间的值，这个值乘到细胞状态 [公式] 上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。</p> 
<h3><a id="_58"></a>第二层</h3> 
<p><img src="https://images2.imgbox.com/97/a3/lOIqTVYj_o.png" alt="在这里插入图片描述"><br> 上一步的细胞状态 Ct-1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项 Ct ，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p> 
<h3><a id="_62"></a>第三层</h3> 
<p><img src="https://images2.imgbox.com/19/e6/R6GssjEN_o.png" alt="在这里插入图片描述"><br> 现在可以让旧的细胞状态 Ct-1 与 ft （f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分 it*Ct （i是input输入门的意思），这就生成了新的细胞状态Ct</p> 
<h3><a id="_66"></a>第四层</h3> 
<p><img src="https://images2.imgbox.com/a7/45/OtGUrM9Z_o.png" alt="在这里插入图片描述"><br> 最后该决定输出什么了。输出值跟细胞状态有关，把 [公式] 输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。</p> 
<h2><a id="GRU_71"></a>三、GRU</h2> 
<p><img src="https://images2.imgbox.com/80/9c/aswZ4dta_o.png" alt="在这里插入图片描述"><br> GRU（Gated Recurrent Unit）是LSTM最流行的一个变体，比LSTM模型要简单。<br> GRU 有两个有两个门，即一个重置门（reset gate）和一个更新门（update gate）。从直观上来说，重置门决定了如何将新的输入信息与前面的记忆相结合，更新门定义了前面记忆保存到当前时间步的量。如果我们将重置门设置为 1，更新门设置为 0，那么我们将再次获得标准 RNN 模型。使用门控机制学习长期依赖关系的基本思想和 LSTM 一致，但还是有一些关键区别：</p> 
<ul><li>GRU 有两个门（重置门与更新门），而 LSTM 有三个门（输入门、遗忘门和输出门）。</li><li>GRU 并不会控制并保留内部记忆（c_t），且没有 LSTM 中的输出门。</li><li>LSTM 中的输入与遗忘门对应于 GRU 的更新门，重置门直接作用于前面的隐藏状态。</li></ul> 
<h2><a id="_LSTMGRU_83"></a>四、 LSTM和GRU区别</h2> 
<ol><li>对memory 的控制</li></ol> 
<p>LSTM： 用output gate 控制，传输给下一个unit</p> 
<p>GRU：直接传递给下一个unit，不做任何控制</p> 
<ol start="2"><li>input gate 和reset gate 作用位置不同</li></ol> 
<p>LSTM: <strong>计算new memory c<sup>(t)c</sup>(t)时 不对上一时刻的信息做任何控制，而是用forget gate 独立的实现这一点</strong></p> 
<p>GRU: 计<strong>算new memory h<sup>(t)h</sup>(t) 时利用reset gate 对上一时刻的信息 进行控制。</strong></p> 
<ol start="3"><li>相似</li></ol> 
<p>最大的相似之处就是， 在从t 到 t-1 的更新时都引入了加法。</p> 
<p>这个加法的好处在于能防止梯度弥散，因此LSTM和GRU都比一般的RNN效果更好。</p> 
<h2><a id="_109"></a>参考</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/37644325" rel="nofollow">RNN、LSTM、GRU基础原理篇</a><br> <a href="https://zhuanlan.zhihu.com/p/46836364" rel="nofollow">RNN,LSTM,GRU</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e619f47e9cfdf249e82528cc406f115b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java中你所需要了解的线程Thread知识</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4833d22fc3d61810fb3b2b61b15cc5a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Expected model to produce a [1 N] shaped tensor where N is the number of labels, instead it produced</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>