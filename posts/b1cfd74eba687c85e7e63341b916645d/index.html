<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CIF10å®æˆ˜(ResNet18) - ç¼–ç¨‹å¤§ç™½çš„åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CIF10å®æˆ˜(ResNet18)" />
<meta property="og:description" content="å®ç°18 å±‚çš„æ·±åº¦æ®‹å·®ç½‘ç»œ ResNet18ï¼Œå¹¶åœ¨ CIFAR10 å›¾ç‰‡æ•°æ®é›†ä¸Šè®­ç»ƒä¸æµ‹è¯•ã€‚æ ‡å‡†çš„ ResNet18 æ¥å—è¾“å…¥ä¸º224 Ã— 224 å¤§å°çš„å›¾ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬å°† ResNet18 è¿›è¡Œé€‚é‡è°ƒæ•´ï¼Œä½¿å¾—å®ƒè¾“å…¥å¤§å°ä¸º32 Ã— 32ï¼Œè¾“å‡ºç»´åº¦ä¸º 10ã€‚è°ƒæ•´åçš„ ResNet18 ç½‘ç»œç»“æ„å¦‚å›¾ï¼š
ä¸€ã€æ•°æ®é›†åŠ è½½ä»¥åŠæ•°æ®é›†é¢„å¤„ç† def preprocess(x, y): # å°†æ•°æ®æ˜ å°„åˆ°-1~1 x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1 y = tf.cast(y, dtype=tf.int32) # ç±»å‹è½¬æ¢ return x, y (x, y), (x_test, y_test) = load.load_data() # åŠ è½½æ•°æ®é›† y = tf.squeeze(y, axis=1) # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦ y_test = tf.squeeze(y_test, axis=1) # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦ print(x.shape, y.shape, x_test.shape, y_test.shape) train_db = tf.data.Dataset.from_tensor_slices((x, y)) # æ„å»ºè®­ç»ƒé›† # éšæœºæ‰“æ•£ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ– train_db = train_db." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/b1cfd74eba687c85e7e63341b916645d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-13T14:55:25+08:00" />
<meta property="article:modified_time" content="2022-02-13T14:55:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§ç™½çš„åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§ç™½çš„åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CIF10å®æˆ˜(ResNet18)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p>å®ç°18 å±‚çš„æ·±åº¦æ®‹å·®ç½‘ç»œ ResNet18ï¼Œå¹¶åœ¨ CIFAR10 å›¾ç‰‡æ•°æ®é›†ä¸Šè®­ç»ƒä¸æµ‹è¯•ã€‚æ ‡å‡†çš„ ResNet18 æ¥å—è¾“å…¥ä¸º224 Ã— 224 å¤§å°çš„å›¾ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬å°† ResNet18 è¿›è¡Œé€‚é‡è°ƒæ•´ï¼Œä½¿å¾—å®ƒè¾“å…¥å¤§å°ä¸º32 Ã— 32ï¼Œè¾“å‡ºç»´åº¦ä¸º 10ã€‚è°ƒæ•´åçš„ ResNet18 ç½‘ç»œç»“æ„å¦‚å›¾ï¼š<br><img alt="" height="216" src="https://images2.imgbox.com/7e/59/6E9431FL_o.png" width="1200"></p> 
<h2>ä¸€ã€æ•°æ®é›†åŠ è½½ä»¥åŠæ•°æ®é›†é¢„å¤„ç†</h2> 
<pre><code class="language-python">def preprocess(x, y):
    # å°†æ•°æ®æ˜ å°„åˆ°-1~1
    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1
    y = tf.cast(y, dtype=tf.int32)  # ç±»å‹è½¬æ¢
    return x, y


(x, y), (x_test, y_test) = load.load_data()  # åŠ è½½æ•°æ®é›†
y = tf.squeeze(y, axis=1)  # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦
y_test = tf.squeeze(y_test, axis=1)  # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦
print(x.shape, y.shape, x_test.shape, y_test.shape)

train_db = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºè®­ç»ƒé›†
# éšæœºæ‰“æ•£ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–
train_db = train_db.shuffle(1000).map(preprocess).batch(512)

test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))  # æ„å»ºæµ‹è¯•é›†
# é¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–
test_db = test_db.map(preprocess).batch(512)
# é‡‡æ ·ä¸€ä¸ªæ ·æœ¬
sample = next(iter(train_db))
</code></pre> 
<p>æ•°æ®é›†çš„å„ç§å¤„ç†è¿˜æ˜¯åˆä»¥å‰ä¸€æ ·ã€‚ç”±äºç½‘ä¸Šåœ¨çº¿ä¸‹è½½æ•°æ®é›†ä¸è¡Œï¼Œä½¿ç”¨çš„æ˜¯è‡ªå®šä¹‰çš„å‡½æ•°æ¥åŠ è½½æ•°æ®çš„ã€‚æ•°æ®é›†åŠ è½½ä»¥åï¼Œæ„å»ºè®­ç»ƒé›†åˆæµ‹è¯•é›†å¹¶è¿›è¡Œç›¸åº”çš„ç±»å‹è½¬æ¢ï¼Œè®­ç»ƒé›†é™¤äº†è¦è¿›è¡Œæ‰¹å¤„ç†è¿˜è¦è¿›è¡Œæ‰“æ•£ã€‚</p> 
<p></p> 
<h2>äºŒã€ç½‘ç»œæ¨¡å‹æ„å»º</h2> 
<h3>1. BasicBlock</h3> 
<p>é¦–å…ˆå®ç°ä¸­é—´ä¸¤ä¸ªå·ç§¯å±‚ï¼Œ Skip Connection 1x1 å·ç§¯å±‚çš„æ®‹å·®æ¨¡å—</p> 
<pre><code class="language-python">class BasicBlock(layers.Layer):
    # æ®‹å·®æ¨¡å—
    def __init__(self, filter_num, stride=1):
        super(BasicBlock, self).__init__()
        # ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ
        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')
        self.bn1 = layers.BatchNormalization()
        self.relu = layers.Activation('relu')
        # ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ
        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')
        self.bn2 = layers.BatchNormalization()
        # paddingéƒ½æ˜¯same,æ‰€ä»¥åªæœ‰stride=1,è¾“å…¥ä¸è¾“å‡ºå½¢çŠ¶ç›¸åŒï¼Œå¦åˆ™é«˜å®½å‡å°‘ä¸ºåŸæ¥çš„1/s
        if stride != 1:  # é€šè¿‡1x1å·ç§¯å®ŒæˆshapeåŒ¹é…
            self.downsample = Sequential()
            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))
        else:  # shapeåŒ¹é…ï¼Œç›´æ¥çŸ­æ¥
            self.downsample = lambda x: x

    def call(self, inputs, training=None):

        # [b, h, w, c]ï¼Œé€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ
        out = self.conv1(inputs)
        out = self.bn1(out)
        out = self.relu(out)
        # é€šè¿‡ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ
        out = self.conv2(out)
        out = self.bn2(out)
        # é€šè¿‡identityæ¨¡å—
        identity = self.downsample(inputs)
        # 2æ¡è·¯å¾„è¾“å‡ºç›´æ¥ç›¸åŠ 
        output = layers.add([out, identity])
        output = tf.nn.relu(output)  # æ¿€æ´»å‡½æ•°
        return output</code></pre> 
<p>é€šè¿‡build_resblock å¯ä»¥ä¸€æ¬¡å®Œæˆå¤šä¸ªæ®‹å·®æ¨¡å—çš„æ–°å»º</p> 
<pre><code class="language-python">    def build_resblock(self, filter_num, blocks, stride=1):
        # è¾…åŠ©å‡½æ•°ï¼Œå †å filter_numä¸ªBasicBlock
        res_blocks = Sequential()
        # åªæœ‰ç¬¬ä¸€ä¸ªBasicBlockçš„æ­¥é•¿å¯èƒ½ä¸ä¸º1ï¼Œå®ç°ä¸‹é‡‡æ ·
        res_blocks.add(BasicBlock(filter_num, stride))
        
        # æ¯ä¸ªæ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ª blocks ä¸ªBasicBlock
        for _ in range(1, blocks):  # å…¶ä»–BasicBlockæ­¥é•¿éƒ½ä¸º1
            res_blocks.add(BasicBlock(filter_num, stride=1))

        return res_blocks</code></pre> 
<h3>2.ResNet</h3> 
<p>åœ¨è®¾è®¡æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€èˆ¬æŒ‰ç…§ç‰¹å¾å›¾é«˜å®½â„/ğ‘¤é€æ¸å‡å°‘ï¼Œé€šé“æ•°ğ‘é€æ¸å¢å¤§çš„ç»éªŒæ³•åˆ™ã€‚å¯ä»¥é€šè¿‡å †å é€šé“æ•°é€æ¸å¢å¤§çš„ Res Block æ¥å®ç°é«˜å±‚ç‰¹å¾çš„æå–ã€‚ä¸‹é¢æ¥å®ç°é€šç”¨çš„ ResNet ç½‘ç»œæ¨¡å‹ã€‚</p> 
<pre><code class="language-python"># é€šç”¨çš„ResNetå®ç°ç±»
class ResNet(keras.Model):
    def __init__(self, layer_dims, num_classes=10):
        # layer_dims:[2, 2, 2, 2]  4ä¸ªResBlock,æ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ªbasicblock  num_classes:åˆ†ç±»æ•°ç›®
        super(ResNet, self).__init__()
        # æ ¹ç½‘ç»œï¼Œé¢„å¤„ç†
        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),
                                layers.BatchNormalization(),
                                layers.Activation('relu'),
                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')
                                ])
        # å †å 4ä¸ªBlockï¼Œæ¯ä¸ªblockåŒ…å«äº†å¤šä¸ªBasicBlock,è®¾ç½®æ­¥é•¿ä¸ä¸€æ ·
        # åœ¨è®¾è®¡æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€èˆ¬æŒ‰ç…§ç‰¹å¾å›¾é«˜å®½â„/ğ‘¤é€æ¸å‡å°‘ï¼Œé€šé“æ•°ğ‘é€æ¸å¢å¤§çš„ç»éªŒæ³•åˆ™ã€‚
        # æ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ªbasicblock
        self.layer1 = self.build_resblock(64, layer_dims[0])
        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)
        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)
        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)

        # è¾“å‡ºï¼š[b, 512, h,w],æ— æ³•ç¡®å®šh,w

        # é€šè¿‡Poolingå±‚å°†é«˜å®½é™ä½ä¸º1x1, ç›¸å½“ç”±äºæ‰“å¹³æˆå…¨è¿æ¥å±‚
        self.avgpool = layers.GlobalAveragePooling2D()
        # æœ€åè¿æ¥ä¸€ä¸ªå…¨è¿æ¥å±‚åˆ†ç±»
        self.fc = layers.Dense(num_classes)

    # å‘å‰è®¡ç®—
    def call(self, inputs, training=None):
        # é€šè¿‡æ ¹ç½‘ç»œ
        x = self.stem(inputs)
        # ä¸€æ¬¡é€šè¿‡4ä¸ªæ¨¡å—
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # é€šè¿‡æ± åŒ–å±‚
        x = self.avgpool(x)
        # é€šè¿‡å…¨è¿æ¥å±‚
        x = self.fc(x)

        return x</code></pre> 
<p>é€šè¿‡è°ƒæ•´æ¯ä¸ª Res Block çš„å †å æ•°é‡å’Œé€šé“æ•°å¯ä»¥äº§ç”Ÿä¸åŒçš„ ResNetï¼Œå¦‚é€šè¿‡ 64-64-128-128-256-256-512-512 é€šé“æ•°é…ç½®ï¼Œå…± 8 ä¸ª Res Blockï¼Œå¯å¾—åˆ° ResNet18 çš„ç½‘ç»œæ¨¡å‹ã€‚æ¯ä¸ªResBlock åŒ…å«äº† 2 ä¸ªä¸»è¦çš„å·ç§¯å±‚ï¼Œå› æ­¤å·ç§¯å±‚æ•°é‡æ˜¯8 âˆ™ 2 = 16ï¼ŒåŠ ä¸Šç½‘ç»œæœ«å°¾çš„å…¨è¿æ¥å±‚ï¼Œå…± 18 å±‚ã€‚</p> 
<p>æ³¨æ„ï¼Œåœ¨å·ç§¯å±‚ä¹‹åä¸€èˆ¬æ˜¯æ·»åŠ å…¨è¿æ¥å±‚æ¥è¿›è¡Œç‰¹å¾çš„å‘é‡åŒ–ï¼Œä½†æ˜¯å…¨è¿æ¥å±‚æœ‰ä¸€ä¸ªéå¸¸è‡´å‘½çš„å¼±ç‚¹å°±æ˜¯<strong>å‚æ•°é‡è¿‡å¤§</strong>ï¼Œç‰¹åˆ«æ˜¯ä¸æœ€åä¸€ä¸ªå·ç§¯å±‚ç›¸è¿çš„å…¨è¿æ¥å±‚ã€‚ä¸€æ–¹é¢å¢åŠ äº†Trainingä»¥åŠtestingçš„è®¡ç®—é‡ï¼Œé™ä½äº†é€Ÿåº¦ï¼›å¦å¤–ä¸€æ–¹é¢å‚æ•°é‡è¿‡å¤§å®¹æ˜“è¿‡æ‹Ÿåˆã€‚è™½ç„¶ä½¿ç”¨äº†ç±»ä¼¼dropoutç­‰æ‰‹æ®µå»å¤„ç†ï¼Œè¿˜æ˜¯è¾¾ä¸åˆ°æ•ˆæœã€‚</p> 
<p><span style="background-color:#ed7976;">å…¨è¿æ¥å±‚å°†å·ç§¯å±‚å±•å¼€æˆå‘é‡ä¹‹åï¼Œå†è¦é’ˆå¯¹æ¯ä¸ªfeature mapè¿›è¡Œåˆ†ç±»ï¼ŒGlobal Average Poolingç›´æ¥å®Œæˆè¿™ä¸¤æ­¥æ“ä½œ</span>ã€‚</p> 
<p><img alt="" height="179" src="https://images2.imgbox.com/ca/31/XJZeDCYF_o.png" width="463"></p> 
<p>Â æ‰€ä»¥ï¼Œè¿™é‡Œè¿™å·ç§¯å±‚ä¹‹åå…ˆè¿›è¡Œäº†GlobalAveragePooling2Dï¼Œç„¶åæ‰è¿›è¡Œå…¨è¿æ¥å±‚çš„è¿æ¥ã€‚<span style="color:#b4b4b4;"><span style="background-color:#262728;">å…¨å±€å¹³å‡æ± åŒ–ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œä¸­ç»å¸¸ä½¿ç”¨çš„ä¸€ä¸ªå±‚ï¼Œä½¿ç”¨å‰åçš„å°ºå¯¸åˆ†åˆ«ä¸º</span></span><span style="background-color:#38d8f0;">[B,H,W,C]-&gt;</span><strong><span style="background-color:#38d8f0;">[B,C]</span></strong><span style="background-color:#38d8f0;">.</span></p> 
<p></p> 
<p><br> åˆ›å»ºResNet18å’ŒResNet34ç½‘ç»œæ¨¡å‹ï¼š</p> 
<pre><code class="language-python">def resnet18():
    # é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlockçš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
    return ResNet([2, 2, 2, 2])

def resnet34():
    # é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlockçš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
    return ResNet([3, 4, 6, 3])
</code></pre> 
<p></p> 
<h2>ä¸‰ã€ç½‘ç»œè£…é…</h2> 
<p>åœ¨å®Œæˆç½‘ç»œæ¨¡å‹çš„æ­å»ºåï¼Œéœ€è¦æŒ‡å®šç½‘ç»œä½¿ç”¨çš„ä¼˜åŒ–å™¨å¯¹è±¡ã€Â æŸå¤±å‡½æ•°ç±»å‹ï¼ŒÂ è¯„ä»·æŒ‡æ ‡ç­‰è®¾å®šï¼Œè¿™ä¸€æ­¥ç§°ä¸ºè£…é…</p> 
<pre><code class="language-python">    model = resnet18()  # ResNet18ç½‘ç»œ
    model.build(input_shape=(None, 32, 32, 3))
    model.summary()  # ç»Ÿè®¡ç½‘ç»œå‚æ•°
    optimizer = optimizers.Adam(lr=1e-4)  # æ„å»ºä¼˜åŒ–å™¨</code></pre> 
<p>ä¼˜åŒ–å™¨ä¸»è¦ä½¿ç”¨apply_gradientsæ–¹æ³•ä¼ å…¥å˜é‡å’Œå¯¹åº”æ¢¯åº¦ä»è€Œæ¥å¯¹ç»™å®šå˜é‡è¿›è¡Œè¿­ä»£</p> 
<p></p> 
<h2>å››ã€è®¡ç®—æ¢¯åº¦ï¼Œä»£ä»·å‡½æ•°å¹¶æ›´æ–°å‚æ•°</h2> 
<p>åœ¨ä½¿ç”¨è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½è®¡ç®—æ¢¯åº¦ï¼Œéœ€è¦å°†å‘å‰è®¡ç®—è¿‡ç¨‹æ”¾ç½®åœ¨tf.GradientTape()ç¯å¢ƒä¸­, åˆ©ç”¨GradientTapeå¯¹è±¡çš„gradient()æ–¹æ³•è‡ªåŠ¨æ±‚è§£å‚æ•°çš„æ¢¯åº¦, å¹¶åˆ©ç”¨optimizerså¯¹è±¡æ›´æ–°å‚æ•°</p> 
<pre><code class="language-python">            with tf.GradientTape() as tape:
                # [b, 32, 32, 3] =&gt; [b, 10],å‰å‘ä¼ æ’­
                logits = model(x)
                # [b] =&gt; [b, 10],one-hotç¼–ç 
                y_onehot = tf.one_hot(y, depth=10)
                # è®¡ç®—äº¤å‰ç†µ
                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)
                loss = tf.reduce_mean(loss)
            # è®¡ç®—æ¢¯åº¦ä¿¡æ¯
            grads = tape.gradient(loss, model.trainable_variables)
            # æ›´æ–°ç½‘ç»œå‚æ•°
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            if step % 50 == 0:
                print(epoch, step, 'loss:', float(loss))
</code></pre> 
<h2>äº”ã€æµ‹è¯•</h2> 
<p>åœ¨æµ‹è¯•é˜¶æ®µï¼Œç”±äºä¸éœ€è¦è®°å½•æ¢¯åº¦ä¿¡æ¯ï¼Œä»£ç ä¸€èˆ¬ä¸éœ€è¦å†™åœ¨ with tf.GradientTape() as tape ç¯å¢ƒä¸­ã€‚å‰å‘è®¡ç®—å¾—åˆ°çš„è¾“å‡ºç»è¿‡ softmax å‡½æ•°åï¼Œä»£è¡¨äº†ç½‘ç»œé¢„æµ‹å½“å‰å›¾ç‰‡è¾“å…¥ğ’™å±äºç±»åˆ«ğ‘–çš„æ¦‚ç‡ğ‘ƒ(ğ’™æ ‡ç­¾æ˜¯ğ‘–|ğ‘¥)ï¼Œ ğ‘– âˆˆ 9 ã€‚é€šè¿‡ argmax å‡½æ•°é€‰å–æ¦‚ç‡æœ€å¤§çš„å…ƒç´ æ‰€åœ¨çš„ç´¢å¼•ï¼Œä½œä¸ºå½“å‰ğ’™çš„é¢„æµ‹ç±»åˆ«ï¼Œä¸çœŸå®æ ‡æ³¨ğ‘¦æ¯”è¾ƒï¼Œé€šè¿‡è®¡ç®—æ¯”è¾ƒç»“æœä¸­é—´ True çš„æ•°é‡å¹¶æ±‚å’Œæ¥ç»Ÿè®¡é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬çš„ä¸ªæ•°ï¼Œæœ€åé™¤ä»¥æ€»æ ·æœ¬çš„ä¸ªæ•°ï¼Œå¾—å‡ºç½‘ç»œçš„æµ‹è¯•å‡†ç¡®åº¦</p> 
<pre><code class="language-python">total_num = 0
        total_correct = 0
        for x, y in test_db:
            logits = model(x)
            prob = tf.nn.softmax(logits, axis=1)
            pred = tf.argmax(prob, axis=1)
            pred = tf.cast(pred, dtype=tf.int32)

            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)
            correct = tf.reduce_sum(correct)

            total_num += x.shape[0]
            total_correct += int(correct)

        acc = total_correct / total_num
        print(epoch, 'acc:', acc)</code></pre> 
<p>æµ‹è¯•ç»“æœï¼š</p> 
<p><img alt="" height="194" src="https://images2.imgbox.com/6e/a2/F5pzJ4E1_o.png" width="361"></p> 
<p>Â è·‘äº†2ä¸ªepoch, å‡†ç¡®ç‡æ¯”è¾ƒä½ï¼Œè·‘å®Œåº”è¯¥èƒ½è¾¾åˆ°80%å·¦å³ã€‚</p> 
<p></p> 
<p></p> 
<h2>å®Œæ•´ç¨‹åº</h2> 
<p>resnet.pyæ–‡ä»¶</p> 
<pre><code class="language-python"># -*- codeing = utf-8 -*-
# @Time : 13:21
# @Author:Paranipd
# @File : resnet.py
# @Software:PyCharm

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Sequential


class BasicBlock(layers.Layer):
    # æ®‹å·®æ¨¡å—
    def __init__(self, filter_num, stride=1):
        super(BasicBlock, self).__init__()
        # ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ
        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')
        self.bn1 = layers.BatchNormalization()
        self.relu = layers.Activation('relu')
        # ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ
        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')
        self.bn2 = layers.BatchNormalization()
        # paddingéƒ½æ˜¯same,æ‰€ä»¥åªæœ‰stride=1,è¾“å…¥ä¸è¾“å‡ºå½¢çŠ¶ç›¸åŒï¼Œå¦åˆ™é«˜å®½å‡å°‘ä¸ºåŸæ¥çš„1/s
        if stride != 1:  # é€šè¿‡1x1å·ç§¯å®ŒæˆshapeåŒ¹é…
            self.downsample = Sequential()
            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))
        else:  # shapeåŒ¹é…ï¼Œç›´æ¥çŸ­æ¥
            self.downsample = lambda x: x

    def call(self, inputs, training=None):

        # [b, h, w, c]ï¼Œé€šè¿‡ç¬¬ä¸€ä¸ªå·ç§¯å•å…ƒ
        out = self.conv1(inputs)
        out = self.bn1(out)
        out = self.relu(out)
        # é€šè¿‡ç¬¬äºŒä¸ªå·ç§¯å•å…ƒ
        out = self.conv2(out)
        out = self.bn2(out)
        # é€šè¿‡identityæ¨¡å—
        identity = self.downsample(inputs)
        # 2æ¡è·¯å¾„è¾“å‡ºç›´æ¥ç›¸åŠ 
        output = layers.add([out, identity])
        output = tf.nn.relu(output)  # æ¿€æ´»å‡½æ•°
        return output


# é€šç”¨çš„ResNetå®ç°ç±»
class ResNet(keras.Model):
    def __init__(self, layer_dims, num_classes=10):
        # layer_dims:[2, 2, 2, 2]  4ä¸ªResBlock,æ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ªbasicblock  num_classes:åˆ†ç±»æ•°ç›®
        super(ResNet, self).__init__()
        # æ ¹ç½‘ç»œï¼Œé¢„å¤„ç†
        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),
                                layers.BatchNormalization(),
                                layers.Activation('relu'),
                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')
                                ])
        # å †å 4ä¸ªBlockï¼Œæ¯ä¸ªblockåŒ…å«äº†å¤šä¸ªBasicBlock,è®¾ç½®æ­¥é•¿ä¸ä¸€æ ·
        # åœ¨è®¾è®¡æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€èˆ¬æŒ‰ç…§ç‰¹å¾å›¾é«˜å®½â„/ğ‘¤é€æ¸å‡å°‘ï¼Œé€šé“æ•°ğ‘é€æ¸å¢å¤§çš„ç»éªŒæ³•åˆ™ã€‚
        # æ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ªbasicblock
        self.layer1 = self.build_resblock(64, layer_dims[0])
        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)
        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)
        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)

        # è¾“å‡ºï¼š[b, 512, h,w],æ— æ³•ç¡®å®šh,w

        # é€šè¿‡Poolingå±‚å°†é«˜å®½é™ä½ä¸º1x1, ç›¸å½“ç”±äºæ‰“å¹³æˆå…¨è¿æ¥å±‚
        self.avgpool = layers.GlobalAveragePooling2D()
        # æœ€åè¿æ¥ä¸€ä¸ªå…¨è¿æ¥å±‚åˆ†ç±»
        self.fc = layers.Dense(num_classes)

    # å‘å‰è®¡ç®—
    def call(self, inputs, training=None):
        # é€šè¿‡æ ¹ç½‘ç»œ
        x = self.stem(inputs)
        # ä¸€æ¬¡é€šè¿‡4ä¸ªæ¨¡å—
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # é€šè¿‡æ± åŒ–å±‚
        x = self.avgpool(x)
        # é€šè¿‡å…¨è¿æ¥å±‚
        x = self.fc(x)

        return x

    def build_resblock(self, filter_num, blocks, stride=1):
        # è¾…åŠ©å‡½æ•°ï¼Œå †å filter_numä¸ªBasicBlock
        res_blocks = Sequential()
        # åªæœ‰ç¬¬ä¸€ä¸ªBasicBlockçš„æ­¥é•¿å¯èƒ½ä¸ä¸º1ï¼Œå®ç°ä¸‹é‡‡æ ·
        res_blocks.add(BasicBlock(filter_num, stride))

        # æ¯ä¸ªæ¯ä¸ªResBlockåŒ…å«ä¸¤ä¸ª blocks ä¸ªBasicBlock
        for _ in range(1, blocks):  # å…¶ä»–BasicBlockæ­¥é•¿éƒ½ä¸º1
            res_blocks.add(BasicBlock(filter_num, stride=1))

        return res_blocks


def resnet18():
    # é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlockçš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
    return ResNet([2, 2, 2, 2])

def resnet34():
    # é€šè¿‡è°ƒæ•´æ¨¡å—å†…éƒ¨BasicBlockçš„æ•°é‡å’Œé…ç½®å®ç°ä¸åŒçš„ResNet
    return ResNet([3, 4, 6, 3])
</code></pre> 
<p></p> 
<p>Â mian.py</p> 
<pre><code class="language-python"># -*- codeing = utf-8 -*-
# @Time : 14:22
# @Author:Paranipd
# @File : resnet18_train.py
# @Software:PyCharm

import tensorflow as tf
from tensorflow.keras import layers, optimizers, datasets, Sequential
import os
from resnet import resnet18
import load

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.random.set_seed(2345)


def preprocess(x, y):
    # å°†æ•°æ®æ˜ å°„åˆ°-1~1
    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1
    y = tf.cast(y, dtype=tf.int32)  # ç±»å‹è½¬æ¢
    return x, y


(x, y), (x_test, y_test) = load.load_data()  # åŠ è½½æ•°æ®é›†
y = tf.squeeze(y, axis=1)  # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦
y_test = tf.squeeze(y_test, axis=1)  # åˆ é™¤ä¸å¿…è¦çš„ç»´åº¦
print(x.shape, y.shape, x_test.shape, y_test.shape)

train_db = tf.data.Dataset.from_tensor_slices((x, y))  # æ„å»ºè®­ç»ƒé›†
# éšæœºæ‰“æ•£ï¼Œé¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–
train_db = train_db.shuffle(1000).map(preprocess).batch(512)

test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))  # æ„å»ºæµ‹è¯•é›†
# é¢„å¤„ç†ï¼Œæ‰¹é‡åŒ–
test_db = test_db.map(preprocess).batch(512)
# é‡‡æ ·ä¸€ä¸ªæ ·æœ¬
sample = next(iter(train_db))


def main():
    # [b, 32, 32, 3] =&gt; [b, 1, 1, 512]
    model = resnet18()  # ResNet18ç½‘ç»œ
    model.build(input_shape=(None, 32, 32, 3))
    model.summary()  # ç»Ÿè®¡ç½‘ç»œå‚æ•°
    optimizer = optimizers.Adam(lr=1e-4)  # æ„å»ºä¼˜åŒ–å™¨

    for epoch in range(100):  # è®­ç»ƒepoch

        for step, (x, y) in enumerate(train_db):

            with tf.GradientTape() as tape:
                # [b, 32, 32, 3] =&gt; [b, 10],å‰å‘ä¼ æ’­
                logits = model(x)
                # [b] =&gt; [b, 10],one-hotç¼–ç 
                y_onehot = tf.one_hot(y, depth=10)
                # è®¡ç®—äº¤å‰ç†µ
                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)
                loss = tf.reduce_mean(loss)
            # è®¡ç®—æ¢¯åº¦ä¿¡æ¯
            grads = tape.gradient(loss, model.trainable_variables)
            # æ›´æ–°ç½‘ç»œå‚æ•°
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            if step % 50 == 0:
                print(epoch, step, 'loss:', float(loss))

        total_num = 0
        total_correct = 0
        for x, y in test_db:
            logits = model(x)
            prob = tf.nn.softmax(logits, axis=1)
            pred = tf.argmax(prob, axis=1)
            pred = tf.cast(pred, dtype=tf.int32)

            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)
            correct = tf.reduce_sum(correct)

            total_num += x.shape[0]
            total_correct += int(correct)

        acc = total_correct / total_num
        print(epoch, 'acc:', acc)


if __name__ == '__main__':
    main()
</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/cf9ee71271cde96c520705a2320f9014/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">æ­¥è¿›ç”µæœºç”ŸæˆSæ›²çº¿ä¸Šä½æœº</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9c6fa023fe6f51de84fb12cab2121a22/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">Rediså•æœºä¸»ä»é…ç½®ï¼ˆä¸€ä¸»äºŒä»ï¼‰&#43;å“¨å…µæ¨¡å¼</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§ç™½çš„åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>