<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HiveCatalog 介绍与使用 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="HiveCatalog 介绍与使用" />
<meta property="og:description" content="作者：苏文鹏，腾讯 CSIG 工程师 一、背景 Apache Hive 已经成为了数据仓库生态系统中的核心。它不仅仅是一个用于大数据分析和 ETL 场景的 SQL 引擎，同样它也是一个数据管理平台，可用于发现、定义和演化数据。Flink 与 Hive 的集成包含两个层面：
一是利用了 Hive 的 Metastore 作为持久化的 Catalog，用户可通过 HiveCatalog 将不同会话中的 Flink 元数据存储到 Hive Metastore 中。例如，用户可以使用 HiveCatalog 将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。
二是利用 Flink 来读写 Hive 的表。
HiveCatalog 的设计提供了与 Hive 良好的兼容性，用户可以&#34;开箱即用&#34;的访问其已有的 Hive 数仓。您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。
二、前置准备 创建私有网络 VPC 私有网络（VPC）是一块在腾讯云上自定义的逻辑隔离网络空间，在构建 Oceanus 集群、Hive 组件等服务时选择的网络建议选择同一个 VPC，网络才能互通。否则需要使用对等连接、NAT 网关、VPN 等方式打通网络。私有网络创建步骤请参考 帮助文档 [1]。
创建流计算 Oceanus 集群 流计算 Oceanus 是大数据产品生态体系的实时化分析利器，是基于 Apache Flink 构建的具备一站开发、无缝连接、亚秒延时、低廉成本、安全稳定等特点的企业级实时大数据分析平台。流计算 Oceanus 以实现企业数据价值最大化为目标，加速企业实时化数字化的建设进程。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/01d6de1e8adbcdfbb4c12807ab7c870c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-22T14:43:12+08:00" />
<meta property="article:modified_time" content="2023-05-22T14:43:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">HiveCatalog 介绍与使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;"><img alt="f1f649bc677b4248304291338e475b1b.gif" src="https://images2.imgbox.com/89/1b/nRg0JaRQ_o.gif"></p> 
 <h3>作者：苏文鹏，腾讯 CSIG 工程师</h3> 
 <h3>一、背景</h3> 
 <p>Apache Hive 已经成为了数据仓库生态系统中的核心。它不仅仅是一个用于大数据分析和 ETL 场景的 SQL 引擎，同样它也是一个数据管理平台，可用于发现、定义和演化数据。Flink 与 Hive 的集成包含两个层面：</p> 
 <ul><li> <p>一是利用了 Hive 的 Metastore 作为持久化的 Catalog，用户可通过 HiveCatalog 将不同会话中的 Flink 元数据存储到 Hive Metastore 中。例如，用户可以使用 HiveCatalog 将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。</p> </li><li> <p>二是利用 Flink 来读写 Hive 的表。</p> </li></ul> 
 <p>HiveCatalog 的设计提供了与 Hive 良好的兼容性，用户可以"开箱即用"的访问其已有的 Hive 数仓。您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。</p> 
 <h3>二、前置准备</h3> 
 <h4>创建私有网络 VPC</h4> 
 <p>私有网络（VPC）是一块在腾讯云上自定义的逻辑隔离网络空间，在构建 Oceanus 集群、Hive 组件等服务时选择的网络建议选择同一个 VPC，网络才能互通。否则需要使用对等连接、NAT 网关、VPN 等方式打通网络。私有网络创建步骤请参考 <em><strong>帮助文档</strong></em> [1]。</p> 
 <h4>创建流计算 Oceanus 集群</h4> 
 <blockquote> 
  <p>流计算 Oceanus 是大数据产品生态体系的实时化分析利器，是基于 Apache Flink 构建的具备一站开发、无缝连接、亚秒延时、低廉成本、安全稳定等特点的企业级实时大数据分析平台。流计算 Oceanus 以实现企业数据价值最大化为目标，加速企业实时化数字化的建设进程。</p> 
 </blockquote> 
 <p>在 Oceanus 控制台的【集群管理】-&gt;【新建集群】页面创建集群，选择地域、可用区、VPC、日志、存储，设置初始密码等。VPC 及子网使用刚刚创建好的网络。创建完后 Oceanus 的集群如下：</p> 
 <p style="text-align:center;"><img alt="405ead0c037b7936d19849b0e3e51e19.png" src="https://images2.imgbox.com/31/a7/chyeG1lL_o.png"></p> 
 <h4>创建 EMR 集群</h4> 
 <blockquote> 
  <p>EMR 是云端托管的弹性开源泛 Hadoop 服务，支持 Hive、Kudu、HDFS、Presto、Flink、Druid 等大数据框架，本次示例主要需要使用 Hive、Zookeeper、HDFS、Yarn、Knox 组件。</p> 
 </blockquote> 
 <p>进入 <em><strong>EMR 控制台</strong></em> [2]，单击左上角【创建集群】进行集群的创建，创建过程中注意选择【产品版本】，不同的版本包含的组件不同，笔者这里选择<code>EMR-V2.2.0</code>版本，另外【集群网络】需选择之前创建好的 VPC 及对应的子网。具体过程可参考 <em><strong>创建 EMR 集群</strong></em> [3]。</p> 
 <p style="text-align:center;"><img alt="7972b04dad364d1a3e066afcaa44fe3a.png" src="https://images2.imgbox.com/0d/84/a0J9hzqE_o.png"></p> 
 <h4>创建 Oceanus SQL 作业</h4> 
 <p>上传依赖</p> 
 <p>在 Oceanus 控制台，点击左侧【依赖管理】，点击左上角【新建】新建依赖，上传 Hive 有关的配置文件。</p> 
 <pre class="has"><code class="language-css">hdfs-site.xml
hive-site.xml
hivemetastore-site.xml
hiveserver2-site.xml</code></pre> 
 <p style="text-align:center;"><img alt="020dce769e0172e2a776442d4729c9cc.png" src="https://images2.imgbox.com/36/d7/0x7uHDGK_o.png"></p> 
 <p>创建 SQL 作业</p> 
 <p>在 <em><strong>流计算 Oceanus 控制台</strong></em> 的 <strong>作业管理 &gt; 新建作业</strong> 中新建 <strong>SQL 作业</strong>，选择在新建的集群中新建作业。</p> 
 <p>创建 HiveCatalog</p> 
 <p style="text-align:center;"><img alt="f8ce689664a86d389effa703a9103608.jpeg" src="https://images2.imgbox.com/9d/d5/Yj9g6Aig_o.jpg"></p> 
 <h3>三、Hive Metastore 的用途</h3> 
 <h4>1. 利用 Hive Metastore 作为持久化的 Catalog</h4> 
 <p>创建 Source</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE datagen_source_table (
  id   INT,  
  name STRING ,  
  dt   STRING,
  hr   STRING
) WITH (
   'connector' = 'datagen',
   'rows-per-second'='1'  -- 每秒产生的数据条数
);</code></pre> 
 <p>‍</p> 
 <p>创建 Sink</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE IF NOT EXISTS `_hive235`.`tinatest1`.`jdbc_upsert_sink_table` (
  id   INT,  
  name STRING ,  
  dt   STRING,
  hr   STRING
) WITH (
   -- 指定数据库连接参数
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://xx.xx.xx.xx:3306/tina?rewriteBatchedStatements=true&amp;serverTimezone=Asia/Shanghai', -- 请替换为您的实际 MySQL 连接参数
   'table-name' = 'testhive', -- 需要写入的数据表
   'username' = 'root',             -- 数据库访问的用户名（需要提供 INSERT 权限）
   'password' = 'xxxxxxxxx',        -- 数据库访问的密码
   'sink.buffer-flush.max-rows' = '70000',  -- 批量输出的条数
   'sink.buffer-flush.interval' = '1s'    -- 批量输出的间隔
);</code></pre> 
 <p>‍</p> 
 <p>算子操作</p> 
 <pre class="has"><code class="language-sql">insert into `_hive235`.`tinatest1`.`jdbc_upsert_sink_table` 
select * from datagen_source_table;</code></pre> 
 <p>‍</p> 
 <p>结果验证</p> 
 <p style="text-align:center;"><img alt="33330bb243eee70f3fac7837ee1b85e6.png" src="https://images2.imgbox.com/90/6b/ETCS40Ja_o.png"></p> 
 <h4>2. 读写 Hive 表的数据</h4> 
 <p>创建 Hive 实体表</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `record`(
 `id`   int,
 `name` string)
PARTITIONED BY (
 `dt` string,
 `hr` string)
ROW FORMAT SERDE
 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
 'field.delim'=',',
 'serialization.format'=',')
STORED AS INPUTFORMAT
 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
 'cosn://tinametatest-xxxxxxxx/xxxxx/xxxxx'
TBLPROPERTIES (
 'hive-version'='2.3.5',
 'streaming-source.consume-order'='create-time',
 'streaming-source.enable'='true',
 'streaming-source.monitor-interval'='10s',
 'streaming-source.partition.include'='all',
);


CREATE TABLE `record_target`(
 `id`   int,
 `name` string,
 `dt`   string,
 `hr`   string)
ROW FORMAT SERDE
 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
 'field.delim'=',',
 'serialization.format'=',')
STORED AS INPUTFORMAT
 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
TBLPROPERTIES (
 'hive-version'='2.3.5');</code></pre> 
 <p>算子操作</p> 
 <pre class="has"><code class="language-sql">insert into `_hive235`.`tinatest1`.`record_target` select * from `_hive235`.`tinatest1`.`record`;</code></pre> 
 <p>结果验证</p> 
 <p style="text-align:center;"><img alt="c306794a673df54a7e2f25ec64851aeb.png" src="https://images2.imgbox.com/8d/b0/dMrtAhCS_o.png"></p> 
 <h4>3. Hive 用做维表</h4> 
 <p><em><strong>基于 processing time join 最新 Hive 分区中的数据</strong></em></p> 
 <p>创建 Hive 实体表</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `record2`(
 `id`   int ,
 `name` string)
PARTITIONED BY (
 `dt` string,
 `hr` string)
LOCATION
 'cosn://tinametatest-xxxxxxxxx/xxxxx/xxxxx'
TBLPROPERTIES (
 'hive-version'='2.3.5',
 'streaming-source.consume-order'='partition-name',
 'streaming-source.enable'='true',
 'streaming-source.monitor-interval'='10s',
 'streaming-source.partition.include'='latest');</code></pre> 
 <p>‍</p> 
 <p>创建 Source</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `mysql_cdc_source_table` (
 `id`   INT,
 `name` STRING,
proctime as proctime(),
 PRIMARY KEY (`id`) NOT ENFORCED -- 如果要同步的数据库表定义了主键, 则这里也需要定义
) WITH (
 'connector' = 'mysql-cdc',      -- 固定值 'mysql-cdc'
 'hostname' = 'xx.xx.xx.xx',     -- 数据库的 IP
 'port' = '3306',                -- 数据库的访问端口
 'username' = 'root',            -- 数据库访问的用户名（需要提供 SHOW DATABASES、REPLICATION SLAVE、REPLICATION CLIENT、SELECT 和 RELOAD 权限）
 'password' = 'xxxxxxxx',        -- 数据库访问的密码
 'database-name' = 'tina',       -- 需要同步的数据库
 'table-name' = 'my_table' ,     -- 需要同步的数据表名
 'server-id'='7400-7412'
);</code></pre> 
 <p>‍</p> 
 <p>创建 Sink</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `jdbc_sink_table` (
    `id`   INT PRIMARY key,
    `name` STRING
) WITH (
    -- 指定数据库连接参数
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://xx.xx.xx.xx:3306/tina?rewriteBatchedStatements=true&amp;amp;serverTimezone=Asia/Shanghai',    -- 请替换为您的实际 MySQL 连接参数
    'table-name' = 'hivecdc',         -- 需要写入的数据表
    'username' = 'root',              -- 数据库访问的用户名（需要提供 SHOW DATABASES、REPLICATION SLAVE、REPLICATION CLIENT、SELECT 和 RELOAD 权限）
    'password' = 'xxxxxxxx',               -- 数据库访问的密码
    'sink.buffer-flush.max-rows' = '200',  -- 批量输出的条数
    'sink.buffer-flush.interval' = '2s'    -- 批量输出的间隔
);</code></pre> 
 <p>‍</p> 
 <p>算子操作</p> 
 <pre class="has"><code class="language-sql">INSERT INTO jdbc_sink_table
 SELECT o.id,o.name FROM mysql_cdc_source_table AS o 
JOIN `_hive235`.`tinatest1`.`record2` FOR SYSTEM_TIME AS OF o.proctime AS dim
ON o.id = dim.id;</code></pre> 
 <p>‍</p> 
 <p><em><strong>基于 processing time join 最新 Hive 表中的数据</strong></em></p> 
 <p>创建 Hive 实体表</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `record_batch`(
  `id`   int ,
  `name` string)
PARTITIONED BY (
  `dt` string,
  `hr` string)
LOCATION
  'cosn://tinametatest-xxxxxxxx/xxxxx/xxxxx'
TBLPROPERTIES (
  'hive-version'='2.3.5',
  'lookup.join.cache.ttl'='10s');</code></pre> 
 <p>‍</p> 
 <p>创建 Source</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `mysql_cdc_source_table` (
  `id`    INT,
  `name`  STRING,
  proctime as proctime(),
  PRIMARY KEY (`id`) NOT ENFORCED -- 如果要同步的数据库表定义了主键, 则这里也需要定义
) WITH (
  'connector' = 'mysql-cdc',    -- 固定值 'mysql-cdc'
  'hostname' = 'xx.xx.xx.xx',   -- 数据库的 IP
  'port' = '3306',              -- 数据库的访问端口
  'username' = 'root',          -- 数据库访问的用户名（需要提供 SHOW DATABASES、REPLICATION SLAVE、REPLICATION CLIENT、SELECT 和 RELOAD 权限）
  'password' = 'xxxxxxxx',      -- 数据库访问的密码
  'database-name' = 'tina',     -- 需要同步的数据库
  'table-name' = 'my_table' ,   -- 需要同步的数据表名
  'server-id'='7400-7412'
);</code></pre> 
 <p>‍</p> 
 <p>创建 Sink</p> 
 <pre class="has"><code class="language-sql">CREATE TABLE `jdbc_sink_table` (
    `id`   INT PRIMARY key,
    `name` STRING
) WITH (
    -- 指定数据库连接参数
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://xx.xx.xx.xx:3306/tina?rewriteBatchedStatements=true&amp;amp;serverTimezone=Asia/Shanghai', -- 请替换为您的实际 MySQL 连接参数
    'table-name' = 'hivecdc',      -- 需要写入的数据表
    'username' = 'root',           -- 数据库访问的用户名（需要提供 SHOW DATABASES、REPLICATION SLAVE、REPLICATION CLIENT、SELECT 和 RELOAD 权限）
    'password' = 'xxxxxxxx',       -- 数据库访问的密码
    'sink.buffer-flush.max-rows' = '200',  -- 批量输出的条数
    'sink.buffer-flush.interval' = '2s'    -- 批量输出的间隔
);</code></pre> 
 <p>‍</p> 
 <p>算子操作</p> 
 <pre class="has"><code class="language-sql">insert into jdbc_sink_table
 SELECT o.id,o.name FROM mysql_cdc_source_table AS o 
JOIN `_hive235`.`tinatest1`.`record_batch` FOR SYSTEM_TIME AS OF o.proctime AS dim
ON o.id = dim.id;</code></pre> 
 <p>‍</p> 
 <h3>四、注意事项</h3> 
 <ul><li> <p>配置文件 hive-site.xml 文件中需要配置 Metastore 的路径；</p> </li><li> <p>同一个 SQL 作业中只能使用一个 HiveCatalog；</p> </li><li> <p>读取 Hive 数仓中的表时需要在配置表的 Properties 属性；</p> </li></ul> 
 <h3>五、参考链接</h3> 
 <p>[1] VPC 帮助文档：https://cloud.tencent.com/document/product/215/36515  </p> 
 <p>[2] EMR 控制台：https://console.cloud.tencent.com/emr/  </p> 
 <p>[3] 创建 EMR 集群：https://cloud.tencent.com/document/product/589/10981  </p> 
 <p>[4] Flink 官网：https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/hive/overview/  </p> 
 <p>[5] 使用 HiveCatalog：https://cloud.tencent.com/document/product/849/71854</p> 
 <p></p> 
 <p></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9ab1f00baedcf8aec377c22281ca2366/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">电动力学专题：计算电磁学简介</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c982db56eb1a6f6adbcde67b141ff360/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SolidWorks装配体中让弹簧随装配体运动的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>