<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python爬虫基础篇1 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python爬虫基础篇1" />
<meta property="og:description" content="Python-Crawler 简介：在Http协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是get请求和post请求。
get请求：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用get请求。
post请求：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用post请求。 以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用get方法的请求就一定要改成post请求，这个要视情况而定。
在http协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在url中，第二个是把数据放在body中（在post请求中），第三个就是把数据放在head中。这里介绍在网络爬虫中经常会用到的一些请求头参数：
User-Agent：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候，服务器通过这个参数就可以知道这个请求是由哪种浏览器发送的。如果我们是通过爬虫发送请求，那么我们的User-Agent就是Python，这对于那些有反爬虫机制的网站来说，可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个值为一些浏览器的值，来伪装我们的爬虫。
Referer：表明当前这个请求是从哪个url过来的。这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。
Cookie：http协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用cookie来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送cookie信息了。
200：请求正常，服务器正常的返回数据。
301：永久重定向。比如在访问www.jingdong.com的时候会重定向到www.jd.com。
302：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录页面。
400：请求的url在服务器上找不到。换句话说就是请求url错误。
403：服务器拒绝访问，权限不够。
500：服务器内部错误。可能是服务器出现bug了。
urllib库 urllib`库是中一个最基本的网络请求库。可以模拟浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。`Python urlopen函数： 在的库中，所有和网络请求相关的方法，都被集到模块下面了，以先来看下函数基本的使用：Python3``urllib``urllib.request``urlopen
from urllib import request resp = request.urlopen(&#39;http://www.baidu.com&#39;) print(resp.read()) 实际上，使用浏览器访问百度，右键查看源代码。你会发现，跟我们刚才打印出来的数据是一模一样的。也就是说，上面的三行代码就已经帮我们把百度的首页的全部代码爬下来了。一个基本的url请求对应的python代码真的非常简单。 以下对函数的进行详细讲解： urlopen
url：请求的url。
data：请求的，如果设置了这个值，那么将变成请求。 data``post
返回值：返回值是一个对象，这个对象是一个类文件句柄对象。有、、以及等方法。http.client.HTTPResponse``read(size)``readline``readlines``getcode
urlretrieve函数： 这个函数可以方便的将网页上的一个文件保存到本地。以下代码可以非常方便的将百度的首页下载到本地：
from urllib import request request.urlretrieve(&#39;http://www.baidu.com/&#39;,&#39;baidu.html&#39;) urlencode函数： 用浏览器发送请求的时候，如果url中包含了中文或者其他特殊字符，那么浏览器会自动的给我们进行编码。而如果使用代码发送请求，那么就必须手动的进行编码，这时候就应该使用函数来实现。可以把字典数据转换为编码的数据。示例代码如下：urlencode``urlencode``URL
from urllib import parse data = {&#39;name&#39;:&#39;爬虫基础&#39;,&#39;greet&#39;:&#39;hello world&#39;,&#39;age&#39;:100} qs = parse.urlencode(data) print(qs) parse_qs函数： 可以将经过编码后的url参数进行解码。示例代码如下：
from urllib import parse qs = &#34;name=%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80&amp;greet=hello&#43;world&amp;age=100&#34; print(parse.parse_qs(qs)) urlparse和urlsplit： 有时候拿到一个url，想要对这个url中的各个组成部分进行分割，那么这时候就可以使用或者是来进行分割。示例代码如下：urlparse``urlsplit
from urllib import request,parse ​ url = &#39;http://www." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/709d1e9591c50ebc5924e5ae0a59917a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-14T20:50:54+08:00" />
<meta property="article:modified_time" content="2024-01-14T20:50:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python爬虫基础篇1</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>Python-Crawler</h2> 
<p>简介：在<code>Http</code>协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是<code>get</code>请求和<code>post</code>请求。</p> 
<ol><li> <p><code>get</code>请求：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用<code>get</code>请求。</p> </li><li> <p><code>post</code>请求：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用<code>post</code>请求。 以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用<code>get</code>方法的请求就一定要改成<code>post</code>请求，这个要视情况而定。</p> </li><li> <p>在<code>http</code>协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在url中，第二个是把数据放在<code>body</code>中（在<code>post</code>请求中），第三个就是把数据放在<code>head</code>中。这里介绍在网络爬虫中经常会用到的一些请求头参数：</p> 
  <ol><li> <p><code>User-Agent</code>：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候，服务器通过这个参数就可以知道这个请求是由哪种浏览器发送的。如果我们是通过爬虫发送请求，那么我们的<code>User-Agent</code>就是<code>Python</code>，这对于那些有反爬虫机制的网站来说，可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个值为一些浏览器的值，来伪装我们的爬虫。</p> </li><li> <p><code>Referer</code>：表明当前这个请求是从哪个<code>url</code>过来的。这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。</p> </li><li> <p><code>Cookie</code>：<code>http</code>协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用<code>cookie</code>来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送<code>cookie</code>信息了。</p> </li><li> <p><code>200</code>：请求正常，服务器正常的返回数据。</p> </li><li> <p><code>301</code>：永久重定向。比如在访问<code>www.jingdong.com</code>的时候会重定向到<code>www.jd.com</code>。</p> </li><li> <p><code>302</code>：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录页面。</p> </li><li> <p><code>400</code>：请求的<code>url</code>在服务器上找不到。换句话说就是请求<code>url</code>错误。</p> </li><li> <p><code>403</code>：服务器拒绝访问，权限不够。</p> </li><li> <p><code>500</code>：服务器内部错误。可能是服务器出现<code>bug</code>了。</p> </li></ol></li></ol> 
<h2>urllib库</h2> 
<pre>urllib`库是中一个最基本的网络请求库。可以模拟浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。`Python</pre> 
<h4>urlopen函数：</h4> 
<p>在的库中，所有和网络请求相关的方法，都被集到模块下面了，以先来看下函数基本的使用：<code>Python3``urllib``urllib.request``urlopen</code></p> 
<pre>from urllib import request
resp = request.urlopen('http://www.baidu.com')
print(resp.read())</pre> 
<p>实际上，使用浏览器访问百度，右键查看源代码。你会发现，跟我们刚才打印出来的数据是一模一样的。也就是说，上面的三行代码就已经帮我们把百度的首页的全部代码爬下来了。一个基本的url请求对应的python代码真的非常简单。 以下对函数的进行详细讲解： <code>urlopen</code></p> 
<ol><li> <p><code>url</code>：请求的url。</p> </li><li> <p><code>data</code>：请求的，如果设置了这个值，那么将变成请求。 <code>data``post</code></p> </li><li> <p>返回值：返回值是一个对象，这个对象是一个类文件句柄对象。有、、以及等方法。<code>http.client.HTTPResponse``read(size)``readline``readlines``getcode</code></p> </li></ol> 
<h4>urlretrieve函数：</h4> 
<p>这个函数可以方便的将网页上的一个文件保存到本地。以下代码可以非常方便的将百度的首页下载到本地：</p> 
<pre>from urllib import request
request.urlretrieve('http://www.baidu.com/','baidu.html')</pre> 
<h4>urlencode函数：</h4> 
<p>用浏览器发送请求的时候，如果url中包含了中文或者其他特殊字符，那么浏览器会自动的给我们进行编码。而如果使用代码发送请求，那么就必须手动的进行编码，这时候就应该使用函数来实现。可以把字典数据转换为编码的数据。示例代码如下：<code>urlencode``urlencode``URL</code></p> 
<pre>from urllib import parse
data = {'name':'爬虫基础','greet':'hello world','age':100}
qs = parse.urlencode(data)
print(qs)</pre> 
<h4>parse_qs函数：</h4> 
<p>可以将经过编码后的url参数进行解码。示例代码如下：</p> 
<pre>from urllib import parse
qs = "name=%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80&amp;greet=hello+world&amp;age=100"
print(parse.parse_qs(qs))</pre> 
<h4>urlparse和urlsplit：</h4> 
<p>有时候拿到一个url，想要对这个url中的各个组成部分进行分割，那么这时候就可以使用或者是来进行分割。示例代码如下：<code>urlparse``urlsplit</code></p> 
<pre>from urllib import request,parse
​
url = 'http://www.baidu.com/s?username=zhiliao'
​
result = parse.urlsplit(url)
# result = parse.urlparse(url)
​
print('scheme:',result.scheme)
print('netloc:',result.netloc)
print('path:',result.path)
print('query:',result.query)
urlparse`和基本上是一模一样的。唯一不一样的地方是，里面多了一个属性，而没有这个属性。比如有一个为：，
那么可以获取到，而不可以获取到。中的也用得比较少。`urlsplit``urlparse``params``urlsplit``params``url``url = 'http://www.baidu.com/s;hello?wd=python&amp;username=abc#1'``urlparse``hello``urlsplit``url``params</pre> 
<h4>request.Request类：</h4> 
<p>如果想要在请求的时候增加一些请求头，那么就必须使用类来实现。比如要增加一个，示例代码如下：<code>request.Request``User-Agent</code></p> 
<pre>from urllib import request
​
headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'
}
req = request.Request("http://www.baidu.com/",headers=headers)
resp = request.urlopen(req)
print(resp.read())</pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/876539ee9ccea7374fe1660d06c90002/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux-sata接口</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c95123481715c00a4ed84472f2d93d55/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【已解决】fatal: Authentication failed for ‘https://github.com/.../‘</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>