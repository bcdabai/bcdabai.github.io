<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>水很深的深度学习Task05 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="水很深的深度学习Task05" />
<meta property="og:description" content="文章目录 计算图循环神经网络RNN的发展历程RNN案例长短时记忆网络LSTM 循环神经网络的主要应用问答系统自动作曲机器翻译 计算图 计算图的引入是为了后面更方便的表示网络，计算图是描述计算结构的一种图，它的元素包括节点(node)和边(edge)，节点表示变量，可以是标量、矢量、张量等，而边表示的是某个操作，即函数。
下面这个计算图表示复合函数
循环神经网络 上一章我们已经介绍了CNN，可能我们会想这里为什么还需要构建一种新的网络RNN呢？因为现实生活中存在很多序列化结构，我们需要建立一种更优秀的序列数据模型。
文本：字母和词汇的序列语音：音节的序列视频：图像帧的序列时态数据：气象观测数据，股票交易数据、房价数据等 RNN的发展历程 循环神经网络是一种人工神经网络，它的节点间的连接形成一个遵循时间序列的有向图，它的核心思想是，样本间存在顺序关系，每个样本和它之前的样本存在关联。通过神经网络在时序上的展开，我们能够找到样本之间的序列相关性。
下面给出RNN的一般结构：
其中各个符号的表示： x t , s t , o t x_ t,s_t, o_t xt​,st​,ot​分别表示t时刻的输入、记忆和输出， U , V , W U,V,W U,V,W是RNN的连接权重， b s , b o b_s, b_o bs​,bo​是RNN的偏置， σ , φ σ, φ σ,φ是激活函数， σ σ σ通常选tanh或sigmoid， φ φ φ通常选用softmax。
RNN案例 比如词性标注，
我/n,爱/v购物/n,我/n在/pre华联/n购物/v
Word Embedding：自然语言处理(NLP)中的 一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。比如这里映射到三个向量然后输入：
将神经元的输出存到memory中，memory中值会作为下一时刻的输入。在最开始时刻，给定 memory初始值，然后逐次更新memory中的值。 各种不同的RNN结构
长短时记忆网络 在RNN中，存在一个很重要的问题，就是梯度消失问题，一开始我们不能有效的解决长时依赖问题，其中梯度消失的原因有两个：BPTT算法和激活函数Tanh
有两种解决方案，分别是ReLU函数和门控RNN(LSTM).
LSTM LSTM，即长短时记忆网络，于1997年被Sepp Hochreiter 和Jürgen Schmidhuber提出来，LSTM是一种用于深度学习领域的人工循环神经网络（RNN）结构。一个LSTM单元由输入门、输出门和遗忘门组成，三个门控制信息进出单元。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/434266f2a452d95bb516d18846abd027/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-01T11:24:02+08:00" />
<meta property="article:modified_time" content="2021-12-01T11:24:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">水很深的深度学习Task05</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">计算图</a></li><li><a href="#_5" rel="nofollow">循环神经网络</a></li><li><a href="#RNN_13" rel="nofollow">RNN的发展历程</a></li><li><a href="#RNN_21" rel="nofollow">RNN案例</a></li><li><a href="#_34" rel="nofollow">长短时记忆网络</a></li><li><ul><li><a href="#LSTM_38" rel="nofollow">LSTM</a></li></ul> 
  </li><li><a href="#_45" rel="nofollow">循环神经网络的主要应用</a></li><li><ul><li><a href="#_47" rel="nofollow">问答系统</a></li><li><a href="#_49" rel="nofollow">自动作曲</a></li><li><a href="#_51" rel="nofollow">机器翻译</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>计算图</h2> 
<p>计算图的引入是为了后面更方便的表示网络，计算图是描述计算结构的一种图，它的元素包括节点(node)和边(edge)，节点表示变量，可以是标量、矢量、张量等，而边表示的是某个操作，即函数。<img src="https://images2.imgbox.com/cd/fa/onIQ0KIt_o.png" alt="在这里插入图片描述"><br> 下面这个计算图表示复合函数<br> <img src="https://images2.imgbox.com/91/d0/uYKP97gr_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_5"></a>循环神经网络</h2> 
<p>上一章我们已经介绍了CNN，可能我们会想这里为什么还需要构建一种新的网络RNN呢？因为现实生活中存在很多序列化结构，我们需要建立一种更优秀的序列数据模型。</p> 
<ul><li>文本：字母和词汇的序列</li><li>语音：音节的序列</li><li>视频：图像帧的序列</li><li>时态数据：气象观测数据，股票交易数据、房价数据等</li></ul> 
<h2><a id="RNN_13"></a>RNN的发展历程</h2> 
<p><img src="https://images2.imgbox.com/7d/ba/wc1sL9og_o.png" alt="在这里插入图片描述"><br> 循环神经网络是一种人工神经网络，它的节点间的连接形成一个遵循时间序列的有向图，它的核心思想是，样本间存在顺序关系，每个样本和它之前的样本存在关联。通过神经网络在时序上的展开，我们能够找到样本之间的序列相关性。</p> 
<p>下面给出RNN的一般结构：<br> <img src="https://images2.imgbox.com/cc/82/i3Yumbfs_o.png" alt="在这里插入图片描述"><br> 其中各个符号的表示：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          t 
         
        
       
         , 
        
        
        
          s 
         
        
          t 
         
        
       
         , 
        
        
        
          o 
         
        
          t 
         
        
       
      
        x_ t,s_t, o_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">o</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>分别表示t时刻的输入、记忆和输出，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         U 
        
       
         , 
        
       
         V 
        
       
         , 
        
       
         W 
        
       
      
        U,V,W 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.87777em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">U</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span></span></span></span></span>是RNN的连接权重，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
        
          s 
         
        
       
         , 
        
        
        
          b 
         
        
          o 
         
        
       
      
        b_s, b_o 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是RNN的偏置，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
         , 
        
       
         φ 
        
       
      
        σ, φ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">φ</span></span></span></span></span>是激活函数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        σ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span></span></span></span></span>通常选tanh或sigmoid，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         φ 
        
       
      
        φ 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault">φ</span></span></span></span></span>通常选用softmax。</p> 
<h2><a id="RNN_21"></a>RNN案例</h2> 
<p>比如词性标注，</p> 
<ul><li>我/n,爱/v购物/n,</li><li>我/n在/pre华联/n购物/v<br> Word Embedding：自然语言处理(NLP)中的 一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。比如这里映射到三个向量然后输入：<br> <img src="https://images2.imgbox.com/7b/2d/P6WqEybX_o.png" alt="在这里插入图片描述"><br> 将神经元的输出存到memory中，memory中值会作为下一时刻的输入。在最开始时刻，给定 memory初始值，然后逐次更新memory中的值。</li></ul> 
<p><img src="https://images2.imgbox.com/cc/b9/FXH9IHv9_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/76/77/OJmkgGH0_o.png" alt="在这里插入图片描述"><br> 各种不同的RNN结构<br> <img src="https://images2.imgbox.com/9a/bc/B0cWblWk_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_34"></a>长短时记忆网络</h2> 
<p>在RNN中，存在一个很重要的问题，就是梯度消失问题，一开始我们不能有效的解决长时依赖问题，其中梯度消失的原因有两个：BPTT算法和激活函数Tanh<br> 有两种解决方案，分别是ReLU函数和门控RNN(LSTM).</p> 
<h3><a id="LSTM_38"></a>LSTM</h3> 
<p>LSTM，即长短时记忆网络，于1997年被Sepp Hochreiter 和Jürgen Schmidhuber提出来，LSTM是一种用于深度学习领域的人工循环神经网络（RNN）结构。一个LSTM单元由输入门、输出门和遗忘门组成，三个门控制信息进出单元。<br> <img src="https://images2.imgbox.com/92/5c/7MxwcYcJ_o.png" alt="在这里插入图片描述"></p> 
<ul><li>LSTM依靠贯穿隐藏层的细胞状态实现隐藏单元之间的信息传递，其中只有少量的线性操作</li><li>LSTM引入了“门”机制对细胞状态信息进行添加或删除，由此实现长程记忆</li><li>“门”机制由一个Sigmoid激活函数层和一个向量点乘操作组成，Sigmoid层的输出控制了信息传递的比例</li></ul> 
<h2><a id="_45"></a>循环神经网络的主要应用</h2> 
<h3><a id="_47"></a>问答系统</h3> 
<p><img src="https://images2.imgbox.com/c0/ec/x0q5ExXk_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_49"></a>自动作曲</h3> 
<p><img src="https://images2.imgbox.com/e4/ee/RYRY3Vj0_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_51"></a>机器翻译</h3> 
<p><img src="https://images2.imgbox.com/81/6c/UIg9Ojxy_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/528b7dd0b2cf66d33ff588df0a99df38/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">查看mysql的binlog日志</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eb4305f469c632b89de83455766bbea1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">wordpress4.9漏洞</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>