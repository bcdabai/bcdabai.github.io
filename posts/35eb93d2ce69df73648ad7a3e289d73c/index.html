<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scikit学习-随机梯度下降 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scikit学习-随机梯度下降" />
<meta property="og:description" content="Scikit学习-随机梯度下降 (Scikit Learn - Stochastic Gradient Descent) Here, we will learn about an optimization algorithm in Sklearn, termed as Stochastic Gradient Descent (SGD).
在这里，我们将学习Sklearn中的优化算法，称为随机梯度下降(SGD)。 Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/35eb93d2ce69df73648ad7a3e289d73c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-22T23:48:04+08:00" />
<meta property="article:modified_time" content="2020-09-22T23:48:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scikit学习-随机梯度下降</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size: 16px;"> 
 <h2> Scikit学习-随机梯度下降 <span style="font-weight: bold;">(</span>Scikit Learn - Stochastic Gradient Descent<span style="font-weight: bold;">)</span></h2> 
 <p>Here, we will learn about an optimization algorithm in Sklearn, termed as Stochastic Gradient Descent (SGD).</p> 
 <p> 在这里，我们将学习Sklearn中的优化算法，称为随机梯度下降(SGD)。 </p> 
 <p>Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.</p> 
 <p> 随机梯度下降(SGD)是一种简单而有效的优化算法，用于查找使成本函数最小化的函数参数/系数值。 换句话说，它用于凸损失函数(例如SVM和Logistic回归)下的线性分类器的判别学习。 它已成功应用于大型数据集，因为是针对每个训练实例(而不是在实例结束时)执行系数更新。 </p> 
 <h3> SGD分类器 <span style="font-weight: bold;">(</span>SGD Classifier<span style="font-weight: bold;">)</span></h3> 
 <p>Stochastic Gradient Descent (SGD) classifier basically implements a plain SGD learning routine supporting various loss functions and penalties for classification. Scikit-learn provides <b>SGDClassifier</b> module to implement SGD classification.</p> 
 <p> 随机梯度下降(SGD)分类器基本上实现了简单的SGD学习例程，该例程支持各种损失函数和分类惩罚。 Scikit-learn提供了<b class="raw_b_node">SGDClassifier</b>模块来实现SGD分类。 </p> 
 <h4> 参量 <span style="font-weight: bold;">(</span>Parameters<span style="font-weight: bold;">)</span></h4> 
 <p>Followings table consist the parameters used by <b>SGDClassifier</b> module −</p> 
 <p> 下表包含<b class="raw_b_node">SGDClassifier</b>模块使用的参数- </p> 
 <table class="table table-bordered"><tbody><tr><th>Sr.No</th><th>Parameter &amp; Description</th></tr><tr><td class="ts">1</td><td> <p><i><b>loss</b> − str, default = ‘hinge’</i></p> <p>It represents the loss function to be used while implementing. The default value is ‘hinge’ which will give us a linear SVM. The other options which can be used are −</p> 
     <ul class="list"><li><p><b>log</b> − This loss will give us logistic regression i.e. a probabilistic classifier.</p></li><li><p><b>modified_huber</b> − a smooth loss that brings tolerance to outliers along with probability estimates.</p></li><li><p><b>squared_hinge</b> − similar to ‘hinge’ loss but it is quadratically penalized.</p></li><li><p><b>perceptron</b> − as the name suggests, it is a linear loss which is used by the perceptron algorithm.</p></li></ul> </td></tr><tr><td class="ts">2</td><td> <p><i><b>penalty</b> − str, ‘none’, ‘l2’, ‘l1’, ‘elasticnet’</i></p> <p>It is the regularization term used in the model. By default, it is L2. We can use L1 or ‘elasticnet; as well but both might bring sparsity to the model, hence not achievable with L2.</p> </td></tr><tr><td class="ts">3</td><td> <p><i><b>alpha</b> − float, default = 0.0001</i></p> <p>Alpha, the constant that multiplies the regularization term, is the tuning parameter that decides how much we want to penalize the model. The default value is 0.0001.</p> </td></tr><tr><td class="ts">4</td><td> <p><i><b>l1_ratio</b> − float, default = 0.15</i></p> <p>This is called the ElasticNet mixing parameter. Its range is 0 &lt; = l1_ratio &lt; = 1. If l1_ratio = 1, the penalty would be L1 penalty. If l1_ratio = 0, the penalty would be an L2 penalty.</p> </td></tr><tr><td class="ts">5</td><td> <p><i><b>fit_intercept</b> − Boolean, Default=True</i></p> <p>This parameter specifies that a constant (bias or intercept) should be added to the decision function. No intercept will be used in calculation and data will be assumed already centered, if it will set to false.</p> </td></tr><tr><td class="ts">6</td><td> <p><i><b>tol</b> − float or none, optional, default = 1.e-3</i></p> <p>This parameter represents the stopping criterion for iterations. Its default value is False but if set to None, the iterations will stop when 𝒍<b><i>loss</i></b> &gt; <b><i>best_loss - tol for n_iter_no_change</i></b>successive epochs.</p> </td></tr><tr><td class="ts">7</td><td> <p><i><b>shuffle</b> − Boolean, optional, default = True</i></p> <p>This parameter represents that whether we want our training data to be shuffled after each epoch or not.</p> </td></tr><tr><td class="ts">8</td><td> <p><i><b>verbose</b> − integer, default = 0</i></p> <p>It represents the verbosity level. Its default value is 0.</p> </td></tr><tr><td class="ts">9</td><td> <p><i><b>epsilon</b> − float, default = 0.1</i></p> <p>This parameter specifies the width of the insensitive region. If loss = ‘epsilon-insensitive’, any difference, between current prediction and the correct label, less than the threshold would be ignored.</p> </td></tr><tr><td class="ts">10</td><td> <p><i><b>max_iter</b> − int, optional, default = 1000</i></p> <p>As name suggest, it represents the maximum number of passes over the epochs i.e. training data.</p> </td></tr><tr><td class="ts">11</td><td> <p><i><b>warm_start</b> − bool, optional, default = false</i></p> <p>With this parameter set to True, we can reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase the previous solution.</p> </td></tr><tr><td class="ts">12</td><td> <p><i><b>random_state</b> − int, RandomState instance or None, optional, default = none</i></p> <p>This parameter represents the seed of the pseudo random number generated which is used while shuffling the data. Followings are the options.</p> 
     <ul class="list"><li><p><b>int</b> − In this case, <b><i>random_state</i></b> is the seed used by random number generator.</p></li><li><p><b>RandomState instance</b> − In this case, <b>random_state</b> is the random number generator.</p></li><li><p><b>None</b> − In this case, the random number generator is the RandonState instance used by np.random.</p></li></ul> </td></tr><tr><td class="ts">13</td><td> <p><i><b>n_jobs</b> − int or none, optional, Default = None</i></p> <p>It represents the number of CPUs to be used in OVA (One Versus All) computation, for multi-class problems. The default value is none which means 1.</p> </td></tr><tr><td class="ts">14</td><td> <p><i><b>learning_rate</b> − string, optional, default = ‘optimal’</i></p> 
     <ul class="list"><li><p>If learning rate is ‘constant’, eta = eta0;</p></li><li><p>If learning rate is ‘optimal’, eta = 1.0/(alpha*(t+t0)), where t0 is chosen by Leon Bottou;</p></li><li><p>If learning rate = ‘invscalling’, eta = eta0/pow(t, power_t).</p></li><li><p>If learning rate = ‘adaptive’, eta = eta0.</p></li></ul></td></tr><tr><td class="ts">15</td><td> <p><i><b>eta0</b> − double, default = 0.0</i></p> <p>It represents the initial learning rate for above mentioned learning rate options i.e. ‘constant’, ‘invscalling’, or ‘adaptive’.</p> </td></tr><tr><td class="ts">16</td><td> <p><i><b>power_t</b> − idouble, default =0.5</i></p> <p>It is the exponent for ‘incscalling’ learning rate.</p> </td></tr><tr><td class="ts">17</td><td> <p><i><b>early_stopping</b> − bool, default = False</i></p> <p>This parameter represents the use of early stopping to terminate training when validation score is not improving. Its default value is false but when set to true, it automatically set aside a stratified fraction of training data as validation and stop training when validation score is not improving.</p> </td></tr><tr><td class="ts">18</td><td> <p><i><b>validation_fraction</b> − float, default = 0.1</i></p> <p>It is only used when early_stopping is true. It represents the proportion of training data to set asides as validation set for early termination of training data..</p> </td></tr><tr><td class="ts">19</td><td> <p><i><b>n_iter_no_change</b> − int, default=5</i></p> <p>It represents the number of iteration with no improvement should algorithm run before early stopping.</p> </td></tr><tr><td class="ts">20</td><td> <p><i><b>classs_weight</b> − dict, {class_label: weight} or “balanced”, or None, optional</i></p> <p>This parameter represents the weights associated with classes. If not provided, the classes are supposed to have weight 1.</p> </td></tr><tr><td class="ts">20</td><td> <p><i><b>warm_start</b> − bool, optional, default = false</i></p> <p>With this parameter set to True, we can reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase the previous solution.</p> </td></tr><tr><td class="ts">21</td><td> <p><i><b>average</b> − iBoolean or int, optional, default = false</i></p> <p>It represents the number of CPUs to be used in OVA (One Versus All) computation, for multi-class problems. The default value is none which means 1.</p> </td></tr></tbody></table> 
 <table class="table table-bordered"><tbody><tr><th> 序号 </th><th> 参数及说明 </th></tr><tr><td class="ts"> 1个 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">损失</b> -str，默认='铰链'</i> </p> <p> 它表示实现时要使用的损失函数。 默认值为“ hinge”，这将为我们提供线性SVM。 可以使用的其他选项是- </p> 
     <ul class="list"><li><p> <b class="raw_b_node">log-</b>这种损失将使我们进行逻辑回归，即概率分类器。 </p></li><li><p> <b class="raw_b_node">modified_huber-</b>平滑损失，对异常值和概率估计值具有容忍度。 </p></li><li><p> <b class="raw_b_node">squared_hinge-</b>与'hinge'损失相似，但二次惩罚。 </p></li><li><p> <b class="raw_b_node">感知器</b> -顾名思义，这是感知器算法使用的线性损耗。 </p></li></ul> </td></tr><tr><td class="ts"> 2 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">惩罚</b> -str，'none'，'l2'，'l1'，'elasticnet'</i> </p> <p> 它是模型中使用的正则化术语。 默认情况下为L2。 我们可以使用L1或'elasticnet; 同样，但是两者都可能给模型带来稀疏性，因此L2无法实现。 </p> </td></tr><tr><td class="ts"> 3 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">alpha-</b>浮点数，默认= 0.0001</i> </p> <p> Alpha(乘以正则项的常数)是调整参数，它决定了我们要对模型进行多少惩罚。 默认值为0.0001。 </p> </td></tr><tr><td class="ts"> 4 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">l1_ratio-</b>浮点数，默认= 0.15</i> </p> <p> 这称为ElasticNet混合参数。 其范围为0 &lt;= l1_ratio &lt;=1。如果l1_ratio = 1，则惩罚为L1惩罚。 如果l1_ratio = 0，则惩罚为L2惩罚。 </p> </td></tr><tr><td class="ts"> 5 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">fit_intercept-</b>布尔值，默认为True</i> </p> <p> 此参数指定应将常量(偏差或截距)添加到决策函数。 如果将其设置为false，则不会在计算中使用截距，并且将假定数据已居中。 </p> </td></tr><tr><td class="ts"> 6 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">tol-</b>浮动或无，可选，默认= 1.e-3</i> </p> <p> 此参数表示迭代的停止标准。 它的默认值为False，但如果设置为None，则当n <b class="raw_b_node"><i class="raw_i_node">损失</i></b> &gt; <b class="raw_b_node"><i class="raw_i_node">best_loss-</i></b>连续<b class="raw_b_node"><i class="raw_i_node">n_iter_no_change</i></b>个时期的<b class="raw_b_node"><i class="raw_i_node">tol</i></b>时，迭代将停止。 </p> </td></tr><tr><td class="ts"> 7 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">shuffle-</b>布尔值，可选，默认= True</i> </p> <p> 此参数表示我们是否希望在每个时期之后对训练数据进行混洗。 </p> </td></tr><tr><td class="ts"> 8 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">详细</b> -整数，默认= 0</i> </p> <p> 它代表了详细程度。 默认值为0。 </p> </td></tr><tr><td class="ts"> 9 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">epsilon-</b>浮动，默认= 0.1</i> </p> <p> 此参数指定不敏感区域的宽度。 如果损失=“对ε不敏感”，则当前预测与正确标签之间的任何差异(小于阈值)将被忽略。 </p> </td></tr><tr><td class="ts"> 10 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">max_iter</b> -int，可选，默认= 1000</i> </p> <p> 顾名思义，它代表历时的最大通过次数，即训练数据。 </p> </td></tr><tr><td class="ts"> 11 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">warm_start</b> -bool，可选，默认= false</i> </p> <p> 通过将此参数设置为True，我们可以重用上一个调用的解决方案以适合初始化。 如果我们选择默认值，即false，它将删除先前的解决方案。 </p> </td></tr><tr><td class="ts"> 12 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">random_state</b> -int，RandomState实例或无，可选，默认=无</i> </p> <p> 此参数表示生成的伪随机数的种子，在对数据进行混洗时会使用该种子。 以下是选项。 </p> 
     <ul class="list"><li><p> <b class="raw_b_node">INT</b> -在这种情况下<b class="raw_b_node"><i class="raw_i_node">，random_state</i></b>是由随机数生成所使用的种子。 </p></li><li><p> <b class="raw_b_node">RandomState实例</b> -在这种情况下， <b class="raw_b_node">random_state</b>是随机数生成器。 </p></li><li><p> <b class="raw_b_node">无</b> -在这种情况下，随机数生成器是np.random使用的RandonState实例。 </p></li></ul> </td></tr><tr><td class="ts"> 13 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_jobs-</b>整数或无，可选，默认=无</i> </p> <p> 它表示用于多类问题的OVA(一个对所有)计算中使用的CPU数量。 默认值为none，表示1。 </p> </td></tr><tr><td class="ts"> 14 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">learning_rate-</b>字符串，可选，默认='最优'</i> </p> 
     <ul class="list"><li><p> 如果学习速率为“恒定”，则eta = eta0; </p></li><li><p> 如果学习率是“最佳”，则eta = 1.0 /(alpha *(t + t0))，其中t0由Leon Bottou选择； </p></li><li><p> 如果学习率='invscalling'，则eta = eta0 / pow(t，power_t)。 </p></li><li><p> 如果学习率=“自适应”，则eta = eta0。 </p></li></ul> </td></tr><tr><td class="ts"> 15 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">eta0-</b>两倍，默认= 0.0</i> </p> <p> 它代表上述学习率选项(即“恒定”，“渐进”或“自适应”)的初始学习率。 </p> </td></tr><tr><td class="ts"> 16 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">power_t</b> -idouble，默认= 0.5</i> </p> <p> 它是“增加”学习率的指数。 </p> </td></tr><tr><td class="ts"> 17 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">early_stopping</b> − bool，默认= False</i> </p> <p> 此参数表示当验证分数没有提高时，使用早期停止来终止训练。 它的默认值是false，但是当设置为true时，它会自动将训练数据的分层部分留作验证，并在验证得分没有提高时停止训练。 </p> </td></tr><tr><td class="ts"> 18 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">validation_fraction-</b>浮点数，默认= 0.1</i> </p> <p> 仅当early_stopping为true时使用。 它表示将训练数据设置为辅助参数以尽早终止训练数据的比例。 </p> </td></tr><tr><td class="ts"> 19 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_iter_no_change-</b>整数，默认= 5</i> </p> <p> 它表示算法的迭代次数，如果算法在尽早停止之前仍未运行，则不会有所改善。 </p> </td></tr><tr><td class="ts"> 20 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">classs_weight</b> -dict，{class_label：weight}或“ balanced”，或者“无”，可选</i> </p> <p> 此参数表示与类关联的权重。 如果未提供，则该类的权重应为1。 </p> </td></tr><tr><td class="ts"> 20 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">warm_start</b> -bool，可选，默认= false</i> </p> <p> 通过将此参数设置为True，我们可以重用上一个调用的解决方案以适合初始化。 如果我们选择默认值，即false，它将删除先前的解决方案。 </p> </td></tr><tr><td class="ts"> 21 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">平均值</b> -iBoolean或int，可选，默认= false</i> </p> <p> 它表示用于多类问题的OVA(一个对所有)计算中使用的CPU数量。 默认值为none，表示1。 </p> </td></tr></tbody></table> 
 <h4> 属性 <span style="font-weight: bold;">(</span>Attributes<span style="font-weight: bold;">)</span></h4> 
 <p>Following table consist the attributes used by <b>SGDClassifier</b> module −</p> 
 <p> 下表包含<b class="raw_b_node">SGDClassifier</b>模块使用的属性- </p> 
 <table class="table table-bordered"><tbody><tr><th>Sr.No</th><th>Attributes &amp; Description</th></tr><tr><td class="ts">1</td><td> <p><i><b>coef_</b> − array, shape (1, n_features) if n_classes==2, else (n_classes, n_features)</i></p> <p>This attribute provides the weight assigned to the features.</p> </td></tr><tr><td class="ts">2</td><td> <p><i><b>intercept_</b> − array, shape (1,) if n_classes==2, else (n_classes,)</i></p> <p>It represents the independent term in decision function.</p> </td></tr><tr><td class="ts">3</td><td> <p><i><b>n_iter_</b> − int</i></p> <p>It gives the number of iterations to reach the stopping criterion.</p> </td></tr></tbody></table> 
 <table class="table table-bordered"><tbody><tr><th> 序号 </th><th> 属性和说明 </th></tr><tr><td class="ts"> 1个 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">coef_-</b>数组，如果n_classes == 2，则形状为(1，n_features)，否则为(n_classes，n_features)</i> </p> <p> 此属性提供分配给要素的权重。 </p> </td></tr><tr><td class="ts"> 2 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">intercept_</b> -阵列的形状(1)如果n_classes == 2，否则(n_classes，)</i> </p> <p> 它代表决策功能中的独立项。 </p> </td></tr><tr><td class="ts"> 3 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_iter_-</b>整数</i> </p> <p> 它给出了达到停止标准的迭代次数。 </p> </td></tr></tbody></table> 
 <p><b>Implementation Example</b></p> 
 <p> <b class="raw_b_node">实施实例</b> </p> 
 <p>Like other classifiers, Stochastic Gradient Descent (SGD) has to be fitted with following two arrays −</p> 
 <p> 像其他分类器一样，随机梯度下降(SGD)必须配备以下两个数组- </p> 
 <ul><li><p>An array X holding the training samples. It is of size [n_samples, n_features].</p><p> 存放训练样本的数组X。 它的大小为[n_samples，n_features]。 </p></li><li><p>An array Y holding the target values i.e. class labels for the training samples. It is of size [n_samples].</p><p> 保存目标值的数组Y，即训练样本的类别标签。 它的大小为[n_samples]。 </p></li></ul> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>Following Python script uses SGDClassifier linear model −</p> 
 <p> 以下Python脚本使用SGDClassifier线性模型- </p> 
 <pre><code class="has">
import numpy as np
from sklearn import linear_model
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
Y = np.array([1, 1, 2, 2])
SGDClf = linear_model.SGDClassifier(max_iter = 1000, tol=1e-3,penalty = "elasticnet")
SGDClf.fit(X, Y)
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
SGDClassifier(
   alpha = 0.0001, average = False, class_weight = None,
   early_stopping = False, epsilon = 0.1, eta0 = 0.0, fit_intercept = True,
   l1_ratio = 0.15, learning_rate = 'optimal', loss = 'hinge', max_iter = 1000,
   n_iter = None, n_iter_no_change = 5, n_jobs = None, penalty = 'elasticnet',
   power_t = 0.5, random_state = None, shuffle = True, tol = 0.001,
   validation_fraction = 0.1, verbose = 0, warm_start = False
)
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>Now, once fitted, the model can predict new values as follows −</p> 
 <p> 现在，一旦拟合，模型可以预测新值，如下所示： </p> 
 <pre><code class="has">
SGDClf.predict([[2.,2.]])
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
array([2])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>For the above example, we can get the weight vector with the help of following python script −</p> 
 <p> 对于上面的示例，我们可以借助以下python脚本获取权重向量- </p> 
 <pre><code class="has">
SGDClf.coef_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
array([[19.54811198, 9.77200712]])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>Similarly, we can get the value of intercept with the help of following python script −</p> 
 <p> 同样，我们可以在以下python脚本的帮助下获取拦截的值- </p> 
 <pre><code class="has">
SGDClf.intercept_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
array([10.])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>We can get the signed distance to the hyperplane by using <b>SGDClassifier.decision_function</b> as used in the following python script −</p> 
 <p> 通过使用以下python脚本中使用的<b class="raw_b_node">SGDClassifier.decision_function</b> ，可以获取到超平面的有符号距离- </p> 
 <pre><code class="has">
SGDClf.decision_function([[2., 2.]])
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
array([68.6402382])
</code>
</pre> 
 <h3> SGD回归器 <span style="font-weight: bold;">(</span>SGD Regressor<span style="font-weight: bold;">)</span></h3> 
 <p>Stochastic Gradient Descent (SGD) regressor basically implements a plain SGD learning routine supporting various loss functions and penalties to fit linear regression models. Scikit-learn provides <b>SGDRegressor</b> module to implement SGD regression.</p> 
 <p> 随机梯度下降(SGD)回归器基本上实现了简单的SGD学习例程，该例程支持各种损失函数和惩罚以适应线性回归模型。 Scikit-learn提供了<b class="raw_b_node">SGDRegressor</b>模块来实现SGD回归。 </p> 
 <h4> 参量 <span style="font-weight: bold;">(</span>Parameters<span style="font-weight: bold;">)</span></h4> 
 <p>Parameters used by <b>SGDRegressor</b> are almost same as that were used in SGDClassifier module. The difference lies in ‘loss’ parameter. For <b>SGDRegressor</b> modules’ loss parameter the positives values are as follows −</p> 
 <p> <b class="raw_b_node">SGDRegressor</b>使用的参数与SGDClassifier模块中使用的参数几乎相同。 区别在于“损失”参数。 对于<b class="raw_b_node">SGDRegressor</b>模块的loss参数，正值如下所示- </p> 
 <ul><li><p><b>squared_loss</b> − It refers to the ordinary least squares fit.</p><p> <b class="raw_b_node">squared_loss-</b>它是指普通的最小二乘拟合。 </p></li><li><p><b>huber: SGDRegressor</b> − correct the outliers by switching from squared to linear loss past a distance of epsilon. The work of ‘huber’ is to modify ‘squared_loss’ so that algorithm focus less on correcting outliers.</p><p> <b class="raw_b_node">Huber：SGDRegressor-</b>通过将平方损失转换为线性损失超过ε距离来校正异常值。 “休伯”的工作是修改“ squared_loss”，以使算法较少关注校正异常值。 </p></li><li><p><b>epsilon_insensitive</b> − Actually, it ignores the errors less than epsilon.</p><p> <b class="raw_b_node">epsilon_insensitive-</b>实际上，它忽略小于epsilon的错误。 </p></li><li><p><b>squared_epsilon_insensitive</b> − It is same as epsilon_insensitive. The only difference is that it becomes squared loss past a tolerance of epsilon.</p><p> <b class="raw_b_node">squared_epsilon_insensitive-</b>与epsilon_insensitive相同。 唯一的区别是，它变成超过ε容差的平方损耗。 </p></li></ul> 
 <p>Another difference is that the parameter named ‘power_t’ has the default value of 0.25 rather than 0.5 as in <b>SGDClassifier</b>. Furthermore, it doesn’t have ‘class_weight’ and ‘n_jobs’ parameters.</p> 
 <p> 另一个区别是名为'power_t'的参数的默认值为0.25，而不是<b class="raw_b_node">SGDClassifier中的</b> 0.5。 此外，它没有'class_weight'和'n_jobs'参数。 </p> 
 <h4> 属性 <span style="font-weight: bold;">(</span>Attributes<span style="font-weight: bold;">)</span></h4> 
 <p>Attributes of SGDRegressor are also same as that were of SGDClassifier module. Rather it has three extra attributes as follows −</p> 
 <p> SGDRegressor的属性也与SGDClassifier模块的属性相同。 相反，它具有三个额外的属性，如下所示： </p> 
 <ul><li><p><b>average_coef_</b> − array, shape(n_features,)</p><p> <b class="raw_b_node">average_coef_</b> −数组，形状(n_features，) </p></li></ul> 
 <p>As name suggest, it provides the average weights assigned to the features.</p> 
 <p> 顾名思义，它提供分配给功能的平均权重。 </p> 
 <ul><li><p><b>average_intercept_</b> − array, shape(1,)</p><p> <b class="raw_b_node">average_intercept_-</b>数组，shape(1，) </p></li></ul> 
 <p>As name suggest, it provides the averaged intercept term.</p> 
 <p> 顾名思义，它提供了平均截距项。 </p> 
 <ul><li><p><b>t_</b> − int</p><p> <b class="raw_b_node">t_-</b>整数 </p></li></ul> 
 <p>It provides the number of weight updates performed during the training phase.</p> 
 <p> 它提供了在训练阶段执行的体重更新次数。 </p> 
 <p><b>Note</b> − the attributes average_coef_ and average_intercept_ will work after enabling parameter ‘average’ to True.</p> 
 <p> <b class="raw_b_node">注意</b> -在将参数“ average”启用为True之后，属性average_coef_和average_intercept_将起作用。 </p> 
 <p><b>Implementation Example</b></p> 
 <p> <b class="raw_b_node">实施实例</b> </p> 
 <p>Following Python script uses <b>SGDRegressor</b> linear model −</p> 
 <p> 以下Python脚本使用<b class="raw_b_node">SGDRegressor</b>线性模型- </p> 
 <pre><code class="has">
import numpy as np
from sklearn import linear_model
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
SGDReg =linear_model.SGDRegressor(
   max_iter = 1000,penalty = "elasticnet",loss = 'huber',tol = 1e-3, average = True
)
SGDReg.fit(X, y)
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
SGDRegressor(
   alpha = 0.0001, average = True, early_stopping = False, epsilon = 0.1,
   eta0 = 0.01, fit_intercept = True, l1_ratio = 0.15,
   learning_rate = 'invscaling', loss = 'huber', max_iter = 1000,
   n_iter = None, n_iter_no_change = 5, penalty = 'elasticnet', power_t = 0.25,
   random_state = None, shuffle = True, tol = 0.001, validation_fraction = 0.1,
   verbose = 0, warm_start = False
)
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>Now, once fitted, we can get the weight vector with the help of following python script −</p> 
 <p> 现在，一旦拟合，我们就可以在以下python脚本的帮助下获得权重向量- </p> 
 <pre><code class="has">
SGDReg.coef_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
array([-0.00423314, 0.00362922, -0.00380136, 0.00585455, 0.00396787])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>Similarly, we can get the value of intercept with the help of following python script −</p> 
 <p> 同样，我们可以在以下python脚本的帮助下获取拦截的值- </p> 
 <pre><code class="has">
SGReg.intercept_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
SGReg.intercept_
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">例</b> </p> 
 <p>We can get the number of weight updates during training phase with the help of the following python script −</p> 
 <p> 我们可以借助以下python脚本获取训练阶段体重更新的次数- </p> 
 <pre><code class="has">
SGDReg.t_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">输出量</b> </p> 
 <pre><code class="has">
61.0
</code>
</pre> 
 <h3> SGD的优点和缺点 <span style="font-weight: bold;">(</span>Pros and Cons of SGD<span style="font-weight: bold;">)</span></h3> 
 <p>Following the pros of SGD −</p> 
 <p> 遵循SGD的优点- </p> 
 <ul><li><p>Stochastic Gradient Descent (SGD) is very efficient.</p><p> 随机梯度下降(SGD)非常有效。 </p></li><li><p>It is very easy to implement as there are lots of opportunities for code tuning.</p><p> 这很容易实现，因为有很多代码调优的机会。 </p></li></ul> 
 <p>Following the cons of SGD −</p> 
 <p> 遵循SGD的缺点- </p> 
 <ul><li><p>Stochastic Gradient Descent (SGD) requires several hyperparameters like regularization parameters.</p><p> 随机梯度下降(SGD)需要一些超参数，例如正则化参数。 </p></li><li><p>It is sensitive to feature scaling.</p><p> 它对特征缩放很敏感。 </p></li></ul> 
 <blockquote> 
  <p>翻译自: <a href="https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm" rel="nofollow">https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm</a></p> 
 </blockquote> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2e62de30a0fd5a4c3a5e8bb2f5291cbd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Java中this的用法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/01ba3bded0d0f90f58346f65ab285f63/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">推荐召回阶段-正负样本选取准则</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>