<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scikitå­¦ä¹ -éšæœºæ¢¯åº¦ä¸‹é™ - ç¼–ç¨‹å¤§ç™½çš„åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scikitå­¦ä¹ -éšæœºæ¢¯åº¦ä¸‹é™" />
<meta property="og:description" content="Scikitå­¦ä¹ -éšæœºæ¢¯åº¦ä¸‹é™ (Scikit Learn - Stochastic Gradient Descent) Here, we will learn about an optimization algorithm in Sklearn, termed as Stochastic Gradient Descent (SGD).
åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å­¦ä¹ Sklearnä¸­çš„ä¼˜åŒ–ç®—æ³•ï¼Œç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™(SGD)ã€‚ Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/35eb93d2ce69df73648ad7a3e289d73c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-22T23:48:04+08:00" />
<meta property="article:modified_time" content="2020-09-22T23:48:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§ç™½çš„åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§ç™½çš„åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scikitå­¦ä¹ -éšæœºæ¢¯åº¦ä¸‹é™</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size: 16px;"> 
 <h2> Scikitå­¦ä¹ -éšæœºæ¢¯åº¦ä¸‹é™ <span style="font-weight: bold;">(</span>Scikit Learn - Stochastic Gradient Descent<span style="font-weight: bold;">)</span></h2> 
 <p>Here, we will learn about an optimization algorithm in Sklearn, termed as Stochastic Gradient Descent (SGD).</p> 
 <p> åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å­¦ä¹ Sklearnä¸­çš„ä¼˜åŒ–ç®—æ³•ï¼Œç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™(SGD)ã€‚ </p> 
 <p>Stochastic Gradient Descent (SGD) is a simple yet efficient optimization algorithm used to find the values of parameters/coefficients of functions that minimize a cost function. In other words, it is used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic regression. It has been successfully applied to large-scale datasets because the update to the coefficients is performed for each training instance, rather than at the end of instances.</p> 
 <p> éšæœºæ¢¯åº¦ä¸‹é™(SGD)æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæŸ¥æ‰¾ä½¿æˆæœ¬å‡½æ•°æœ€å°åŒ–çš„å‡½æ•°å‚æ•°/ç³»æ•°å€¼ã€‚ æ¢å¥è¯è¯´ï¼Œå®ƒç”¨äºå‡¸æŸå¤±å‡½æ•°(ä¾‹å¦‚SVMå’ŒLogisticå›å½’)ä¸‹çš„çº¿æ€§åˆ†ç±»å™¨çš„åˆ¤åˆ«å­¦ä¹ ã€‚ å®ƒå·²æˆåŠŸåº”ç”¨äºå¤§å‹æ•°æ®é›†ï¼Œå› ä¸ºæ˜¯é’ˆå¯¹æ¯ä¸ªè®­ç»ƒå®ä¾‹(è€Œä¸æ˜¯åœ¨å®ä¾‹ç»“æŸæ—¶)æ‰§è¡Œç³»æ•°æ›´æ–°ã€‚ </p> 
 <h3> SGDåˆ†ç±»å™¨ <span style="font-weight: bold;">(</span>SGD Classifier<span style="font-weight: bold;">)</span></h3> 
 <p>Stochastic Gradient Descent (SGD) classifier basically implements a plain SGD learning routine supporting various loss functions and penalties for classification. Scikit-learn provides <b>SGDClassifier</b> module to implement SGD classification.</p> 
 <p> éšæœºæ¢¯åº¦ä¸‹é™(SGD)åˆ†ç±»å™¨åŸºæœ¬ä¸Šå®ç°äº†ç®€å•çš„SGDå­¦ä¹ ä¾‹ç¨‹ï¼Œè¯¥ä¾‹ç¨‹æ”¯æŒå„ç§æŸå¤±å‡½æ•°å’Œåˆ†ç±»æƒ©ç½šã€‚ Scikit-learnæä¾›äº†<b class="raw_b_node">SGDClassifier</b>æ¨¡å—æ¥å®ç°SGDåˆ†ç±»ã€‚ </p> 
 <h4> å‚é‡ <span style="font-weight: bold;">(</span>Parameters<span style="font-weight: bold;">)</span></h4> 
 <p>Followings table consist the parameters used by <b>SGDClassifier</b> module âˆ’</p> 
 <p> ä¸‹è¡¨åŒ…å«<b class="raw_b_node">SGDClassifier</b>æ¨¡å—ä½¿ç”¨çš„å‚æ•°- </p> 
 <table class="table table-bordered"><tbody><tr><th>Sr.No</th><th>Parameter &amp; Description</th></tr><tr><td class="ts">1</td><td> <p><i><b>loss</b> âˆ’ str, default = â€˜hingeâ€™</i></p> <p>It represents the loss function to be used while implementing. The default value is â€˜hingeâ€™ which will give us a linear SVM. The other options which can be used are âˆ’</p> 
     <ul class="list"><li><p><b>log</b> âˆ’ This loss will give us logistic regression i.e. a probabilistic classifier.</p></li><li><p><b>modified_huber</b> âˆ’ a smooth loss that brings tolerance to outliers along with probability estimates.</p></li><li><p><b>squared_hinge</b> âˆ’ similar to â€˜hingeâ€™ loss but it is quadratically penalized.</p></li><li><p><b>perceptron</b> âˆ’ as the name suggests, it is a linear loss which is used by the perceptron algorithm.</p></li></ul> </td></tr><tr><td class="ts">2</td><td> <p><i><b>penalty</b> âˆ’ str, â€˜noneâ€™, â€˜l2â€™, â€˜l1â€™, â€˜elasticnetâ€™</i></p> <p>It is the regularization term used in the model. By default, it is L2. We can use L1 or â€˜elasticnet; as well but both might bring sparsity to the model, hence not achievable with L2.</p> </td></tr><tr><td class="ts">3</td><td> <p><i><b>alpha</b> âˆ’ float, default = 0.0001</i></p> <p>Alpha, the constant that multiplies the regularization term, is the tuning parameter that decides how much we want to penalize the model. The default value is 0.0001.</p> </td></tr><tr><td class="ts">4</td><td> <p><i><b>l1_ratio</b> âˆ’ float, default = 0.15</i></p> <p>This is called the ElasticNet mixing parameter. Its range is 0 &lt; = l1_ratio &lt; = 1. If l1_ratio = 1, the penalty would be L1 penalty. If l1_ratio = 0, the penalty would be an L2 penalty.</p> </td></tr><tr><td class="ts">5</td><td> <p><i><b>fit_intercept</b> âˆ’ Boolean, Default=True</i></p> <p>This parameter specifies that a constant (bias or intercept) should be added to the decision function. No intercept will be used in calculation and data will be assumed already centered, if it will set to false.</p> </td></tr><tr><td class="ts">6</td><td> <p><i><b>tol</b> âˆ’ float or none, optional, default = 1.e-3</i></p> <p>This parameter represents the stopping criterion for iterations. Its default value is False but if set to None, the iterations will stop when ğ’<b><i>loss</i></b> &gt; <b><i>best_loss - tol for n_iter_no_change</i></b>successive epochs.</p> </td></tr><tr><td class="ts">7</td><td> <p><i><b>shuffle</b> âˆ’ Boolean, optional, default = True</i></p> <p>This parameter represents that whether we want our training data to be shuffled after each epoch or not.</p> </td></tr><tr><td class="ts">8</td><td> <p><i><b>verbose</b> âˆ’ integer, default = 0</i></p> <p>It represents the verbosity level. Its default value is 0.</p> </td></tr><tr><td class="ts">9</td><td> <p><i><b>epsilon</b> âˆ’ float, default = 0.1</i></p> <p>This parameter specifies the width of the insensitive region. If loss = â€˜epsilon-insensitiveâ€™, any difference, between current prediction and the correct label, less than the threshold would be ignored.</p> </td></tr><tr><td class="ts">10</td><td> <p><i><b>max_iter</b> âˆ’ int, optional, default = 1000</i></p> <p>As name suggest, it represents the maximum number of passes over the epochs i.e. training data.</p> </td></tr><tr><td class="ts">11</td><td> <p><i><b>warm_start</b> âˆ’ bool, optional, default = false</i></p> <p>With this parameter set to True, we can reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase the previous solution.</p> </td></tr><tr><td class="ts">12</td><td> <p><i><b>random_state</b> âˆ’ int, RandomState instance or None, optional, default = none</i></p> <p>This parameter represents the seed of the pseudo random number generated which is used while shuffling the data. Followings are the options.</p> 
     <ul class="list"><li><p><b>int</b> âˆ’ In this case, <b><i>random_state</i></b> is the seed used by random number generator.</p></li><li><p><b>RandomState instance</b> âˆ’ In this case, <b>random_state</b> is the random number generator.</p></li><li><p><b>None</b> âˆ’ In this case, the random number generator is the RandonState instance used by np.random.</p></li></ul> </td></tr><tr><td class="ts">13</td><td> <p><i><b>n_jobs</b> âˆ’ int or none, optional, Default = None</i></p> <p>It represents the number of CPUs to be used in OVA (One Versus All) computation, for multi-class problems. The default value is none which means 1.</p> </td></tr><tr><td class="ts">14</td><td> <p><i><b>learning_rate</b> âˆ’ string, optional, default = â€˜optimalâ€™</i></p> 
     <ul class="list"><li><p>If learning rate is â€˜constantâ€™, eta = eta0;</p></li><li><p>If learning rate is â€˜optimalâ€™, eta = 1.0/(alpha*(t+t0)), where t0 is chosen by Leon Bottou;</p></li><li><p>If learning rate = â€˜invscallingâ€™, eta = eta0/pow(t, power_t).</p></li><li><p>If learning rate = â€˜adaptiveâ€™, eta = eta0.</p></li></ul></td></tr><tr><td class="ts">15</td><td> <p><i><b>eta0</b> âˆ’ double, default = 0.0</i></p> <p>It represents the initial learning rate for above mentioned learning rate options i.e. â€˜constantâ€™, â€˜invscallingâ€™, or â€˜adaptiveâ€™.</p> </td></tr><tr><td class="ts">16</td><td> <p><i><b>power_t</b> âˆ’ idouble, default =0.5</i></p> <p>It is the exponent for â€˜incscallingâ€™ learning rate.</p> </td></tr><tr><td class="ts">17</td><td> <p><i><b>early_stopping</b> âˆ’ bool, default = False</i></p> <p>This parameter represents the use of early stopping to terminate training when validation score is not improving. Its default value is false but when set to true, it automatically set aside a stratified fraction of training data as validation and stop training when validation score is not improving.</p> </td></tr><tr><td class="ts">18</td><td> <p><i><b>validation_fraction</b> âˆ’ float, default = 0.1</i></p> <p>It is only used when early_stopping is true. It represents the proportion of training data to set asides as validation set for early termination of training data..</p> </td></tr><tr><td class="ts">19</td><td> <p><i><b>n_iter_no_change</b> âˆ’ int, default=5</i></p> <p>It represents the number of iteration with no improvement should algorithm run before early stopping.</p> </td></tr><tr><td class="ts">20</td><td> <p><i><b>classs_weight</b> âˆ’ dict, {class_label: weight} or â€œbalancedâ€, or None, optional</i></p> <p>This parameter represents the weights associated with classes. If not provided, the classes are supposed to have weight 1.</p> </td></tr><tr><td class="ts">20</td><td> <p><i><b>warm_start</b> âˆ’ bool, optional, default = false</i></p> <p>With this parameter set to True, we can reuse the solution of the previous call to fit as initialization. If we choose default i.e. false, it will erase the previous solution.</p> </td></tr><tr><td class="ts">21</td><td> <p><i><b>average</b> âˆ’ iBoolean or int, optional, default = false</i></p> <p>It represents the number of CPUs to be used in OVA (One Versus All) computation, for multi-class problems. The default value is none which means 1.</p> </td></tr></tbody></table> 
 <table class="table table-bordered"><tbody><tr><th> åºå· </th><th> å‚æ•°åŠè¯´æ˜ </th></tr><tr><td class="ts"> 1ä¸ª </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">æŸå¤±</b> -strï¼Œé»˜è®¤='é“°é“¾'</i> </p> <p> å®ƒè¡¨ç¤ºå®ç°æ—¶è¦ä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚ é»˜è®¤å€¼ä¸ºâ€œ hingeâ€ï¼Œè¿™å°†ä¸ºæˆ‘ä»¬æä¾›çº¿æ€§SVMã€‚ å¯ä»¥ä½¿ç”¨çš„å…¶ä»–é€‰é¡¹æ˜¯- </p> 
     <ul class="list"><li><p> <b class="raw_b_node">log-</b>è¿™ç§æŸå¤±å°†ä½¿æˆ‘ä»¬è¿›è¡Œé€»è¾‘å›å½’ï¼Œå³æ¦‚ç‡åˆ†ç±»å™¨ã€‚ </p></li><li><p> <b class="raw_b_node">modified_huber-</b>å¹³æ»‘æŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼å’Œæ¦‚ç‡ä¼°è®¡å€¼å…·æœ‰å®¹å¿åº¦ã€‚ </p></li><li><p> <b class="raw_b_node">squared_hinge-</b>ä¸'hinge'æŸå¤±ç›¸ä¼¼ï¼Œä½†äºŒæ¬¡æƒ©ç½šã€‚ </p></li><li><p> <b class="raw_b_node">æ„ŸçŸ¥å™¨</b> -é¡¾åæ€ä¹‰ï¼Œè¿™æ˜¯æ„ŸçŸ¥å™¨ç®—æ³•ä½¿ç”¨çš„çº¿æ€§æŸè€—ã€‚ </p></li></ul> </td></tr><tr><td class="ts"> 2 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">æƒ©ç½š</b> -strï¼Œ'none'ï¼Œ'l2'ï¼Œ'l1'ï¼Œ'elasticnet'</i> </p> <p> å®ƒæ˜¯æ¨¡å‹ä¸­ä½¿ç”¨çš„æ­£åˆ™åŒ–æœ¯è¯­ã€‚ é»˜è®¤æƒ…å†µä¸‹ä¸ºL2ã€‚ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨L1æˆ–'elasticnet; åŒæ ·ï¼Œä½†æ˜¯ä¸¤è€…éƒ½å¯èƒ½ç»™æ¨¡å‹å¸¦æ¥ç¨€ç–æ€§ï¼Œå› æ­¤L2æ— æ³•å®ç°ã€‚ </p> </td></tr><tr><td class="ts"> 3 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">alpha-</b>æµ®ç‚¹æ•°ï¼Œé»˜è®¤= 0.0001</i> </p> <p> Alpha(ä¹˜ä»¥æ­£åˆ™é¡¹çš„å¸¸æ•°)æ˜¯è°ƒæ•´å‚æ•°ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬è¦å¯¹æ¨¡å‹è¿›è¡Œå¤šå°‘æƒ©ç½šã€‚ é»˜è®¤å€¼ä¸º0.0001ã€‚ </p> </td></tr><tr><td class="ts"> 4 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">l1_ratio-</b>æµ®ç‚¹æ•°ï¼Œé»˜è®¤= 0.15</i> </p> <p> è¿™ç§°ä¸ºElasticNetæ··åˆå‚æ•°ã€‚ å…¶èŒƒå›´ä¸º0 &lt;= l1_ratio &lt;=1ã€‚å¦‚æœl1_ratio = 1ï¼Œåˆ™æƒ©ç½šä¸ºL1æƒ©ç½šã€‚ å¦‚æœl1_ratio = 0ï¼Œåˆ™æƒ©ç½šä¸ºL2æƒ©ç½šã€‚ </p> </td></tr><tr><td class="ts"> 5 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">fit_intercept-</b>å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸ºTrue</i> </p> <p> æ­¤å‚æ•°æŒ‡å®šåº”å°†å¸¸é‡(åå·®æˆ–æˆªè·)æ·»åŠ åˆ°å†³ç­–å‡½æ•°ã€‚ å¦‚æœå°†å…¶è®¾ç½®ä¸ºfalseï¼Œåˆ™ä¸ä¼šåœ¨è®¡ç®—ä¸­ä½¿ç”¨æˆªè·ï¼Œå¹¶ä¸”å°†å‡å®šæ•°æ®å·²å±…ä¸­ã€‚ </p> </td></tr><tr><td class="ts"> 6 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">tol-</b>æµ®åŠ¨æˆ–æ— ï¼Œå¯é€‰ï¼Œé»˜è®¤= 1.e-3</i> </p> <p> æ­¤å‚æ•°è¡¨ç¤ºè¿­ä»£çš„åœæ­¢æ ‡å‡†ã€‚ å®ƒçš„é»˜è®¤å€¼ä¸ºFalseï¼Œä½†å¦‚æœè®¾ç½®ä¸ºNoneï¼Œåˆ™å½“n <b class="raw_b_node"><i class="raw_i_node">æŸå¤±</i></b> &gt; <b class="raw_b_node"><i class="raw_i_node">best_loss-</i></b>è¿ç»­<b class="raw_b_node"><i class="raw_i_node">n_iter_no_change</i></b>ä¸ªæ—¶æœŸçš„<b class="raw_b_node"><i class="raw_i_node">tol</i></b>æ—¶ï¼Œè¿­ä»£å°†åœæ­¢ã€‚ </p> </td></tr><tr><td class="ts"> 7 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">shuffle-</b>å¸ƒå°”å€¼ï¼Œå¯é€‰ï¼Œé»˜è®¤= True</i> </p> <p> æ­¤å‚æ•°è¡¨ç¤ºæˆ‘ä»¬æ˜¯å¦å¸Œæœ›åœ¨æ¯ä¸ªæ—¶æœŸä¹‹åå¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ··æ´—ã€‚ </p> </td></tr><tr><td class="ts"> 8 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">è¯¦ç»†</b> -æ•´æ•°ï¼Œé»˜è®¤= 0</i> </p> <p> å®ƒä»£è¡¨äº†è¯¦ç»†ç¨‹åº¦ã€‚ é»˜è®¤å€¼ä¸º0ã€‚ </p> </td></tr><tr><td class="ts"> 9 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">epsilon-</b>æµ®åŠ¨ï¼Œé»˜è®¤= 0.1</i> </p> <p> æ­¤å‚æ•°æŒ‡å®šä¸æ•æ„ŸåŒºåŸŸçš„å®½åº¦ã€‚ å¦‚æœæŸå¤±=â€œå¯¹Îµä¸æ•æ„Ÿâ€ï¼Œåˆ™å½“å‰é¢„æµ‹ä¸æ­£ç¡®æ ‡ç­¾ä¹‹é—´çš„ä»»ä½•å·®å¼‚(å°äºé˜ˆå€¼)å°†è¢«å¿½ç•¥ã€‚ </p> </td></tr><tr><td class="ts"> 10 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">max_iter</b> -intï¼Œå¯é€‰ï¼Œé»˜è®¤= 1000</i> </p> <p> é¡¾åæ€ä¹‰ï¼Œå®ƒä»£è¡¨å†æ—¶çš„æœ€å¤§é€šè¿‡æ¬¡æ•°ï¼Œå³è®­ç»ƒæ•°æ®ã€‚ </p> </td></tr><tr><td class="ts"> 11 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">warm_start</b> -boolï¼Œå¯é€‰ï¼Œé»˜è®¤= false</i> </p> <p> é€šè¿‡å°†æ­¤å‚æ•°è®¾ç½®ä¸ºTrueï¼Œæˆ‘ä»¬å¯ä»¥é‡ç”¨ä¸Šä¸€ä¸ªè°ƒç”¨çš„è§£å†³æ–¹æ¡ˆä»¥é€‚åˆåˆå§‹åŒ–ã€‚ å¦‚æœæˆ‘ä»¬é€‰æ‹©é»˜è®¤å€¼ï¼Œå³falseï¼Œå®ƒå°†åˆ é™¤å…ˆå‰çš„è§£å†³æ–¹æ¡ˆã€‚ </p> </td></tr><tr><td class="ts"> 12 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">random_state</b> -intï¼ŒRandomStateå®ä¾‹æˆ–æ— ï¼Œå¯é€‰ï¼Œé»˜è®¤=æ— </i> </p> <p> æ­¤å‚æ•°è¡¨ç¤ºç”Ÿæˆçš„ä¼ªéšæœºæ•°çš„ç§å­ï¼Œåœ¨å¯¹æ•°æ®è¿›è¡Œæ··æ´—æ—¶ä¼šä½¿ç”¨è¯¥ç§å­ã€‚ ä»¥ä¸‹æ˜¯é€‰é¡¹ã€‚ </p> 
     <ul class="list"><li><p> <b class="raw_b_node">INT</b> -åœ¨è¿™ç§æƒ…å†µä¸‹<b class="raw_b_node"><i class="raw_i_node">ï¼Œrandom_state</i></b>æ˜¯ç”±éšæœºæ•°ç”Ÿæˆæ‰€ä½¿ç”¨çš„ç§å­ã€‚ </p></li><li><p> <b class="raw_b_node">RandomStateå®ä¾‹</b> -åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ <b class="raw_b_node">random_state</b>æ˜¯éšæœºæ•°ç”Ÿæˆå™¨ã€‚ </p></li><li><p> <b class="raw_b_node">æ— </b> -åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéšæœºæ•°ç”Ÿæˆå™¨æ˜¯np.randomä½¿ç”¨çš„RandonStateå®ä¾‹ã€‚ </p></li></ul> </td></tr><tr><td class="ts"> 13 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_jobs-</b>æ•´æ•°æˆ–æ— ï¼Œå¯é€‰ï¼Œé»˜è®¤=æ— </i> </p> <p> å®ƒè¡¨ç¤ºç”¨äºå¤šç±»é—®é¢˜çš„OVA(ä¸€ä¸ªå¯¹æ‰€æœ‰)è®¡ç®—ä¸­ä½¿ç”¨çš„CPUæ•°é‡ã€‚ é»˜è®¤å€¼ä¸ºnoneï¼Œè¡¨ç¤º1ã€‚ </p> </td></tr><tr><td class="ts"> 14 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">learning_rate-</b>å­—ç¬¦ä¸²ï¼Œå¯é€‰ï¼Œé»˜è®¤='æœ€ä¼˜'</i> </p> 
     <ul class="list"><li><p> å¦‚æœå­¦ä¹ é€Ÿç‡ä¸ºâ€œæ’å®šâ€ï¼Œåˆ™eta = eta0; </p></li><li><p> å¦‚æœå­¦ä¹ ç‡æ˜¯â€œæœ€ä½³â€ï¼Œåˆ™eta = 1.0 /(alpha *(t + t0))ï¼Œå…¶ä¸­t0ç”±Leon Bottoué€‰æ‹©ï¼› </p></li><li><p> å¦‚æœå­¦ä¹ ç‡='invscalling'ï¼Œåˆ™eta = eta0 / pow(tï¼Œpower_t)ã€‚ </p></li><li><p> å¦‚æœå­¦ä¹ ç‡=â€œè‡ªé€‚åº”â€ï¼Œåˆ™eta = eta0ã€‚ </p></li></ul> </td></tr><tr><td class="ts"> 15 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">eta0-</b>ä¸¤å€ï¼Œé»˜è®¤= 0.0</i> </p> <p> å®ƒä»£è¡¨ä¸Šè¿°å­¦ä¹ ç‡é€‰é¡¹(å³â€œæ’å®šâ€ï¼Œâ€œæ¸è¿›â€æˆ–â€œè‡ªé€‚åº”â€)çš„åˆå§‹å­¦ä¹ ç‡ã€‚ </p> </td></tr><tr><td class="ts"> 16 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">power_t</b> -idoubleï¼Œé»˜è®¤= 0.5</i> </p> <p> å®ƒæ˜¯â€œå¢åŠ â€å­¦ä¹ ç‡çš„æŒ‡æ•°ã€‚ </p> </td></tr><tr><td class="ts"> 17 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">early_stopping</b> âˆ’ boolï¼Œé»˜è®¤= False</i> </p> <p> æ­¤å‚æ•°è¡¨ç¤ºå½“éªŒè¯åˆ†æ•°æ²¡æœ‰æé«˜æ—¶ï¼Œä½¿ç”¨æ—©æœŸåœæ­¢æ¥ç»ˆæ­¢è®­ç»ƒã€‚ å®ƒçš„é»˜è®¤å€¼æ˜¯falseï¼Œä½†æ˜¯å½“è®¾ç½®ä¸ºtrueæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å°†è®­ç»ƒæ•°æ®çš„åˆ†å±‚éƒ¨åˆ†ç•™ä½œéªŒè¯ï¼Œå¹¶åœ¨éªŒè¯å¾—åˆ†æ²¡æœ‰æé«˜æ—¶åœæ­¢è®­ç»ƒã€‚ </p> </td></tr><tr><td class="ts"> 18 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">validation_fraction-</b>æµ®ç‚¹æ•°ï¼Œé»˜è®¤= 0.1</i> </p> <p> ä»…å½“early_stoppingä¸ºtrueæ—¶ä½¿ç”¨ã€‚ å®ƒè¡¨ç¤ºå°†è®­ç»ƒæ•°æ®è®¾ç½®ä¸ºè¾…åŠ©å‚æ•°ä»¥å°½æ—©ç»ˆæ­¢è®­ç»ƒæ•°æ®çš„æ¯”ä¾‹ã€‚ </p> </td></tr><tr><td class="ts"> 19 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_iter_no_change-</b>æ•´æ•°ï¼Œé»˜è®¤= 5</i> </p> <p> å®ƒè¡¨ç¤ºç®—æ³•çš„è¿­ä»£æ¬¡æ•°ï¼Œå¦‚æœç®—æ³•åœ¨å°½æ—©åœæ­¢ä¹‹å‰ä»æœªè¿è¡Œï¼Œåˆ™ä¸ä¼šæœ‰æ‰€æ”¹å–„ã€‚ </p> </td></tr><tr><td class="ts"> 20 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">classs_weight</b> -dictï¼Œ{class_labelï¼šweight}æˆ–â€œ balancedâ€ï¼Œæˆ–è€…â€œæ— â€ï¼Œå¯é€‰</i> </p> <p> æ­¤å‚æ•°è¡¨ç¤ºä¸ç±»å…³è”çš„æƒé‡ã€‚ å¦‚æœæœªæä¾›ï¼Œåˆ™è¯¥ç±»çš„æƒé‡åº”ä¸º1ã€‚ </p> </td></tr><tr><td class="ts"> 20 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">warm_start</b> -boolï¼Œå¯é€‰ï¼Œé»˜è®¤= false</i> </p> <p> é€šè¿‡å°†æ­¤å‚æ•°è®¾ç½®ä¸ºTrueï¼Œæˆ‘ä»¬å¯ä»¥é‡ç”¨ä¸Šä¸€ä¸ªè°ƒç”¨çš„è§£å†³æ–¹æ¡ˆä»¥é€‚åˆåˆå§‹åŒ–ã€‚ å¦‚æœæˆ‘ä»¬é€‰æ‹©é»˜è®¤å€¼ï¼Œå³falseï¼Œå®ƒå°†åˆ é™¤å…ˆå‰çš„è§£å†³æ–¹æ¡ˆã€‚ </p> </td></tr><tr><td class="ts"> 21 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">å¹³å‡å€¼</b> -iBooleanæˆ–intï¼Œå¯é€‰ï¼Œé»˜è®¤= false</i> </p> <p> å®ƒè¡¨ç¤ºç”¨äºå¤šç±»é—®é¢˜çš„OVA(ä¸€ä¸ªå¯¹æ‰€æœ‰)è®¡ç®—ä¸­ä½¿ç”¨çš„CPUæ•°é‡ã€‚ é»˜è®¤å€¼ä¸ºnoneï¼Œè¡¨ç¤º1ã€‚ </p> </td></tr></tbody></table> 
 <h4> å±æ€§ <span style="font-weight: bold;">(</span>Attributes<span style="font-weight: bold;">)</span></h4> 
 <p>Following table consist the attributes used by <b>SGDClassifier</b> module âˆ’</p> 
 <p> ä¸‹è¡¨åŒ…å«<b class="raw_b_node">SGDClassifier</b>æ¨¡å—ä½¿ç”¨çš„å±æ€§- </p> 
 <table class="table table-bordered"><tbody><tr><th>Sr.No</th><th>Attributes &amp; Description</th></tr><tr><td class="ts">1</td><td> <p><i><b>coef_</b> âˆ’ array, shape (1, n_features) if n_classes==2, else (n_classes, n_features)</i></p> <p>This attribute provides the weight assigned to the features.</p> </td></tr><tr><td class="ts">2</td><td> <p><i><b>intercept_</b> âˆ’ array, shape (1,) if n_classes==2, else (n_classes,)</i></p> <p>It represents the independent term in decision function.</p> </td></tr><tr><td class="ts">3</td><td> <p><i><b>n_iter_</b> âˆ’ int</i></p> <p>It gives the number of iterations to reach the stopping criterion.</p> </td></tr></tbody></table> 
 <table class="table table-bordered"><tbody><tr><th> åºå· </th><th> å±æ€§å’Œè¯´æ˜ </th></tr><tr><td class="ts"> 1ä¸ª </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">coef_-</b>æ•°ç»„ï¼Œå¦‚æœn_classes == 2ï¼Œåˆ™å½¢çŠ¶ä¸º(1ï¼Œn_features)ï¼Œå¦åˆ™ä¸º(n_classesï¼Œn_features)</i> </p> <p> æ­¤å±æ€§æä¾›åˆ†é…ç»™è¦ç´ çš„æƒé‡ã€‚ </p> </td></tr><tr><td class="ts"> 2 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">intercept_</b> -é˜µåˆ—çš„å½¢çŠ¶(1)å¦‚æœn_classes == 2ï¼Œå¦åˆ™(n_classesï¼Œ)</i> </p> <p> å®ƒä»£è¡¨å†³ç­–åŠŸèƒ½ä¸­çš„ç‹¬ç«‹é¡¹ã€‚ </p> </td></tr><tr><td class="ts"> 3 </td><td> <p> <i class="raw_i_node"><b class="raw_b_node">n_iter_-</b>æ•´æ•°</i> </p> <p> å®ƒç»™å‡ºäº†è¾¾åˆ°åœæ­¢æ ‡å‡†çš„è¿­ä»£æ¬¡æ•°ã€‚ </p> </td></tr></tbody></table> 
 <p><b>Implementation Example</b></p> 
 <p> <b class="raw_b_node">å®æ–½å®ä¾‹</b> </p> 
 <p>Like other classifiers, Stochastic Gradient Descent (SGD) has to be fitted with following two arrays âˆ’</p> 
 <p> åƒå…¶ä»–åˆ†ç±»å™¨ä¸€æ ·ï¼Œéšæœºæ¢¯åº¦ä¸‹é™(SGD)å¿…é¡»é…å¤‡ä»¥ä¸‹ä¸¤ä¸ªæ•°ç»„- </p> 
 <ul><li><p>An array X holding the training samples. It is of size [n_samples, n_features].</p><p> å­˜æ”¾è®­ç»ƒæ ·æœ¬çš„æ•°ç»„Xã€‚ å®ƒçš„å¤§å°ä¸º[n_samplesï¼Œn_features]ã€‚ </p></li><li><p>An array Y holding the target values i.e. class labels for the training samples. It is of size [n_samples].</p><p> ä¿å­˜ç›®æ ‡å€¼çš„æ•°ç»„Yï¼Œå³è®­ç»ƒæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ã€‚ å®ƒçš„å¤§å°ä¸º[n_samples]ã€‚ </p></li></ul> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>Following Python script uses SGDClassifier linear model âˆ’</p> 
 <p> ä»¥ä¸‹Pythonè„šæœ¬ä½¿ç”¨SGDClassifierçº¿æ€§æ¨¡å‹- </p> 
 <pre><code class="has">
import numpy as np
from sklearn import linear_model
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
Y = np.array([1, 1, 2, 2])
SGDClf = linear_model.SGDClassifier(max_iter = 1000, tol=1e-3,penalty = "elasticnet")
SGDClf.fit(X, Y)
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
SGDClassifier(
   alpha = 0.0001, average = False, class_weight = None,
   early_stopping = False, epsilon = 0.1, eta0 = 0.0, fit_intercept = True,
   l1_ratio = 0.15, learning_rate = 'optimal', loss = 'hinge', max_iter = 1000,
   n_iter = None, n_iter_no_change = 5, n_jobs = None, penalty = 'elasticnet',
   power_t = 0.5, random_state = None, shuffle = True, tol = 0.001,
   validation_fraction = 0.1, verbose = 0, warm_start = False
)
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>Now, once fitted, the model can predict new values as follows âˆ’</p> 
 <p> ç°åœ¨ï¼Œä¸€æ—¦æ‹Ÿåˆï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹æ–°å€¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š </p> 
 <pre><code class="has">
SGDClf.predict([[2.,2.]])
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
array([2])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>For the above example, we can get the weight vector with the help of following python script âˆ’</p> 
 <p> å¯¹äºä¸Šé¢çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ©ä»¥ä¸‹pythonè„šæœ¬è·å–æƒé‡å‘é‡- </p> 
 <pre><code class="has">
SGDClf.coef_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
array([[19.54811198, 9.77200712]])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>Similarly, we can get the value of intercept with the help of following python script âˆ’</p> 
 <p> åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»¥ä¸‹pythonè„šæœ¬çš„å¸®åŠ©ä¸‹è·å–æ‹¦æˆªçš„å€¼- </p> 
 <pre><code class="has">
SGDClf.intercept_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
array([10.])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>We can get the signed distance to the hyperplane by using <b>SGDClassifier.decision_function</b> as used in the following python script âˆ’</p> 
 <p> é€šè¿‡ä½¿ç”¨ä»¥ä¸‹pythonè„šæœ¬ä¸­ä½¿ç”¨çš„<b class="raw_b_node">SGDClassifier.decision_function</b> ï¼Œå¯ä»¥è·å–åˆ°è¶…å¹³é¢çš„æœ‰ç¬¦å·è·ç¦»- </p> 
 <pre><code class="has">
SGDClf.decision_function([[2., 2.]])
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
array([68.6402382])
</code>
</pre> 
 <h3> SGDå›å½’å™¨ <span style="font-weight: bold;">(</span>SGD Regressor<span style="font-weight: bold;">)</span></h3> 
 <p>Stochastic Gradient Descent (SGD) regressor basically implements a plain SGD learning routine supporting various loss functions and penalties to fit linear regression models. Scikit-learn provides <b>SGDRegressor</b> module to implement SGD regression.</p> 
 <p> éšæœºæ¢¯åº¦ä¸‹é™(SGD)å›å½’å™¨åŸºæœ¬ä¸Šå®ç°äº†ç®€å•çš„SGDå­¦ä¹ ä¾‹ç¨‹ï¼Œè¯¥ä¾‹ç¨‹æ”¯æŒå„ç§æŸå¤±å‡½æ•°å’Œæƒ©ç½šä»¥é€‚åº”çº¿æ€§å›å½’æ¨¡å‹ã€‚ Scikit-learnæä¾›äº†<b class="raw_b_node">SGDRegressor</b>æ¨¡å—æ¥å®ç°SGDå›å½’ã€‚ </p> 
 <h4> å‚é‡ <span style="font-weight: bold;">(</span>Parameters<span style="font-weight: bold;">)</span></h4> 
 <p>Parameters used by <b>SGDRegressor</b> are almost same as that were used in SGDClassifier module. The difference lies in â€˜lossâ€™ parameter. For <b>SGDRegressor</b> modulesâ€™ loss parameter the positives values are as follows âˆ’</p> 
 <p> <b class="raw_b_node">SGDRegressor</b>ä½¿ç”¨çš„å‚æ•°ä¸SGDClassifieræ¨¡å—ä¸­ä½¿ç”¨çš„å‚æ•°å‡ ä¹ç›¸åŒã€‚ åŒºåˆ«åœ¨äºâ€œæŸå¤±â€å‚æ•°ã€‚ å¯¹äº<b class="raw_b_node">SGDRegressor</b>æ¨¡å—çš„losså‚æ•°ï¼Œæ­£å€¼å¦‚ä¸‹æ‰€ç¤º- </p> 
 <ul><li><p><b>squared_loss</b> âˆ’ It refers to the ordinary least squares fit.</p><p> <b class="raw_b_node">squared_loss-</b>å®ƒæ˜¯æŒ‡æ™®é€šçš„æœ€å°äºŒä¹˜æ‹Ÿåˆã€‚ </p></li><li><p><b>huber: SGDRegressor</b> âˆ’ correct the outliers by switching from squared to linear loss past a distance of epsilon. The work of â€˜huberâ€™ is to modify â€˜squared_lossâ€™ so that algorithm focus less on correcting outliers.</p><p> <b class="raw_b_node">Huberï¼šSGDRegressor-</b>é€šè¿‡å°†å¹³æ–¹æŸå¤±è½¬æ¢ä¸ºçº¿æ€§æŸå¤±è¶…è¿‡Îµè·ç¦»æ¥æ ¡æ­£å¼‚å¸¸å€¼ã€‚ â€œä¼‘ä¼¯â€çš„å·¥ä½œæ˜¯ä¿®æ”¹â€œ squared_lossâ€ï¼Œä»¥ä½¿ç®—æ³•è¾ƒå°‘å…³æ³¨æ ¡æ­£å¼‚å¸¸å€¼ã€‚ </p></li><li><p><b>epsilon_insensitive</b> âˆ’ Actually, it ignores the errors less than epsilon.</p><p> <b class="raw_b_node">epsilon_insensitive-</b>å®é™…ä¸Šï¼Œå®ƒå¿½ç•¥å°äºepsilonçš„é”™è¯¯ã€‚ </p></li><li><p><b>squared_epsilon_insensitive</b> âˆ’ It is same as epsilon_insensitive. The only difference is that it becomes squared loss past a tolerance of epsilon.</p><p> <b class="raw_b_node">squared_epsilon_insensitive-</b>ä¸epsilon_insensitiveç›¸åŒã€‚ å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œå®ƒå˜æˆè¶…è¿‡Îµå®¹å·®çš„å¹³æ–¹æŸè€—ã€‚ </p></li></ul> 
 <p>Another difference is that the parameter named â€˜power_tâ€™ has the default value of 0.25 rather than 0.5 as in <b>SGDClassifier</b>. Furthermore, it doesnâ€™t have â€˜class_weightâ€™ and â€˜n_jobsâ€™ parameters.</p> 
 <p> å¦ä¸€ä¸ªåŒºåˆ«æ˜¯åä¸º'power_t'çš„å‚æ•°çš„é»˜è®¤å€¼ä¸º0.25ï¼Œè€Œä¸æ˜¯<b class="raw_b_node">SGDClassifierä¸­çš„</b> 0.5ã€‚ æ­¤å¤–ï¼Œå®ƒæ²¡æœ‰'class_weight'å’Œ'n_jobs'å‚æ•°ã€‚ </p> 
 <h4> å±æ€§ <span style="font-weight: bold;">(</span>Attributes<span style="font-weight: bold;">)</span></h4> 
 <p>Attributes of SGDRegressor are also same as that were of SGDClassifier module. Rather it has three extra attributes as follows âˆ’</p> 
 <p> SGDRegressorçš„å±æ€§ä¹Ÿä¸SGDClassifieræ¨¡å—çš„å±æ€§ç›¸åŒã€‚ ç›¸åï¼Œå®ƒå…·æœ‰ä¸‰ä¸ªé¢å¤–çš„å±æ€§ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š </p> 
 <ul><li><p><b>average_coef_</b> âˆ’ array, shape(n_features,)</p><p> <b class="raw_b_node">average_coef_</b> âˆ’æ•°ç»„ï¼Œå½¢çŠ¶(n_featuresï¼Œ) </p></li></ul> 
 <p>As name suggest, it provides the average weights assigned to the features.</p> 
 <p> é¡¾åæ€ä¹‰ï¼Œå®ƒæä¾›åˆ†é…ç»™åŠŸèƒ½çš„å¹³å‡æƒé‡ã€‚ </p> 
 <ul><li><p><b>average_intercept_</b> âˆ’ array, shape(1,)</p><p> <b class="raw_b_node">average_intercept_-</b>æ•°ç»„ï¼Œshape(1ï¼Œ) </p></li></ul> 
 <p>As name suggest, it provides the averaged intercept term.</p> 
 <p> é¡¾åæ€ä¹‰ï¼Œå®ƒæä¾›äº†å¹³å‡æˆªè·é¡¹ã€‚ </p> 
 <ul><li><p><b>t_</b> âˆ’ int</p><p> <b class="raw_b_node">t_-</b>æ•´æ•° </p></li></ul> 
 <p>It provides the number of weight updates performed during the training phase.</p> 
 <p> å®ƒæä¾›äº†åœ¨è®­ç»ƒé˜¶æ®µæ‰§è¡Œçš„ä½“é‡æ›´æ–°æ¬¡æ•°ã€‚ </p> 
 <p><b>Note</b> âˆ’ the attributes average_coef_ and average_intercept_ will work after enabling parameter â€˜averageâ€™ to True.</p> 
 <p> <b class="raw_b_node">æ³¨æ„</b> -åœ¨å°†å‚æ•°â€œ averageâ€å¯ç”¨ä¸ºTrueä¹‹åï¼Œå±æ€§average_coef_å’Œaverage_intercept_å°†èµ·ä½œç”¨ã€‚ </p> 
 <p><b>Implementation Example</b></p> 
 <p> <b class="raw_b_node">å®æ–½å®ä¾‹</b> </p> 
 <p>Following Python script uses <b>SGDRegressor</b> linear model âˆ’</p> 
 <p> ä»¥ä¸‹Pythonè„šæœ¬ä½¿ç”¨<b class="raw_b_node">SGDRegressor</b>çº¿æ€§æ¨¡å‹- </p> 
 <pre><code class="has">
import numpy as np
from sklearn import linear_model
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
SGDReg =linear_model.SGDRegressor(
   max_iter = 1000,penalty = "elasticnet",loss = 'huber',tol = 1e-3, average = True
)
SGDReg.fit(X, y)
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
SGDRegressor(
   alpha = 0.0001, average = True, early_stopping = False, epsilon = 0.1,
   eta0 = 0.01, fit_intercept = True, l1_ratio = 0.15,
   learning_rate = 'invscaling', loss = 'huber', max_iter = 1000,
   n_iter = None, n_iter_no_change = 5, penalty = 'elasticnet', power_t = 0.25,
   random_state = None, shuffle = True, tol = 0.001, validation_fraction = 0.1,
   verbose = 0, warm_start = False
)
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>Now, once fitted, we can get the weight vector with the help of following python script âˆ’</p> 
 <p> ç°åœ¨ï¼Œä¸€æ—¦æ‹Ÿåˆï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨ä»¥ä¸‹pythonè„šæœ¬çš„å¸®åŠ©ä¸‹è·å¾—æƒé‡å‘é‡- </p> 
 <pre><code class="has">
SGDReg.coef_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
array([-0.00423314, 0.00362922, -0.00380136, 0.00585455, 0.00396787])
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>Similarly, we can get the value of intercept with the help of following python script âˆ’</p> 
 <p> åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»¥ä¸‹pythonè„šæœ¬çš„å¸®åŠ©ä¸‹è·å–æ‹¦æˆªçš„å€¼- </p> 
 <pre><code class="has">
SGReg.intercept_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
SGReg.intercept_
</code>
</pre> 
 <p><b>Example</b></p> 
 <p> <b class="raw_b_node">ä¾‹</b> </p> 
 <p>We can get the number of weight updates during training phase with the help of the following python script âˆ’</p> 
 <p> æˆ‘ä»¬å¯ä»¥å€ŸåŠ©ä»¥ä¸‹pythonè„šæœ¬è·å–è®­ç»ƒé˜¶æ®µä½“é‡æ›´æ–°çš„æ¬¡æ•°- </p> 
 <pre><code class="has">
SGDReg.t_
</code>
</pre> 
 <p><b>Output</b></p> 
 <p> <b class="raw_b_node">è¾“å‡ºé‡</b> </p> 
 <pre><code class="has">
61.0
</code>
</pre> 
 <h3> SGDçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ <span style="font-weight: bold;">(</span>Pros and Cons of SGD<span style="font-weight: bold;">)</span></h3> 
 <p>Following the pros of SGD âˆ’</p> 
 <p> éµå¾ªSGDçš„ä¼˜ç‚¹- </p> 
 <ul><li><p>Stochastic Gradient Descent (SGD) is very efficient.</p><p> éšæœºæ¢¯åº¦ä¸‹é™(SGD)éå¸¸æœ‰æ•ˆã€‚ </p></li><li><p>It is very easy to implement as there are lots of opportunities for code tuning.</p><p> è¿™å¾ˆå®¹æ˜“å®ç°ï¼Œå› ä¸ºæœ‰å¾ˆå¤šä»£ç è°ƒä¼˜çš„æœºä¼šã€‚ </p></li></ul> 
 <p>Following the cons of SGD âˆ’</p> 
 <p> éµå¾ªSGDçš„ç¼ºç‚¹- </p> 
 <ul><li><p>Stochastic Gradient Descent (SGD) requires several hyperparameters like regularization parameters.</p><p> éšæœºæ¢¯åº¦ä¸‹é™(SGD)éœ€è¦ä¸€äº›è¶…å‚æ•°ï¼Œä¾‹å¦‚æ­£åˆ™åŒ–å‚æ•°ã€‚ </p></li><li><p>It is sensitive to feature scaling.</p><p> å®ƒå¯¹ç‰¹å¾ç¼©æ”¾å¾ˆæ•æ„Ÿã€‚ </p></li></ul> 
 <blockquote> 
  <p>ç¿»è¯‘è‡ª: <a href="https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm" rel="nofollow">https://www.tutorialspoint.com/scikit_learn/scikit_learn_stochastic_gradient_descent.htm</a></p> 
 </blockquote> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2e62de30a0fd5a4c3a5e8bb2f5291cbd/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Javaä¸­thisçš„ç”¨æ³•</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/01ba3bded0d0f90f58346f65ab285f63/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">æ¨èå¬å›é˜¶æ®µ-æ­£è´Ÿæ ·æœ¬é€‰å–å‡†åˆ™</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§ç™½çš„åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>