<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>目标跟踪经典算法汇总(持续更新...) - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="目标跟踪经典算法汇总(持续更新...)" />
<meta property="og:description" content="如题，虽然这个问题是经典目标跟踪算法，但事实上，可能我们并不需要那些曾经辉煌但已被拍在沙滩上的tracker(目标跟踪算法)，而是那些即将成为经典的，或者就目前来说最好用、速度和性能都看的过去tracker。我比较关注目标跟踪中的相关滤波方向，接下来我介绍下我所认识的目标跟踪，尤其是相关滤波类方法
benchmark: https://github.com/foolwood/benchmark_results
知乎：https://www.zhihu.com/question/26493945/answer/156025576
背景介绍 作者：YaqiLYU
链接：https://www.zhihu.com/question/26493945/answer/156025576
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
经典判别类方法推荐Struck和TLD，都能实时性能还行，Struck是2012年之前最好的方法，TLD是经典long-term的代表，思想非常值得借鉴：
Hare S, Golodetz S, Saffari A, et al. Struck: Structured output tracking with kernels [J]. IEEE TPAMI, 2016.Kalal Z, Mikolajczyk K, Matas J. Tracking-learning-detection [J]. IEEE TPAMI, 2012. 长江后浪推前浪，前面的已被排在沙滩上，这个后浪就是相关滤波和深度学习。相关滤波类方法correlation filter简称CF，也叫做discriminative correlation filter简称DCF，注意和后面的DCF算法区别，包括前面提到的那几个，也是后面要着重介绍的。深度学习（Deep ConvNet based）类方法，因为深度学习类目前不适合落地就不瞎推荐了，可以参考Winsty的几篇 Naiyan Wang - Home，还有VOT2015的冠军MDNet Learning Multi-Domain Convolutional Neural Networks for Visual Tracking，以及VOT2016的冠军TCNN http://www.votchallenge.net/vot2016/download/44_TCNN.zip，速度方面比较突出的如80FPS的SiamFC SiameseFC tracker和100FPS的GOTURN davheld/GOTURN，注意都是在GPU上。基于ResNet的SiamFC-R(ResNet)在VOT2016表现不错，很看好后续发展，有兴趣也可以去VALSE听作者自己讲解 VALSE-20160930-LucaBertinetto-Oxford-JackValmadre-Oxford-pu，至于GOTURN，效果比较差，但优势是跑的很快100FPS，如果以后效果也能上来就好了。做科研的同学深度学习类是关键，能兼顾速度就更好了。
最后强力推荐两个资源：
王强@Qiang Wang维护的benchmark_results foolwood/benchmark_results：大量顶级方法在OTB库上的性能对比，各种论文代码应有尽有，大神自己C&#43;&#43;实现并开源的CSK, KCF和DAT，还有他自己的DCFNet论文加源码，找不着路的同学请跟紧。
@H Hakase维护的相关滤波类资源 HakaseH/CF_benchmark_results ，详细分类和论文代码资源，走过路过别错过，相关滤波类算法非常全面，非常之用心！" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/465fc543b3f4d6cc8313f65438da7e02/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-24T09:51:22+08:00" />
<meta property="article:modified_time" content="2018-07-24T09:51:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">目标跟踪经典算法汇总(持续更新...)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>    如题，虽然这个问题是<strong>经典目标跟踪算法</strong>，但事实上，可能我们并不需要那些曾经辉煌但已被拍在沙滩上的tracker(目标跟踪算法)，而是那些即将成为经典的，或者就目前来说最好用、速度和性能都看的过去tracker。我比较关注目标跟踪中的相关滤波方向，接下来我介绍下我所认识的目标跟踪，尤其是相关滤波类方法</p> 
<p> </p> 
<p>benchmark: <a href="https://github.com/foolwood/benchmark_results">https://github.com/foolwood/benchmark_results</a></p> 
<p>知乎：<a href="https://www.zhihu.com/question/26493945/answer/156025576" rel="nofollow">https://www.zhihu.com/question/26493945/answer/156025576</a></p> 
<p> </p> 
<h3><strong>背景介绍</strong></h3> 
<p>作者：YaqiLYU<br> 链接：https://www.zhihu.com/question/26493945/answer/156025576<br> 来源：知乎<br> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。<br>  </p> 
<p>经典判别类方法推荐<strong>Struck</strong>和<strong>TLD</strong>，都能实时性能还行，Struck是2012年之前最好的方法，TLD是经典long-term的代表，思想非常值得借鉴：</p> 
<ul><li>Hare S, Golodetz S, Saffari A, et al. <strong>Struck: Structured output tracking with kernels </strong>[J]. IEEE TPAMI, 2016.</li><li>Kalal Z, Mikolajczyk K, Matas J. <strong>Tracking-learning-detection </strong>[J]. IEEE TPAMI, 2012.</li></ul> 
<p>长江后浪推前浪，前面的已被排在沙滩上，这个后浪就是相关滤波和深度学习。<strong>相关滤波</strong>类方法correlation filter简称CF，也叫做discriminative correlation filter简称DCF，注意和后面的DCF算法区别，包括前面提到的那几个，也是后面要着重介绍的。<strong>深度学习</strong>（Deep ConvNet based）类方法，因为深度学习类目前不适合落地就不瞎推荐了，可以参考Winsty的几篇 <a href="https://link.zhihu.com/?target=http%3A//www.winsty.net/" rel="nofollow">Naiyan Wang - Home</a>，还有VOT2015的冠军<strong>MDNet </strong><a href="https://link.zhihu.com/?target=http%3A//cvlab.postech.ac.kr/research/mdnet/" rel="nofollow">Learning Multi-Domain Convolutional Neural Networks for Visual Tracking</a>，以及VOT2016的冠军<strong>TCNN </strong><a href="https://link.zhihu.com/?target=http%3A//www.votchallenge.net/vot2016/download/44_TCNN.zip" rel="nofollow">http://www.votchallenge.net/vot2016/download/44_TCNN.zip</a>，速度方面比较突出的如80FPS的<strong>SiamFC </strong><a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/~luca/siamese-fc.html" rel="nofollow">SiameseFC tracker</a>和100FPS的<strong>GOTURN </strong><a href="https://link.zhihu.com/?target=https%3A//github.com/davheld/GOTURN" rel="nofollow">davheld/GOTURN</a>，注意都是在GPU上。基于ResNet的<strong>SiamFC-R</strong>(ResNet)在VOT2016表现不错，很看好后续发展，有兴趣也可以去VALSE听作者自己讲解 <a href="https://link.zhihu.com/?target=http%3A//www.iqiyi.com/w_19ruirwrel.html%23vfrm%3D8-8-0-1" rel="nofollow">VALSE-20160930-LucaBertinetto-Oxford-JackValmadre-Oxford-pu</a>，至于GOTURN，效果比较差，但优势是跑的很快100FPS，如果以后效果也能上来就好了。做科研的同学深度学习类是关键，能兼顾速度就更好了。</p> 
<p>最后强力推荐两个资源：</p> 
<p>王强<a href="//www.zhihu.com/people/2d6e027e5d25744f64f437b50df5dea1" rel="nofollow">@Qiang Wang</a>维护的<strong>benchmark_results </strong> <a href="https://link.zhihu.com/?target=https%3A//github.com/foolwood/benchmark_results" rel="nofollow">foolwood/benchmark_results</a>：大量顶级方法在OTB库上的性能对比，各种论文代码应有尽有，大神自己C++实现并开源的CSK, KCF和DAT，还有他自己的<strong>DCFNet</strong>论文加源码，找不着路的同学请跟紧。</p> 
<p><a href="//www.zhihu.com/people/24f182250444c956a8ba54a520ceb91b" rel="nofollow">@H Hakase</a>维护的<strong>相关滤波类资源</strong> <a href="https://link.zhihu.com/?target=https%3A//github.com/HakaseH/CF_benchmark_results" rel="nofollow">HakaseH/CF_benchmark_results</a> ，详细分类和论文代码资源，走过路过别错过，相关滤波类算法非常全面，非常之用心！<br>  </p> 
<h3> </h3> 
<h3>相关滤波</h3> 
<p><strong>最经典的高速相关滤波类跟踪算法CSK, KCF/DCF, CN</strong></p> 
<p>KCF/DCF算法在OTB50上(<em>2014年4月就挂arVix了, 那时候OTB100还没有发表</em>)的实验结果，Precision和FPS碾压了OTB50上最好的Struck，看惯了勉强实时的Struck和TLD，飙到高速的KCF/DCF突然有点让人不敢相信，其实KCF/DCF就是在OTB上大放异彩的CSK的多通道特征改进版本。注意到那个超高速615FPS的MOSSE(<em>严重超速这是您的罚单</em>)，这是目标跟踪领域的第一篇相关滤波类方法，这其实是真正第一次显示了相关滤波的潜力。和KCF同一时期的还有个CN，在2014'CVPR上引起剧烈反响的颜色特征方法，其实也是CSK的多通道颜色特征改进算法。从MOSSE(615)到 CSK(362) 再到 KCF(172FPS), DCF(292FPS), CN(152FPS), CN2(202FPS)，速度虽然是越来越慢，但效果越来越好，而且始终保持在高速水平：</p> 
<ul><li>Bolme D S, Beveridge J R, Draper B A, et al. Visual object tracking using adaptive correlation filters [C]// CVPR, 2010.</li><li>Henriques J F, Caseiro R, Martins P, et al. Exploiting the circulant structure of tracking-by- detection with kernels [C]// ECCV, 2012.</li><li>Henriques J F, Rui C, Martins P, et al. High-Speed Tracking with Kernelized Correlation Filters [J]. IEEE TPAMI, 2015.</li><li>Danelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.</li></ul> 
<p> </p> 
<p>CSK和KCF都是Henriques J F(<em>牛津大学</em>)<a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/~joao/index.html%23" rel="nofollow">João F. Henriques</a> 大神先后两篇论文，影响后来很多工作，核心部分的岭回归，循环移位的近似密集采样，还给出了整个相关滤波算法的详细推导。还有岭回归加kernel-trick的封闭解，多通道HOG特征。</p> 
<p>Martin Danelljan大牛(林雪平大学)用多通道颜色特征Color Names(CN)去扩展CSK得到了不错的效果，算法也简称CN <a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html" rel="nofollow">Coloring Visual Tracking</a> 。</p> 
<p>MOSSE是单通道灰度特征的相关滤波，CSK在MOSSE的基础上扩展了密集采样(加padding)和kernel-trick，KCF在CSK的基础上扩展了多通道梯度的HOG特征，CN在CSK的基础上扩展了多通道颜色的Color Names。HOG是梯度特征，而CN是颜色特征，两者可以互补，所以HOG+CN在近两年的跟踪算法中成为了hand-craft特征标配。最后，根据KCF/DCF的实验结果，讨论两个问题：</p> 
<ul><li><strong>1. 为什么只用单通道灰度特征的KCF和用了多通道HOG特征的KCF速度差异很小？</strong></li></ul> 
<p>第一，作者用了HOG的快速算法fHOG，来自Piotr's Computer Vision Matlab Toolbox，C代码而且做了SSE优化。如对fHOG有疑问，请参考论文Object Detection with Discriminatively Trained Part Based Models第12页。<br> 第二，HOG特征常用cell size是4，这就意味着，100*100的图像，HOG特征图的维度只有25*25，而Raw pixels是灰度图归一化，维度依然是100*100，我们简单算一下：27通道HOG特征的复杂度是27*625*log(625)=47180，单通道灰度特征的复杂度是10000*log(10000)=40000，理论上也差不多，符合表格。</p> 
<p>看代码会发现，作者在扩展后目标区域面积较大时，会先对提取到的图像块做因子2的下采样到50*50，这样复杂度就变成了2500*log(2500)=8495，下降了非常多。那你可能会想，如果下采样再多一点，复杂度就更低了，但这是以牺牲跟踪精度为代价的，再举个例子，如果图像块面积为200*200，先下采样到100*100，再提取HOG特征，分辨率降到了25*25，这就意味着响应图的分辨率也是25*25，也就是说，响应图每位移1个像素，原始图像中跟踪框要移动8个像素，这样就降低了跟踪精度。在精度要求不高时，完全可以稍微牺牲下精度提高帧率(<em>但看起来真的不能再下采样了</em>)。</p> 
<p><strong>2. HOG特征的KCF和DCF哪个更好？</strong></p> 
<p>大部分人都会认为KCF效果超过DCF，而且各属性的准确度都在DCF之上，然而，如果换个角度来看，以DCF为基准，再来看加了kernel-trick的KCF，mean precision仅提高了0.4%，而FPS下降了41%，这么看是不是挺惊讶的呢？除了图像块像素总数，KCF的复杂度还主要和kernel-trick相关。所以，下文中的CF方法如果没有kernel-trick，就简称基于DCF，如果加了kernel-trick，就简称基于KCF(<em>剧透基本各占一半</em>)。当然这里的CN也有kernel-trick，但请注意，这是Martin Danelljan大神第一次使用kernel-trick，也是最后一次。。。</p> 
<p>这就会引发一个疑问，kernel-trick这么强大的东西，怎么才提高这么点？这里就不得不提到Winsty的另一篇大作：</p> 
<p>Wang N, Shi J, Yeung D Y, et al. Understanding and diagnosing visual tracking systems[C]// ICCV, 2015.</p> 
<p>一句话总结，别看那些五花八门的机器学习方法，那都是虚的，目标跟踪算法中特征才是最重要的（<em>就是因为这篇文章我粉了WIN叔哈哈</em>），以上就是最经典的三个高速算法，CSK, KCF/DCF和CN，推荐。</p> 
<p><strong>总体来说，相关滤波类方法对快速变形和快速运动情况的跟踪效果不好</strong>。</p> 
<p>快速变形主要因为CF是模板类方法。容易跟丢这个比较好理解，前面分析了相关滤波是模板类方法，如果目标快速变形，那基于HOG的梯度模板肯定就跟不上了，如果快速变色，那基于CN的颜色模板肯定也就跟不上了。这个还和模型更新策略与更新速度有关，固定学习率的线性加权更新，如果学习率太大，部分或短暂遮挡和任何检测不准确，模型就会学习到背景信息，积累到一定程度模型跟着背景私奔了，一去不复返。如果学习率太小，目标已经变形了而模板还是那个模板，就会变得不认识目标。</p> 
<p> </p> 
<p><strong>检测阶段，相关滤波对快速运动的目标检测比较乏力</strong>。相关滤波训练的图像块和检测的图像块大小必须是一样的，这就是说你训练了一个100*100的滤波器，那你也只能检测100*100的区域，如果打算通过加更大的padding来扩展检测区域，那样除了扩展了复杂度，并不会有什么好处。目标运动可能是目标自身移动，或摄像机移动，按照目标在检测区域的位置分四种情况来看：</p> 
<ol><li>如果目标在中心附近，检测准确且成功。</li><li>如果目标移动到了边界附近但还没有出边界，加了余弦窗以后，部分目标像素会被过滤掉，这时候就没法保证这里的响应是全局最大的，而且，这时候的检测样本和训练过程中的那些不合理样本很像，所以很可能会失败。</li><li>如果目标的一部分已经移出了这个区域，而我们还要加余弦窗，很可能就过滤掉了仅存的目标像素，检测失败。</li><li>如果整个目标已经位移出了这个区域，那肯定就检测失败了。</li></ol> 
<p>以上就是边界效应(Boundary Effets)，推荐两个主流的解决边界效应的方法，其中SRDCF速度比较慢，并不适合实时场合。</p> 
<p>Martin Danelljan的SRDCF <a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html" rel="nofollow">Learning Spatially Regularized Correlation Filters for Visual Tracking</a>，主要思路：既然边界效应发生在边界附近，那就忽略所有移位样本的边界部分像素，或者说限制让边界附近滤波器系数接近0：</p> 
<ul><li>Danelljan M, Hager G, Shahbaz Khan F, et al. Learning spatially regularized correlation filters for visual tracking [C]// ICCV. 2015.</li></ul> 
<p><img alt="" class="has" src="https://images2.imgbox.com/f5/09/wpQ8Kndg_o.jpg" width="1200"></p> 
<p>SRDCF基于DCF，类SAMF多尺度，采用更大的检测区域(padding = 4)，同时加入空域正则化，惩罚边界区域的滤波器系数，由于没有闭合解，采用高斯-塞德尔方法迭代优化。检测区域扩大(1.5-&gt;4)，迭代优化(破坏了闭合解)导致SRDCF只有5FP，但效果非常好是2015年的baseline。</p> 
<p>另一种方法是Hamed Kiani提出的MOSSE改进算法，基于灰度特征的<strong>CFLM</strong> <a href="https://link.zhihu.com/?target=http%3A//www.hamedkiani.com/cfwlb.html" rel="nofollow">Correlation Filters with Limited Boundaries</a> 和基于HOG特征的<strong>BACF</strong> <a href="https://link.zhihu.com/?target=http%3A//www.hamedkiani.com/bacf.html" rel="nofollow">Learning Background-Aware Correlation Filters for Visual Tracking</a>，主要思路是<strong>采用较大尺寸检测图像块和较小尺寸滤波器来提高真实样本的比例，或者说滤波器填充0以保持和检测图像一样大，同样没有闭合解，</strong>采用ADMM迭代优化：</p> 
<ul><li>Kiani Galoogahi H, Sim T, Lucey S. Correlation filters with limited boundaries [C]// CVPR, 2015.</li><li>Kiani Galoogahi H, Fagg A, Lucey S. Learning Background-Aware Correlation Filters for Visual Tracking [C]// ICCV, 2017.</li></ul> 
<p> </p> 
<p>CFLB仅单通道灰度特征，虽然速度比较快167FPS，但性能远不如KCF，不推荐；最新<strong>BACF将特征扩展为多通道HOG特征，性能超过了SRDCF，而且速度比较快35FPS，非常推荐。</strong></p> 
<p>其实这两个解决方案挺像的，都是用更大的检测及更新图像块，训练作用域比较小的相关滤波器，不同点是SRDCF的滤波器系数从中心到边缘平滑过渡到0，而CFLM直接用0填充滤波器边缘。</p> 
<p>VOT2015相关滤波方面还有排在第二名，结合深度特征的DeepSRDCF，因为深度特征都非常慢，在CPU上别说高速，实时都到不了，虽然性能非常高，但这里就不推荐，先跳过。</p> 
<p> </p> 
<p> </p> 
<h3>Benchmark Results</h3> 
<p><strong>The trackers are ordered by the average overlap scores.</strong></p> 
<ul><li><code>AUC</code> and <code>Precision</code> are the standard metrics.</li><li><code>Deep Learning</code>: deep learning features, deep learning method and RL.</li><li><code>RealTime</code>: Speeds from the original paper, not test on the same platform. (just focus magnitude)</li></ul> 
<table><thead><tr><th>Tracker</th><th>AUC-CVPR2013</th><th>Precision-CVPR2013</th><th>AUC-OTB100</th><th>Precision-OTB100</th><th>AUC-OTB50</th><th>Precision-OTB50</th><th>Deep Learning</th><th>RealTime</th></tr></thead><tbody><tr><td>ECO</td><td><strong>0.709</strong></td><td>0.93</td><td><strong>0.694</strong></td><td><strong>0.910</strong></td><td><em>0.643</em></td><td><em>0.874</em></td><td>Y</td><td>N(6)</td></tr><tr><td>MDNet</td><td><em>0.708</em></td><td><em>0.948</em></td><td><em>0.678</em></td><td><em>0.909</em></td><td><strong>0.645</strong></td><td><strong>0.890</strong></td><td>Y</td><td>N(1)</td></tr><tr><td>SANet</td><td>0.686</td><td><strong>0.95</strong></td><td>0.692</td><td>0.928</td><td>-</td><td>-</td><td>Y</td><td>N(1)</td></tr><tr><td>BranchOut</td><td> </td><td> </td><td>0.678</td><td>0.917</td><td> </td><td> </td><td>Y</td><td>N(1)</td></tr><tr><td>TCNN</td><td>0.682</td><td>0.937</td><td>0.654</td><td>0.884</td><td>-</td><td>-</td><td>Y</td><td>N(1)</td></tr><tr><td>TSN</td><td> </td><td> </td><td>0.644</td><td>0.868</td><td>0.58</td><td>0.809</td><td>Y</td><td>N(1)</td></tr><tr><td>CRT</td><td>-</td><td>-</td><td>0.642</td><td>0.875</td><td>0.594</td><td>0.835</td><td>Y</td><td>N(1.3)</td></tr><tr><td>BACF</td><td>0.678</td><td> </td><td>0.63</td><td> </td><td> </td><td> </td><td>N</td><td>Y(35)</td></tr><tr><td>MCPF</td><td>0.677</td><td>0.916</td><td>0.628</td><td>0.873</td><td> </td><td> </td><td>Y</td><td>N(0.5)</td></tr><tr><td>CREST</td><td>0.673</td><td>0.908</td><td>0.623</td><td>0.837</td><td>-</td><td>-</td><td>Y</td><td>N(1)</td></tr><tr><td>C-COT</td><td>0.672</td><td>0.899</td><td>0.682</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(0.3)</td></tr><tr><td>DNT</td><td>0.664</td><td>0.907</td><td>0.627</td><td>0.851</td><td>-</td><td>-</td><td>Y</td><td>N(5)</td></tr><tr><td>PTAV</td><td>0.663</td><td>0.894</td><td>0.635</td><td>0.849</td><td> </td><td> </td><td>Y</td><td>Y(25)</td></tr><tr><td>ADNet</td><td>0.659</td><td>0.903</td><td>0.646</td><td>0.88</td><td> </td><td> </td><td>Y</td><td>N(3)</td></tr><tr><td>DSiamM</td><td>0.656</td><td>0.891</td><td> </td><td> </td><td> </td><td> </td><td>Y</td><td>Y(25)</td></tr><tr><td>SINT+</td><td>0.655</td><td>0.882</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(4)</td></tr><tr><td>DRT</td><td>0.655</td><td>0.892</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(0.8)</td></tr><tr><td>RDT</td><td>0.654</td><td>-</td><td>0.603</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>Y(43)</td></tr><tr><td>SRDCFdecon</td><td>0.653</td><td>0.87</td><td>0.627</td><td>0.825</td><td>0.56</td><td>0.764</td><td>N</td><td>N(1)</td></tr><tr><td>DeepLMCF</td><td>0.643</td><td>0.892</td><td> </td><td> </td><td> </td><td> </td><td>Y</td><td>N(8)</td></tr><tr><td>MUSTer</td><td>0.641</td><td>0.865</td><td>0.575</td><td>0.774</td><td>-</td><td>-</td><td>N</td><td>N(4)</td></tr><tr><td>DeepSRDCF</td><td>0.641</td><td>0.849</td><td>0.635</td><td>0.851</td><td>0.56</td><td>0.772</td><td>Y</td><td>N(&lt;1)</td></tr><tr><td>EAST</td><td>0.638</td><td> </td><td> </td><td> </td><td> </td><td> </td><td>Y</td><td>Y(23/159)</td></tr><tr><td>SINT</td><td>0.635</td><td>0.851</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(4)</td></tr><tr><td>LCT</td><td>0.628</td><td>0.848</td><td>0.562</td><td>0.762</td><td>0.492</td><td>0.691</td><td>N</td><td>Y(27)</td></tr><tr><td>SRDCF</td><td>0.626</td><td>0.838</td><td>0.598</td><td>0.789</td><td>0.539</td><td>0.732</td><td>N</td><td>N(5)</td></tr><tr><td>LMCF</td><td>0.624</td><td>0.839</td><td>0.568</td><td> </td><td> </td><td> </td><td>N</td><td>Y(85)</td></tr><tr><td>SCF</td><td>0.623</td><td>0.874</td><td>-</td><td>-</td><td>-</td><td>-</td><td>N</td><td>Y(35)</td></tr><tr><td>Staple_CA</td><td>0.621</td><td>0.833</td><td>0.598</td><td>0.81</td><td> </td><td> </td><td>N</td><td>Y(35)</td></tr><tr><td>RaF</td><td>0.615</td><td>0.919</td><td> </td><td> </td><td> </td><td> </td><td>Y</td><td>N(2)</td></tr><tr><td>SiamFC</td><td>0.612</td><td>0.815</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>Y(58)</td></tr><tr><td>RFL</td><td> </td><td> </td><td>0.581</td><td> </td><td> </td><td> </td><td>Y</td><td>Y(15)</td></tr><tr><td>CFNet_conv2</td><td>0.611</td><td>0.807</td><td>0.568</td><td>0.748</td><td>0.53</td><td>0.702</td><td>Y</td><td>Y(75)</td></tr><tr><td>SiamFC_{3s}</td><td>0.608</td><td>0.809</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>Y(86)</td></tr><tr><td>ACFN</td><td>0.607</td><td>0.86</td><td>0.575</td><td>0.802</td><td> </td><td> </td><td>Y</td><td>Y(15)</td></tr><tr><td>CF2</td><td>0.605</td><td>0.891</td><td>0.562</td><td>0.837</td><td>0.513</td><td>0.803</td><td>Y</td><td>N(11)</td></tr><tr><td>HDT</td><td>0.603</td><td>0.889</td><td>0.654</td><td>0.848</td><td>0.515</td><td>0.804</td><td>Y</td><td>N(10)</td></tr><tr><td>Staple</td><td>0.6</td><td>0.793</td><td>0.578</td><td>0.784</td><td>-</td><td>-</td><td>N</td><td>Y(80)</td></tr><tr><td>CSR-DCF</td><td>0.599</td><td>0.8</td><td>0.598</td><td>0.733</td><td> </td><td> </td><td>N</td><td>Y(13)</td></tr><tr><td>FCNT</td><td>0.599</td><td>0.856</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(1)</td></tr><tr><td>CNN-SVM</td><td>0.597</td><td>0.852</td><td>0.554</td><td>0.814</td><td>0.512</td><td>0.769</td><td>Y</td><td>N</td></tr><tr><td>SCT</td><td>0.595</td><td>0.845</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>Y(40)</td></tr><tr><td>SO-DLT</td><td>0.595</td><td>0.81</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N</td></tr><tr><td>BIT</td><td>0.593</td><td>0.817</td><td>-</td><td>-</td><td>-</td><td>-</td><td>N</td><td>Y(45)</td></tr><tr><td>DLSSVM</td><td>0.589</td><td>0.829</td><td>0.541</td><td>0.767</td><td>-</td><td>-</td><td>Y</td><td>N(10)</td></tr><tr><td>SAMF</td><td>0.579</td><td>0.785</td><td>0.535</td><td>0.743</td><td>-</td><td>-</td><td>N</td><td>N(7)</td></tr><tr><td>RPT</td><td>0.577</td><td>0.805</td><td>-</td><td>-</td><td>-</td><td>-</td><td>N</td><td>N(4)</td></tr><tr><td>MEEM</td><td>0.566</td><td>0.83</td><td>0.53</td><td>0.781</td><td>0.473</td><td>0.712</td><td>N</td><td>N(10)</td></tr><tr><td>DSST</td><td>0.554</td><td>0.737</td><td>0.52</td><td>0.693</td><td>0.463</td><td>0.625</td><td>N</td><td>Y(24)</td></tr><tr><td>CNT</td><td>0.545</td><td>0.723</td><td>-</td><td>-</td><td>-</td><td>-</td><td>Y</td><td>N(1.5)</td></tr><tr><td>TGPR</td><td>0.529</td><td>0.766</td><td>0.458</td><td>0.643</td><td>-</td><td>-</td><td>N</td><td>N(1)</td></tr><tr><td>KCF</td><td>0.514</td><td>0.74</td><td>0.477</td><td>0.693</td><td>0.403</td><td>0.611</td><td>N</td><td>Y(<strong>172</strong>)</td></tr><tr><td>GOTURN</td><td>0.444</td><td>0.62</td><td>0.427</td><td>0.572</td><td>-</td><td>-</td><td>Y</td><td>Y(<em>16</em></td></tr></tbody></table> 
<p> </p> 
<h2><a href="https://www.cnblogs.com/jjwu/p/8512730.html" rel="nofollow" id="cb_post_title_url">目标跟踪简介</a></h2> 
<p> </p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/38/41/sU5ClYCn_o.gif"></p> 
<p>视觉目标跟踪是计算机视觉中的一个重要研究方向，有着广泛的应用，如：视频监控，人机交互， 无人驾驶等。过去二三十年视觉目标跟踪技术取得了长足的进步，特别是最近两年利用深度学习的目标跟踪方法取得了令人满意的效果，使目标跟踪技术获得了突破性的进展。本文旨在简要介绍：目标跟踪的基本流程与框架，目标跟踪存在的挑战，目标跟踪相关方法，以及目标跟踪最新的进展等，希望通过这篇文章能让读者对视觉目标跟踪领域有一个较为全面的认识。</p> 
<p><strong>1.</strong><strong>视觉目标跟踪基本流程与框架</strong></p> 
<p>        视觉目标（单目标）跟踪任务就是在给定某视频序列初始帧的目标大小与位置的情况下，预测后续帧中该目标的大小与位置。这一基本任务流程可以按如下的框架划分：</p> 
<p><img alt="" class="has" height="297" src="https://images2.imgbox.com/e0/4b/ypGkgSK3_o.png" width="648"></p> 
<p> </p> 
<p>        输入初始化目标框，在下一帧中产生众多候选框（Motion Model），提取这些候选框的特征（Feature Extractor），然后对这些候选框评分（Observation Model），最后在这些评分中找一个得分最高的候选框作为预测的目标（Prediction A），或者对多个预测值进行融合（Ensemble）得到更优的预测目标。</p> 
<p>        根据如上的框架，我们可以把目标跟踪划分为5项主要的研究内容. （1）运动模型：如何产生众多的候选样本。（2）特征提取：利用何种特征表示目标。（3）观测模型：如何为众多候选样本进行评分。（4）模型更新：如何更新观测模型使其适应目标的变化。（5）集成方法：如何融合多个决策获得一个更优的决策结果。下面分别简要介绍这5项研究内容。</p> 
<p><strong>运动模型（Motion Model）：</strong>生成候选样本的速度与质量直接决定了跟踪系统表现的优劣。常用的有两种方法：粒子滤波（Particle Filter）和滑动窗口（Sliding Window）。粒子滤波是一种序贯贝叶斯推断方法，通过递归的方式推断目标的隐含状态。而滑动窗口是一种穷举搜索方法，它列出目标附近的所有可能的样本作为候选样本。</p> 
<p><strong>特征提取（Feature Extractor）: </strong>鉴别性的特征表示是目标跟踪的关键之一。常用的特征被分为两种类型：手工设计的特征（Hand-crafted feature）和深度特征（Deep feature）。常用的手工设计的特征有灰度特征（Gray），方向梯度直方图（HOG），哈尔特征（Haar-like），尺度不变特征（SIFT）等。与人为设计的特征不同，深度特征是通过大量的训练样本学习出来的特征，它比手工设计的特征更具有鉴别性。因此，利用深度特征的跟踪方法通常很轻松就能获得一个不错的效果。</p> 
<p><strong>观测模型（Observation Model）:</strong>大多数的跟踪方法主要集中在这一块的设计上。根据不同的思路，观测模型可分为两类：生成式模型（Generative Model）和判别式模型（Discriminative Model）. 生成式模型通常寻找与目标模板最相似的候选作为跟踪结果，这一过程可以视为模板匹配。常用的理论方法包括：子空间，稀疏表示，字典学习等。而判别式模型通过训练一个分类器去区分目标与背景，选择置信度最高的候选样本作为预测结果。判别式方法已经成为目标跟踪中的主流方法，因为有大量的机器学习方法可以利用。常用的理论方法包括：逻辑回归，岭回归，支持向量机，多示例学习，相关滤波等。</p> 
<p><strong>模型更新（Model Update）: </strong>模型更新主要是更新观测模型，以适应目标表观的变化，防止跟踪过程发生漂移。模型更新没有一个统一的标准，通常认为目标的表观连续变化，所以常常会每一帧都更新一次模型。但也有人认为目标过去的表观对跟踪很重要，连续更新可能会丢失过去的表观信息，引入过多的噪音，因此利用长短期更新相结合的方式来解决这一问题。</p> 
<p><strong>集成方法（Ensemble Method）: </strong>集成方法有利于提高模型的预测精度，也常常被视为一种提高跟踪准确率的有效手段。可以把集成方法笼统的划分为两类：在多个预测结果中选一个最好的，或是利用所有的预测加权平均。</p> 
<p> </p> 
<p><strong>2.</strong><strong>视觉目标跟踪面临的挑战</strong></p> 
<p>      视觉运动目标跟踪是一个极具挑战性的任务，因为对于运动目标而言，其运动的场景非常复杂并且经常发生变化，或是目标本身也会不断变化。那么如何在复杂场景中识别并跟踪不断变化的目标就成为一个挑战性的任务。如下图我们列出了目标跟踪中几个主要的挑战因素：</p> 
<p> </p> 
<p> <img alt="" class="has" src="https://images2.imgbox.com/ca/8d/HzGRDvms_o.png"><img alt="" class="has" height="418" src="https://images2.imgbox.com/02/ce/rCTISUsZ_o.png" width="715"></p> 
<p>        其中<strong>遮挡（Occlusion）</strong>是目标跟踪中最常见的挑战因素之一，遮挡又分为部分遮挡（Partial Occlusion）和完全遮挡（Full Occlusion）。解决部分遮挡通常有两种思路：（1）利用检测机制判断目标是否被遮挡，从而决定是否更新模板，保证模板对遮挡的鲁棒性。（2）把目标分成多个块，利用没有被遮挡的块进行有效的跟踪。对于目标被完全遮挡的情况，当前也并没有有效的方法能够完全解决。</p> 
<p><strong>形变（Deformation）</strong>也是目标跟踪中的一大难题，目标表观的不断变化，通常导致跟踪发生漂移（Drift）。解决漂移问题常用的方法是更新目标的表观模型，使其适应表观的变化，那么模型更新方法则成为了关键。什么时候更新，更新的频率多大是模型更新需要关注的问题。</p> 
<p><strong>背景杂斑（Background Clutter）</strong>指得是要跟踪的目标周围有非常相似的目标对跟踪造成了干扰。解决这类问题常用的手段是利用目标的运动信息，预测运动的大致轨迹，防止跟踪器跟踪到相似的其他目标上，或是利用目标周围的大量样本框对分类器进行更新训练，提高分类器对背景与目标的辨别能力。</p> 
<p><strong>尺度变换（Scale Variation）</strong>是目标在运动过程中的由远及近或由近及远而产生的尺度大小变化的现象。预测目标框的大小也是目标跟踪中的一项挑战，如何又快又准确的预测出目标的尺度变化系数直接影响了跟踪的准确率。通常的做法有：在运动模型产生候选样本的时候，生成大量尺度大小不一的候选框，或是在多个不同尺度目标上进行目标跟踪，产生多个预测结果，选择其中最优的作为最后的预测目标。</p> 
<p>        当然，除了上述几个常见的挑战外，还有一些其他的挑战性因素：光照（illumination）,低分辨率（Low Resolution）,运动模糊（Motion Blur）,快速运动（Fast Motion），超出视野（Out of View），旋转（Rotation）等。所有的这些挑战因数共同决定了目标跟踪是一项极为复杂的任务。更多信息请参考<a href="http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html" rel="nofollow">http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html</a>。</p> 
<p><strong>3.</strong><strong>视觉目标跟踪方法</strong></p> 
<p>        视觉目标跟踪方法根据观测模型是生成式模型或判别式模型可以被分为生成式方法（Generative Method）和判别式方法（Discriminative Method）。前几年最火的生成式跟踪方法大概是稀疏编码（Sparse Coding）了, 而近来判别式跟踪方法逐渐占据了主流地位，以相关滤波（Correlation Filter）和深度学习（Deep Learning）为代表的判别式方法也取得了令人满意的效果。下面我们分别简要概括这几种方法的大体思想和其中的一些具体的跟踪方法。</p> 
<p><strong>稀疏表示(Sparse Representation)：</strong>给定一组过完备字典，将输入信号用这组过完备字典线性表示，对线性表示的系数做一个稀疏性的约束（即使得系数向量的分量尽可能多的为0），那么这一过程就称为稀疏表示。基于稀疏表示的目标跟踪方法则将跟踪问题转化为稀疏逼近问题来求解。如稀疏跟踪的开山之作L1Tracker, 认为候选样本可以被稀疏的表示通过目标模板和琐碎模板，而一个好的候选样本应该拥有更稀疏的系数向量。稀疏性可通过解决一个L1正则化的最小二乘优化问题获得，最后将与目标模板拥有最小重构误差的候选样本作为跟踪结果。L1Tracker中利用琐碎模板处理遮挡，利用对稀疏系数的非负约束解决背景杂斑问题。随后在L1Tracker基础上的改进则有很多，比较有代表性的有ALSA，L1APG等。</p> 
<p><strong>相关滤波(Correlation Filter)：</strong>相关滤波源于信号处理领域，相关性用于表示两个信号之间的相似程度，通常用卷积表示相关操作。那么基于相关滤波的跟踪方法的基本思想就是，寻找一个滤波模板，让下一帧的图像与我们的滤波模板做卷积操作，响应最大的区域则是预测的目标。根据这一思想先后提出了大量的基于相关滤波的方法，如最早的平方误差最小输出和（MOSSE）利用的就是最朴素的相关滤波思想的跟踪方法。随后基于MOSSE有了很多相关的改进，如引入核方法（Kernel Method）的CSK，KCF等都取得了很好的效果，特别是利用循环矩阵计算的KCF，跟踪速度惊人。在KCF的基础上又发展了一系列的方法用于处理各种挑战。如：DSST可以处理尺度变化，基于分块的（Reliable Patches）相关滤波方法可处理遮挡等。但是所有上述的基于相关滤波的方法都受到边界效应（Boundary Effect）的影响。为了克服这个问题SRDCF应运而生，SRDCF利用空间正则化惩罚了相关滤波系数获得了可与深度学习跟踪方法相比的结果。</p> 
<p><strong>深度学习（CNN-Based）：</strong>因为深度特征对目标拥有强大的表示能力，深度学习在计算机视觉的其他领域，如：检测，人脸识别中已经展现出巨大的潜力。但早前两年，深度学习在目标跟踪领域的应用并不顺利，因为目标跟踪任务的特殊性，只有初始帧的图片数据可以利用，因此缺乏大量的数据供神经网络学习。只到研究人员把在分类图像数据集上训练的卷积神经网络迁移到目标跟踪中来，基于深度学习的目标跟踪方法才得到充分的发展。如：CNN-SVM利用在ImageNet分类数据集上训练的卷积神经网络提取目标的特征，再利用传统的SVM方法做跟踪。与CNN-SVM提取最后一层的深度特征不同的是，FCN利用了目标的两个卷积层的特征构造了可以选择特征图的网络，这种方法比只利用最后的全连接层的CNN-SVM效果有些许的提升。随后HCF, HDT等方法则更加充分的利用了卷积神经网络各层的卷积特征，这些方法在相关滤波的基础上结合多层次卷积特征进一步的提升了跟踪效果。然而，跟踪任务与分类任务始终是不同的，分类任务关心的是区分类间差异，忽视类内的区别。目标跟踪任务关心的则是区分特定目标与背景，抑制同类目标。两个任务有着本质的区别，因此在分类数据集上预训练的网络可能并不完全适用于目标跟踪任务。于是，Nam设计了一个专门在跟踪视频序列上训练的多域（Multi-Domain）卷积神经网络（MDNet），结果取得了VOT2015比赛的第一名。但是MDNet在标准集上进行训练多少有一点过拟合的嫌疑，于是VOT2016比赛中禁止在标准跟踪数据集上进行训练。2016年SRDCF的作者继续发力，也利用了卷积神经网络提取目标特征然后结合相关滤波提出了C-COT的跟踪方法取得了VOT2016的冠军。</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p><strong>4.</strong><strong>视觉目标跟踪最新进展</strong></p> 
<p>        目标跟踪最近几年发展迅速，以基于相关滤波（Correlation Filter）和卷积神经网络（CNN）的跟踪方法已经占据了目标跟踪的大半江山。如下图给出的2014-2017年以来表现排名靠前的一些跟踪方法。</p> 
<p> <img alt="" class="has" height="480" src="https://images2.imgbox.com/71/a3/KruqSCvU_o.png" width="651"></p> 
<p>可以看到前三名的方法不是基于相关滤波的方法就是基于卷积神经网络的方法,或是两者结合的方法。比如ECCV2016的C-COT就是在相关滤波的基础上结合卷积神经网络的杰作。下图给出这些方法在标准跟踪数据集OTB2013上的跟踪结果：</p> 
<p> <img alt="" class="has" height="354" src="https://images2.imgbox.com/2a/7e/xcJLWxQa_o.png" width="600"></p> 
<p>可以看到基于卷积神经网络的方法取得了惊人的突破。预计未来两年相关滤波和卷积神经网络将仍然会是目标跟踪领域的主角。</p> 
<p> </p> 
<p> </p> 
<h2>卷积特征</h2> 
<p>最后这部分是Martin Danelljan的专场，主要介绍他的一些列工作，尤其是结合深度特征的相关滤波方法，代码都在他主页<a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/" rel="nofollow">Visual Tracking</a>，就不一一贴出了。</p> 
<ul><li>Danelljan M, Shahbaz Khan F, Felsberg M, et al. Adaptive color attributes for real-time visual tracking [C]// CVPR, 2014.</li></ul> 
<p>在CN中提出了非常重要的多通道颜色特征Color Names，用于CSK框架取得非常好得效果，还提出了加速算法CN2，通过类PCA的自适应降维方法，对特征通道数量降维(10 -&gt; 2)，平滑项增加跨越不同特征子空间时的代价，也就是PCA中的协方差矩阵线性更新防止降维矩阵变化太大。</p> 
<ul><li>Danelljan M, Hager G, Khan F S, et al. Discriminative Scale Space Tracking [J]. IEEE TPAMI, 2017.</li></ul> 
<p>DSST是VOT2014的第一名，开创了平移滤波+尺度滤波的方式。在fDSST中对DSST进行加速，PCA方法将平移滤波HOG特征的通道降维(31 -&gt; 18)，QR方法将尺度滤波器~1000*17的特征降维到17*17，最后用三角插值(频域插值)将尺度数量从17插值到33以获得更精确的尺度定位。</p> 
<p>SRDCF是VOT2015的第四名，为了减轻边界效应扩大检测区域，优化目标增加了空间约束项，用高斯-塞德尔方法迭代优化，并用牛顿法迭代优化平移检测的子网格精确目标定位。</p> 
<ul><li>Danelljan M, Hager G, Shahbaz Khan F, et al. Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking [C]// CVPR, 2016.</li></ul> 
<p><img alt="" class="has" src="https://images2.imgbox.com/6a/a3/6x7lavA5_o.jpg" width="826"></p> 
<p>SRDCFdecon在SRDCF的基础上，改进了样本和学习率问题。以前的相关滤波都是固定学习率线性加权更新模型，虽然这样比较简单不用保存以前样本，但在定位不准确、遮挡、背景扰动等情况会污染模型导致漂移。SRDCFdecon选择保存以往样本(图像块包括正，负样本)，在优化目标函数中添加样本权重参数和正则项，采用交替凸搜索，首先固定样本权重，高斯-塞德尔方法迭代优化模型参数，然后固定模型参数，凸二次规划方法优化样本权重。</p> 
<ul><li>Danelljan M, Hager G, Shahbaz Khan F, et al. Convolutional features for correlation filter based visual tracking [C]// ICCVW, 2015.</li></ul> 
<p>DeepSRDCF是VOT2015的第二名，将SRDCF中的HOG特征替换为CNN中单层卷积层的深度特征(也就是卷积网络的激活值)，效果有了极大提升。这里用imagenet-vgg-2048 network，VGG网络的迁移能力比较强，而且MatConvNet就是VGG组的，MATLAB调用非常方便。论文还测试了不同卷积层在目标跟踪任务中的表现：</p> 
<p><img alt="" class="has" height="273" src="https://images2.imgbox.com/26/bb/LATmoTCF_o.png" width="611"></p> 
<p>第1层表现最好，第2和第5次之。由于卷积层数越高语义信息越多，但纹理细节越少，从1到4层越来越差的原因之一就是特征图的分辨率越来越低，但第5层反而很高，是因为包括完整的语义信息，判别力比较强(本来就是用来做识别的)。</p> 
<p><img alt="" class="has" height="110" src="https://images2.imgbox.com/20/58/SONSfrq6_o.png" width="621"></p> 
<p>注意区分这里的深度特征和基于深度学习的方法，深度特征来自ImageNet上预训练的图像分类网络，没有fine-turn这一过程，不存在过拟合的问题。而基于深度学习的方法大多需要在跟踪序列上end-to-end训练或fine-turn，如果样本数量和多样性有限就很可能过拟合。</p> 
<ul><li>Ma C, Huang J B, Yang X, et al. Hierarchical convolutional features for visual tracking [C]// ICCV, 2015.</li></ul> 
<p><img alt="" class="has" height="356" src="https://images2.imgbox.com/1c/46/yioVVF6d_o.png" width="598"></p> 
<p>值得一提的还有Chao Ma的HCF，结合多层卷积特征提升效果，用了VGG19的Conv5-4, Conv4-4和Conv3-4的激活值作为特征，所有特征都缩放到图像块分辨率，虽然按照论文应该是由粗到细确定目标，但代码中比较直接，三种卷积层的响应以固定权值1, 0.5, 0.02线性加权作为最终响应。虽然用了多层卷积特征，但没有关注边界效应而且线性加权的方式过于简单，HCF在VOT2016仅排在28名（单层卷积深度特征的DeepSRDCF是第13名）。</p> 
<ul><li>Danelljan M, Robinson A, Khan F S, et al. Beyond correlation filters: Learning continuous convolution operators for visual tracking [C]// ECCV, 2016.</li></ul> 
<p><img alt="" class="has" height="550" src="https://images2.imgbox.com/17/8e/I8EtZlbL_o.png" width="868"></p> 
<p>C-COT是VOT2016的第一名，综合了SRDCF的空域正则化和SRDCFdecon的自适应样本权重，还将DeepSRDCF的单层卷积的深度特征扩展为多成卷积的深度特征（VGG第1和5层），为了应对不同卷积层分辨率不同的问题，提出了连续空间域插值转换操作，在训练之前通过频域隐式插值将特征图插值到连续空域，方便集成多分辨率特征图，并且保持定位的高精度。目标函数通过共轭梯度下降方法迭代优化，比高斯-塞德尔方法要快，自适应样本权值直接采用先验权值，没有交替凸优化过程，检测中用牛顿法迭代优化目标位置。</p> 
<p>注意<strong>以上SRDCF, SRDCFdecon，DeepSRDCF，C-COT都无法实时</strong>，这一系列工作虽然效果越来越好，但也越来越复杂，在相关滤波越来越慢失去速度优势的时候，Martin Danelljan在2017CVPR的ECO来了一脚急刹车，大神来告诉我们什么叫又好又快，不忘初心：</p> 
<ul><li>Danelljan M, Bhat G, Khan F S, et al. ECO: Efficient Convolution Operators for Tracking [C]// CVPR, 2017.</li></ul> 
<p><strong>ECO是C-COT的加速版，从模型大小、样本集大小和更新策略三个方便加速，速度比C-COT提升了20倍</strong>，加量还减价，EAO提升了13.3%，最最最厉害的是， hand-crafted features的ECO-HC有60FPS。。吹完了，来看看具体做法。</p> 
<p> </p> 
<p>第一减少模型参数，定义了factorized convolution operator(分解卷积操作)，效果类似PCA，用PCA初始化，然后仅在第一帧优化这个降维矩阵，以后帧都直接用，简单来说就是有监督降维，深度特征时模型参数减少了80%。</p> 
<p><img alt="" class="has" height="476" src="https://images2.imgbox.com/6b/0e/JWYLjZy2_o.png" width="1200"></p> 
<p>第二减少样本数量， compact generative model(紧凑的样本集生成模型)，采用Gaussian Mixture Model (GMM)合并相似样本，建立更具代表性和多样性的样本集，需要保存和优化的样本集数量降到C-COT的1/8。</p> 
<p><img alt="" class="has" height="767" src="https://images2.imgbox.com/36/6c/Vsd81gnk_o.png" width="1200"></p> 
<p>第三改变更新策略，sparser updating scheme(稀疏更新策略)，每隔5帧做一次优化更新模型参数，不但提高了算法速度，而且提高了对突变，遮挡等情况的稳定性。但样本集是每帧都更新的，稀疏更新并不会错过间隔期的样本变化信息。</p> 
<p>ECO的成功当然还有很多细节，而且有些我也看的不是很懂，总之很厉害就是了。。ECO实验跑了四个库(VOT2016, UAV123, OTB-2015, and TempleColor)都是第一，而且没有过拟合的问题，仅性能来说ECO是目前最好的相关滤波算法，也有可能是最好的目标跟踪算法。hand-crafted features版本的ECO-HC，降维部分原来HOG+CN的42维特征降到13维，其他部分类似，实验结果ECO-HC超过了大部分深度学习方法，而且论文给出速度是CPU上60FPS。</p> 
<p>最后是来自Luca Bertinetto的CFNet <a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/~luca/cfnet.html" rel="nofollow">End-to-end representation learning for Correlation Filter based tracking</a>，除了上面介绍的相关滤波结合深度特征，相关滤波也可以end-to-end方式在CNN中训练了：</p> 
<ul><li>Valmadre J, Bertinetto L, Henriques J F, et al. End-to-end representation learning for Correlation Filter based tracking [C]// CVPR, 2017.</li></ul> 
<p><img alt="" class="has" height="453" src="https://images2.imgbox.com/37/e8/IcYvOhUq_o.png" width="1078"></p> 
<p>在SiamFC的基础上，将相关滤波也作为CNN中的一层，最重要的是cf层的前向传播和反向传播公式推导，两层卷积层的CFNet在GPU上是75FPS，综合表现并没有很多惊艳，可能是难以处理CF层的边界效应吧，持观望态度。</p> 
<p> </p> 
<h3>2017年CVPR和ICCV结果</h3> 
<p>下面是CVPR 2017的目标跟踪算法结果：可能MD大神想说，一个能打的都没有！</p> 
<p><img alt="" class="has" height="589" src="https://images2.imgbox.com/c9/d6/Rh6qFiSm_o.png" width="1200"></p> 
<p>仿照上面的表格，整理了ICCV 2017的相关论文结果对比ECO：哎，还是一个能打的都没有！</p> 
<p><img alt="" class="has" height="569" src="https://images2.imgbox.com/04/69/6iR01tTs_o.png" width="979"></p> 
<p> </p> 
<p><strong>=================================分 割 线 ====2018============================================</strong></p> 
<p>VOT2018 paper: <a href="http://link.zhihu.com/?target=http%3A//prints.vicos.si/publications/files/365" rel="nofollow">http://prints.vicos.si/publications/files/365</a></p> 
<p>VOT2018 presentation: <a href="http://link.zhihu.com/?target=http%3A//data.votchallenge.net/vot2018/presentations/vot2018_presentation.pdf" rel="nofollow">http://data.votchallenge.net/vot2018/presentations/vot2018_presentation.pdf</a></p> 
<p>计算机视觉两大盛会CVPR 2018和ECCV 2018，与Visual Object Tracking领域一年一度最权威竞赛VOT2018 <a href="http://link.zhihu.com/?target=http%3A//votchallenge.net/vot2018/index.html" rel="nofollow">VOT2018 Challenge</a> 随着ECCV落下帷幕，一起来看看今年单目标跟踪方向又有什么重大突破，寻找高含金量的好paper，发现速度性能双高的实用算法。</p> 
<p>（<strong><em>看点：DCF是否依然是中流砥柱，未来之星SiamNet发展到了什么程度</em></strong>）</p> 
<h3><strong>Short-term tracking challenge</strong></h3> 
<p>短期跟踪竞赛依然是VOT2017的60个<strong>public dataset</strong>公开序列（公测）和60个<strong>sequestered dataset</strong>隐藏序列（内测），序列和评价指标完全相同。今年共有72个算法参赛，下面是前50名的公测结果（高亮标出了一些baseline）：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/eb/28/JulwOSIH_o.jpg" width="1148"></p> 
<p>72个参赛算法中占比最高的是<strong>DCF</strong>(discriminative correlation filter)类和<strong>SiamNet</strong>(Siamese network)类，其中38个是DCF类方法占比51%，feature以来自VGGNet的深度特征为主；14个是SiamNet类占比18%，backbone以SiamFC中的AlexNet为主，今年SiamNet类占比大幅提高。</p> 
<ul><li><strong>EAO</strong>：两个baseline，VOT2016和VOT2017的神话<strong>CCOT</strong>，和2017年最好算法<strong>ECO</strong>都只能排在20左右，已经被大幅超越，甚至前几名都与ECO拉开了0.1以上的差距。</li><li><strong>R鲁棒性</strong>前四名：<strong>MFT, LADCF, RCO, UPDT</strong>，都是DCF类方法，CNN特征提取的backbone都是ResNet-50。</li><li><strong>A准确性</strong>前两名：<strong>SiamRPN, SA-Siam-R</strong>, 都是SiamNet类方法，这两个算法都表现出准确性奇高，而鲁棒性前十最差的特点。</li></ul> 
<p>（准确<em>性奇高而鲁棒性较差，这是算法设计的缺陷，还是SiamNet类别天生的劣势？</em>）</p> 
<p>VOT竞赛是各新算法的试金石，在前20名中我们也能看到2018年CVPR和ECCV的一些论文：</p> 
<p><strong>CVPR 2018：SiamRPN, DRT, STRCF, SA-Siam, LSART</strong></p> 
<p><strong>ECCV 2018：DaSiamRPN, UPDT</strong></p> 
<p>以上就是性能比较突出的2018年顶会了，好文推荐！其中SiamRPN, SA-Siam, DaSiamRPN三篇是Siamese Net类方法，DRT, STRCF, UPDT三篇是DCF类方法。</p> 
<p> </p> 
<p> </p> 
<h3><strong>总结</strong></h3> 
<p>DCF依然领跑性能，short-term榜依旧以DCF+CNN为主，尤其UPDT贡献巨大，对前几名算法都有影响，但速度越来越慢看不到边，KCF不堪回首。</p> 
<p>SiamNet速度快性能也不差，real-time榜和long-term榜都是SiamNet登顶，尤其SiamRPN潜力巨大，打通了目标跟踪和目标检测，性价比很高，接下来会快速发展壮大，希望速度优势能保得住。</p> 
<p> </p> 
<p> </p> 
<p><strong>=================================分===割===线======2019.11.22======================================</strong></p> 
<p>时隔一年，再次回来目标跟踪领域，不得不感叹，大神们发paper的速度实在是太快了，去年封王的算法今年看来也是被虐的渣都不剩，深感落后了啊，赶紧抓一把来看看最新的算法情况。</p> 
<h4>首先看<strong>SiamRPN系列文章</strong></h4> 
<p><strong>如下：</strong></p> 
<p>[0] SiamFC文章，对SINT（Siamese Instance Search for Tracking，in CVPR2016）改进，第一个提出用全卷积孪生网络结构来解决tracking问题的paper，可以视为只有一个anchor的SiamRPN</p> 
<p>论文题目：Fully-convolutional siamese networks for object tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.09549" rel="nofollow">https://arxiv.org/abs/1606.09549</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=https%3A//www.robots.ox.ac.uk/~luca/siamese-fc.html" rel="nofollow">https://www.robots.ox.ac.uk/~luca/siamese-fc.html</a></p> 
<p>tf实现：<a href="https://link.zhihu.com/?target=https%3A//github.com/torrvision/siamfc-tf" rel="nofollow">https://github.com/torrvision/siamfc-tf</a></p> 
<p>pytorch实现：<a href="https://link.zhihu.com/?target=https%3A//github.com/rafellerc/Pytorch-SiamFC" rel="nofollow">https://github.com/rafellerc/Pytorch-SiamFC</a></p> 
<p>[0.1] 后面的v2版本即CFNet，用cf操作代替了correlation操作。</p> 
<p>论文题目：End-To-End Representation Learning for Correlation Filter Based Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/html/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.html" rel="nofollow">http://openaccess.thecvf.com/content_cvpr_2017/html/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.html</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/~luca/cfnet.html" rel="nofollow">http://www.robots.ox.ac.uk/~luca/cfnet.html</a></p> 
<p>MatConvNet实现：<a href="https://link.zhihu.com/?target=https%3A//github.com/bertinetto/cfnet" rel="nofollow">https://github.com/bertinetto/cfnet</a></p> 
<p>SiamFC之后有诸多的改进工作，例如</p> 
<p>[0.2] StructSiam，在跟踪中考虑Local structures</p> 
<p>论文题目：Structured Siamese Network for Real-Time Visual Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf</a></p> 
<p>[0.3] SiamFC-tri，在Saimese跟踪网络中引入了Triplet Loss</p> 
<p>论文题目：Triplet Loss in Siamese Network for Object Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf</a></p> 
<p>[0.4] DSiam，动态Siamese网络</p> 
<p>论文题目：Learning Dynamic Siamese Network for Visual Object Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf</a></p> 
<p>代码地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/tsingqguo/DSiam" rel="nofollow">https://github.com/tsingqguo/DSiam</a></p> 
<p>[0.5] SA-Siam，Twofold Siamese网络</p> 
<p>论文题目：A Twofold Siamese Network for Real-Time Object Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf</a></p> 
<p>[1] SiamRPN文章，将anchor应用在候选区域的每个位置，同时进行分类和回归，one-shot local detection。</p> 
<p>论文题目：High Performance Visual Tracking with Siamese Region Proposal Network</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf" rel="nofollow">http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=http%3A//bo-li.info/SiamRPN/" rel="nofollow">http://bo-li.info/SiamRPN/</a></p> 
<p>[2] DaSiamRPN, SiamRPN文章的follow-up，重点强调了训练过程中样本不均衡的问题，增加了正样本的种类和有语义的负样本。</p> 
<p>论文题目：Distractor-aware Siamese Networks for Visual Object Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.06048" rel="nofollow">https://arxiv.org/abs/1808.06048</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=http%3A//bo-li.info/DaSiamRPN/" rel="nofollow">http://bo-li.info/DaSiamRPN/</a></p> 
<p>test code：<a href="https://link.zhihu.com/?target=https%3A//github.com/foolwood/DaSiamRPN" rel="nofollow">https://github.com/foolwood/DaSiamRPN</a></p> 
<p>[3] Cascaded SiamRPN，将若干RPN模块cascade起来，同时利用了不同layer的feature。</p> 
<p>论文题目：Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.06148" rel="nofollow">https://arxiv.org/abs/1812.06148</a></p> 
<p>[4] SiamMask，在SiamRPN的结构中增加了一个mask分支，同时进行tracking和video segmentation。</p> 
<p>论文题目：Fast Online Object Tracking and Segmentation: A Unifying Approach</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.05050" rel="nofollow">https://arxiv.org/abs/1812.05050</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=http%3A//www.robots.ox.ac.uk/~qwang/SiamMask/" rel="nofollow">http://www.robots.ox.ac.uk/~qwang/SiamMask/</a></p> 
<p>[5] SiamRPN++, SiamRPN文章的follow-up，让现代网络例如ResNet在tracking中work了，基本上所有数据集都是SOTA。</p> 
<p>论文题目：SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.11703" rel="nofollow">https://arxiv.org/abs/1812.11703</a></p> 
<p>项目地址：<a href="https://link.zhihu.com/?target=http%3A//bo-li.info/SiamRPN%2B%2B/" rel="nofollow">http://bo-li.info/SiamRPN++/</a></p> 
<p>[6] Deeper and Wider SiamRPN，将网络加深加宽来提升性能，重点关注感受野和padding的影响。</p> 
<p>论文题目：Deeper and Wider Siamese Networks for Real-Time Visual Tracking</p> 
<p>论文地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.01660" rel="nofollow">https://arxiv.org/abs/1901.01660</a></p> 
<p>test code：<u><a href="https://link.zhihu.com/?target=https%3A//gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers" rel="nofollow">https://gitlab.com/MSRA_NLPR/deeper_wider_siamese_trackers</a></u></p> 
<p> </p> 
<h2>2019论文盘点</h2> 
<p>跟踪在计算机视觉里有很广泛的内涵，本文所指的跟踪<strong>为通用目标跟踪</strong>，不包括比如人脸特征点跟踪、视线跟踪等特定领域。</p> 
<p>本文总结了 19 篇相关论文，列出了代码地址，并大致分类为<strong>单目标跟踪（最多）、多目标跟踪、跟踪与分割、3D目标跟踪、跟踪数据集</strong>几部分。</p> 
<p>比较有意思的是，跟踪领域的人也开始关注分割了，ICCV上也有相关的workshop。这给算法带来新挑战。</p> 
<p>可以在以下网站下载这些论文：</p> 
<p><a href="http://openaccess.thecvf.com/CVPR2019.py" rel="nofollow">http://openaccess.thecvf.com/CVPR2019.py</a></p> 
<p> </p> 
<h3><strong>单目标跟踪</strong></h3> 
<p> </p> 
<p><strong>一种无监督的方式对大规模无标记视频进行训练的。动机是一个健壮的跟踪器应该在视频前向和后向预测中都是有效的，在Siamese相关滤波网络上构建了算法框架，该网络使用未标记的原始视频进行训练，达到了有监督跟踪器的baseline精度。</strong></p> 
<p>Unsupervised Deep Tracking</p> 
<p>Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li</p> 
<p><a href="https://github.com/594422814/UDT/">https://github.com/594422814/UDT/</a></p> 
<p> </p> 
<p><strong>为了实现多目标跟踪的无标签和端到端学习，提出了一种用动画来跟踪的框架，其中可微分神经模型首先跟踪输入帧中的对象，然后将这些对象动画化为重建帧。然后通过反向传播通过重建误差来驱动学习。</strong></p> 
<p>军事医学研究院、国防科技大学、伦敦大学学院、阿兰图灵研究所</p> 
<p>Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers</p> 
<p>Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber</p> 
<p><a href="https://github.com/zhen-he/tracking-by-animation">https://github.com/zhen-he/tracking-by-animation</a></p> 
<p> </p> 
<p><strong>通过特定视点的鉴别相关滤波的重建进行目标跟踪</strong></p> 
<p>Tampere University、University of Ljubljana</p> 
<p>Object Tracking by Reconstruction With View-Specific Discriminative Correlation Filters</p> 
<p>Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas</p> 
<p> </p> 
<p><strong>提出了一种新的学习目标感知特征的方法，该方法能够比预训练的深度特征更好地识别发生显著外观变化的目标。目标感知特征与Siamese匹配网络集成，用于视觉跟踪。大量的实验结果表明，该算法在精度和速度上均优于现有的算法。</strong></p> 
<p>哈工大、上海交大、腾讯AI实验室、加州大学默塞德分校、Google Cloud AI</p> 
<p>Target-Aware Deep Tracking</p> 
<p>Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang</p> 
<p><a href="https://github.com/XinLi-zn/TADT">https://github.com/XinLi-zn/TADT</a></p> 
<p><a href="https://github.com/ZikunZhou/TADT-python">https://github.com/ZikunZhou/TADT-python</a></p> 
<p> </p> 
<p><strong>SPM跟踪器：用于实时视觉对象跟踪的串并联匹配。提出了串并联匹配（Series-Parallel Matching）的结构。整个结构分为两个部分，称之为“粗匹配”与“细匹配”。</strong></p> 
<p>中国科技大学、微软亚洲研究院</p> 
<p>SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking</p> 
<p>Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng</p> 
<p> </p> 
<p><strong>SiamRPN++：目前精度最高的单目标跟踪</strong></p> 
<p>商汤研究院、中科院自动化所、中科院计算所</p> 
<p>SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks</p> 
<p>Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan</p> 
<p><a href="https://github.com/STVIR/pysot">https://github.com/STVIR/pysot</a></p> 
<p> </p> 
<p><strong>对影响跟踪精度的主干网络因素进行了系统的研究，为Siamese跟踪框架提供了一个架构设计的指导；基于文章提出的无填充残差单元，设计了一种新的用于Siamese跟踪的更深、更宽的网络架构。实验结果显示新的架构对基准跟踪算法确实有很明显的性能提升效果。</strong></p> 
<p>中科院大学&amp;中科院自动化所、微软研究院</p> 
<p>Deeper and Wider Siamese Networks for Real-Time Visual Tracking</p> 
<p>Zhipeng Zhang, Houwen Peng</p> 
<p><a href="https://github.com/researchmm/SiamDW">https://github.com/researchmm/SiamDW</a></p> 
<p> </p> 
<p><strong>一种在siamese网络下训练GCNS的视觉追踪方法，实现了存在遮挡、突然运动、背景杂波情景下的鲁棒视觉追踪</strong></p> 
<p>中科院、中科院大学、鹏城实验室、中国科技大学</p> 
<p>Graph Convolutional Tracking</p> 
<p>Junyu Gao, Tianzhu Zhang, Changsheng Xu</p> 
<p><a href="http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html" rel="nofollow">http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html</a>#</p> 
<p> </p> 
<p><strong>通过预测target和estimated bounding box的overlap来实现目标估计。本文提出的ATOM跟踪模型在5个benchmark数据集上实现了state-of-the-art性能；在TrackingNet数据集上，相对于之前的最佳方法提升了15%，同时运行速度超过30 FPS。</strong></p> 
<p>CVL, Linko ̈ping University, Sweden、CVL, ETH Zu ̈rich, Switzerland、起源人工智能研究院</p> 
<p>ATOM: Accurate Tracking by Overlap Maximization</p> 
<p>Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg</p> 
<p><a href="https://github.com/visionml/pytracking">https://github.com/visionml/pytracking</a> （378颗星！）</p> 
<p> </p> 
<p><strong>基于自适应空间加权相关滤波的视觉跟踪</strong></p> 
<p>大连理工大学、鹏城实验室、腾讯优图</p> 
<p>Visual Tracking via Adaptive Spatially-Regularized Correlation Filters</p> 
<p>Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, Jianhua Li</p> 
<p><a href="https://github.com/Daikenan/ASRCF">https://github.com/Daikenan/ASRCF</a></p> 
<p> </p> 
<p><strong>基于兴趣区域的池化相关滤波方法的视觉跟踪</strong></p> 
<p>大连理工大学、腾讯优图、海军航空兵学院、鹏城实验室</p> 
<p>ROI Pooled Correlation Filters for Visual Tracking</p> 
<p>Yuxuan Sun, Chong Sun, Dong Wang, You He, Huchuan Lu</p> 
<p> </p> 
<p><strong>孪生级联候选区域网络，用于实时目标跟踪</strong></p> 
<p>美国天普大学</p> 
<p>Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</p> 
<p>Heng Fan, Haibin Ling</p> 
<p> </p> 
<h2><strong>多目标跟踪</strong></h2> 
<h3><strong>深度学习模型</strong></h3> 
<p>近两年，深度学习算法开始在MOT领域发展，一般分为这么几类：</p> 
<ul><li>以Re-ID为主的表观特征提取网络，如《Aggregate Tracklet Appearance Features for Multi-Object Tracking》；</li><li>基于单目标跟踪领域中成熟的Siam类框架构建的多目标跟踪框架，如《Multi-object tracking with multiple cues and switcher-aware classification》；</li><li>联合目标检测框架和单目标跟踪框架的多任务框架，如《Detect to track and track to detect》；</li><li>端到端的数据关联类算法，如《DeepMOT: A Differentiable Framework for Training Multiple Object Trackers》；</li><li>联合运动、表观和数据观联的集成框架，如《FAMNet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking》；</li><li>基于LSTM类算法实现的运动估计、表观特征选择和融合等等算法。</li></ul> 
<h3><strong>MOTChallenge评价体系</strong></h3> 
<p>由于MOTChallenge是最主流的MOT数据集，所以我这里就以它为例进行介绍。其包含有MOT15~17三个数据集，其中MOT15提供了3D的坐标信息，包含5500帧训练集和5783帧测试集，提供了基于ACF检测器的观测结果。而MOT16和MOT17则包含5316帧训练集和5919帧测试集，其中MOT16仅提供了基于DPM检测器的观测，而MOT17则提供了SDP、FasterRcnn、DPM三种检测结果。</p> 
<p>MOT提供的目标检测结果标注格式为： frame_id 、 target_id、 bb_left 、bb_top、 bb_width、bb_height 、confidence 、x 、y 、z 。即视频帧序号、目标编号（由于暂时未定，所以均为-1）、目标框左上角坐标和宽高、检测置信度（不一定是0~1）、三维坐标（2D数据集中默认为-1）.</p> 
<p>那么我们所需要提供的跟踪结果格式也是同上面一致的，不过需要我们填写对应的target_id和对应的目标框信息，而confidence,x,y,z均任意，保持默认即可。</p> 
<p>相应地，官方所采用的跟踪groudtruth格式则为： frame_id、 target_id 、bb_left、bb_top、bb_width bb_height 、is_active、label_id、visibility_ratio 。其中is_active代表此目标是否考虑，label_id表示该目标所属类别，visibility_ratio表示目标的可视程度，目标类别分类如下：Pedestrian-1 Static Person-7 Person on vehicle-2 Distractor-8 Car-3 Occluder-9 Bicycle-4 Occluder on the ground-10 Motorbike-5 Occluder full-11 Non motorized vehicle-6 Reflection-12</p> 
<p>最后，数据集还提供了各个视频的视频信息seqinfo.ini，主要包括视频名称、视频集路径、帧率fps、视频长度、图像宽高、图像格式等。</p> 
<p>根据MOT官方工具箱中的评价工具，可分析如下的评价规则：</p> 
<p><strong>Step1</strong> <strong>数据清洗</strong></p> 
<p>对于跟踪结果进行简单的格式转换，这个主要是方便计算，意义不大，其中根据官方提供的跟踪groundtruth，<strong>只保留is_active = 1的目标</strong>（根据观察，只考虑了类别为1，即处于运动状态的无遮挡的行人）。另外将groudtruth中完全没有跟踪结果的目标清除，并保持groudtruth中的视频帧序号与视频帧数一一对应。为了统一跟踪结果和groudtruth的目标ID，首先建立目标的映射表，即将跟踪结果中离散的目标ID按照从1开始的数字ID替代。</p> 
<p><strong>Step2</strong> <strong>数据匹配</strong></p> 
<p>将跟踪结果和groundtruth中同属一帧的目标取出来，并计算两两之间的IOU，并将其转换为cost矩阵（可理解为距离矩阵，假定Thresh=0.5）。利用cost矩阵，通过匈牙利算法建立匹配矩阵，从而将跟踪结果中的目标和groundtruth中的目标一一对应起来。</p> 
<p><strong>Step3</strong> <strong>数据分析</strong></p> 
<p>对视频每一帧进行分析，利用每一帧中的跟踪目标和groudtruth目标之间的匹配关系，可作出以下几个设定：</p> 
<ul><li>对于当前帧检测到但未匹配的目标轨迹记作falsepositive；</li><li>对于当前帧groudtruth中未匹配的目标轨迹记作missed；</li><li>对于groudtruth中的某一目标，如果与之匹配的跟踪目标ID前后不一致，则记作IDswitch；</li><li>对于已匹配的轨迹记作covered，总轨迹为gt。</li></ul> 
<p>其中，对于匹配和未匹配到的目标都有各自的评价依据，评价指标很多，这里就不细讲了，网上都有。</p> 
<p> </p> 
<p><strong>提出一种新的训练模式，用于改进多目标跟踪算法中目标交叠时身份切换问题</strong></p> 
<p>洛桑联邦理工学院（EPFL）</p> 
<p>Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</p> 
<p>Andrii Maksai, Pascal Fua</p> 
<p> </p> 
<p><strong>多目标跟踪与分割，提出问题、构建了数据集、建造了一个基线模型，并全部开源了</strong></p> 
<p>亚琛工业大学、MPI for Intelligent Systems and University of Tubingen</p> 
<p>MOTS: Multi-Object Tracking and Segmentation</p> 
<p>Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe</p> 
<p><a href="http://www.vision.rwth-aachen.de/page/mots" rel="nofollow">http://www.vision.rwth-aachen.de/page/mots</a></p> 
<p> </p> 
<h3>Top 算法（2020.04.15更新）</h3> 
<p>Tracking Objects as Points | <u><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/2004.01177" rel="nofollow">[pdf]</a><a href="https://link.zhihu.com/?target=https%3A//github.com/xingyizhou/CenterTrack" rel="nofollow">[code]</a> | </u>arXiv(2019) | CenterTrack</p> 
<p>Refinements in Motion and Appearance for Online Multi-Object Tracking| <u><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.07177" rel="nofollow">[pdf]</a><a href="https://link.zhihu.com/?target=https%3A//github.com/nightmaredimple/libmot" rel="nofollow">[code]</a> |</u>arXiv(2019) | MIFT</p> 
<p>Multiple Object Tracking by Flowing and Fusing |<u><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2001.11180" rel="nofollow">[pdf]</a> |</u>arXiv(2019) |FFT</p> 
<p>A Unified Object Motion and Affinity Model for Online Multi-Object Tracking |<u><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2003.11291" rel="nofollow">[pdf]</a><a href="https://link.zhihu.com/?target=https%3A//github.com/yinjunbo/UMA-MOT" rel="nofollow">[code]</a>|</u>CVPR2020 |UMA</p> 
<p><strong>Towards Real-Time Multi-Object Tracking | <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.12605v1.pdf" rel="nofollow">[pdf]</a></strong><strong><a href="https://link.zhihu.com/?target=https%3A//github.com/Zhongdao/Towards-Realtime-MOT" rel="nofollow">[code]</a> | arXiv(2019) | JDE(private)</strong></p> 
<p><strong>A Simple Baseline for Multi-Object Tracking | 【<a href="https://arxiv.org/pdf/2004.01888.pdf" rel="nofollow">pdf</a>】| [<a href="https://github.com/ifzhang/FairMOT">code</a>] arXiv(2020) | FairMOT(public&amp;private)</strong></p> 
<p> </p> 
<h3><strong>跟踪与分割</strong></h3> 
<p> </p> 
<p><strong>SiamMask，在视频跟踪任务上达到最优性能，并且在视频目标分割上取得了当前最快的速度。</strong></p> 
<p>Fast Online Object Tracking and Segmentation: A Unifying Approach</p> 
<p>Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr</p> 
<p><a href="https://github.com/foolwood/SiamMask">https://github.com/foolwood/SiamMask</a></p> 
<p> </p> 
<p><strong>多目标跟踪与分割，提出问题、构建了数据集、建造了一个基线模型，并全部开源了</strong></p> 
<p>亚琛工业大学、MPI for Intelligent Systems and University of Tubingen</p> 
<p>MOTS: Multi-Object Tracking and Segmentation</p> 
<p>Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe</p> 
<p><a href="http://www.vision.rwth-aachen.de/page/mots" rel="nofollow">http://www.vision.rwth-aachen.de/page/mots</a></p> 
<p> </p> 
<h3><strong>3D目标跟踪</strong></h3> 
<p> </p> 
<p><strong>形状补全用于LIDAR点云Siamese网络三维目标跟踪</strong></p> 
<p>沙特阿卜杜拉国王科技大学</p> 
<p>Leveraging Shape Completion for 3D Siamese Tracking</p> 
<p>Silvio Giancola, Jesus Zarzar, Bernard Ghanem</p> 
<p><a href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking">https://github.com/SilvioGiancola/ShapeCompletion3DTracking</a></p> 
<p> </p> 
<h3><strong>跟踪数据集</strong></h3> 
<p> </p> 
<p><strong>大规模高质量单目标跟踪数据集LaSOT</strong></p> 
<p>美国天普大学、华南理工大学、亮风台公司</p> 
<p>LaSOT: A High-Quality Benchmark for Large-Scale Single Object Tracking</p> 
<p>Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling</p> 
<p><a href="https://cis.temple.edu/lasot/index.html" rel="nofollow">https://cis.temple.edu/lasot/index.html</a></p> 
<p> </p> 
<p><strong>用于支持研究自动驾驶汽车感知任务（3D 跟踪与运动预测）的数据集Argoverse</strong></p> 
<p>Argo AI、卡内基梅隆大学、佐治亚理工学院</p> 
<p>Argoverse: 3D Tracking and Forecasting With Rich Maps</p> 
<p>Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays</p> 
<p><a href="https://www.argoverse.org/" rel="nofollow">https://www.argoverse.org/</a></p> 
<p> </p> 
<p><strong>英伟达推出首个跨摄像头汽车跟踪与冲重识别数据集</strong></p> 
<p>华盛顿大学、NIVIDIA、圣何塞州立大学</p> 
<p>CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification</p> 
<p>Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang</p> 
<p><a href="https://www.aicitychallenge.org/" rel="nofollow">https://www.aicitychallenge.org/</a></p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p>更多参见：</p> 
<p>1.<a href="https://blog.csdn.net/weixin_40245131/article/details/79754531">https://blog.csdn.net/weixin_40245131/article/details/79754531</a></p> 
<p>2.<a href="https://zhuanlan.zhihu.com/p/46669238" rel="nofollow">VOT2018：SiamNet大崛起</a></p> 
<p>3.CVPR 2019 <a href="http://bbs.cvmart.net/articles/523" rel="nofollow">论文大盘点</a>-目标跟踪篇 </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/39ddb75f43383007f12544a60ebfd291/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">颠倒给定的 32 位无符号整数的二进制位。（python）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bcf42db1b16ff963b769fae434e45965/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android窗口动画和过渡动画（activity和dialog）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>