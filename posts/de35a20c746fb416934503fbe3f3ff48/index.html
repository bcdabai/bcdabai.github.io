<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>卷积和全连接层的模型参数计算详解，详细到神经元个数一个个算，天啊，以VGG16为例 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="卷积和全连接层的模型参数计算详解，详细到神经元个数一个个算，天啊，以VGG16为例" />
<meta property="og:description" content="卷积和全连接层的模型参数计算详解，以VGG16为例 神经网络参数计算基础（卷积、全连接层）卷积核参数量计算单个卷积核大小卷积层卷积核大小的计算 全连接层参数计算全连接层概念全连接层参数计算 VGG16参数计算模型架构计算参数量第一部分卷积层其他卷积层参数量计算全连接层输入说明全连接层计算参数量其他部分参数量 验证 神经网络参数计算基础（卷积、全连接层） 卷积核参数量计算 单个卷积核大小 一个卷积核的大小是 kernel = channel(input) × kernel_width × kernel_height，如下图卷积过程，一个卷积核能卷积得到一个通道的特征图。
其中，channel(input) 就是输入的通道数，kernel_width，kernel_height 分别表示一个卷积核的宽和高。通过一个这么大的卷积核对一层多通道的特征图层进行卷积我可以获得一个（1，W，H）的特征图层。
如果需要输出 N 个通道的特征图层，我们就需要 N 个这样的卷积核进行卷积计算，即可获取（N，W，H）的特征图层。这里的 N 个卷积核在建立网络的时候都是系统自动初始化的，如随机初始化，全部为0初始化，高斯分布初始化等，看你的库是用什么方法初始化。
卷积层卷积核大小的计算 由上面的说明，我们可以知道，一个卷积层的卷积核参数量计算其实就受输入通道、输出通道、卷积核大小的影响，而实际卷积过程中我们还会给每个卷积核都加个偏置，因此公式如下：
卷积核参数量
= kernel.shape × kernel.num &#43; bias.num
= ( channel(input) × kernel_width × kernel_height ) × channel(output) &#43; channel(output)
其中，channel(input) 是输入卷积层的通道数，channel(output)是输出卷积层的通道数。
也就是说，如果输入的特征图层的 shape 是（3，224，224），需要 3 × 3 卷积之后****输出的特征图层的 shape 是（64，224，224）。我们就可以知道输入通道数为 3，输出通道数为64，kernel_width 为 3，kernel_height 为 3，因此，我们可以计算这个 3 × 3 卷积层的卷积核参数量大小为
参数量 = 3 × 3 × 3 × 64 &#43; 64 = 1792。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/de35a20c746fb416934503fbe3f3ff48/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-29T10:22:41+08:00" />
<meta property="article:modified_time" content="2022-10-29T10:22:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">卷积和全连接层的模型参数计算详解，详细到神经元个数一个个算，天啊，以VGG16为例</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>卷积和全连接层的模型参数计算详解，以VGG16为例</h4> 
 <ul><li><a href="#_1" rel="nofollow">神经网络参数计算基础（卷积、全连接层）</a></li><li><ul><li><a href="#_2" rel="nofollow">卷积核参数量计算</a></li><li><ul><li><a href="#_3" rel="nofollow">单个卷积核大小</a></li><li><a href="#_9" rel="nofollow">卷积层卷积核大小的计算</a></li></ul> 
   </li><li><a href="#_18" rel="nofollow">全连接层参数计算</a></li><li><ul><li><a href="#_19" rel="nofollow">全连接层概念</a></li><li><a href="#_25" rel="nofollow">全连接层参数计算</a></li></ul> 
  </li></ul> 
  </li><li><a href="#VGG16_32" rel="nofollow">VGG16参数计算</a></li><li><ul><li><a href="#_33" rel="nofollow">模型架构</a></li><li><a href="#_36" rel="nofollow">计算参数量</a></li><li><ul><li><a href="#_37" rel="nofollow">第一部分卷积层</a></li><li><a href="#_49" rel="nofollow">其他卷积层参数量计算</a></li><li><a href="#_64" rel="nofollow">全连接层输入说明</a></li><li><a href="#_69" rel="nofollow">全连接层计算参数量</a></li><li><a href="#_76" rel="nofollow">其他部分参数量</a></li></ul> 
   </li><li><a href="#_80" rel="nofollow">验证</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>神经网络参数计算基础（卷积、全连接层）</h2> 
<h3><a id="_2"></a>卷积核参数量计算</h3> 
<h4><a id="_3"></a>单个卷积核大小</h4> 
<p><strong>一个卷积核的大小是 kernel = channel(input) × kernel_width × kernel_height，如下图卷积过程，一个卷积核能卷积得到一个通道的特征图。</strong><br> <img src="https://images2.imgbox.com/1e/9d/MRIk92NZ_o.png" alt="在这里插入图片描述"></p> 
<p>其中，channel(input) 就是输入的通道数，kernel_width，kernel_height 分别表示一个卷积核的宽和高。通过一个这么大的卷积核对一层多通道的特征图层进行卷积我可以获得一个（1，W，H）的特征图层。<br> <strong>如果需要输出 N 个通道的特征图层，我们就需要 N 个这样的卷积核进行卷积计算</strong>，即可获取（N，W，H）的特征图层。这里的 N 个卷积核在建立网络的时候都是系统自动初始化的，如随机初始化，全部为0初始化，高斯分布初始化等，看你的库是用什么方法初始化。</p> 
<h4><a id="_9"></a>卷积层卷积核大小的计算</h4> 
<p>由上面的说明，我们可以知道，一个卷积层的卷积核参数量计算其实就受<strong>输入通道、输出通道、卷积核大小</strong>的影响，而实际卷积过程中我们<strong>还会给每个卷积核都加个偏置</strong>，因此公式如下：<br> <strong>卷积核参数量<br> = kernel.shape × kernel.num + bias.num<br> = ( channel(input) × kernel_width × kernel_height ) × channel(output) + channel(output)</strong></p> 
<p>其中，channel(input) 是输入卷积层的通道数，channel(output)是输出卷积层的通道数。<br> 也就是说，<strong>如果输入的特征图层的 shape 是（3，224，224）</strong>，<strong>需要 3 × 3 卷积之后****输出的特征图层的 shape 是（64，224，224）</strong>。我们就可以知道输入通道数为 3，输出通道数为64，kernel_width 为 3，kernel_height 为 3，因此，我们可以计算这个 3 × 3 卷积层的卷积核参数量大小为<br> <strong>参数量 = 3 × 3 × 3 × 64 + 64 = 1792。</strong></p> 
<h3><a id="_18"></a>全连接层参数计算</h3> 
<h4><a id="_19"></a>全连接层概念</h4> 
<p>全连接层其实就是一个权重矩阵，计算过程其实就是通过一个矩阵，将一个向量转换成另外一个维度的向量。公式如下：<br> <strong>Vector(output) = Vector(input) × Weight(FC) + Bias(FC)</strong><br> 其中，Weight(FC) 表示全连接层的权重，Bias(FC)表示全连接层的偏置。<br> 也就是说，如果输入全连接层向量的 shape 是（1，4），输出全连接层向量的 shape 是（1，3），显然学过线性代数的同学就知道了，这个全连接层权重的 shape 是（4，3），偏置的shape是（1，3）。<strong>如输入一个（2，2，2，2）的四维向量，需要输出一个三维向量</strong>，当权重矩阵的数值全为1，偏置矩阵的数值全为3时，计算过程如下：<br> <img src="https://images2.imgbox.com/69/3f/H6xi3NpS_o.png" alt="全连接层计算过程"></p> 
<h4><a id="_25"></a>全连接层参数计算</h4> 
<p>全连接层的参数量，其实就是参数权重和偏置的大小。因此全连接层的参数量就有公式：<br> <strong>全连接层参数量<br> = Weight(FC).height × Weight(FC).width + bias(FC).width<br> = Vector(input).shape × Vector(output).shape + Vector(output).shape</strong><br> 因此，如上面的例子，<strong>输入是1×4的向量，输出是1×3的向量，全连接层的参数量 = 4 × 3 + 3 = 15。</strong></p> 
<h2><a id="VGG16_32"></a>VGG16参数计算</h2> 
<h3><a id="_33"></a>模型架构</h3> 
<p>VGG16就是下图红框部分，我加上了在这里面的特征图层的大小。因此VGG16输入的大小是（224，224，3），最后一层的输出是（1000）。<br> <img src="https://images2.imgbox.com/29/6c/hdP6pAGN_o.png" alt="VGG16"></p> 
<h3><a id="_36"></a>计算参数量</h3> 
<h4><a id="_37"></a>第一部分卷积层</h4> 
<p><img src="https://images2.imgbox.com/7b/eb/zmqw037B_o.png" alt="VGG16"><br> 第一部分就是蓝色框的位置，其中有两个3*3的卷积层：</p> 
<ol><li>第一个卷积层把输入（224，224，3）的图片转换成（224，224，64）的特征图层；</li><li>第二个卷积层把（224，224，64）的特征图层转换成（224，224，64）的特征图层；</li></ol> 
<p><strong>num11表示第 1 部分的第 1 个卷积核，num12表示第 1 部分的第 2 个卷积核</strong>，后面以此类推。<br> 根据上述公式我们可以知道第一层卷积的参数量为<br> num11 = 3×3×3×64 + 64 = 1792<br> 第二层卷积的参数量为<br> num12 = 64×3×3×64 + 64 = 36928</p> 
<h4><a id="_49"></a>其他卷积层参数量计算</h4> 
<p><img src="https://images2.imgbox.com/e0/c8/mrPRFUKw_o.png" alt="其他卷积层计算"><br> num21 = 64 × 128 × 3 × 3 + 128 = 73856<br> num22 = 128 × 128 × 3 × 3 + 128 = 147,584<br> num31 = 128 × 256 × 3 × 3 + 256 = 295,168<br> num32 = 256 × 256 × 3 × 3 + 256 = 590,080<br> num33 = 256 × 256 × 3 × 3 + 256 = 590,080<br> num41 = 256 × 512 × 3 × 3 + 512 = 1,180,160<br> num42 = 512 × 512 × 3 × 3 + 512 = 2,359,808<br> num43 = 512 × 512 × 3 × 3 + 512 = 2,359,808<br> num51 = 512 × 512 × 3 × 3 + 512 = 2,359,808<br> num52 = 512 × 512 × 3 × 3 + 512 = 2,359,808<br> num53 = 512 × 512 × 3 × 3 + 512 = 2,359,808<br> 即所有卷积层的参数总量为<br> num_conv_total = 14,714,688</p> 
<h4><a id="_64"></a>全连接层输入说明</h4> 
<p>全连接层是整个模型中参数量占比较多的一部分，我们计算蓝框这一部分的全连接层。<br> <img src="https://images2.imgbox.com/41/96/WUdd6BWY_o.png" alt="全连接层计算"><br> 首先我们看全连接层上面的maxpool，通常这种<strong>maxpool操作后直接展平为一维向量输入进全连接层</strong>（展平：其实就是将（m，n）的向量变成（1，m×n）的向量）。<br> 因此，我们需要知道上一层的特征图层大小是多少，根据推算，maxpooling后的特征图大小是 7 × 7 × 512，即可直接展平成（1，7×7×512）大小的向量，即（1，25088）的向量。</p> 
<h4><a id="_69"></a>全连接层计算参数量</h4> 
<p>根据上述全连接层的参数量计算公式，可知各个全连接层的参数量是：<br> num_fc1 = 25088 × 4096 + 4096 = 102,764,544<br> num_fc2 = 4096 × 4096 + 4096 = 16,781,312<br> num_fc3 = 4096 × 1000 + 1000 = 4,097,000<br> 即所有的全连接层参数量为<br> num_fc_total = 123,642,856</p> 
<h4><a id="_76"></a>其他部分参数量</h4> 
<p>其他部分如maxpool，softmax等，我记得是没有参数，所以为0。<br> <strong>VGG16总体参数量为 num_total = num_conv_total + num_fc_total = 138,357,544</strong><br> 即VGG16的参数计算就算完啦。</p> 
<h3><a id="_80"></a>验证</h3> 
<p><strong>我用torch调用VGG16看下模型大小的时候发现，跟计算出来的结果相差了很多。</strong><br> <img src="https://images2.imgbox.com/61/28/gTVPyFxV_o.png" alt="pt模型保存结果"><br> 然后就去看了下他的源码，看是不是模型设置跟论文的不一样，发现除了最后的maxpooling换成了平均池化，加了dropout层，其他没什么大的变化。<br> <img src="https://images2.imgbox.com/e4/41/6QMqm71r_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/cf/0a/ThF18M54_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ff/04/FVvxfroW_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/92/b1/cKSLYoVh_o.png" alt="卷积部分"><br> 后面发现是自己傻逼了，我们的计算参数量是138,357,544，<strong>也就是138M，保存成文件，需要用4bytes的float类型来存储</strong>，<br> <strong>所以保存后的模型大小是138357544 × 4 = 553,430,176，就是553M，验证完毕</strong>。<br> 而实际保存的参数量为540M。<br> 个人想法是由于dropout层去除了一些参数量，所以容量减小了，以上。</p> 
<p>个人学习，如有不正，请大佬指正，欢迎交流。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3943771bcfda46e9eac45669ce274903/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">NewStarCTF2022-Week4-Web</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8201cebbc9f3bb25489f678da554245a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">oracle中替换字符串的不同写法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>