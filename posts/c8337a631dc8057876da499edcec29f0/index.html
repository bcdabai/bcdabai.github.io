<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大模型训练踩坑记录 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大模型训练踩坑记录" />
<meta property="og:description" content="按照博客进行模型的微调，遇到的bug记录。
【1】执行
python train_qlora.py --train_args_file train_args/sft/qlora/chatglm3-6b-sft-qlora.json
报错：
libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11
解决方法，详见：这里
中间重新安装pytorch都不行，上面写的 pip uninstall nvidia_cublas_cu11 可以work。
【2】个人使用设备双卡T4，每个卡16G，2张卡32G，qlora方法微调chatglm3-6b。
执行
python train_qlora.py --train_args_file train_args/sft/qlora/chatglm3-6b-sft-qlora.json
之后，跑起来之后OOM，报错如下：
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB (GPU 0; 14.76 GiB total capacity; 13.16 GiB already allocated; 315.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/c8337a631dc8057876da499edcec29f0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-11T10:10:37+08:00" />
<meta property="article:modified_time" content="2024-01-11T10:10:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大模型训练踩坑记录</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>按照<a class="link-info" href="https://zhuanlan.zhihu.com/p/643950663" rel="nofollow" title="博客">博客</a>进行模型的微调，遇到的bug记录。</p> 
<p>【1】执行</p> 
<blockquote> 
 <p>python train_qlora.py --train_args_file train_args/sft/qlora/chatglm3-6b-sft-qlora.json</p> 
</blockquote> 
<p>报错：</p> 
<blockquote> 
 <p>libcublas.so.11: undefined symbol: cublasLtGetStatusString, version libcublasLt.so.11</p> 
</blockquote> 
<p><strong><span style="color:#fe2c24;">解决方法</span></strong>，详见：<a class="link-info" href="https://blog.csdn.net/bcfd_yundou/article/details/129206267" title="这里">这里</a></p> 
<p>中间重新安装pytorch都不行，上面写的 pip uninstall nvidia_cublas_cu11 可以work。</p> 
<p></p> 
<p>【2】个人使用设备双卡T4，每个卡16G，2张卡32G，qlora方法微调chatglm3-6b。</p> 
<p>执行</p> 
<blockquote> 
 <p>python train_qlora.py --train_args_file train_args/sft/qlora/chatglm3-6b-sft-qlora.json</p> 
</blockquote> 
<p>之后，跑起来之后OOM，报错如下：</p> 
<blockquote> 
 <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.98 GiB (GPU 0; 14.76 GiB total capacity; 13.16 GiB already allocated; 315.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p> 
</blockquote> 
<p>但实际上第2张卡显存空间还剩很多：</p> 
<p><img alt="" height="796" src="https://images2.imgbox.com/78/84/S7RzM0iD_o.png" width="1152"></p> 
<p><strong><span style="color:#fe2c24;">解决方法</span></strong>：查了一些资料，在stackoverflow上看到有人训练别的模型说还是batchsize设置的大了，可以减小一下试试。当我将batch_size=1时，发现OK了。</p> 
<p>其他的诸如</p> 
<blockquote> 
 <p>os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:xxx"</p> 
</blockquote> 
<p>都不work。按道理来说应该当第一张卡不足，使用第二张卡上，但目前不知道原因为什么没弄好，有经验的朋友可以不吝赐教。</p> 
<p>补充：</p> 
<p>可以使用命令：</p> 
<blockquote> 
 <p>torchrun --nproc_per_node=2 train_qlora.py --train_args_file train_args/sft/qlora/chatglm3-6b-sft-qlora.json</p> 
</blockquote> 
<p>这里通过设置--nproc_per_node=2来指定训练的显卡数。global batch = per_device_train_batch_size * gradient_accumulation_steps * num_gpus。</p> 
<p>这样时间能减少一半。</p> 
<p></p> 
<p>【3】使用lora方法微调后合并的模型推理时报错：</p> 
<blockquote> 
 <p>AttributeError: can't set attribute 'eos_token'</p> 
</blockquote> 
<p><strong><span style="color:#fe2c24;">解决方法</span></strong>：将原来模型的tokenizer_config.json移动到微调模型的下面，详细见<a class="link-info" href="https://github.com/hiyouga/LLaMA-Factory/issues/1307" title="GitHub的issue">GitHub的issue</a></p> 
<p></p> 
<p>【4】大模型训练好之后的效果（中文场景）跟哪些因素有关</p> 
<p>这一部分还没做实验，但参考北大做的ChatLaw模型（由几个模型的组合，算个解决方案或者系统）:<a class="link-info" href="https://huggingface.co/FarReelAILab/ChatLaw-13B" rel="nofollow" title="参考简介里的说明">参考简介里的说明</a></p> 
<p>a. 大模型本身的文本能力，所以一定选个基座就比较好的，我们做微调的，尤其是训练数据不够，训练机器不行的，一定要选个好的基座；</p> 
<p>b. 数据质量：多清洗，多增强，要求严格，少量的精品胜过大量的普通数据；</p> 
<p>c. 大参数：一般来说33B好于13B，以此类推；</p> 
<p>d. 实际落地：真正能解决业务上的问题，可能单靠一个大模型是不够的，需要配合，知识库是必须的，配合做知识库的模型，比如相似度，embedding模型等都要搞好，这对于落地很重要！</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5647093dcec95eaef204f3f7ff44fbec/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Curl命令POST请求</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/cf7d00d68ad0bcf104ca0edf4007bfa9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Flowable工作流入门</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>