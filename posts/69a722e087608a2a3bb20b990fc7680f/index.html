<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>探索MediaPipe检测人脸关键点 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="探索MediaPipe检测人脸关键点" />
<meta property="og:description" content="MediaPipe是Google开源的计算机视觉处理框架，基于TensorFlow来训练模型，支持人脸识别、人脸关键点、目标检测追踪、图像分类、人像分割、手势识别、文本分类、语音分类等。我们可以使用CPU来推理，也可以选择GPU加速推理。在滤镜特效场景，经常需要用到人脸关键点。
目录
一、配置参数与模型
1、配置参数
2、检测模型 二、工程配置
三、初始化工作
1、初始化模型
2、初始化Camera
四、检测实时流
1、检测人脸关键点
2、绘制人脸关键点
五、检测结果​​​​​​​
一、配置参数与模型 1、配置参数 检测人脸关键点的配置参数有运行模式、人脸数、最小的检测人脸置信度、最小的显示人脸置信度、最小的追踪人脸置信度、结果回调，具体如下表所示：
选项描述取值范围默认值running_mode IMAGE: 单个图像 VIDEO: 视频帧
LIVE_STREAM: 实时流
{IMAGE,VIDEO,
LIVE_STREAM}
IMAGEnum_faces最多检测的人脸数大于01 min_face_detection
_confidence
人脸检测最小置信度[0.0, 1.0]0.5 min_face_presence
_confidence
人脸显示最小置信度[0.0, 1.0]0.5min_tracking_confidence人脸追踪最小置信度[0.0, 1.0]0.5output_face_blendshapes是否输出混合形状(用于3D人脸模型)Booleanfalse output_facial_transformation
_matrixes
是否输出变换矩阵(用于滤镜特效)Booleanfalseresult_callback异步回调结果(LIVE_STREAM模式)ResultListener / 2、检测模型 检测人脸关键点分为三步：首先检测人脸，然后定位关键点，最后识别面部特征。使用到的模型如下：
人脸检测模型：根据人脸关键点特征来检测人脸；人脸网格模型：包含478个坐标点的3D人脸标识；混合形状模型：预测52个混合形状的分数，表示不同表情的系数； 二、工程配置 以Android平台为例，在gradle导入MediaPipe相关包：
implementation &#39;com.google.mediapipe:tasks-vision:0.10.0&#39; 然后运行下载模型的task，并且指定模型保存路径：
project.ext.ASSET_DIR = projectDir.toString() &#43; &#39;/src/main/assets&#39; apply from: &#39;download_tasks.gradle&#39; 这里用到的模型是face_landmarker，设置src和dest：
task downloadTaskFile(type: Download) { src &#39;https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task&#39; dest project.ext.ASSET_DIR &#43; &#39;/face_landmarker.task&#39; overwrite false } preBuild." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/69a722e087608a2a3bb20b990fc7680f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-01T15:03:34+08:00" />
<meta property="article:modified_time" content="2023-07-01T15:03:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">探索MediaPipe检测人脸关键点</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>MediaPipe是Google开源的计算机视觉处理框架，基于TensorFlow来训练模型，支持人脸识别、人脸关键点、目标检测追踪、图像分类、人像分割、手势识别、文本分类、语音分类等。我们可以使用CPU来推理，也可以选择GPU加速推理。在滤镜特效场景，经常需要用到人脸关键点。</p> 
<p> </p> 
<p></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B" rel="nofollow">一、配置参数与模型</a></p> 
<p id="1%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0" rel="nofollow">1、配置参数</a></p> 
<p id="2%E3%80%81%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%C2%A0-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%C2%A0" rel="nofollow">2、检测模型 </a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%B7%A5%E7%A8%8B%E9%85%8D%E7%BD%AE-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%B7%A5%E7%A8%8B%E9%85%8D%E7%BD%AE" rel="nofollow">二、工程配置</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B7%A5%E4%BD%9C-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B7%A5%E4%BD%9C" rel="nofollow">三、初始化工作</a></p> 
<p id="1%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B" rel="nofollow">1、初始化模型</a></p> 
<p id="2%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96Camera-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96Camera" rel="nofollow">2、初始化Camera</a></p> 
<p id="%E5%9B%9B%E3%80%81%E6%A3%80%E6%B5%8B%E5%AE%9E%E6%97%B6%E6%B5%81-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81%E6%A3%80%E6%B5%8B%E5%AE%9E%E6%97%B6%E6%B5%81" rel="nofollow">四、检测实时流</a></p> 
<p id="1%E3%80%81%E6%A3%80%E6%B5%8B%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9-toc" style="margin-left:80px;"><a href="#1%E3%80%81%E6%A3%80%E6%B5%8B%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9" rel="nofollow">1、检测人脸关键点</a></p> 
<p id="2%E3%80%81%E7%BB%98%E5%88%B6%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9-toc" style="margin-left:80px;"><a href="#2%E3%80%81%E7%BB%98%E5%88%B6%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9" rel="nofollow">2、绘制人脸关键点</a></p> 
<p id="%E4%BA%94%E3%80%81%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C" rel="nofollow">五、检测结果</a>​​​​​​​</p> 
<h3 id="%E4%B8%80%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%B8%8E%E6%A8%A1%E5%9E%8B">一、配置参数与模型</h3> 
<h4 id="1%E3%80%81%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0">1、配置参数</h4> 
<p>检测人脸关键点的配置参数有运行模式、人脸数、最小的检测人脸置信度、最小的显示人脸置信度、最小的追踪人脸置信度、结果回调，具体如下表所示：</p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:596px;"><tbody><tr><td style="width:200px;">选项</td><td style="width:177px;">描述</td><td style="width:114px;">取值范围</td><td style="width:107px;">默认值</td></tr><tr><td style="width:200px;">running_mode</td><td style="width:177px;"> <p>IMAGE: 单个图像 </p> <p>VIDEO: 视频帧</p> <p>LIVE_STREAM: 实时流</p> </td><td style="width:114px;"> <p>{IMAGE,VIDEO,</p> <p>LIVE_STREAM}</p> </td><td style="width:107px;">IMAGE</td></tr><tr><td style="width:200px;">num_faces</td><td style="width:177px;">最多检测的人脸数</td><td style="width:114px;">大于0</td><td style="width:107px;">1</td></tr><tr><td style="width:200px;"> <p>min_face_detection</p> <p>_confidence</p> </td><td style="width:177px;">人脸检测最小置信度</td><td style="width:114px;">[0.0, 1.0]</td><td style="width:107px;">0.5</td></tr><tr><td style="width:200px;"> <p>min_face_presence</p> <p>_confidence</p> </td><td style="width:177px;">人脸显示最小置信度</td><td style="width:114px;">[0.0, 1.0]</td><td style="width:107px;">0.5</td></tr><tr><td style="width:200px;">min_tracking_confidence</td><td style="width:177px;">人脸追踪最小置信度</td><td style="width:114px;">[0.0, 1.0]</td><td style="width:107px;">0.5</td></tr><tr><td style="width:200px;">output_face_blendshapes</td><td style="width:177px;">是否输出混合形状(用于3D人脸模型)</td><td style="width:114px;">Boolean</td><td style="width:107px;">false</td></tr><tr><td style="width:200px;"> <p>output_facial_transformation</p> <p>_matrixes</p> </td><td style="width:177px;">是否输出变换矩阵(用于滤镜特效)</td><td style="width:114px;">Boolean</td><td style="width:107px;">false</td></tr><tr><td style="width:200px;">result_callback</td><td style="width:177px;">异步回调结果(LIVE_STREAM模式)</td><td style="width:114px;">ResultListener</td><td style="width:107px;">   /</td></tr></tbody></table> 
<h4 id="2%E3%80%81%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%C2%A0">2、检测模型 </h4> 
<p>检测人脸关键点分为三步：首先检测人脸，然后定位关键点，最后识别面部特征。使用到的模型如下：</p> 
<ul><li>人脸检测模型：根据人脸关键点特征来检测人脸；</li><li>人脸网格模型：包含478个坐标点的3D人脸标识；</li><li>混合形状模型：预测52个混合形状的分数，表示不同表情的系数； </li></ul> 
<h3 id="%E4%BA%8C%E3%80%81%E5%B7%A5%E7%A8%8B%E9%85%8D%E7%BD%AE">二、工程配置</h3> 
<p>以Android平台为例，在gradle导入MediaPipe相关包：</p> 
<pre><code class="language-Kotlin">implementation 'com.google.mediapipe:tasks-vision:0.10.0'</code></pre> 
<p>然后运行下载模型的task，并且指定模型保存路径：</p> 
<pre><code class="language-Kotlin">project.ext.ASSET_DIR = projectDir.toString() + '/src/main/assets'
apply from: 'download_tasks.gradle'</code></pre> 
<p>这里用到的模型是face_landmarker，设置src和dest：</p> 
<pre><code class="language-Kotlin">task downloadTaskFile(type: Download) {
    src 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task'
    dest project.ext.ASSET_DIR + '/face_landmarker.task'
    overwrite false
}

preBuild.dependsOn downloadTaskFile</code></pre> 
<h3 id="%E4%B8%89%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B7%A5%E4%BD%9C">三、初始化工作</h3> 
<h4 id="1%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B">1、初始化模型</h4> 
<p>模型的初始化包括：设置运行模式、模型路径、检测人脸数、回调结果等，示例代码如下：</p> 
<pre><code class="language-Kotlin">    fun setupFaceLandmark() {
        val baseOptionBuilder = BaseOptions.builder()

        // 设置运行模式，默认CPU
        when (currentDelegate) {
            DELEGATE_CPU -&gt; {
                baseOptionBuilder.setDelegate(Delegate.CPU)
            }
            DELEGATE_GPU -&gt; {
                baseOptionBuilder.setDelegate(Delegate.GPU)
            }
        }
        // 设置模型路径
        baseOptionBuilder.setModelAssetPath(MP_FACE_LANDMARKER_TASK)

        try {
            val baseOptions = baseOptionBuilder.build()
            // 设置检测的人脸数、最小的检测人脸置信度
            val optionsBuilder =
                FaceLandmarker.FaceLandmarkerOptions.builder()
                    .setBaseOptions(baseOptions)
                    .setMinFaceDetectionConfidence(minFaceDetectionConfidence)
                    .setMinTrackingConfidence(minFaceTrackingConfidence)
                    .setMinFacePresenceConfidence(minFacePresenceConfidence)
                    .setNumFaces(maxNumFaces)
                    .setRunningMode(runningMode)

            // LIVE_STREAM模式：设置回调结果
            if (runningMode == RunningMode.LIVE_STREAM) {
                optionsBuilder
                    .setResultListener(this::returnLivestreamResult)
                    .setErrorListener(this::returnLivestreamError)
            }

            val options = optionsBuilder.build()
            faceLandmarker =
                FaceLandmarker.createFromOptions(context, options)
        } catch (e: IllegalStateException) {
            faceLandmarkerHelperListener?.onError(
                "Face Landmark failed to initialize, error: " + e.message)
        } catch (e: RuntimeException) {
            faceLandmarkerHelperListener?.onError(
                "Face Landmark failed to initialize. See error logs for details", GPU_ERROR)
        }
    }</code></pre> 
<h4 id="2%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96Camera">2、初始化Camera</h4> 
<p>这里以LIVE_STREAM模式为例，Camera的初始化包括：设置像素格式、预览宽高比、绑定生命周期、关联SurfaceProvider。示例代码如下：</p> 
<pre><code class="language-Kotlin">    private fun bindCameraUseCases() {
        val cameraProvider = cameraProvider ?: throw IllegalStateException("Camera init failed.")

        val cameraSelector =
            CameraSelector.Builder().requireLensFacing(cameraFacing).build()

        // 预览的宽高比为4:3
        preview = Preview.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)
            .setTargetRotation(fragmentCameraBinding.viewFinder.display.rotation)
            .build()

        // 设置像素格式为RGBA_8888，预览的旋转角度
        imageAnalyzer =
            ImageAnalysis.Builder().setTargetAspectRatio(AspectRatio.RATIO_4_3)
                .setTargetRotation(fragmentCameraBinding.viewFinder.display.rotation)
                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
                .setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)
                .build()
                .also {
                    it.setAnalyzer(backgroundExecutor) { image -&gt;
                        // 执行检测人脸关键点
                        faceLandmarkerHelper.detectLiveStream(image, cameraFacing == CameraSelector.LENS_FACING_FRONT)
                    }
                }

        // 绑定之前，先解除绑定
        cameraProvider.unbindAll()

        try {
            // 绑定Lifecycle
            camera = cameraProvider.bindToLifecycle(
                this, cameraSelector, preview, imageAnalyzer)
            // 关联SurfaceProvider
            preview?.setSurfaceProvider(fragmentCameraBinding.viewFinder.surfaceProvider)
        } catch (exc: Exception) {
            Log.e(TAG, "bind lifecycle failed", exc)
        }
    }</code></pre> 
<h3 id="%E5%9B%9B%E3%80%81%E6%A3%80%E6%B5%8B%E5%AE%9E%E6%97%B6%E6%B5%81">四、检测实时流</h3> 
<h4 id="1%E3%80%81%E6%A3%80%E6%B5%8B%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9">1、检测人脸关键点</h4> 
<p>在检测之前，先拷贝数据，图像帧预处理，然后执行检测：</p> 
<pre><code class="language-Kotlin">    fun detectLiveStream(
        imageProxy: ImageProxy,
        isFrontCamera: Boolean) {
        val frameTime = SystemClock.uptimeMillis()

        // 拷贝RGB数据到缓冲区
        val bitmapBuffer =
            Bitmap.createBitmap(
                imageProxy.width,
                imageProxy.height,
                Bitmap.Config.ARGB_8888
            )
        imageProxy.use { bitmapBuffer.copyPixelsFromBuffer(imageProxy.planes[0].buffer) }
        imageProxy.close()

        val matrix = Matrix().apply {
            // 图像旋转
            postRotate(imageProxy.imageInfo.rotationDegrees.toFloat())

            // 如果是前置摄像头，需要左右镜像
            if (isFrontCamera) {
                postScale(-1f, 1f, imageProxy.width.toFloat(), imageProxy.height.toFloat())
            }
        }
        val rotatedBitmap = Bitmap.createBitmap(
            bitmapBuffer, 0, 0, bitmapBuffer.width, bitmapBuffer.height,
            matrix, true)

        // 转换Bitmap为MPImage
        val mpImage = BitmapImageBuilder(rotatedBitmap).build()
        // 异步检测人脸关键点
        faceLandmarker?.detectAsync(mpImage, frameTime)
    }</code></pre> 
<h4 id="2%E3%80%81%E7%BB%98%E5%88%B6%E4%BA%BA%E8%84%B8%E5%85%B3%E9%94%AE%E7%82%B9">2、绘制人脸关键点</h4> 
<p>检测到人脸关键点结果后，然后回调到主线程：</p> 
<pre><code class="language-Kotlin">   override fun onResults(resultBundle: FaceLandmarkerHelper.ResultBundle) {
        activity?.runOnUiThread {
            if (_fragmentCameraBinding != null) {
                // 显示推理时长
                fragmentCameraBinding.bottomSheetLayout.inferenceTimeVal.text =
                    String.format("%d ms", resultBundle.inferenceTime)

                // 传递结果给OverlayView
                fragmentCameraBinding.overlay.setResults(
                    resultBundle.result,
                    resultBundle.inputImageHeight,
                    resultBundle.inputImageWidth,
                    RunningMode.LIVE_STREAM
                )

                // 主动触发渲染
                fragmentCameraBinding.overlay.invalidate()
            }
        }
    }</code></pre> 
<p>最后绘制人脸关键点，包括面部表情、轮廓：</p> 
<pre><code class="language-Kotlin">   override fun draw(canvas: Canvas) {
        super.draw(canvas)
        if(results == null || results!!.faceLandmarks().isEmpty()) {
            clear()
            return
        }

        results?.let { faceLandmarkResult -&gt;
            // 绘制关键点
            for(landmark in faceLandmarkResult.faceLandmarks()) {
                for(normalizedLandmark in landmark) {
                    canvas.drawPoint(normalizedLandmark.x() * imageWidth * scaleFactor,
                        normalizedLandmark.y() * imageHeight * scaleFactor, pointPaint)
                }
            }
            // 绘制线条
            FaceLandmarker.FACE_LANDMARKS_CONNECTORS.forEach {
                canvas.drawLine(
                    faceLandmarkResult.faceLandmarks()[0][it!!.start()].x() * imageWidth * scaleFactor,
                    faceLandmarkResult.faceLandmarks()[0][it.start()].y() * imageHeight * scaleFactor,
                    faceLandmarkResult.faceLandmarks()[0][it.end()].x() * imageWidth * scaleFactor,
                    faceLandmarkResult.faceLandmarks()[0][it.end()].y() * imageHeight * scaleFactor,
                    linePaint)
            }
        }
    }</code></pre> 
<h3 id="%E4%BA%94%E3%80%81%E6%A3%80%E6%B5%8B%E7%BB%93%E6%9E%9C">五、检测结果</h3> 
<p>输入数据可以是静态图像、实时视频流、文件视频帧。输出数据有人脸边界框、人脸网格、关键点坐标。其中，人脸关键点包括：脸部轮廓、嘴巴、鼻子、眼睛、眉毛、脸颊等，属于3D的landmark模型。如下图所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2c/cd/qrZnFDdW_o.jpg"></p> 
<p> 在人脸识别、人脸关键点基础上，还支持换脸，变成可爱的卡通效果。眨眼睛、摇头、张嘴这些表情动作，都会有实时的卡通头像变化。如下图所示：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fc/82/WaHdER0c_o.png"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5807a6204b14bfdfb5a47c4e2b7f516a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Nginx超详细入门教程</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/eca94ac454337c1d79a7052035201bc7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Ubuntu安装gcc和g&#43;&#43;图文教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>