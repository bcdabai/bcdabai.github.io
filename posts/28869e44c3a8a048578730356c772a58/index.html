<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>å¦‚ä½•å»è®¾è®¡ä¸€ä¸ªæ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨ï¼Ÿ - ç¼–ç¨‹å¤§ç™½çš„åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="å¦‚ä½•å»è®¾è®¡ä¸€ä¸ªæ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨ï¼Ÿ" />
<meta property="og:description" content="How to make your own deep learning accelerator chip! Currently, there are more than 100 companies all over the world building ASICs (Application Specific Integrated Circuit) or SOCâ€™s (System on Chip) targeted towards deep learning applications. There is a long list of companies here. In addition to these startup big companies like Google (TPU), Facebook, Amazon (Inferentia), Tesla etc are all developing custom ASICâ€™s for deep learning training and inference." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/28869e44c3a8a048578730356c772a58/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-01T15:02:22+08:00" />
<meta property="article:modified_time" content="2020-03-01T15:02:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§ç™½çš„åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§ç™½çš„åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">å¦‚ä½•å»è®¾è®¡ä¸€ä¸ªæ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨ï¼Ÿ</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="How_to_make_your_own_deep_learning_accelerator_chip_0"></a>How to make your own deep learning accelerator chip!</h2> 
<p>Currently, there are more than 100 companies all over the world building ASICs (Application Specific Integrated Circuit) or SOCâ€™s (System on Chip) targeted towards deep learning applications. There is a long list of companies here. In addition to these startup big companies like Google (TPU), Facebook, Amazon (Inferentia), Tesla etc are all developing custom ASICâ€™s for deep learning training and inference. These can be categorized into two types â€”</p> 
<ul><li><strong>Training and Inference</strong> â€” These ASICâ€™s are designed to handle both training the deep neural network and also performing inference. Training a large neural network like Resnet-50 is a much more compute-intensive task involving gradient descent and back-propagation. Compared to training inference is very simple and requires less computation. NVidia GPUâ€™s, which are most popular today for deep learning, can do both training and inference. Some other examples are Graphcore IPU, Google TPU V3, Cerebras, etc. OpenAI has great analysis showing the recent increase in compute required for training large networks.</li><li><strong>Inference</strong> â€” These ASICs are designed to run DNNâ€™s (Deep neural networks) which have been trained on GPU or other ASIC and then trained network is modified (quantized, pruned etc) to run on a different ASIC (like Google Coral Edge TPU, NVidia Jetson Nano). Most people say that the market for deep learning inference is much bigger than the training. Even very small microcontrollers (MCUâ€™s) based on ARM Cortex-M0, M3, M4 etc can do inference as shown by the TensorFlow Lite team.</li></ul> 
<p><img src="https://images2.imgbox.com/12/d4/XupkeDb8_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Making any chip (ASIC, SOC etc) is a costly, difficult and lengthy process typically done by teams of 10 to 1000â€™s of people depending on the size and complexity of the chip.</p> 
<p>Here I am only providing a brief overview specific to deep learning inference accelerator. If you have already designed chips you may find this too simple. If you are still interested, read on! If you like it share and ğŸ‘ .</p> 
<h3><a id="Architecture_of_Existing_ASICs_10"></a>Architecture of Existing ASICâ€™s</h3> 
<p>Lets first look at the high-level architecture of some of the accelerators currently being developed.</p> 
<p><strong>Habana Goya</strong> â€” Habana labs is a start-up which is developing separate chips for training â€” Gaudi and inference â€” Goya.<br> <img src="https://images2.imgbox.com/15/26/yVC4qZX3_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> GEMM Engine â€” General matrix and multiply Engine. Matrix multiplication is the core operation in all DNNâ€™s â€” convolution can be represented as matrix multiplication and fully connected layers are straight forward matrix multiplication.</p> 
<p>TPC â€” Tensor processing Core â€” this is a block which actually performs the multiplication or multiply and accumulate (MAC) operation.</p> 
<p>Local Memory and Shared Memory â€” These are both some form of cache commonly implemented using SRAM (Static Random Access Memory) and Register file (also type of static volatile memory just less dense than SRAM).</p> 
<p><strong>Eyeriss</strong> â€” The Eyeriss team from MIT has been working on deep learning inference accelerators and have published several papers about their two chips namely Eyeriss V1 and V2.<br> <img src="https://images2.imgbox.com/8d/e8/BdELKfik_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <strong>Nvidia Deep Learning Accelerator (NVDLA)</strong></p> 
<p><img src="https://images2.imgbox.com/f5/b5/nCYuQshK_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Dataflow Architecture â€” Dataflow architectures has been in research since the 1970s at least. Wave Computing came up with Dataflow processing unit (DPU) to accelerate training of DNNâ€™s. Hailo also uses some form of dataflow architecture.</p> 
<p><img src="https://images2.imgbox.com/c7/3c/dUaFILZ5_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <strong>Gyrfalcon</strong> â€” They have already released some chips like the Lightspeeur 2801S targeted towards low power Edge AI applications.</p> 
<p><img src="https://images2.imgbox.com/74/87/x3hoBS2P_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Google TPU also has a systolic data flow engine.</p> 
<p><img src="https://images2.imgbox.com/9d/dc/bPLZR3hY_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> <img src="https://images2.imgbox.com/8e/61/7VlrlWDM_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Unified Buffer â€” This is basically local memory/cache probably implemented using SRAM.</p> 
<p>DRAM â€” These are interfaces to access external DRAM, with two of them you can access 2x the data.</p> 
<h3><a id="Key_Blocks_40"></a>Key Blocks</h3> 
<p>Based on some of the above examples we can say that below are the key components required to make a deep learning inference accelerator. Also, we will only focus on 8-bit inference engine which has been shown to be good enough for many applications.</p> 
<p><strong>Matrix multiplication Unit</strong> â€” This is referred by different names like TPC (Tensor processing core), PE, etc. GEMM is the core computation involved in DNNâ€™s, to learn more about GEMM read this great post.</p> 
<p><strong>SRAM</strong> â€” This is the local memory used to store the weights or intermediate outputs/activations.</p> 
<p><img src="https://images2.imgbox.com/01/f7/Zfloa7za_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> To reduce energy consumption the memory should be located as close as possible to the processing unit and should be accessed as little as possible.<br> Interconnect/Fabric â€” This is the logic which connects all the different processing units and memory so that output from one layer or block can be transferred to the next block. Also referred to as Network on Chip (NoC).</p> 
<p><strong>Interfaces (DDR, PCIE)</strong> â€” These blocks are needed to connect to external memory (DRAM) and an external processor.</p> 
<p><strong>Controller</strong> â€” This can be a RISC-V or ARM processor or custom logic which is used to control and communicate with all the other blocks and the external processor.</p> 
<h3><a id="Architecture_and_Instruction_Set_55"></a>Architecture and Instruction Set</h3> 
<p>If we look at all the architectures we will see memory is always placed as close as possible to the compute. The reason is that moving data consumes more energy than compute. Letâ€™s look at the computation and memory involved in AlexNet architecture, which broke the ImageNet record in 2012 â€”</p> 
<p><img src="https://images2.imgbox.com/6e/f9/d64kjPQH_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> AlexNet consists of 5 Constitutional layers and 3 fully connected layers. The total number of parameters/weights for AlexNet is around 62 million. Letâ€™s say after weight quantization each weight is stored as an 8-bit value so if we want to keep all the weights in on-chip memory it would require at least 62 MB of SRAM or 62<em>8 Mega-bits = 496 Million SRAM cells. If we use the 6T (six transistor) SRAM cell just the memory would require â€” 496M</em>6~2.9 Billion transistors. So while deciding architecture we have to keep in mind which DNN architectures we can support without keeping weights off-chip (which increases power consumption). For this reason lot of startups demonstrate using newer architectures like MobileNetV2 which use much fewer parameters and less compute, for example, one checkpoint of MobileNetV2 with Top-5 accuracy of 92.5% on ImageNet has only 6.06M parameters and performs 582M MACs (multiply and accumulate) operations during single image inference.</p> 
<p><img src="https://images2.imgbox.com/28/0e/elRpTOow_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Weight pruning is another technique which can be used to reduce the model size (hence memory footprint). See results for model compression.</p> 
<p>MobileNetV2 uses depthwise separable convolutions which are different from traditional convolution so the accelerator architecture has to be flexible enough so that if researchers come up with different operations they can still be represented in terms of the instruction set available on the accelerator.</p> 
<p>We can come up with a very simple set of instructions for our simple accelerator like â€”</p> 
<ul><li>Load Data â€” Takes source and destination address</li><li>MAC (Multiply and accumulate) â€” Assumes data is already in the local register.</li><li>Store result â€” Store the intermediate result</li><li>PAD â€” To add zeros</li></ul> 
<h3><a id="Compilers_for_Hardware_Accelerators_74"></a>Compilers for Hardware Accelerators</h3> 
<p>Compilers convert the high-level code written in python using PyTorch or Tensorflow to the instruction set for the specific chip. Below are some of the frameworks in development/use to work with these custom ASICâ€™s. This process can be very hard and complicated because different ASICâ€™s support different instruction sets and if the compiler doesnâ€™t generate optimized code then you may not be taking full advantage of the capabilities of the ASIC.</p> 
<p><strong>Facebook Glow</strong> â€” Habana labs has developed a backend for their ASIC using the Glow framework.</p> 
<p><strong>TVM</strong> â€” This is an open-source deep learning compiler stack started by researchers at the University of Washington. The TVM framework also includes Versatile Tensor Accelerator (VTA) which is a programmable standalone accelerator. Amazon</p> 
<p><strong>Sagemaker Neo</strong> uses TVM to compile deep learning models and deploy on different hardware.</p> 
<p><strong>TensorFlow MLIR</strong> â€” MLIR is the compiler infrastructure from Google for TensorFlow and has been recently been made part of the LLVM project.</p> 
<p><strong>Intel ngraph</strong> â€” This was developed by Nervana and used for the Nervana/Intel deep learning accelerators.</p> 
<h3><a id="EDA_Tools_and_HighLevel_Synthesis_87"></a>EDA Tools and High-Level Synthesis</h3> 
<p>Chisel â€” Chisel is hardware construction/description language initially developed by researchers at Berkeley. Itâ€™s actually written in Scala and is used for the design of many RISC-V based processors.</p> 
<p>Synthesis, Timing and Layout â€” RTL Synthesis is the process of converting high-level code written in Verilog/VHDL etc to logic gates. Timing tools use the pre- and post- layout delay information of the logic gates and routing to make sure the design is correct. In sequential design, everything happens with respect to the clock edge so timing is very important. Layout tools generate the layout from the synthesized netlist. Synopsys (Design Compiler, PrimeTime) and Cadence tools are most commonly used for these steps.</p> 
<p>High-Level Synthesis (HLS) â€” HLS refers to the process when the hardware is described in a high-level language like C/C++ etc and then converted to a RTL (Register transfer level) language like VHDL/Verilog. There is even a python package http://www.myhdl.org/ â€” to convert python code to Verilog or VHDL. Cadence has commercial tools which support C/C++ etc, these tools can be very helpful for custom designs. Google used Mentor Graphics Catapult HLS tool to develop the WebM decompression IP.</p> 
<h3><a id="Available_IP_95"></a>Available IP</h3> 
<p>Now that we have identified the key blocks needed, letâ€™s look at what existing IP we use (free or paid).</p> 
<p>Nvidia Deep Learning Accelerator (NVDLA) â€” NVDLA is a free and open architecture released by Nvidia for the design of deep learning inference accelerators. The source code, drivers, documentation etc are available on GitHub.</p> 
<p>SRAM â€” Different types of SRAM IP â€” Single port, dual port, lower power, high speed etc for different process nodes is available from Synopsys and others. Typically they provide a SRAM compiler which is used to generate specific SRAM block as per the chip requirements.</p> 
<p>Register File â€” This IP is also available from Synopsys and various types of logic standard cells.</p> 
<p>Interconnect/Fabric/NoC â€” One of the options for this IP is Arteris, they have the FlexNoC AI Package targeted towards deep learning accelerators.</p> 
<p>Processors â€” Various RISC-V processor cores are available for free. Even ARM gives licenses to startups for free or very low upfront cost. ARM Ethos NPUs are specially designed for neural networks â€” Ethos N37, N57, N77.</p> 
<p><img src="https://images2.imgbox.com/54/6b/g9ggax1a_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Cadence Tensilica DNA 100 â€” Cadence provides IP which can be configured from 0.5 to 100â€™s of TMAC operations depending on the application/industry we are targeting.</p> 
<p>There are lots of other IP available so my advice is to look for already tested IP from companies like ARM, Ceva, NXP etc before designing your own.</p> 
<h3><a id="Design_Process_114"></a>Design Process</h3> 
<p>There are a lot of resources (books, lectures etc) available on ASIC design flow, digital design process etc so I will not cover it.<br> <img src="https://images2.imgbox.com/90/2c/wtv7wt1v_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
<h4><a id="Foundries_and_Process_technology_118"></a>Foundries and Process technology</h4> 
<p>The manufacturing of chips is done is huge fabs (fabrication plants or foundries) and currently, there are very few companies like Intel, Samsung, Texas Instruments, NXP etc which own their own fabs. Even huge companies like Qualcomm, AMD etc use external foundries and all such companies are called fabless. Below are some of the biggest semiconductor foundries</p> 
<p>TSMC (Taiwan Semiconductor manufacturing company) â€” TSMC is the worlds largest foundry and makes chips for companies like Qualcomm, Apple, etc. It can be challenging for small startups to manufacture at TSMC because most of their manufacturing capacity is used by big companies.</p> 
<p>UMC (United Microelectronics Corporation) â€” UMC also works with a large number of customers including small startups. Currently, the smallest process available at UMC is 14nm.<br> There are several other foundries like Global foundry, Samsung foundry, etc</p> 
<h4><a id="Process_Selection_127"></a>Process Selection</h4> 
<p><img src="https://images2.imgbox.com/56/1f/2tTFQIew_o.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> IC manufacturing processes are measured by the size of of the transistors and the width of the metal connections. For a long time, the process dimensions have been going down (Mooreâ€™s law) and thatâ€™s modern IC contain more and more transistors every year (this used to be governed by Mooreâ€™s law). Currently, the most advanced process node is 7nm and products using 7nm process were only launched in 2019. So most of the products are currently using chips made using 14nm/16nm process. The more advanced the process the more expensive itâ€™s going to be hence most small startups will initially use a slightly older process to keeps costs low. Many of the startups developing deep learning accelerators are using 28nm processor or, in some cases, even 40nm</p> 
<p>process. Leakage is a big concern in modern processes and can contribute to significant power consumption if the chip is not designed properly.</p> 
<h4><a id="Simple_Cost_Estimation_133"></a>Simple Cost Estimation</h4> 
<p>Wafer costs depend on the process node and various other things like number of processing steps (layers used). The cost can vary from around thousand dollars for relatively older processes to several thousand of dollars for the latest process node and depends a lot of how many wafers one is buying etc.</p> 
<p><img src="https://images2.imgbox.com/2b/ca/CvEYnoBt_o.jpg" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> Most foundries produce 300 mm (~12 inch) diameter wafers for the digital processes. Let do simple calculation of die cost for a 12 inch wafer</p> 
<p>Total Area ~ Ï€ * r Â² (r = radius of wafer) ~ 70,650 mmÂ² Total Wafer Cost~$1000 (Just used for example)<br> Die Area ~ 10 mm x 10 mm ~ 100 mmÂ² ( TPU V1 Die Size ~ 331 mmÂ², SRAM Cell in 32 nm area ~ 0.18 umÂ²)</p> 
<p>Dies per wafer ~ 70,650 / 100 ~ 706 (Actually less due to edge defects etc) Actually good dies with 95% yield ~ 0.95 * 706 ~ 670<br> Single Die Cost ~ $1000/670 ~ $1.5 Packaging and testing also add to the final cost.</p> 
<p>This is a huge field and this post only touched the surface of some of these topics. There are so many other things to cover like FPGA for deep learning, layout, testing, yield, low power design, etc. I may write another post if people like this one.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ed2fca1d2c7530f706eb6a10314b961d/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Game101è¯¾ç¨‹ç¬”è®°_lecture01_è®¡ç®—æœºå›¾å½¢å­¦æ¦‚è¿°</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f0681c22929d5ce23625863060b9d2b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">linuxè‡ªåŠ¨è§£å‹å¼å®‰è£…java</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§ç™½çš„åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>