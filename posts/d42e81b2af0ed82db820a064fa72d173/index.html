<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch 深度学习之余弦相似度 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch 深度学习之余弦相似度" />
<meta property="og:description" content="文章目录 用处定理代码F.normalize() 和 F.norm() 的区别 用处 此方法特别重要，经常可以用来修改论文，提出创新点.
定理 余弦相似度是通过计算两个向量之间的夹角余弦值来衡量它们的相似性。给定两个非零向量 x 和 y，它们之间的余弦相似度可以使用以下公式计算：
cosine_similarity(x, y) = (x · y) / (||x|| * ||y||)
其中，
(x · y) 表示向量 x 和 y 的点积（内积），是两个向量对应元素相乘再求和的结果。||x|| 表示向量 x 的范数，通常使用 L2 范数表示，即向量 x 的所有元素平方和的平方根。||y|| 表示向量 y 的范数，也是使用 L2 范数进行计算。 使用上述公式，我们可以将两个向量的点积除以它们的范数的乘积，得到余弦相似度的标量结果，取值范围在 -1 到 1 之间。越接近 1 表示两个向量越相似，越接近 -1 表示两个向量越不相似，0 表示两个向量正交（无关）。
代码 代码1： 如果您想在指定的维度（channels, height, width）上计算范数并保持计算过程中的维度，可以进行如下修改：
import torch.nn.functional as F def cosine_similarity(tensor_1, tensor_2): normalized_tensor_1 = F.normalize(tensor_1, p=2, dim=(1, 2, 3)) normalized_tensor_2 = F." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d42e81b2af0ed82db820a064fa72d173/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-15T21:32:38+08:00" />
<meta property="article:modified_time" content="2023-11-15T21:32:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch 深度学习之余弦相似度</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">用处</a></li><li><a href="#_3" rel="nofollow">定理</a></li><li><a href="#_15" rel="nofollow">代码</a></li><li><a href="#Fnormalize__Fnorm__55" rel="nofollow">F.normalize() 和 F.norm() 的区别</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>用处</h3> 
<p>此方法特别重要，经常可以用来修改论文，提出创新点.</p> 
<h3><a id="_3"></a>定理</h3> 
<p>余弦相似度是通过计算两个向量之间的夹角余弦值来衡量它们的相似性。给定两个非零向量 x 和 y，它们之间的余弦相似度可以使用以下公式计算：</p> 
<p>cosine_similarity(x, y) = (x · y) / (||x|| * ||y||)</p> 
<p>其中，</p> 
<ul><li>(x · y) 表示向量 x 和 y 的点积（内积），是两个向量对应元素相乘再求和的结果。</li><li>||x|| 表示向量 x 的范数，通常使用 L2 范数表示，即向量 x 的所有元素平方和的平方根。</li><li>||y|| 表示向量 y 的范数，也是使用 L2 范数进行计算。</li></ul> 
<p>使用上述公式，我们可以将两个向量的点积除以它们的范数的乘积，得到余弦相似度的标量结果，取值范围在 -1 到 1 之间。越接近 1 表示两个向量越相似，越接近 -1 表示两个向量越不相似，0 表示两个向量正交（无关）。</p> 
<h3><a id="_15"></a>代码</h3> 
<ul><li>代码1：</li></ul> 
<p>如果您想在指定的维度（channels, height, width）上计算范数并保持计算过程中的维度，可以进行如下修改：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

<span class="token keyword">def</span> <span class="token function">cosine_similarity</span><span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    normalized_tensor_1 <span class="token operator">=</span> F<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    normalized_tensor_2 <span class="token operator">=</span> F<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>tensor_2<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    cosine_sim <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>normalized_tensor_1 <span class="token operator">*</span> normalized_tensor_2<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> cosine_sim
</code></pre> 
<p>在这里，我们使用 <code>dim=(1, 2, 3)</code> 将计算范数的维度指定为 (channels, height, width)，并使用 <code>keepdim=True</code> 保持了计算过程中的维度。</p> 
<p>这样，函数将在指定的维度上进行范数计算，并返回一个与输入张量形状相同的张量，其中的每个元素是沿着指定维度（channels, height, width）计算得到的余弦相似度值，并保持了指定维度的维度大小。</p> 
<ul><li>代码2：</li></ul> 
<p>如果你希望使用 <code>torch.norm()</code> 函数计算张量的范数，可以对上述代码进行如下修改：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">cosine_similarity</span><span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    normalized_tensor_1 <span class="token operator">=</span> tensor_1 <span class="token operator">/</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    normalized_tensor_2 <span class="token operator">=</span> tensor_2 <span class="token operator">/</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>tensor_2<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    cosine_sim <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>normalized_tensor_1 <span class="token operator">*</span> normalized_tensor_2<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> cosine_sim
</code></pre> 
<p>在这个修改后的代码中，我们使用了 <code>torch.norm()</code> 函数计算指定维度上的张量范数，并将其作为分母来归一化输入张量。参数 <code>p=2</code> 表示使用 L2 范数计算。</p> 
<p>然后，我们使用 <code>torch.sum()</code> 函数在指定的维度上求和，并保持计算过程中的维度，得到余弦相似度的向量。</p> 
<p>请确保已经导入了 <code>torch</code> 模块。</p> 
<h3><a id="Fnormalize__Fnorm__55"></a>F.normalize() 和 F.norm() 的区别</h3> 
<p><code>F.normalize()</code> 和 <code>F.norm()</code> 是两个不同的函数，它们在功能和使用方式上有所不同。</p> 
<ol><li> <p><code>F.normalize()</code> 函数是用来对张量进行归一化处理的。它接受一个输入张量和一个参数 <code>p</code>，并根据指定的范数类型对输入张量进行归一化。常见的范数类型包括 L1 范数、L2 范数等。归一化后的张量将具有单位长度，方便进行一些距离度量或相似度计算的操作。</p> </li><li> <p><code>F.norm()</code> 函数是用来计算张量的范数的。它接受一个输入张量和一个参数 <code>p</code>，并返回指定范数类型的计算结果。常见的范数类型包括 L1 范数、L2 范数等。<code>F.norm()</code> 函数返回的是一个标量结果，而不是对输入张量进行归一化处理。</p> </li></ol> 
<p>总结：<br> <code>F.normalize()</code> 函数用于对张量进行归一化处理，返回归一化后的张量；<br> <code>F.norm()</code> 函数用于计算张量的范数，返回范数的标量结果。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7617183b74a1eb35e7dbc49865599957/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《Hello Solidity！》之 payable</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9b056fe0c0e51a6d7e8fb814ad6d4da6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《Hello Solidity！》之 随机数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>