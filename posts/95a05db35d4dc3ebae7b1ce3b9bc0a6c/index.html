<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>torch.nn.Embedding()中的padding_idx参数解读 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="torch.nn.Embedding()中的padding_idx参数解读" />
<meta property="og:description" content="torch.nn.Embedding() Word Embedding 词嵌入，就是把一个词典，随机初始化映射为一个向量矩阵。
列如：有一组词典，有两个词“hello”和“world”，对应的值为0和1.通过pytorch中的torch.nn.Embedding()建立一个2x10的向量矩阵，其中2表示词典中词的数量，10表示每个词对应的向量大小。
word_to_id = {&#39;hello&#39;:0, &#39;world&#39;:1} embeds = nn.Embedding(2, 10) hello_idx = torch.LongTensor([word_to_id[&#39;hello&#39;]]) hello_embed = embeds(hello_idx) print(hello_embed) 应用pytorch自带的nn.Enbedding构建词嵌入向量。
torch.nn.Embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False, _weight: Optional[torch.Tensor] = None) num_embeddings：表示词典中词的数量
embedding_dim：表示每个词对应的向量维度
自然语言中使用批处理时候, 每个句子的长度并不一定是等长的, 这时候就需要对较短的句子进行padding, 填充的数据一般是0, 这个时候, 在进行词嵌入的时候就会进行相应的处理, nn.embedding会将填充位置的值映射为0。
padding_idx padding_idx：表示用于填充的参数索引，比如用3填充，嵌入向量索引为3的向量设置为0
import torch import torch.nn as nn embed = nn.Embedding(10, 3, padding_idx=3) # padding_idx 默认是0 embed." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/95a05db35d4dc3ebae7b1ce3b9bc0a6c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-29T17:20:54+08:00" />
<meta property="article:modified_time" content="2020-09-29T17:20:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">torch.nn.Embedding()中的padding_idx参数解读</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="torchnnEmbedding_0"></a>torch.nn.Embedding()</h2> 
<h3><a id="Word_Embedding_1"></a>Word Embedding</h3> 
<p>词嵌入，就是把一个词典，随机初始化映射为一个向量矩阵。<br> 列如：有一组词典，有两个词“hello”和“world”，对应的值为0和1.通过pytorch中的torch.nn.Embedding()建立一个2x10的向量矩阵，其中2表示词典中词的数量，10表示每个词对应的向量大小。</p> 
<pre><code>word_to_id = {'hello':0, 'world':1}
embeds = nn.Embedding(2, 10)
hello_idx = torch.LongTensor([word_to_id['hello']])
hello_embed = embeds(hello_idx)
print(hello_embed)
</code></pre> 
<p>应用pytorch自带的nn.Enbedding构建词嵌入向量。</p> 
<pre><code>torch.nn.Embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False, _weight: Optional[torch.Tensor] = None)
</code></pre> 
<p>num_embeddings：表示词典中词的数量<br> embedding_dim：表示每个词对应的向量维度</p> 
<p>自然语言中使用批处理时候, 每个句子的长度并不一定是等长的, 这时候就需要对较短的句子进行padding, 填充的数据一般是0, 这个时候, 在进行词嵌入的时候就会进行相应的处理, nn.embedding会将填充位置的值映射为0。</p> 
<h3><a id="padding_idx_20"></a>padding_idx</h3> 
<p>padding_idx：表示用于填充的参数索引，比如用3填充，嵌入向量索引为3的向量设置为0</p> 
<pre><code>import torch
import torch.nn as nn
 
embed = nn.Embedding(10, 3, padding_idx=3) # padding_idx 默认是0
embed.weight
</code></pre> 
<p><img src="https://images2.imgbox.com/bb/83/EpfqtrSK_o.png" alt="在这里插入图片描述"><br> 让我们再举个栗子，所有索引为3的都用0来填充,。</p> 
<pre><code>x = torch.tensor([[2, 2, 3, 3], [1, 2, 5, 4]])
embed(x)
</code></pre> 
<p><img src="https://images2.imgbox.com/06/ff/Rd8VFfFG_o.png" alt="在这里插入图片描述"><br> 也就是说，为了便于模型训练需统一句子长度，对那些句子长度不够的句子进行填充，比如用值3进行填充，当用nn.Embedding()进行词向量嵌入时，对应的索引为3的向量将变为全为0的向量。这样就减少了填充值对模型训练的影响。<br> 把padding_idx设置为填充的值，如padding_idx=3，训练过程中索引为3的将始终设置为0,不进行参数更新.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/63b61e65c844783f6805cf43bbe94819/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【java队列】LinkedBlockingDeque</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/56fe48773416ff78220e357a3b3db5b3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">线程和进程/阻塞和挂起以及那些sleep,wait()和notify()方法详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>