<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Tensorflow 1.0】—— Graph（图）和Session（会话） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【Tensorflow 1.0】—— Graph（图）和Session（会话）" />
<meta property="og:description" content="借鉴：tensorflow中的Graph（图）和Session（会话）的关系 - 简书
借鉴：TensorFlow学习笔记1：graph、session和op - Jiax - 博客园
一、Graph和Session的关系 TensorFlow是一种“符号式编程框架”，首先要构造一个图（graph），然后在会话（Session）上根据这个图做真实的运算（op）。打个比方，graph就是“输入-处理-输出”这个pipeline中的处理部分，一个session就是建立了一个pipeline进行“输入-处理-输出”。graph具有一系列的加工步骤（加减乘除等运算），session把输入投进去，就能得到输出。不同的session都可以使用同一个graph，只要他们的加工步骤是一样的就行。同样的，一个graph可以供多个session使用，而一个session不一定需要使用graph的全部，可以只使用其中的一部分。
graph即tf.Graph()，session即tf.Session()，是两个完全独立的概念。
graph定义了计算方式，是一些加减乘除等运算的组合。它本身不会进行任何计算，也不保存任何中间计算结果。session用来运行一个graph，或者运行graph的一部分。它类似于一个执行者，给graph灌入输入数据，得到输出，并保存中间的计算结果。同时它也给graph分配计算资源（如内存、显卡等） 下图是用tensorflow制作大盘鸡和红烧肉的过程，以此为例来说明graph和session的区别：
左图中绿色矩形为数据，黄色圆圈为中间结果，红色圆圈为最终结果，这是一个完整的制作大盘鸡和红烧肉的graph（相当于是一个菜谱），图中每一个独立单元都可以看成是一个op（操作，包括数据）。在tensorflow中只有graph是没法得到结果的，这就像只有菜谱不可能得到红烧肉是一个道理。于是就有了tf.Session(），他根据graph制定的步骤，将graph变成现实。
tf.Session(）就相当于一个厨师长，他下面有很多办事的人（Session()下的各种方法），其中有一个非常厉害人叫tf.Session.run()，他不仅会烧菜，还会杀猪、酿酒、制作酱料等一系列工作，比如：
我的酱料 = sess.run(酱料)：run收到制作“酱料”的命令，于是他看了下graph，需要“酵母”和“大豆”来制作酱料，最终他把酱料制作好了（这里酵母和大豆是graph定义好的，但也可以根据自己的喜好来换）。
我的料酒 = sess.run(料酒，feed_dic={米:泰国籼米})：run又收到要制作“料酒”的命令，而且不用graph规定的“米”来做，需要用“泰国籼米”，没关系，run跑去买了泰国籼米，又把料酒给做了。
我的红烧肉 = sess.run(红烧肉)：傍晚，run又收到了做一份完整红烧肉的命令，这下他有的忙了，必须将整个流程走一遍，才能完成个任务。
我的大盘鸡 = sess.run(大盘鸡): 后来，run又收到做大盘鸡的任务，这是一个独立的任务，跟红烧肉没有半点关系，但不影响，他只要按照步骤照做就可以了。
二、关于Graph 2.1 定义一个图：graph #定义一个graph g = tf.Graph() #默认在g中定义下面op a = tf.constant(2) b = tf.constant(3) x = tf.add(a, b) 上面就定义了一个graph。tensorflow会默认给我们建立一个graph，所以g = tf.Graph()这句其实是可以省略的。上面的graph包含3个操作（即op），但凡是op，都需要通过session运行之后，才能得到结果。如果你直接执行print(a)，那么输出结果是：Tensor(&#34;a:0&#34;, shape=(), dtype=int32)。执行print(tf.Session().run(a))或with方法，才能得到2。可见，在tensorflow中，即使是最基本的对象Tensor（张量）也需要在Session中才能得到其值
2.2 定义多个图：多个graph 你可以定义多个graph，例如一个graph实现z = x &#43; y，另一个graph实现u = 2 * v
g1 = tf.Graph() #定义一个graph g2 = tf.Graph() #定义另一个graph with g1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e057b4222eb6c39e16f0f69ffc7e09a3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-24T20:41:37+08:00" />
<meta property="article:modified_time" content="2022-03-24T20:41:37+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Tensorflow 1.0】—— Graph（图）和Session（会话）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="udadbd375"><strong>借鉴</strong>：<a href="https://www.jianshu.com/p/b636de7c251a" rel="nofollow" title="tensorflow中的Graph（图）和Session（会话）的关系 - 简书">tensorflow中的Graph（图）和Session（会话）的关系 - 简书</a></p> 
<p id="u6fedb039"><strong>借鉴</strong>：<a href="https://www.cnblogs.com/jiaxblog/p/9054051.html" rel="nofollow" title="TensorFlow学习笔记1：graph、session和op - Jiax - 博客园">TensorFlow学习笔记1：graph、session和op - Jiax - 博客园</a></p> 
<h2>一、Graph和Session的关系</h2> 
<p id="ufd510195">        TensorFlow是一种“<strong>符号式编程框架</strong>”，首先要构造一个图（graph），然后在会话（Session）上根据这个图做真实的运算（op）。打个比方，<span style="background-color:#f9eda6;">graph就是“输入-处理-输出”这个pipeline中的处理部分，一个session就是建立了一个pipeline进行“输入-处理-输出”</span>。graph具有一系列的加工步骤（加减乘除等运算），session把输入投进去，就能得到输出。不同的session都可以使用同一个graph，只要他们的加工步骤是一样的就行。同样的，<strong>一个graph可以供多个session使用，而一个session不一定需要使用graph的全部，可以只使用其中的一部分</strong>。</p> 
<p>        <strong>graph即tf.Graph()，session即tf.Session()，是两个完全独立的概念。</strong></p> 
<ul><li id="u9d4af444"><strong>graph定义了计算方式，是一些加减乘除等运算的组合。它本身不会进行任何计算，也不保存任何中间计算结果。</strong></li><li id="uc4724728"><strong>session用来运行一个graph，或者运行graph的一部分。它类似于一个执行者，给graph灌入输入数据，得到输出，并保存中间的计算结果。同时它也给graph分配计算资源（如内存、显卡等）</strong></li></ul> 
<p id="u13cef380"><strong>下图是用tensorflow制作大盘鸡和红烧肉的过程，以此为例来说明graph和session的区别：</strong><br>         左图中绿色矩形为数据，黄色圆圈为中间结果，红色圆圈为最终结果，这是一个完整的制作大盘鸡和红烧肉的<span style="background-color:#f9eda6;">graph（相当于是一个菜谱）</span>，图中每一个独立单元都可以看成是一个op（操作，包括数据）。在tensorflow中只有graph是没法得到结果的，这就像只有菜谱不可能得到红烧肉是一个道理。于是就有了tf.Session(），他根据graph制定的步骤，将graph变成现实。<br>         <span style="background-color:#f9eda6;">tf.Session(）就相当于一个厨师长，他下面有很多办事的人（Session()下的各种方法），其中有一个非常厉害人叫tf.Session.run()</span>，他不仅会烧菜，还会杀猪、酿酒、制作酱料等一系列工作，比如：<br><strong>我的酱料 = sess.run(酱料)</strong>：run收到制作“酱料”的命令，于是他看了下graph，需要“酵母”和“大豆”来制作酱料，最终他把酱料制作好了（这里酵母和大豆是graph定义好的，但也可以根据自己的喜好来换）。<br><strong>我的料酒 = sess.run(料酒，feed_dic={米:泰国籼米})</strong>：run又收到要制作“料酒”的命令，而且<span style="background-color:#f9eda6;">不用graph规定的“米”来做，需要用“泰国籼米”</span>，没关系，run跑去买了泰国籼米，又把料酒给做了。<br><strong>我的红烧肉 = sess.run(红烧肉)</strong>：傍晚，run又收到了做一份完整红烧肉的命令，这下他有的忙了，必须将整个流程走一遍，才能完成个任务。<br><strong>我的大盘鸡 = sess.run(大盘鸡)</strong>: 后来，run又收到做大盘鸡的任务，这是一个独立的任务，跟红烧肉没有半点关系，但不影响，他只要按照步骤照做就可以了。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/17/8a/3ySv3XTm_o.png"></p> 
<h2 id="GERN3">二、关于Graph</h2> 
<h3>2.1 定义一个图：graph</h3> 
<pre><code class="language-python">#定义一个graph
g = tf.Graph() 
#默认在g中定义下面op
a = tf.constant(2)
b = tf.constant(3)
x = tf.add(a, b) </code></pre> 
<p id="u12c0ef1b">上面就定义了一个graph。<strong>tensorflow会默认给我们建立一个graph，所以g = tf.Graph()这句其实是可以省略的</strong>。上面的graph包含3个操作（即op），但凡是op，都需要通过session运行之后，才能得到结果。如果你直接执行print(a)，那么输出结果是：Tensor("a:0", shape=(), dtype=int32)。执行print(tf.Session().run(a))或with方法，才能得到2。可见，<strong>在tensorflow中，即使是最基本的对象Tensor（张量）也需要在Session中才能得到其值</strong></p> 
<h3>2.2 定义多个图：多个graph</h3> 
<p id="u3a930765">你可以定义多个graph，例如一个graph实现z = x + y，另一个graph实现u = 2 * v</p> 
<pre><code class="language-python">g1 = tf.Graph() #定义一个graph
g2 = tf.Graph() #定义另一个graph
with g1.as_default(): #在指定graph中定义op
    x = tf.constant(2)
    y = tf.constant(3)
    z = tf.add(x, y)
with g2.as_default(): #在指定graph中定义op
    v = tf.constant(4)
    u = tf.mul(2, v)</code></pre> 
<p id="uca2cf89a">上述代码<span style="background-color:#f9eda6;">定义了两个graph并分别在其中定义了不同的op，</span><strong><span style="background-color:#f9eda6;">但通常不建议这么做</span></strong>，原因如下：</p> 
<ul><li id="ua81fcd0c"><strong>运行多个graph需要多个session，而每个session会试图耗尽所有的计算资源，开销太大；</strong></li><li id="ud8210b5a"><strong>graph之间没有数据通道，要人为通过python/numpy传数据。</strong></li></ul> 
<p></p> 
<p id="uef323426">        事实上，我们可以把所有的op都定义在一个graph中：</p> 
<pre><code class="language-python"># 没有显示定义graph，系统自动提供一个默认的graph
# 并在其中定义下面op
x = tf.constant(2)
y = tf.constant(3)
z = tf.add(x, y)
v = tf.constant(4)
u = tf.mul(2, v)</code></pre> 
<p id="uaa808282">        从上面graph的定义可以看到，x/y/z是一波，u/v是另一波，二者没有任何交集。这相当于在一个graph里有两个独立的subgraph。当你要计算z = x + y时，执行tf.Session().run(z)；当你想计算u = 2 * v，就执行tf.Session().run(u)，二者完全独立。<strong>但更重要的是，二者在同一个session上运行，<span style="background-color:#f9eda6;">系</span></strong><strong><span style="background-color:#f9eda6;">统会均衡地给两个subgraph分配合适的计算资源</span></strong>。</p> 
<ul><li id="u63d0ada8"><strong>我们可以把所有的op（op之间不一定相互有联系，如上图中的红烧肉和大盘鸡，两者完全独立）都定义在同一个graph上，Session在执行某个op时，只执行跟该op有关联的其他op，与其不想关的op是不会被执行的。</strong></li></ul> 
<p></p> 
<h3 id="mroWi">三、 关于session</h3> 
<p id="ud83730a2">        所有的节点计算都在session中完成，tf.Session()是一个大类，使用最多的方法是tf.Session.run()。</p> 
<p id="u2f8e4e09"><strong>通常我们会显示地定义一个session来运行graph，通常采用with的方式（推荐），也可用其他方式：</strong></p> 
<pre><code class="language-python"># 以下在一个默认graph中定义了三个op
x = tf.constant(2)
y = tf.constant(3)
z = tf.add(x, y)

# 以下显示启动了一个Session并在其中执行z
with tf.Session() as sess:
    result = sess.run(z)
    print(result)</code></pre> 
<p id="u17080b3c">输出结果是5</p> 
<p id="u3c0b02c5"></p> 
<h2 id="PG3CW">四、关于op</h2> 
<p id="u0023c04a">        graph就是由一系列op构成的。</p> 
<h4 id="mbXzS">        一个特殊的op: tf.placeholder()</h4> 
<p id="u4631810d">        placeholder，翻译过来就是<strong>占位符</strong>。其实它类似于函数里的自变量。比如z = x + y，那么x和y就可以定义成占位符。占位符，顾名思义，就这是占一个位子，平时不用关心它们的值，当你做运算的时候，你再把你的数据灌进去就行了。是不是和自变量很像？看下面的代码：</p> 
<pre><code class="language-python">a = tf.placeholder(tf.float32, shape=[3]) # a是一个3维向量
b = tf.constant([5, 5, 5], tf.float32)
c = a + b
with tf.Session() as sess:
    print sess.run(c, feed_dict = {a: [1, 2, 3]}) # 把[1, 2, 3]灌到a里去</code></pre> 
<p id="u8dfc8c27">        输出结果是[6, 7, 8]。上面代码中出现了feed_dict的概念，其实就是用[1, 2, 3]代替a的意思。相当于在本轮计算中，自变量a的取值为[1, 2, 3]。其实不仅仅是tf.placeholder才可以用feed_dict，很多op都可以。只要tf.Graph.is_feedable(tensor)返回值是True，那么这个tensor就可用用feed_dict来灌入数据。</p> 
<p id="u28139761"><strong>        那为什么不直接用tf.constant()去定义呢</strong>？</p> 
<p id="u912eeaaf">        tf.constant()是直接定义在graph里的，它是graph的一部分，会随着graph一起加载。如果通过tf.constant()定义了一个维度很高的张量，那么graph占用的内存就会变大，加载也会变慢。而tf.placeholder就没有这个问题，所以<span style="background-color:#f9eda6;">如果数据维度很高的话，定义成tf.placeholder是更好的选择</span>。</p> 
<p></p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f08e5457e25316b3948903f9315521df/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">网页禁止缩放</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9108689676a88f6a0d60f3fd0ca9b68e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">蓝桥云课 激光样式（两种方法）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>