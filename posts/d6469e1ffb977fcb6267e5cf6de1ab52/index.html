<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>RT-2(robotics-transformer2)论文翻译——1 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="RT-2(robotics-transformer2)论文翻译——1" />
<meta property="og:description" content="说明： ChatGPT等基于Transformer的大语言模型(LLM)的成功带给了人们巨大的震撼，其表现的few-shot甚至zero-shot能力仿佛让人们看到了通用人工智能(AGI)的曙光。而CLIP等跨模态模型打通了NLP和CV的界限，这些VLM模型将AGI发展继续向前推进。
AGI在机器人领域同样有很大的提升空间。试想如果机器人拥有了LLM般的思考能力，那这个世界将变得多么智能！届时的机器人不是只能机械执行预先设定好的程序，而是能够真的读懂人类，和人类自然交互。
RT2是谷歌deepmind团队今年7月底结合VLM和机器人控制技术研发的横跨语言-视觉-动作三模态的大模型。真正赋予了机器人的泛化能力！
——————————————————————————————————————————
本文包括论文的摘要、介绍、相关工作及模型介绍；论文其他部分翻译和解读见本专栏其他文章。翻译仅供参考。
论文地址：arxiv.org/pdf/2307.15818.pdf 翻译：ChatGPT Abstract 我们研究了在互联网规模数据上训练的视觉-语言模型如何直接融入端到端的机器人控制中，以提升泛化能力并实现新兴的语义推理。我们的目标是使单一的端到端训练模型既能够学习将机器人的观测映射到动作，又能够享受来自网络的大规模语言和视觉-语言数据的好处。为此，我们提出了将最先进的视觉-语言模型在机器人轨迹数据和互联网规模的视觉-语言任务（如视觉问答）上进行协同微调的方法。与其他方法不同的是，我们提出了一个简单通用的方法来实现这个目标：为了将自然语言回复和机器人动作都适应相同的格式，我们将动作表达为文本标记，并将其与自然语言标记一样直接并入模型的训练集中。我们将这类模型称为视觉-语言-动作模型（VLA），并且实例化了一个这样的模型，我们称之为RT-2。我们广泛的评估（6,000个评估试验）显示，我们的方法导致了高性能的机器人策略，并使RT-2能够从互联网规模的训练中获得一系列新兴的能力。其中包括对新颖对象的显著改进的泛化能力，解释机器人训练数据中不存在的命令（例如将物体放在特定的数字或图标上），以及能够对用户命令做出基本推理（例如拾取最小或最大的物体，或距离另一个物体最近的物体）。我们进一步展示，引入思维链推理使得RT-2能够进行多阶段的语义推理，例如找出哪个物体适合用作临时的锤子（一块石头），或者哪种类型的饮料最适合疲劳的人（能量饮料）。
Introduction 在广泛的网络规模数据集上预训练的高容量模型为各种下游任务提供了一个有效且强大的平台：大型语言模型不仅可以实现流畅的文本生成（Anil等，2023；Brohan等，2022；OpenAI，2023），还能够实现新兴的问题解决（Cobbe等，2021；Lewkowycz等，2022；Polu等，2022）和创造性的散文（Brown等，2020；OpenAI，2023）和代码生成（Chen等，2021），而视觉-语言模型可以实现开放词汇的视觉识别（Kirillov等，2023；Minderer等，2022；Radford等，2021），甚至可以对图像中的物体-代理交互进行复杂推断（Alayrac等，2022；Chen等，2023a,b；Driess等，2023；Hao等，2022；Huang等，2023；Wang等，2022）。这种语义推理、问题解决和视觉解释能力对于必须在现实环境中执行各种任务的通用型机器人将非常有用。然而，机器人应该如何获得这些能力尚不清楚。虽然一种蛮力方法可能涉及收集数百万次机器人交互试验，但最能胜任的语言和视觉-语言模型是在来自网络的数十亿标记和图像上进行训练的（Alayrac等，2022；Chen等，2023a,b；Huang等，2023）——在不久的将来，机器人数据不太可能达到这个数量级。另一方面，直接将这些模型应用于机器人任务也很困难：这些模型推理语义、标签和文本提示，而机器人需要基于实际的低级行动，比如笛卡尔末端执行器命令。尽管近期有许多工作试图将语言模型（LLMs）和视觉-语言模型（VLMs）融入机器人技术中（Ahn等，2022；Driess等，2023；Vemprala等，2023），但这些方法通常只涉及机器人规划的“高级”方面，本质上扮演着解释命令并将其解析为单独基元（如拾取和放置物体）的状态机的角色，这些基元然后由独立的低级控制器执行，而这些控制器本身在训练过程中并未从互联网规模模型的丰富语义知识中受益。因此，在本文中，我们提出了一个问题：是否可以将大型预训练的视觉-语言模型直接集成到低级机器人控制中，以提升泛化能力并实现新兴的语义推理？
为了实现这一目标，我们探索了一个既简单又出奇有效的方法：我们直接训练设计用于开放词汇视觉问答和视觉对话的视觉-语言模型，使其输出低级机器人动作，并解决其他互联网规模的视觉-语言任务。尽管这些模型通常是针对生成自然语言标记进行训练的，但我们可以通过将动作标记为文本标记，并创建“多模态句子”（Driess等，2023），通过将动作与摄像头观测相配对来“响应”机器人指令，从而训练它们在机器人轨迹上。这种方式，视觉-语言模型可以直接被训练成为遵循指令的机器人策略。这种简单方法与将VLMs纳入机器人策略的先前替代方法（Shridhar等，2022a），或者从头开始设计新的视觉-语言-动作架构（Reed等，2022）形成了鲜明对比：相反，现有的视觉-语言模型，已经经过了大量的计算投入，可以在不引入任何新参数的情况下进行训练，以输出文本编码的动作。我们将这类模型称为视觉-语言-动作（VLA）模型。我们通过在RT-1（Brohan等，2022）提出的协议基础上进行扩展来实例化VLA模型，使用类似的数据集，但将模型扩展为使用大型视觉-语言主干。因此，我们将我们的模型称为RT-2。我们在图1中提供了一个概览。
图1：左侧是利用互联网上的VQA(视觉问答数据)，但是把回答的内容从自然语言替换成了动作的表示。中间是RT-2模型架构，它的核心是一个transformer，视觉图像通过ViT模型进行输入；输出是机器人的一系列动作token并解码表示为动作。右侧是最终的机器人闭环控制效果。 我们观察到，从这些视觉-语言模型衍生出的机器人策略表现出一系列引人注目的能力，将从机器人数据中学到的物理运动能力与从网络数据中学到的图像和文本解释能力融合到一个单一的模型中。除了显著提高对新颖物体和语义多样指令的泛化能力的预期好处外，我们还观察到一些新兴的能力。虽然模型的物理技能仍然局限于机器人数据中看到的技能分布，但模型通过使用从网络获取的知识来解释图像和语言命令，获得了将这些技能以新方式应用的能力。一些示例亮点如图2所示。模型能够将从机器人数据中学到的拾取和放置技能重新用于将物体放置在语义指示的位置附近，例如特定的数字或图标，尽管这些提示在机器人数据中不存在。模型还可以解释物体之间的关系，以确定选择哪个物体以及在哪里放置它，尽管机器人演示中没有提供这样的关系。此外，如果我们使用思维链提示来增强命令，模型甚至可以进行更复杂的语义推理，例如找出哪个物体适合用作临时的锤子（一块石头），或者哪种类型的饮料最适合疲劳的人（能量饮料）。
我们的主要贡献是RT-2，这是一系列从在网络规模数据上进行训练的大型视觉-语言模型进行微调得到的模型，直接用作具有泛化能力和语义感知的机器人策略。我们的实验研究了在互联网数据和来自先前工作的带有指令注释的机器人轨迹上训练的多达550亿参数的模型（Brohan等，2022）。在6000次机器人评估过程中，我们展示了RT-2能够显著提高对物体、场景和指令的泛化能力，并且表现出从网络规模的视觉-语言预训练中继承而来的广泛新兴能力。
Related Work 视觉-语言模型 视觉-语言模型（VLMs）可以分为几个类别（Gan等，2022），其中可能有两个最相关的类别：（1）表示学习模型，例如CLIP（Radford等，2021），它们学习了两种模态的共同嵌入，以及（2）形式为{视觉，文本} → {文本}的视觉语言模型，它们学习以视觉和语言作为输入，并提供自由格式的文本输出。这两个类别已被用于为各种应用于下游应用程序的预训练，如物体分类（Radford等，2021）、检测（Gu等，2021）和分割（Ghiasi等，2021）。在这项工作中，我们关注后一类别（Alayrac等，2022；Chen等，2023a,b；Driess等，2023；Hao等，2022；Li等，2023, 2019；Lu等，2019）。这些模型通常在许多不同的任务上进行训练，例如图像字幕生成、视觉问答（VQA）以及多个数据集上的通用语言任务。虽然先前的研究在广泛的问题和设置中研究了VLMs，包括在机器人技术中，但我们的重点是如何将VLMs的能力扩展到机器人闭环控制，赋予它们预测机器人动作的能力，从而利用VLMs中已经存在的知识，实现新水平的泛化能力。
机器人学习中的泛化 在机器人研究中，开发能够在各种情景中广泛成功的机器人控制器是一个长期存在的目标（Kaelbling，2020；Smith和Coles，1973）。在机器人操作中实现泛化的一个有希望的方法是从大规模和多样化的数据集中学习（Dasari等，2019；Levine等，2018；Pinto和Gupta，2016）。通过这样做，先前的方法已经证明了机器人如何能够对新的物体实例（Finn和Levine，2017；Levine等，2018；Mahler等，2017；Pinto和Gupta，2016；Young等，2021），对涉及新对象和技能组合的任务（Dasari和Gupta，2021；Finn等，2017；James等，2018；Jang等，2021；Yu等，2018），对新目标或语言指令（Jang等，2021；Jiang等，2022；Liu等，2022；Mees等，2022；Nair等，2022a；Pong等，2019），对新的语义物体类别的任务（Shridhar等，2021；Stone等，2023），以及对未见环境（Cui等，2022；Du等，2023a；Hansen等，2020）进行泛化。与大多数这些先前的工作不同，我们的目标是开发和研究一个单一模型，能够在所有这些方面泛化到未见条件。我们方法的关键要素之一是利用预训练模型，这些模型已经接触到比机器人所见的数据更广泛的数据。
预训练机器人操作 预训练在机器人学习中有着悠久的历史。大多数工作都集中在预训练的视觉表示上，这些表示可以用来初始化机器人摄像头观测的编码器，可以通过监督的ImageNet分类（Shah和Kumar，2021）、数据增强（Kostrikov等，2020；Laskin等，2020a,b；Pari等，2021）或专门用于机器人控制的目标（Karamcheti等，2023；Ma等，2022；Majumdar等，2023b；Nair等，2022b；Xiao等，2022b）来实现。其他工作已经将预训练的语言模型纳入，通常是作为指令编码器（Brohan等，2022；Hill等，2020；Jang等，2021；Jiang等，2022；Lynch和Sermanet，2020；Nair等，2022a；Shridhar等，2022b）或用于高层规划（Ahn等，2022；Driess等，2023；Huang等，2022；Mu等，2023；Singh等，2023；Wu等，2023）。与使用预训练视觉模型或预训练语言模型不同，我们特别考虑使用预训练的视觉-语言模型（VLMs），这些模型为世界提供了丰富的、基于现实的知识。先前的工作已经研究了VLMs在机器人技术中的使用（Driess等，2023；Du等，2023b；Gadre等，2022；Karamcheti等，2023；Shah等，2023；Shridhar等，2021；Stone等，2023），并且为本工作提供了灵感。这些先前的方法使用VLMs用于视觉状态表示（Karamcheti等，2023），用于识别物体（Gadre等，2022；Stone等，2023），用于高级规划（Driess等，2023），或用于提供监督或成功检测（Du等，2023b；Ma等，2023；Sumers等，2023；Xiao等，2022a；Zhang等，2023）。虽然CLIPort（Shridhar等，2021）和MOO（Stone等，2023）将预训练的VLMs整合到端到端的视觉运动控制策略中，但两者都将重要结构纳入策略中，从而限制了其适用性。值得注意的是，我们的工作不依赖于受限制的二维动作空间，也不需要校准的摄像头。此外，一个关键区别在于，与这些工作不同，我们利用了生成语言的VLMs，并且我们的统一输出空间使得模型权重可以在语言和动作任务之间完全共享，而不需要引入仅针对动作的模型层组件。
Vision-Language-Action Models 在本节中，我们介绍我们的模型系列以及使训练VLMs能够直接进行闭环机器人控制的设计选择。首先，我们描述了我们模型的总体架构，以及它们如何从通常用于视觉-语言任务的模型中派生出来。然后，我们介绍了将在网络规模数据上预训练的大型VLMs进行微调的方法和挑战，使其能够直接输出机器人动作，成为VLA模型。最后，我们描述了如何使这些模型在机器人任务中变得实用，解决了模型大小和推理速度方面的挑战，以实现实时控制。
预训练视觉-语言模型 我们在本研究中所使用的视觉-语言模型（Chen等，2023a；Driess等，2023）将一个或多个图像作为输入，并生成一系列标记，通常代表自然语言文本。这样的模型可以执行广泛的视觉解释和推理任务，从推断图像的构成到回答关于单个对象及其与其他对象关系的问题（Alayrac等，2022；Chen等，2023a；Driess等，2023；Huang等，2023）。代表执行如此广泛任务所需的知识需要大型模型和网络规模的数据集。在本研究中，我们对两个先前提出的VLMs进行了调整，使其能够充当VLA模型：PaLI-X（Chen等，2023a）和PaLM-E（Driess等，2023）。我们将这些模型的视觉-语言-动作版本称为RT-2-PaLI-X和RT-2-PaLM-E。我们利用了这些模型的实例，其参数数量从数十亿到数百亿不等。我们在附录D中详细描述了这两个模型的架构。
机器人动作微调 为了使视觉-语言模型能够控制机器人，它们必须被训练为输出动作。我们采用了一种直接的方法来解决这个问题，将动作表示为模型输出中的标记，这些标记与语言标记相同对待。我们的动作编码基于Brohan等人（2022）在RT-1模型中提出的离散化方法。动作空间包括机器人末端执行器的6自由度位置和旋转位移，以及机器人夹持器的伸展程度和特殊的离散终止命令(用于终止机器人行为)。这应该由策略触发以表示成功完成。连续的维度（除了离散终止命令外的所有维度）被均匀地离散成256个箱子。因此，机器人动作可以用离散箱子的序数表示为8个整数。为了将这些离散化动作用于对视觉-语言进行微调以创建视觉-语言-动作模型，我们需要将模型现有标记的标记与离散动作箱子关联起来。这需要预留256个标记作为动作标记。选择哪些标记取决于每个VLM使用的特定标记化方法，我们稍后在本节中讨论这一点。为了为VLM微调定义一个目标，我们通过简单地将每个维度的动作标记与空格字符连接起来，将动作向量转换为单个字符串：
这样一个目标的可能实例化为：“1 128 91 241 5 101 127”。我们在实验中微调的两个VLMs，PaLI-X（Chen等，2023a）和PaLM-E（Driess等，2023），使用不同的标记化。对于PaLI-X，整数最多到1000，每个都有一个唯一的标记，因此我们只需将动作箱子与表示相应整数的标记相关联。对于PaLM-E模型，它没有提供这种方便的数字表示，我们只需覆盖256个最不常用的标记，以表示动作词汇。值得注意的是，训练VLMs覆盖现有标记以动作标记是符号调谐（Wei等，2023）的一种形式，在先前的工作中已被证明对VLMs非常有效。
采用上述所述的动作表示，我们将我们的机器人数据转换为适合VLM模型微调的格式，其中我们的输入包括机器人摄像头图像和文本任务描述（使用标准的视觉问答格式“Q：机器人应该采取什么动作来[任务说明]？A：”），我们的输出被格式化为表示机器人动作的数字/最不常用标记的字符串。
共同微调。正如我们将在实验中展示的那样，改善机器人性能的训练方法的一个关键技术细节是将机器人数据与原始网络数据一起进行共同微调，而不是仅在机器人数据上进行朴素微调。我们注意到，共同微调可以带来更具有泛化性的策略，因为在微调过程中，策略不仅暴露于来自网络规模数据的抽象视觉概念，还暴露于低级机器人动作，而不仅仅是机器人动作。在共同微调中，我们通过增加机器人数据集的采样权重来平衡每个训练批次中机器人和网络数据的比例。
输出约束。RT -2与标准的VLMs之间一个重要的区别是，RT -2需要输出可在真实机器人上执行的有效动作标记。因此，为了确保在解码过程中RT -2输出有效的动作标记，我们通过在模型被提示进行机器人动作任务时只对有效的动作标记进行采样来限制其输出词汇表，而在标准视觉-语言任务中，模型仍然可以输出完整范围的自然语言标记。
实时推理 现代VLMs的规模可以达到数百亿甚至数千亿个参数（Chen等，2023a；Driess等，2023）。本研究中训练的最大模型使用了550亿个参数。在通常用于实时机器人控制的标准台式机或机器人GPU上直接运行这样的模型是不可行的。据我们所知，我们的模型是有史以来用于直接闭环机器人控制的规模最大的模型，超过了一个数量级，因此需要一组新的解决方案来实现高效的实时推理。我们制定了一个协议，允许我们在多TPU云服务中部署RT-2模型，并通过网络查询此服务以在机器人上运行。通过这种解决方案，我们可以实现合适的控制频率，并且使用同一云服务为多个机器人提供服务。我们评估的最大模型是55B参数的RT-2-PaLI-X-55B模型，可以以1-3赫兹的频率运行。该模型的较小版本，由50亿个参数组成，可以以约5赫兹的频率运行。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d6469e1ffb977fcb6267e5cf6de1ab52/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-27T23:51:10+08:00" />
<meta property="article:modified_time" content="2023-08-27T23:51:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">RT-2(robotics-transformer2)论文翻译——1</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4>说明：</h4> 
<p>ChatGPT等基于Transformer的大语言模型(LLM)的成功带给了人们巨大的震撼，其表现的few-shot甚至zero-shot能力仿佛让人们看到了通用人工智能(AGI)的曙光。而CLIP等跨模态模型打通了NLP和CV的界限，这些VLM模型将AGI发展继续向前推进。</p> 
<p>AGI在机器人领域同样有很大的提升空间。试想如果机器人拥有了LLM般的思考能力，那这个世界将变得多么智能！届时的机器人不是只能机械执行预先设定好的程序，而是能够真的读懂人类，和人类自然交互。</p> 
<p>RT2是谷歌deepmind团队今年7月底结合VLM和机器人控制技术研发的横跨语言-视觉-动作三模态的大模型。真正赋予了机器人的泛化能力！</p> 
<p>——————————————————————————————————————————</p> 
<p>本文包括论文的摘要、介绍、相关工作及模型介绍；论文其他部分翻译和解读见本专栏其他文章。翻译仅供参考。</p> 
<h4>论文地址：<a href="https://arxiv.org/pdf/2307.15818.pdf" rel="nofollow" title="arxiv.org/pdf/2307.15818.pdf">arxiv.org/pdf/2307.15818.pdf</a></h4> 
<h4>翻译：ChatGPT</h4> 
<p></p> 
<h2>Abstract</h2> 
<p>我们研究了在互联网规模数据上训练的视觉-语言模型如何直接融入端到端的机器人控制中，以提升泛化能力并实现新兴的语义推理。我们的目标是使单一的端到端训练模型既能够学习将机器人的观测映射到动作，又能够享受来自网络的大规模语言和视觉-语言数据的好处。为此，我们提出了将最先进的视觉-语言模型在机器人轨迹数据和互联网规模的视觉-语言任务（如视觉问答）上进行协同微调的方法。与其他方法不同的是，我们提出了一个简单通用的方法来实现这个目标：为了将自然语言回复和机器人动作都适应相同的格式，我们将动作表达为文本标记，并将其与自然语言标记一样直接并入模型的训练集中。<strong>我们将这类模型称为视觉-语言-动作模型（VLA），并且实例化了一个这样的模型，我们称之为RT-2。</strong>我们广泛的评估（6,000个评估试验）显示，我们的方法导致了高性能的机器人策略，并使RT-2能够从互联网规模的训练中获得一系列新兴的能力。其中包括对新颖对象的显著改进的泛化能力，解释机器人训练数据中不存在的命令（例如将物体放在特定的数字或图标上），以及能够对用户命令做出基本推理（例如拾取最小或最大的物体，或距离另一个物体最近的物体）。我们进一步展示，引入思维链推理使得RT-2能够进行多阶段的语义推理，例如找出哪个物体适合用作临时的锤子（一块石头），或者哪种类型的饮料最适合疲劳的人（能量饮料）。</p> 
<h2>Introduction</h2> 
<p>在广泛的网络规模数据集上预训练的高容量模型为各种下游任务提供了一个有效且强大的平台：大型语言模型不仅可以实现流畅的文本生成（Anil等，2023；Brohan等，2022；OpenAI，2023），还能够实现新兴的问题解决（Cobbe等，2021；Lewkowycz等，2022；Polu等，2022）和创造性的散文（Brown等，2020；OpenAI，2023）和代码生成（Chen等，2021），而<strong>视觉-语言模型可以实现开放词汇的视觉识别</strong>（Kirillov等，2023；Minderer等，2022；Radford等，2021），甚至可以对<strong>图像中的物体-代理交互进行复杂推断</strong>（Alayrac等，2022；Chen等，2023a,b；Driess等，2023；Hao等，2022；Huang等，2023；Wang等，2022）。这种语义推理、问题解决和视觉解释能力对于必须在现实环境中执行各种任务的通用型机器人将非常有用。然而，机器人应该如何获得这些能力尚不清楚。虽然一种蛮力方法可能涉及收集数百万次机器人交互试验，但最能胜任的语言和视觉-语言模型是在来自网络的数十亿标记和图像上进行训练的（Alayrac等，2022；Chen等，2023a,b；Huang等，2023）——在不久的将来，机器人数据不太可能达到这个数量级。另一方面，直接将这些模型应用于机器人任务也很困难：这些模型推理语义、标签和文本提示，而机器人需要基于实际的低级行动，比如笛卡尔末端执行器命令。尽管近期有许多工作试图将语言模型（LLMs）和视觉-语言模型（VLMs）融入机器人技术中（Ahn等，2022；Driess等，2023；Vemprala等，2023），但这些方法通常只涉及机器人规划的“高级”方面，本质上扮演着<strong>解释命令并将其解析为单独基元（如拾取和放置物体）的状态机的角色，这些基元然后由独立的低级控制器执行，而这些控制器本身在训练过程中并未从互联网规模模型的丰富语义知识中受益</strong>。因此，在本文中，我们提出了一个问题：是否可以将大型预训练的视觉-语言模型直接集成到低级机器人控制中，以提升泛化能力并实现新兴的语义推理？</p> 
<p>为了实现这一目标，我们探索了一个既简单又出奇有效的方法：我们直接<strong>训练设计用于开放词汇视觉问答和视觉对话的视觉-语言模型，使其输出低级机器人动作</strong>，并解决其他互联网规模的视觉-语言任务。尽管这些模型通常是针对生成自然语言标记进行训练的，但我们可以通过将动作标记为文本标记，并创建“多模态句子”（Driess等，2023），通过将动作与摄像头观测相配对来“响应”机器人指令，从而训练它们在机器人轨迹上。这种方式，视觉-语言模型可以直接被训练成为遵循指令的机器人策略。这种简单方法与将VLMs纳入机器人策略的先前替代方法（Shridhar等，2022a），或者从头开始设计新的视觉-语言-动作架构（Reed等，2022）形成了鲜明对比：相反，现有的视觉-语言模型，已经经过了大量的计算投入，可以在不引入任何新参数的情况下进行训练，以输出文本编码的动作。我们将这类模型称为视觉-语言-动作（VLA）模型。我们通过在RT-1（Brohan等，2022）提出的协议基础上进行扩展来实例化VLA模型，使用类似的数据集，但将模型扩展为使用大型视觉-语言主干。因此，我们将我们的模型称为RT-2。我们在图1中提供了一个概览。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ef/76/RH5bZ6tW_o.png"></p> 
<p>图1：<span style="color:#0d0016;">左侧是利用互联网上的VQA(视觉问答数据)，但是把回答的内容从自然语言替换成了动作的表示。中间是RT-2模型架构，它的核心是一个transformer，视觉图像通过ViT模型进行输入；输出是机器人的一系列动作token并解码表示为动作。右侧是最终的机器人闭环控制效果。 </span></p> 
<p>我们观察到，从这些视觉-语言模型衍生出的机器人策略表现出一系列引人注目的能力，将从机器人数据中学到的物理运动能力与从网络数据中学到的图像和文本解释能力融合到一个单一的模型中。除了显著提高<strong>对新颖物体和语义多样指令的泛化能力</strong>的预期好处外，我们还观察到一些新兴的能力。虽然模型的物理技能仍然局限于机器人数据中看到的技能分布，但模型通过使用从网络获取的知识来解释图像和语言命令，获得了将这些技能以新方式应用的能力。一些示例亮点如图2所示。模型能够将从机器人数据中学到的拾取和放置技能重新用于将物体放置在语义指示的位置附近，例如特定的数字或图标，尽管这些提示在机器人数据中不存在。模型还可以解释物体之间的关系，以确定选择哪个物体以及在哪里放置它，尽管机器人演示中没有提供这样的关系。此外，如果我们使用<strong>思维链提示来增强命令</strong>，模型甚至可以进行更复杂的语义推理，例如找出哪个物体适合用作临时的锤子（一块石头），或者哪种类型的饮料最适合疲劳的人（能量饮料）。</p> 
<p><img alt="" src="https://images2.imgbox.com/9a/08/zw7JSc52_o.png"></p> 
<p>我们的主要贡献是RT-2，这是一系列从在网络规模数据上进行训练的大型视觉-语言模型进行微调得到的模型，直接用作具有泛化能力和语义感知的机器人策略。我们的实验研究了在互联网数据和来自先前工作的带有指令注释的机器人轨迹上训练的多达550亿参数的模型（Brohan等，2022）。在6000次机器人评估过程中，我们展示了RT-2能够显著提高对物体、场景和指令的泛化能力，并且表现出从网络规模的视觉-语言预训练中继承而来的广泛新兴能力。</p> 
<h2>Related Work</h2> 
<h3>视觉-语言模型</h3> 
<p>视觉-语言模型（VLMs）可以分为几个类别（Gan等，2022），其中可能有两个最相关的类别：（1）表示学习模型，例如CLIP（Radford等，2021），它们学习了两种模态的共同嵌入，以及（2）形式为{视觉，文本} → {文本}的视觉语言模型，它们学习以视觉和语言作为输入，并提供自由格式的文本输出。这两个类别已被用于为各种应用于下游应用程序的预训练，如物体分类（Radford等，2021）、检测（Gu等，2021）和分割（Ghiasi等，2021）。在这项工作中，我们关注后一类别（Alayrac等，2022；Chen等，2023a,b；Driess等，2023；Hao等，2022；Li等，2023, 2019；Lu等，2019）。这些模型通常在许多不同的任务上进行训练，例如图像字幕生成、视觉问答（VQA）以及多个数据集上的通用语言任务。虽然先前的研究在广泛的问题和设置中研究了VLMs，包括在机器人技术中，但我们的重点是如何将VLMs的能力扩展到机器人闭环控制，赋予它们预测机器人动作的能力，从而利用VLMs中已经存在的知识，实现新水平的泛化能力。</p> 
<h3>机器人学习中的泛化</h3> 
<p>在机器人研究中，开发能够在各种情景中广泛成功的机器人控制器是一个长期存在的目标（Kaelbling，2020；Smith和Coles，1973）。在机器人操作中实现泛化的一个有希望的方法是从大规模和多样化的数据集中学习（Dasari等，2019；Levine等，2018；Pinto和Gupta，2016）。通过这样做，先前的方法已经证明了机器人如何能够对新的物体实例（Finn和Levine，2017；Levine等，2018；Mahler等，2017；Pinto和Gupta，2016；Young等，2021），对涉及新对象和技能组合的任务（Dasari和Gupta，2021；Finn等，2017；James等，2018；Jang等，2021；Yu等，2018），对新目标或语言指令（Jang等，2021；Jiang等，2022；Liu等，2022；Mees等，2022；Nair等，2022a；Pong等，2019），对新的语义物体类别的任务（Shridhar等，2021；Stone等，2023），以及对未见环境（Cui等，2022；Du等，2023a；Hansen等，2020）进行泛化。与大多数这些先前的工作不同，我们的目标是开发和研究一个单一模型，能够在所有这些方面泛化到未见条件。我们方法的关键要素之一是<strong>利用预训练模型，这些模型已经接触到比机器人所见的数据更广泛的数据。</strong></p> 
<h3>预训练机器人操作</h3> 
<p>预训练在机器人学习中有着悠久的历史。大多数工作都集中在预训练的视觉表示上，这些表示可以用来初始化机器人摄像头观测的编码器，可以通过监督的ImageNet分类（Shah和Kumar，2021）、数据增强（Kostrikov等，2020；Laskin等，2020a,b；Pari等，2021）或专门用于机器人控制的目标（Karamcheti等，2023；Ma等，2022；Majumdar等，2023b；Nair等，2022b；Xiao等，2022b）来实现。其他工作已经将预训练的语言模型纳入，通常是作为指令编码器（Brohan等，2022；Hill等，2020；Jang等，2021；Jiang等，2022；Lynch和Sermanet，2020；Nair等，2022a；Shridhar等，2022b）或用于高层规划（Ahn等，2022；Driess等，2023；Huang等，2022；Mu等，2023；Singh等，2023；Wu等，2023）。与使用预训练视觉模型或预训练语言模型不同，我们特别考虑使用预训练的视觉-语言模型（VLMs），这些模型为世界提供了丰富的、基于现实的知识。先前的工作已经研究了VLMs在机器人技术中的使用（Driess等，2023；Du等，2023b；Gadre等，2022；Karamcheti等，2023；Shah等，2023；Shridhar等，2021；Stone等，2023），并且为本工作提供了灵感。这些先前的方法使用VLMs用于视觉状态表示（Karamcheti等，2023），用于识别物体（Gadre等，2022；Stone等，2023），用于高级规划（Driess等，2023），或用于提供监督或成功检测（Du等，2023b；Ma等，2023；Sumers等，2023；Xiao等，2022a；Zhang等，2023）。虽然CLIPort（Shridhar等，2021）和MOO（Stone等，2023）将预训练的VLMs整合到端到端的视觉运动控制策略中，但两者都将重要结构纳入策略中，从而限制了其适用性。值得注意的是，我们的工作不依赖于受限制的二维动作空间，也不需要校准的摄像头。此外，一个关键区别在于，与这些工作不同，我们利用了生成语言的VLMs，并且我们的统一输出空间使得模型权重可以在语言和动作任务之间完全共享，而不需要引入仅针对动作的模型层组件。</p> 
<h2>Vision-Language-Action Models</h2> 
<p>在本节中，我们介绍我们的模型系列以及使训练VLMs能够直接进行闭环机器人控制的设计选择。首先，我们描述了我们模型的总体架构，以及它们如何从通常用于视觉-语言任务的模型中派生出来。然后，我们介绍了将在网络规模数据上预训练的大型VLMs进行微调的方法和挑战，使其能够直接输出机器人动作，成为VLA模型。最后，我们描述了如何使这些模型在机器人任务中变得实用，解决了模型大小和推理速度方面的挑战，以实现实时控制。</p> 
<h3>预训练视觉-语言模型</h3> 
<p>我们在本研究中所使用的视觉-语言模型（Chen等，2023a；Driess等，2023）将一个或多个图像作为输入，并生成一系列标记，通常代表自然语言文本。这样的模型可以执行广泛的视觉解释和推理任务，从推断图像的构成到回答关于单个对象及其与其他对象关系的问题（Alayrac等，2022；Chen等，2023a；Driess等，2023；Huang等，2023）。代表执行如此广泛任务所需的知识需要大型模型和网络规模的数据集。在本研究中，<strong>我们对两个先前提出的VLMs进行了调整，使其能够充当VLA模型：PaLI-X（Chen等，2023a）和PaLM-E</strong>（Driess等，2023）。我们将这些模型的视觉-语言-动作版本称为RT-2-PaLI-X和RT-2-PaLM-E。我们利用了这些模型的实例，其参数数量从数十亿到数百亿不等。我们在附录D中详细描述了这两个模型的架构。</p> 
<h3>机器人动作微调</h3> 
<p>为了使视觉-语言模型能够控制机器人，它们必须被训练为输出动作。我们采用了一种直接的方法来解决这个问题，将动作表示为模型输出中的标记，这些标记与语言标记相同对待。我们的动作编码基于Brohan等人（2022）在RT-1模型中提出的离散化方法。动作空间包括机器人末端执行器的6自由度位置和旋转位移，以及机器人夹持器的伸展程度和特殊的离散终止命令(用于终止机器人行为)。这应该由策略触发以表示成功完成。连续的维度（除了离散终止命令外的所有维度）被均匀地离散成256个箱子。因此，机器人动作可以用离散箱子的序数表示为8个整数。为了将这些离散化动作用于对视觉-语言进行微调以创建视觉-语言-动作模型，我们需要将模型现有标记的标记与离散动作箱子关联起来。这需要预留256个标记作为动作标记。选择哪些标记取决于每个VLM使用的特定标记化方法，我们稍后在本节中讨论这一点。为了为VLM微调定义一个目标，我们通过简单地将每个维度的动作标记与空格字符连接起来，将动作向量转换为单个字符串：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/76/f9/upQ93Unp_o.png"></p> 
<p></p> 
<p>这样一个目标的可能实例化为：“1 128 91 241 5 101 127”。我们在实验中微调的两个VLMs，PaLI-X（Chen等，2023a）和PaLM-E（Driess等，2023），使用不同的标记化。对于PaLI-X，整数最多到1000，每个都有一个唯一的标记，因此我们只需将动作箱子与表示相应整数的标记相关联。对于PaLM-E模型，它没有提供这种方便的数字表示，我们只需覆盖256个最不常用的标记，以表示动作词汇。值得注意的是，训练VLMs覆盖现有标记以动作标记是符号调谐（Wei等，2023）的一种形式，在先前的工作中已被证明对VLMs非常有效。</p> 
<p>采用上述所述的动作表示，我们将我们的机器人数据转换为适合VLM模型微调的格式，其中我们的输入包括机器人摄像头图像和文本任务描述（使用标准的视觉问答格式“Q：机器人应该采取什么动作来[任务说明]？A：”），我们的输出被格式化为表示机器人动作的数字/最不常用标记的字符串。</p> 
<p><strong>共同微调。</strong>正如我们将在实验中展示的那样，改善机器人性能的训练方法的一个关键技术细节是将机器人数据与原始网络数据一起进行共同微调，而不是仅在机器人数据上进行朴素微调。我们注意到，共同微调可以带来更具有泛化性的策略，因为在微调过程中，策略不仅暴露于来自网络规模数据的抽象视觉概念，还暴露于低级机器人动作，而不仅仅是机器人动作。在共同微调中，我们通过增加机器人数据集的采样权重来平衡每个训练批次中机器人和网络数据的比例。</p> 
<p><strong>输出约束。</strong>RT -2与标准的VLMs之间一个重要的区别是，RT -2需要输出可在真实机器人上执行的有效动作标记。因此，为了确保在解码过程中RT -2输出有效的动作标记，我们通过在模型被提示进行机器人动作任务时只对有效的动作标记进行采样来限制其输出词汇表，而在标准视觉-语言任务中，模型仍然可以输出完整范围的自然语言标记。</p> 
<h3>实时推理</h3> 
<p>现代VLMs的规模可以达到数百亿甚至数千亿个参数（Chen等，2023a；Driess等，2023）。本研究中训练的最大模型使用了550亿个参数。在通常用于实时机器人控制的标准台式机或机器人GPU上直接运行这样的模型是不可行的。据我们所知，我们的模型是有史以来用于直接闭环机器人控制的规模最大的模型，超过了一个数量级，因此需要一组新的解决方案来实现高效的实时推理。我们制定了一个协议，允许我们在多TPU云服务中部署RT-2模型，并通过网络查询此服务以在机器人上运行。通过这种解决方案，我们可以实现合适的控制频率，并且使用同一云服务为多个机器人提供服务。我们评估的最大模型是55B参数的RT-2-PaLI-X-55B模型，可以以1-3赫兹的频率运行。该模型的较小版本，由50亿个参数组成，可以以约5赫兹的频率运行。</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/420898f8d92191a6dd05948cae5b523c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">RBAC鉴权实验操作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e5939af41d5f842a1956265748522bc1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kafka学习笔记</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>