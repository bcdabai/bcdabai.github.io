<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>训练分类器为什么要用cross entropy loss（交叉熵损失函数）而不能用mean square error loss（MSE,最小平方差损失函数）? - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="训练分类器为什么要用cross entropy loss（交叉熵损失函数）而不能用mean square error loss（MSE,最小平方差损失函数）?" />
<meta property="og:description" content="在一个人工智能群里，有人问起，训练分类器为什么要用cross entropy loss（交叉熵损失函数）而不能用mean square error loss（MSE,最小平方差损失函数）呢?
正好，在我的那本《深度学习之美》（第11章）提及这个问题，于是复制了一部分内容，作为回答，群里的同学觉得通俗易懂，于是，把我的回答贴到这里，算是一个总结：
---------
对于多分类的标签（即教师信号），从本质上看，通过One-hot操作，就是把具体的标签（Label）空间，变换到一个概率测度空间（设为 p），如[1，0，0]（表示它是第一个品类）。可以这样理解这个概率，如果标签分类的标量输出为1（即概率为100%），其它值为0（即概率为0%）。
而对于多分类问题，在Softmax函数的“加工”下，神经网络的实际输出值就是一个概率向量，如[0.96, 0.04, 0]，设其概率分布为q。现在我们想衡量p和q之间的差异（即损失），一种简单粗暴的方式，自然是可以比较p和q的差值，如MSE（不过效果不好而已）[1]。
一种更好的方式是衡量这二者的概率分布的差异，就是交叉熵，因为它的设计初衷，就是要衡量两个概率分布之间的差异。
图1 Softmax输出层示意图 为什么要用softmax一下呢？exp函数是单调递增的，它能很好地模拟max的行为，而且它能让“大者更大”。其背后的潜台词则是让“小者更小”，这个有点类似“马太效应”，强者愈强、弱者愈弱。这个特性，对于分类来说尤为重要，它能让学习效率更高。
举例来说，在图1中，原始的分类分数（或者说特征值）是[4, 1, -2]，其中“4”和“1”的差值看起来没有那么大，但经过Softmax“渲染”之后，前者的分类概率接近于96%，而后者则仅在4%左右。而分值为“-2”的概率就“更惨”了，直接趋近于0。这正是Softmax回归的魅力所在。 这样一来，分类标签可以看做是概率分布（由one-hot变换而来），神经网络输出（经过softmax加工）也是一个概率分布，现在想衡量二者的差异（即损失），自然用交叉熵最好了。
参考文献：
[1] Golik P, Doetsch P, Ney H. Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison[C]// Interspeech. 2013:1756-1760.
节选自 张玉宏 《深度学习之美》第11章，电子工业出版社，博文视点，2018年6月出版" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/321499b2977f0d6db1942e835caaf67d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-05-10T06:37:45+08:00" />
<meta property="article:modified_time" content="2018-05-10T06:37:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">训练分类器为什么要用cross entropy loss（交叉熵损失函数）而不能用mean square error loss（MSE,最小平方差损失函数）?</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>在一个人工智能群里，有人问起，训练分类器为什么要用cross entropy loss（交叉熵损失函数）而不能用mean square error loss（MSE,最小平方差损失函数）呢?</p> 
<p>正好，在我的那本《深度学习之美》（第11章）提及这个问题，于是复制了一部分内容，作为回答，群里的同学觉得通俗易懂，于是，把我的回答贴到这里，算是一个总结：</p> 
<p></p> 
<p>---------</p> 
<p>对于多分类的标签（即教师信号），从本质上看，通过One-hot操作，就是把具体的标签（Label）空间，变换到一个概率测度空间（设为 p），如[1，0，0]（表示它是第一个品类）。可以这样理解这个概率，如果标签分类的标量输出为1（即概率为100%），其它值为0（即概率为0%）。</p> 
<p><br></p> 
<p>而对于多分类问题，在Softmax函数的“加工”下，神经网络的实际输出值就是一个概率向量，如[0.96, 0.04, 0]，设其概率分布为q。现在我们想衡量p和q之间的差异（即损失），一种简单粗暴的方式，自然是可以比较p和q的差值，如MSE（不过效果不好而已）[1]。</p> 
<p>一种更好的方式是衡量这二者的概率分布的差异，就是交叉熵，因为它的设计初衷，就是要衡量两个概率分布之间的差异。</p> 
<p><br></p> 
<p></p> 
<div style="text-align:center;"> 
 <img src="https://images2.imgbox.com/f7/e2/VDVwT2Ch_o.png" alt=""> 
</div> 
<div style="text-align:center;">
  图1  Softmax输出层示意图 
 <br> 
</div> 
<br> 
<p>为什么要用softmax一下呢？exp函数是单调递增的，它能很好地模拟max的行为，而且它能让“大者更大”。其背后的潜台词则是让“小者更小”，这个有点类似“马太效应”，强者愈强、弱者愈弱。这个特性，对于分类来说尤为重要，它能让学习效率更高。</p> 
<p><br></p>举例来说，在图1中，原始的分类分数（或者说特征值）是[4, 1, -2]，其中“4”和“1”的差值看起来没有那么大，但经过Softmax“渲染”之后，前者的分类概率接近于96%，而后者则仅在4%左右。而分值为“-2”的概率就“更惨”了，直接趋近于0。这正是Softmax回归的魅力所在。 
<br> 
<br> 
<p>这样一来，分类标签可以看做是概率分布（由one-hot变换而来），神经网络输出（经过softmax加工）也是一个概率分布，现在想衡量二者的差异（即损失），自然用交叉熵最好了。</p> 
<p>参考文献：</p> 
<p>[1] <span style="color:rgb(0,0,0);font-family:Arial;font-size:13px;background-color:rgb(255,255,255);">Golik P, Doetsch P, Ney H. Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison[C]// Interspeech. 2013:1756-1760.</span></p> 
<br> 
<p><span style="font-size:18px;"><span style="background-color:rgb(255,255,255);">节选自 张玉宏 <a href="https://item.jd.com/12382640.html" rel="nofollow">《深度学习之美》</a></span><span style="background-color:rgb(255,255,255);">第11章，电子工业出版社，博文视点，2018年6月出版</span></span><br></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2be0ce9e5736ab5e44fb5287c9d27e2a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Tensorflow入门1-CNN网络及MNIST例子讲解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9708f5c1b10c437aeafec34123ea1026/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">阿里云服端口无法访问，已解决</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>