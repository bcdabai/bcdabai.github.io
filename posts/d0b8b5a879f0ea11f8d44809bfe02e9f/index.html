<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Spring-Kafka 3.0 消费者消费失败处理方案 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Spring-Kafka 3.0 消费者消费失败处理方案" />
<meta property="og:description" content="一、背景 我们作为Kafka在使用Kafka是，必然考虑消息消费失败的重试次数，重试后仍然失败如何处理，要么阻塞，要么丢弃，或者保存
二、设置消费失败重试次数 1 默认重试次数在哪里看 Kafka3.0 版本默认失败重试次数为10次，准确讲应该是1次正常调用&#43;9次重试，这个在这个类可以看到 org.springframework.kafka.listener.SeekUtils
2 如何修改重试次数 据我的实验，spring-kafka3.0版本通过application.yml 配置是行不通的，也没有找到任何一项配置可以改重试次数的（网上很多说的通过配置spring.kafka.consumer.retries 可以配置，我尝试过了，至少3.0版本是不行的，如果有人成功试过可以通过application.yml 配置消费者的消费的重试次数可以留言通知我，谢谢）
经过我不懈努力和尝试，只能通过Java代码配置的方式才可以，并且这种方式相对于application.yml配置更加灵活细致，上代码
public CommonErrorHandler commonErrorHandler() { BackOff backOff = new FixedBackOff(5000L, 3L); return new DefaultErrorHandler(backOff); } 然后把这个handler 添加到ConcurrentKafkaListenerContainerFactory中就行了
三、设置消费失败处理方式 1 保存到数据库重试 我们需要在创建DefaultErrorHandler类时加入一个ConsumerAwareRecordRecoverer参数就可以了，这样在重试3次后仍然失败就会保存到数据库中，注意这里save to db成功之后，我认为没有必要执行consumer.commitSync方法，首先这个consumer.commitSync这个方法默认是提交当前批次的最大的offset（可能会导致丢失消息），其次不提交Kafka的消费者仍然回去消费后面的消息，只要后面的消息，消费成功了，那么依然会提交offset，覆盖了这个offset
public CommonErrorHandler commonErrorHandler() { // 创建 FixedBackOff 对象 BackOff backOff = new FixedBackOff(5000L, 3L); DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler((ConsumerAwareRecordRecoverer) (record, consumer, exception) -&gt; { log.info(&#34;save to db &#34; &#43; record.value().toString()); }, backOff); return defaultErrorHandler; } 如果你硬要提交也可以试试下面这种，指定提交当前的offset" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d0b8b5a879f0ea11f8d44809bfe02e9f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-26T01:12:39+08:00" />
<meta property="article:modified_time" content="2024-01-26T01:12:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Spring-Kafka 3.0 消费者消费失败处理方案</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>一、背景</h3> 
<p>我们作为Kafka在使用Kafka是，必然考虑消息消费失败的重试次数，重试后仍然失败如何处理，要么阻塞，要么丢弃，或者保存</p> 
<h3>二、设置消费失败重试次数</h3> 
<h4>1 默认重试次数在哪里看</h4> 
<p>Kafka3.0 版本默认失败重试次数为10次，准确讲应该是1次正常调用+9次重试，这个在这个类可以看到 org.springframework.kafka.listener.SeekUtils</p> 
<p><img alt="" height="384" src="https://images2.imgbox.com/73/6b/Ej1hK7gr_o.png" width="1200"></p> 
<h4>2 如何修改重试次数</h4> 
<p>据我的实验，spring-kafka3.0版本通过application.yml 配置是行不通的，也没有找到任何一项配置可以改重试次数的（网上很多说的通过配置spring.kafka.consumer.retries 可以配置，我尝试过了，至少3.0版本是不行的，如果有人成功试过可以通过application.yml 配置消费者的消费的重试次数可以留言通知我，谢谢）</p> 
<p>经过我不懈努力和尝试，只能通过Java代码配置的方式才可以，并且这种方式相对于application.yml配置更加灵活细致，上代码</p> 
<pre><code class="language-java">    public CommonErrorHandler commonErrorHandler() {
        BackOff backOff = new FixedBackOff(5000L, 3L);
        return  new DefaultErrorHandler(backOff);
    }</code></pre> 
<p>然后把这个handler 添加到ConcurrentKafkaListenerContainerFactory中就行了</p> 
<h3>三、设置消费失败处理方式</h3> 
<h4>1 保存到数据库重试</h4> 
<p>我们需要在创建DefaultErrorHandler类时加入一个ConsumerAwareRecordRecoverer参数就可以了，这样在重试3次后仍然失败就会保存到数据库中，注意这里save to db成功之后，我认为没有必要执行consumer.commitSync方法，首先这个consumer.commitSync这个方法默认是提交当前批次的最大的offset（可能会导致丢失消息），其次不提交Kafka的消费者仍然回去消费后面的消息，只要后面的消息，消费成功了，那么依然会提交offset，覆盖了这个offset</p> 
<pre><code class="language-java">    public CommonErrorHandler commonErrorHandler() {
        // 创建 FixedBackOff 对象
        BackOff backOff = new FixedBackOff(5000L, 3L);
        DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler((ConsumerAwareRecordRecoverer) (record, consumer, exception) -&gt; {
            log.info("save to db " + record.value().toString());
        }, backOff);
        return defaultErrorHandler;
    }</code></pre> 
<p>如果你硬要提交也可以试试下面这种，指定提交当前的offset</p> 
<pre><code class="language-java">    public CommonErrorHandler commonErrorHandler() {
        // 创建 FixedBackOff 对象
        BackOff backOff = new FixedBackOff(5000L, 3L);
        DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler((ConsumerAwareRecordRecoverer) (record, consumer, exception) -&gt; {
            log.info("save to db " + record.value().toString());
            Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();
            offsets.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset()));
            consumer.commitSync(offsets);
        }, backOff);
        return defaultErrorHandler;
    }</code></pre> 
<h4>2 发送到Kafka死信队列</h4> 
<p>仍然在创建DefaultErrorHandler类时加入一个DeadLetterPublishingRecoverer 类就行了，默认会把消息发到kafkaTemplate 配置的topic名字为your_topic+.DLT</p> 
<pre><code class="language-java">@Autowired
private KafkaTemplate&lt;String, String&gt; kafkaTemplate;
   

 public CommonErrorHandler commonErrorHandler() {
        // 创建 FixedBackOff 对象
        BackOff backOff = new FixedBackOff(5000L, 3L);

        DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler(new DeadLetterPublishingRecoverer(kafkaTemplate), backOff);

        return defaultErrorHandler;
    }</code></pre> 
<pre>ConsumerRecordRecoverer 接口总共就这2种实现方式</pre> 
<p><img alt="" height="127" src="https://images2.imgbox.com/f2/5c/JtPySNX9_o.png" width="963"></p> 
<h3>四、整体消费者代码粘贴</h3> 
<h4>1 application.yml</h4> 
<pre><code class="language-java">kafka-consumer:
  bootstrapServers: 192.168.31.114:9092
  groupId: goods-center
  #后台的心跳线程必须在30秒之内提交心跳,否则会reBalance
  sessionTimeOut: 30000
  autoOffsetReset: latest
  #取消自动提交,即便如此 spring会帮助我们自动提交
  enableAutoCommit: false
  #自动提交间隔
  autoCommitInterval: 1000
  #拉取的最小字节
  fetchMinSize: 1
  #拉去最小字节的最大等待时间
  fetchMaxWait: 500
  maxPollRecords: 50
  #300秒的提交间隔,如果程序大于300秒提交,会报错
  maxPollInterval: 300000
  #心跳间隔
  heartbeatInterval: 10000
  keyDeserializer: org.apache.kafka.common.serialization.LongDeserializer
  valueDeserializer: org.springframework.kafka.support.serializer.JsonDeserializer</code></pre> 
<h4>2 KafkaListenerProperties</h4> 
<pre><code class="language-java">package com.ychen.goodscenter.fafka;

import lombok.Getter;
import lombok.Setter;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

@Configuration
//指定配置文件的前缀
@ConfigurationProperties(prefix = "kafka-consumer")
@Getter
@Setter
public class KafkaListenerProperties {
 
    private String groupId;
 
    private String sessionTimeOut;
 
    private String bootstrapServers;
 
    private String autoOffsetReset;
 
    private boolean enableAutoCommit;
 
    private String autoCommitInterval;
 
    private String fetchMinSize;
 
    private String fetchMaxWait;
 
    private String maxPollRecords;
 
    private String maxPollInterval;
 
    private String heartbeatInterval;
 
    private String keyDeserializer;
 
    private String valueDeserializer;
 
}</code></pre> 
<h4>3 KafkaConsumerConfig</h4> 
<pre><code class="language-java">package com.ychen.goodscenter.fafka;

import com.alibaba.fastjson2.JSONObject;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.context.properties.EnableConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.config.KafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.*;
import org.springframework.util.backoff.BackOff;
import org.springframework.util.backoff.FixedBackOff;

import java.util.HashMap;
import java.util.Map;

@Configuration
@EnableConfigurationProperties(KafkaListenerProperties.class)
@Slf4j
public class KafkaConsumerConfig {
    @Autowired
    private KafkaListenerProperties kafkaListenerProperties;

    @Autowired
    private KafkaTemplate&lt;String, String&gt; kafkaTemplate;

    @Bean
    public KafkaListenerContainerFactory&lt;ConcurrentMessageListenerContainer&lt;String, String&gt;&gt; kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory&lt;String, String&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();
        factory.setConsumerFactory(consumerFactory());
        // 并发数 多个微服务实例会均分
        factory.setConcurrency(2);
//        factory.setBatchListener(true);
        factory.setCommonErrorHandler(commonErrorHandler());

        ContainerProperties containerProperties = factory.getContainerProperties();
        // 是否设置手动提交
        containerProperties.setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);

        return factory;
    }


    private ConsumerFactory&lt;String, String&gt; consumerFactory() {
        Map&lt;String, Object&gt; consumerConfigs = consumerConfigs();
        log.info("消费者的配置信息:{}", JSONObject.toJSONString(consumerConfigs));
        return new DefaultKafkaConsumerFactory&lt;&gt;(consumerConfigs);
    }


    public CommonErrorHandler commonErrorHandler() {
        // 创建 FixedBackOff 对象
        BackOff backOff = new FixedBackOff(5000L, 3L);

        DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler(new DeadLetterPublishingRecoverer(kafkaTemplate), backOff);
//        DefaultErrorHandler defaultErrorHandler = new DefaultErrorHandler((ConsumerAwareRecordRecoverer) (record, consumer, exception) -&gt; {
//            log.info("save to db " + record.value().toString());
//            Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();
//            offsets.put(new TopicPartition(record.topic(),record.partition()),new OffsetAndMetadata(record.offset()));
//            consumer.commitSync(offsets);
//        }, backOff);
        return defaultErrorHandler;
    }

    public Map&lt;String, Object&gt; consumerConfigs() {
        Map&lt;String, Object&gt; propsMap = new HashMap&lt;&gt;();
        // 服务器地址
        propsMap.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaListenerProperties.getBootstrapServers());
        // 是否自动提交
        propsMap.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, kafkaListenerProperties.isEnableAutoCommit());
        // 自动提交间隔
        propsMap.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, kafkaListenerProperties.getAutoCommitInterval());

        //会话时间
        propsMap.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, kafkaListenerProperties.getSessionTimeOut());
        //key序列化
        propsMap.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, kafkaListenerProperties.getKeyDeserializer());
        //value序列化
        propsMap.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, kafkaListenerProperties.getValueDeserializer());
        // 心跳时间
        propsMap.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, kafkaListenerProperties.getHeartbeatInterval());

        // 分组id
        propsMap.put(ConsumerConfig.GROUP_ID_CONFIG, kafkaListenerProperties.getGroupId());
        //消费策略
        propsMap.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, kafkaListenerProperties.getAutoOffsetReset());
        // poll记录数
        propsMap.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, kafkaListenerProperties.getMaxPollRecords());
        //poll时间
        propsMap.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, kafkaListenerProperties.getMaxPollInterval());

        propsMap.put("spring.json.trusted.packages", "com.ychen.**");

        return propsMap;
    }


}
</code></pre> 
<h4>4 MessageListener</h4> 
<pre><code class="language-java">package com.ychen.goodscenter.fafka;

import com.ychen.goodscenter.service.OrderService;
import com.ychen.goodscenter.vo.req.SubmitOrderReq;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.dao.DuplicateKeyException;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Component;

@Component
@Slf4j
public class MessageListener {
    @Autowired
    private OrderService orderService;

    @KafkaListener(topics = "order-message-topic", containerFactory = "kafkaListenerContainerFactory")
    public void processMessage(ConsumerRecord&lt;Long, SubmitOrderReq&gt; record, Acknowledgment acknowledgment) {
        log.info("order-message-topic message Listener, Thread ID: " + Thread.currentThread().getId());

        try {
            log.info("order-message-topic message received, orderId: {}", record.value().getOrderId());

            orderService.submitOrder(record.value());
            // 同步提交
            acknowledgment.acknowledge();
            log.info("order-message-topic message acked: orderId: {}", record.value().getOrderId());
        } catch (DuplicateKeyException dupe) {
            // 处理异常情况
            log.error("order-message-topic message error DuplicateKeyException", dupe);
            // 重复数据，忽略掉，同步提交
            acknowledgment.acknowledge();
        }
    }
}</code></pre> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a7c132e9ba17024f8288bb90ff395607/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">自然语言处理中的文本纠错</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a10b4f53ad6954effb06b16e1248790a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于本地缓存制作一个分库分表的分布式ID生成器</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>