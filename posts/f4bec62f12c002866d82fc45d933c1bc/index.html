<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>HDFS的Java API开发详解（文件的创建上传下载删除、IO流操作HDFS、小文件合并） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="HDFS的Java API开发详解（文件的创建上传下载删除、IO流操作HDFS、小文件合并）" />
<meta property="og:description" content="目录
一、准备工作
第一步：windows中的hadoop环境配置
第二步：创建maven工程并导入jar包
二、Java API开发实操
一、准备工作 windows操作系统需要配置一下hadoop环境
mac本质上是unix系统，不需要配置
第一步：windows中的hadoop环境配置 windows操作系统需要配置一下hadoop环境
mac本质上是unix系统，不需要配置
1. 解压Hadoop安装包压缩文件到一个没有中文没有空格的目录下，类似下图路径
2. 然后在windows当中配置hadoop的环境变量
3. bin、sbin目录添加到path中
4. 然后将下图中的hadoop.dll文件拷贝到C:\Windows\System32
5. 将Linux上安装部署好的hadoop集群的以下5个配置文件core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、slaves，拷贝到windows下hadoop的C:\hadoop-2.6.0-cdh5.14.0\etc\hadoop目录下
6. cmd中运行hadoop，出现如下效果
在Linux虚拟机中hdfs集群启动的前提下，cmd中运行hdfs dfs -ls /，查询出hdfs根目录的内容
注意：如果没有配置好windows的hadoop的环境变量，在windows下用IDEA编程时，会报以下错误
第二步：创建maven工程并导入jar包 由于cdh版本的所有的软件涉及版权的问题，所以并没有将所有的jar包托管到maven仓库当中去，而是托管在了CDH自己的服务器上面，所以我们默认去maven的仓库下载不到，需要自己手动的添加repository去CDH仓库进行下载，以下两个地址是官方文档说明，请仔细查阅
https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html
https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html
&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0-mr1-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f4bec62f12c002866d82fc45d933c1bc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-05T14:41:44+08:00" />
<meta property="article:modified_time" content="2020-08-05T14:41:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">HDFS的Java API开发详解（文件的创建上传下载删除、IO流操作HDFS、小文件合并）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C" rel="nofollow">一、准备工作</a></p> 
<p id="1%E3%80%81windows%E4%B8%AD%E7%9A%84hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-toc" style="margin-left:40px;"><a href="#1%E3%80%81windows%E4%B8%AD%E7%9A%84hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE" rel="nofollow">第一步：windows中的hadoop环境配置</a></p> 
<p id="2%E3%80%81%E5%88%9B%E5%BB%BAmaven%E5%B7%A5%E7%A8%8B%E5%B9%B6%E5%AF%BC%E5%85%A5jar%E5%8C%85-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E5%88%9B%E5%BB%BAmaven%E5%B7%A5%E7%A8%8B%E5%B9%B6%E5%AF%BC%E5%85%A5jar%E5%8C%85" rel="nofollow">第二步：创建maven工程并导入jar包</a></p> 
<p id="%E4%BA%8C%E3%80%81Java%20API%E5%BC%80%E5%8F%91%E5%AE%9E%E6%93%8D-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81Java%20API%E5%BC%80%E5%8F%91%E5%AE%9E%E6%93%8D" rel="nofollow">二、Java API开发实操</a></p> 
<hr id="hr-toc"> 
<h2 id="%E4%B8%80%E3%80%81%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">一、准备工作</h2> 
<ul><li> <p>windows操作系统需要配置一下hadoop环境</p> </li><li> <p>mac本质上是unix系统，不需要配置</p> </li></ul> 
<h3 id="1%E3%80%81windows%E4%B8%AD%E7%9A%84hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">第一步：windows中的hadoop环境配置</h3> 
<ul><li> <p>windows操作系统需要配置一下hadoop环境</p> </li><li> <p>mac本质上是unix系统，不需要配置</p> </li></ul> 
<p>1. 解压Hadoop<code>安装包</code>压缩文件到一个没有中文没有空格的目录下，类似下图路径</p> 
<p><img alt="" height="485" src="https://images2.imgbox.com/61/5b/M2QI9FsC_o.png" width="957"></p> 
<p>2. 然后在windows当中配置hadoop的环境变量</p> 
<p><img alt="" height="644" src="https://images2.imgbox.com/a9/b9/gCKfhF1L_o.png" width="795"></p> 
<p>3. bin、sbin目录添加到path中</p> 
<p><img alt="" height="776" src="https://images2.imgbox.com/93/00/YCzRa5pl_o.png" width="793"></p> 
<p>4. 然后将下图中的hadoop.dll文件拷贝到C:\Windows\System32</p> 
<p><img alt="" height="570" src="https://images2.imgbox.com/c8/47/DC9J5UB5_o.png" width="942"></p> 
<p>5. 将Linux上安装部署好的hadoop集群的以下5个配置文件<code>core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml、slaves</code>，拷贝到windows下hadoop的<code>C:\hadoop-2.6.0-cdh5.14.0\etc\hadoop</code>目录下</p> 
<p>6. cmd中运行<code>hadoop</code>，出现如下效果</p> 
<p><img alt="" height="583" src="https://images2.imgbox.com/db/c2/oAFJAPbu_o.png" width="959"></p> 
<p>在Linux虚拟机中hdfs集群启动的前提下，cmd中运行<code>hdfs dfs -ls /</code>，查询出hdfs根目录的内容</p> 
<p><img alt="" height="171" src="https://images2.imgbox.com/c8/fc/PtqfxPEl_o.png" width="863"></p> 
<p>注意：如果没有配置好windows的hadoop的环境变量，在windows下用IDEA编程时，会报以下错误</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e8/76/HQNEHNdq_o.png"></p> 
<h3 id="2%E3%80%81%E5%88%9B%E5%BB%BAmaven%E5%B7%A5%E7%A8%8B%E5%B9%B6%E5%AF%BC%E5%85%A5jar%E5%8C%85">第二步：创建maven工程并导入jar包</h3> 
<ul><li> <p>由于cdh版本的所有的软件涉及版权的问题，所以并没有将所有的jar包托管到maven仓库当中去，而是托管在了CDH自己的服务器上面，所以我们默认去maven的仓库下载不到，需要自己手动的添加repository去CDH仓库进行下载，以下两个地址是官方文档说明，请仔细查阅</p> <p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html" rel="nofollow">https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo.html</a></p> <p><a href="https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html" rel="nofollow">https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh5_maven_repo_514x.html</a></p> </li></ul> 
<pre><code> &lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
    &lt;/repository&gt;
 &lt;/repositories&gt;
 &lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;version&gt;2.6.0-mr1-cdh5.14.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;
    &lt;/dependency&gt;
 
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
        &lt;version&gt;2.6.0-cdh5.14.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- https://mvnrepository.com/artifact/junit/junit --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;junit&lt;/groupId&gt;
        &lt;artifactId&gt;junit&lt;/artifactId&gt;
        &lt;version&gt;4.11&lt;/version&gt;
        &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.testng&lt;/groupId&gt;
        &lt;artifactId&gt;testng&lt;/artifactId&gt;
        &lt;version&gt;RELEASE&lt;/version&gt;
    &lt;/dependency&gt;
 &lt;/dependencies&gt;
 &lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.0&lt;/version&gt;
            &lt;configuration&gt;
                &lt;source&gt;1.8&lt;/source&gt;
                &lt;target&gt;1.8&lt;/target&gt;
                &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                &lt;!--   &lt;verbal&gt;true&lt;/verbal&gt;--&gt;
            &lt;/configuration&gt;
        &lt;/plugin&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;phase&gt;package&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;shade&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;minimizeJar&gt;true&lt;/minimizeJar&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
 &lt;/build&gt;</code></pre> 
<h2 id="%E4%BA%8C%E3%80%81Java%20API%E5%BC%80%E5%8F%91%E5%AE%9E%E6%93%8D">二、Java API开发实操</h2> 
<pre><code class="language-java">package com.xsluo;

import org.apache.commons.io.IOUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

public class HdfsJavaAPI {

    public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException {
        HdfsJavaAPI o = new HdfsJavaAPI();
        //1.创建
        o.mkdirToHdfs();
        //2.上传
        o.uploadFile();
        //3.下载
        o.downloadFile();
        //4.删除
        o.deleteFile();
        //5.重命名
        o.renameFile();
        //6.查看文件的描述信息
        o.testlistFiles();
        //7.IO流上传
        o.ioPutFile();
        //8.IO流下载
        o.ioGetFile();
        //9.合并小文件
        o.mergeSmallFiles();
    }

    /**
     * 获取HDFS文件系统
     * @return
     * @throws IOException
     */
    public FileSystem getFileSystem() throws IOException {
        Configuration configuration = new Configuration();
        configuration.set("fs.defaultFS","hdfs://node01:8020");
        FileSystem fileSystem = FileSystem.get(configuration);
        return fileSystem;
    }

    /**
     * 创建文件夹
     * @throws IOException
     */
    public void mkdirToHdfs() throws IOException {
        FileSystem fileSystem = getFileSystem();
        boolean b = fileSystem.mkdirs(new Path("/xsluo/dir1"));//若目录已经存在，则创建失败，返回false
        if (b){
            System.out.println("/xsluo/dir1创建成功！");
        }else {
            System.out.println("/xsluo/dir1可能已存在，创建失败！");
        }
        fileSystem.close();
    }

    /**
     * 文件上传
     * @throws IOException
     */
    public void uploadFile() throws IOException {
        FileSystem fileSystem = getFileSystem();
        fileSystem.copyFromLocalFile(
                new Path("file:///F:\\testDatas\\a.txt"),
                new Path("hdfs://node01:8020/xsluo"));//hdfs路径也可以直接写成/xsluo/dir1
        fileSystem.close();
    }

    /**
     * 文件下载
     * @throws IOException
     */
    public void downloadFile() throws IOException {
        FileSystem fileSystem = getFileSystem();
        fileSystem.copyToLocalFile(new Path("hdfs://node01:8020/xsluo/a.txt"),new Path("file:///F:\\testDatas\\a2.txt"));
        fileSystem.close();
    }

    /**
     * 文件删除
     * @throws IOException
     */
    public void deleteFile() throws IOException {
        FileSystem fileSystem = getFileSystem();
        fileSystem.delete(new Path("hdfs://node01:8020/xsluo/b.txt"),true);
        fileSystem.close();
    }

    /**
     * 文件重命名
     * @throws IOException
     */
    public void renameFile() throws IOException {
        FileSystem fileSystem = getFileSystem();
        fileSystem.rename(new Path("/xsluo/a.txt"),new Path("/xsluo/a_new.txt"));
        fileSystem.close();
    }

    /**
     * 文件相关信息查看
     * @throws IOException
     */
    public void testlistFiles() throws IOException {
        FileSystem fileSystem = getFileSystem();
        //获取文件详情
        RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path("/xsluo"), true);
        while (listFiles.hasNext()){
            LocatedFileStatus status = listFiles.next();
            //输出详情
            System.out.println(status.getPath().getName());//文件名称
            System.out.println(status.getLen());//长度
            System.out.println(status.getPermission());//权限
            System.out.println(status.getOwner());//所属用户
            System.out.println(status.getGroup());//分组
            System.out.println(status.getModificationTime());//修改时间
            //获取存储的块信息
            BlockLocation[] blockLocations = status.getBlockLocations();
            for (BlockLocation blockLocation : blockLocations) {
                //获取块存储的主机节点
                String[] hosts = blockLocation.getHosts();
                for (String host : hosts) {
                    System.out.println(host);
                }
            }
        }
        fileSystem.close();
    }

    /**
     * IO流操作hdfs文件：上传
     * @throws IOException
     */
    public void ioPutFile() throws IOException {
        // 1 获取文件系统
        FileSystem fileSystem = getFileSystem();
        // 2 创建输入流；路径前不需要加file:///，否则报错
        FileInputStream fis = new FileInputStream(new File("F:\\testDatas\\hhh.txt"));
        // 3 创建输出流
        FSDataOutputStream fos = fileSystem.create(new Path("hdfs://node01:8020/xsluo/hhh.txt"));
        // 4 流对拷 org.apache.commons.io.IOUtils
        IOUtils.copy(fis,fos);
        // 5 关闭资源
        IOUtils.closeQuietly(fos);
        IOUtils.closeQuietly(fis);
        fileSystem.close();
    }

    /**
     * IO流操作hdfs文件：下载
     * @throws IOException
     */
    public void ioGetFile() throws IOException {
        // 1 获取文件系统
        FileSystem fileSystem = getFileSystem();
        // 2 创建输入流
        FSDataInputStream fis = fileSystem.open(new Path("/xsluo/a_new.txt"));
        // 3 创建输出流
        FileOutputStream fos = new FileOutputStream(new File("F:\\testDatas\\a_new.txt"));
        // 4 流对拷
        IOUtils.copy(fis,fos);
        // 5 关闭资源
        IOUtils.closeQuietly(fos);
        IOUtils.closeQuietly(fis);
        fileSystem.close();
    }

    /**
     * 小文件合并
     * @throws URISyntaxException
     * @throws IOException
     * @throws InterruptedException
     */
    public void mergeSmallFiles() throws URISyntaxException, IOException, InterruptedException {
        //获取分布式文件系统hdfs；第三个参数指定hdfs的用户
        FileSystem fileSystem = FileSystem.get(new URI("hdfs://node01:8020"), new Configuration(), "hadoop");
        FSDataOutputStream fsDataOutputStream = fileSystem.create(new Path("hdfs://node01:8020/xsluo/bigfile.txt"));

        //读取所有本地小文件，写入到hdfs的大文件里面去
        //获取本地文件系统 localFileSystem
        LocalFileSystem localFileSystem = FileSystem.getLocal(new Configuration());
        //读取本地的小文件们
        FileStatus[] fileStatuses = localFileSystem.listStatus(new Path("F:\\testDatas"));
        for (FileStatus fileStatus : fileStatuses) {
            //获取每一个本地小文件的路径
            Path path = fileStatus.getPath();
            //读取本地小文件
            FSDataInputStream fsDataInputStream = localFileSystem.open(path);
            IOUtils.copy(fsDataInputStream,fsDataOutputStream);
            IOUtils.closeQuietly(fsDataInputStream);
        }
        IOUtils.closeQuietly(fsDataOutputStream);
        localFileSystem.close();
        fileSystem.close();
    }

}

</code></pre> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6c1e4f0726f8b8cd3cecf580a3d4996f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux报错Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c43254f3ccdb18731aa258a288de4220/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用电脑远程操作Jetson nano终端3-更改linux权限</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>