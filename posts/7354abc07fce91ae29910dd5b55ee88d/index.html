<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>hive与sqoop - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="hive与sqoop" />
<meta property="og:description" content="hive：基于Hadoop的关系数据库离线同步到数仓的工具，将结构化数据映射成一张数据库表，实现数据的提取、转换、加载。
Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。
Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。
Hive 没有专门的数据格式。 Hive 可以很好的工作在 Thrift 之上，控制分隔符，也允许用户指定数据格式。
二.架构
1.用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是CLI，Cli启动的时候，会同时启动一个Hive副本。Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 WUI是通过浏览器访问Hive。
2.Hive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。
3.解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。 4.Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（包含*的及类似的查询除外，比如select * from tbl不会生成MapRedcue任务）。
sqoop导入有两种方式：
1）直接导入到hdfs，然后再load到hive表中
2）直接导入到hive中
一般生产情况导业务库数据，都是指定导入，不能全部导入，对业务库压力大。
1）需要已知的参数
mysql 的jdbc连接参数：jdbc链接、用户名、密码
mysql同步表的信息：列、表名
hive链接参数 ：是否有链接限制
hive目标表的信息：表名、分隔符、是否分区
2）具体参数
两种方式：1）指定mysql列导入 2）写相应的mysql的query sql语句导入
1）制定mysql表的某列、某表
#（必须参数）sqoop 导入，-D 指定参数，当前参数是集群使用队列名称
sqoop import -D mapred.job.queue.name=q #（必须参数）链接mysql jdbs参数 xxxx路径/yy库?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/7354abc07fce91ae29910dd5b55ee88d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-12T15:45:22+08:00" />
<meta property="article:modified_time" content="2021-04-12T15:45:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">hive与sqoop</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>hive：基于Hadoop的关系数据库离线同步到数仓的工具，将结构化数据映射成一张数据库表，实现数据的提取、转换、加载。<br> Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。</p> 
<p>Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。<br> 　　Hive 没有专门的数据格式。 Hive 可以很好的工作在 Thrift 之上，控制分隔符，也允许用户指定数据格式。<br> 二.架构<br> 　　1.用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是CLI，Cli启动的时候，会同时启动一个Hive副本。Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 WUI是通过浏览器访问Hive。</p> 
<p>2.Hive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</p> 
<p>3.解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。 　　4.Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（包含*的及类似的查询除外，比如select * from tbl不会生成MapRedcue任务）。</p> 
<p>sqoop导入有两种方式：<br> 1）直接导入到hdfs，然后再load到hive表中<br> 2）直接导入到hive中</p> 
<p>一般生产情况导业务库数据，都是指定导入，不能全部导入，对业务库压力大。<br> 1）需要已知的参数</p> 
<p>mysql 的jdbc连接参数：jdbc链接、用户名、密码<br> mysql同步表的信息：列、表名<br> hive链接参数 ：是否有链接限制<br> hive目标表的信息：表名、分隔符、是否分区<br> 2）具体参数<br> 两种方式：1）指定mysql列导入 2）写相应的mysql的query sql语句导入</p> 
<p>1）制定mysql表的某列、某表<br> #（必须参数）sqoop 导入，-D 指定参数，当前参数是集群使用队列名称<br> sqoop import -D mapred.job.queue.name=q <br> #（必须参数）链接mysql jdbs参数 xxxx路径/yy库?mysql的参数（多个参数用&amp;隔开）<br> #tinyInt1isBit=false这个参数 主要解决 从Sqoop导入MySQL导入TINYINT(1)类型数据到hive（tinyint），数据为null<br> –connect jdbc:mysql:xxxx/yy?tinyInt1isBit=false <br> #（必须参数）用户名、密码、具体的表<br> –username xx --password xx<br> –table xx <br> –delete-target-dir <br> #（非必须）系统默认是textfile格式，当然也可以是parquet<br> –as-textfile <br> #（非必须）指定想要的列<br> –columns ‘id,title’ <br> #（必须参数）导入到hive的参数<br> –hive-import <br> #（必须参数）指定分隔符，和hive 的目标表分隔符一致<br> –fields-terminated-by ‘\t’ <br> #（必须参数）hive的库表名<br> –hive-database xx <br> –hive-table xx <br> #（必须参数）是不是string 为null的都要变成真正为null<br> –null-string ‘\N’ --null-non-string ‘\N’ <br> #（非必须）写入到hive的分区信息,hive无分区无需这步<br> –hive-partition-key dt <br> –hive-partition-value $day <br> #写入hive的方式<br> –hive-overwrite <br> –num-mappers 1 <br> #（必须参数）导入到hive时删除 \n, \r, and \01<br> -hive-drop-import-delims <br> #sqoop完会生成java文件，可以指定文件的路径，方便删除和管理<br> –outdir xxx</p> 
<p>2）写查询sql 导入<br> #（必须参数）sqoop 导入，-D 指定参数，当前参数是集群使用队列名称<br> sqoop import -D mapred.job.queue.name=q <br> #（必须参数）链接mysql jdbs参数 xxxx路径/yy库?mysql的参数（多个参数用&amp;隔开）<br> #tinyInt1isBit=false这个参数 主要解决 从Sqoop导入MySQL导入TINYINT(1)类型数据到hive（tinyint），数据为null<br> –connect jdbc:mysql:xxxx/yy?tinyInt1isBit=false <br> #（必须参数）用户名、密码、具体的表<br> –username xx --password xx <br> –delete-target-dir <br> #（非必须）系统默认是textfile格式，当然也可以是parquet<br> –as-textfile <br> #（query sql最后要加上$CONDITIONS）<br> –query ‘select xxx from xxx where $CONDITIONS’ <br> #（必须参数）导入到hive的参数<br> –hive-import <br> #（必须参数）指定分隔符，和hive 的目标表分隔符一致<br> –fields-terminated-by ‘\t’ <br> #（必须参数）hive的库表名<br> –hive-database xx <br> –hive-table xx <br> #（必须参数）是不是string 为null的都要变成真正为null<br> –null-string ‘\N’ --null-non-string ‘\N’ <br> #（非必须）写入到hive的分区信息,hive无分区无需这步<br> –hive-partition-key dt <br> –hive-partition-value $day <br> #写入hive的方式<br> –hive-overwrite <br> –num-mappers 1 <br> #（必须参数）导入到hive时删除 \n, \r, and \01<br> -hive-drop-import-delims <br> #sqoop完会生成java文件，可以指定文件的路径，方便删除和管理<br> –outdir xxx</p> 
<p>https://blog.csdn.net/Vivi_static/article/details/107867085</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/080f22ec5ece5038561296aa144a23de/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java实现头像上传功能</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/88c63599688d502a0a73fe5f72f20d3f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">论文阅读笔记（6）: GNN-快速局部光谱滤波</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>