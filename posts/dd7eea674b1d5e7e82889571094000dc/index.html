<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>k8s集群性能优化之kubelet配置资源预留 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="k8s集群性能优化之kubelet配置资源预留" />
<meta property="og:description" content="本篇目录链接 疑问分析解决思路环境说明Kubelet Node Allocatable 介绍配置方式（基于yum安装）配置资源预留 Eviction 与 OOM可分配约束 疑问分析 在k8s集群中，默认情况下 Pod 能够使用节点全部可用容量，同样就会伴随一个新的问题，pod消耗的内存会挤占掉系统服务本身的资源，这就好比我们在宿主机上运行java服务一样，会出现java程序将宿主机上的资源（内存、cpu）耗尽，从而导致系统登陆不上或者卡顿现象。同理在k8s集群中也会出现类似的问题，从而也会导致一系列不可控的故障产生。因此为系统守护进程预留出部分资源，也是很有必要的。
解决思路 要解决这个问题就需要为 k8s 集群配置资源预留，kubelet 暴露了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源，k8s 也是推荐集群管理员按照每个节点上的工作负载来配置 Node Allocatable。
环境说明 本文的操作环境为 Kubernetes v1.18.0 版本，使用docker的容器运行时，docker 和 Kubelet 采用的 cgroup 驱动为 systemd。
Kubelet Node Allocatable 介绍 Kubelet Node Allocatable用来为Kube组件和System进程预留资源，从而保证当节点出现满负荷时也能保证Kube和System进程有足够的资源。目前支持cpu, memory, ephemeral-storage三种资源预留。
计算方式
Node Allocatable Resource = Node Capacity - Kube-reserved - system-reserved - eviction-threshold 其中：
Node Capacity：Node的所有硬件资源，kube-reserved是给kube组件预留的资源Kube-reserved：kube 组件预留的资源system-reserved：给system进程预留的资源eviction-threshold（阈值）：kubelet eviction(收回)的阈值设定 只有allocatable才是真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable） ，关系图如下：
配置方式（基于yum安装） 我们先通过 kubectl describe node 命令查看节点可分配资源的数据：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/dd7eea674b1d5e7e82889571094000dc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-14T22:09:38+08:00" />
<meta property="article:modified_time" content="2021-09-14T22:09:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">k8s集群性能优化之kubelet配置资源预留</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>本篇目录链接</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">疑问分析</a></li><li><a href="#_4" rel="nofollow">解决思路</a></li><li><a href="#_7" rel="nofollow">环境说明</a></li><li><a href="#Kubelet_Node_Allocatable__10" rel="nofollow">Kubelet Node Allocatable 介绍</a></li><li><a href="#yum_25" rel="nofollow">配置方式（基于yum安装）</a></li><li><ul><li><ul><li><a href="#_48" rel="nofollow">配置资源预留</a></li></ul> 
   </li></ul> 
   </li><li><a href="#Eviction__OOM_114" rel="nofollow">Eviction 与 OOM</a></li><li><a href="#_121" rel="nofollow">可分配约束</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>疑问分析</h3> 
<p>在k8s集群中，默认情况下 Pod 能够使用节点全部可用容量，同样就会伴随一个新的问题，pod消耗的内存会挤占掉系统服务本身的资源，这就好比我们在宿主机上运行java服务一样，会出现java程序将宿主机上的资源（内存、cpu）耗尽，从而导致系统登陆不上或者卡顿现象。同理在k8s集群中也会出现类似的问题，从而也会导致一系列不可控的故障产生。因此为系统守护进程预留出部分资源，也是很有必要的。</p> 
<h3><a id="_4"></a>解决思路</h3> 
<p>要解决这个问题就需要为 k8s 集群配置资源预留，kubelet 暴露了一个名为 Node Allocatable 的特性，有助于为系统守护进程预留计算资源，k8s 也是推荐集群管理员按照每个节点上的工作负载来配置 Node Allocatable。</p> 
<h3><a id="_7"></a>环境说明</h3> 
<p>本文的操作环境为 Kubernetes v1.18.0 版本，使用docker的容器运行时，docker 和 Kubelet 采用的 cgroup 驱动为 systemd。<br> <img src="https://images2.imgbox.com/f0/20/dqfhsuxU_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Kubelet_Node_Allocatable__10"></a>Kubelet Node Allocatable 介绍</h3> 
<p>Kubelet Node Allocatable用来为Kube组件和System进程预留资源，从而保证当节点出现满负荷时也能保证Kube和System进程有足够的资源。目前支持cpu, memory, ephemeral-storage三种资源预留。</p> 
<p>计算方式</p> 
<pre><code class="prism language-bash">Node Allocatable Resource <span class="token operator">=</span> Node Capacity - Kube-reserved - system-reserved - eviction-threshold
</code></pre> 
<p>其中：</p> 
<ul><li>Node Capacity：Node的所有硬件资源，kube-reserved是给kube组件预留的资源</li><li>Kube-reserved：kube 组件预留的资源</li><li>system-reserved：给system进程预留的资源</li><li>eviction-threshold（阈值）：kubelet eviction(收回)的阈值设定</li></ul> 
<p>只有allocatable才是真正scheduler调度Pod时的参考值（保证Node上所有Pods的request resource不超过Allocatable） ，关系图如下：<br> <img src="https://images2.imgbox.com/56/b1/fwJHl5AG_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="yum_25"></a>配置方式（基于yum安装）</h3> 
<p>我们先通过 kubectl describe node 命令查看节点可分配资源的数据：<br> kubectl describe nodes k8s-m1</p> 
<pre><code class="prism language-bash"><span class="token punctuation">..</span>.
Capacity:
  cpu:                <span class="token number">8</span>
  ephemeral-storage:  206292644Ki
  hugepages-1Gi:      <span class="token number">0</span>
  hugepages-2Mi:      <span class="token number">0</span>
  memory:             15731900Ki
  pods:               <span class="token number">110</span>
Allocatable:
  cpu:                <span class="token number">8</span>
  ephemeral-storage:  <span class="token number">190119300396</span>
  hugepages-1Gi:      <span class="token number">0</span>
  hugepages-2Mi:      <span class="token number">0</span>
  memory:             15629500Ki
  pods:               <span class="token number">110</span>
<span class="token punctuation">..</span>.
</code></pre> 
<p>可以看到其中有 Capacity 与 Allocatable 两项内容，其中的 Allocatable 就是节点可被分配的资源，我们这里没有配置资源预留，所以默认情况下 Capacity 与 Allocatable 的值基本上是一致的。</p> 
<h5><a id="_48"></a>配置资源预留</h5> 
<p>比如我们现在需要为系统预留一定的资源，我们可以使用如下的几个 kubelet 参数来进行配置：</p> 
<pre><code class="prism language-bash">--enforce-node-allocatable<span class="token operator">=</span>pods
--kube-reserved<span class="token operator">=</span>memory<span class="token operator">=</span><span class="token punctuation">..</span>.
--system-reserved<span class="token operator">=</span>memory<span class="token operator">=</span><span class="token punctuation">..</span>.
--eviction-hard<span class="token operator">=</span><span class="token punctuation">..</span>.
</code></pre> 
<p>这里我们暂时不设置对应的 cgroup，比如我们这里先只对 master1 节点添加资源预留，我们可以直接修改 /var/lib/kubelet/config.yaml 文件来动态配置 kubelet，添加如下所示的资源预留配置：</p> 
<pre><code class="prism language-bash">apiVersion: kubelet.config.k8s.io/v1beta1
<span class="token punctuation">..</span><span class="token punctuation">..</span><span class="token punctuation">..</span>
enforceNodeAllocatable:
 <span class="token number">1</span>. pods
kubeReserved:  <span class="token comment"># 配置 kube 资源预留</span>
  cpu: 500m
  memory: 1Gi
  ephemeral-storage: 1Gi
systemReserved:  <span class="token comment"># 配置系统资源预留</span>
  memory: 1Gi
evictionHard:  <span class="token comment"># 配置硬驱逐阈值</span>
  memory.available: <span class="token string">"300Mi"</span>
  nodefs.available: <span class="token string">"10%"</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/f8/c3/AWgzmBGr_o.png" alt="在这里插入图片描述"><br> 修改完成后，重启 kubelet，启动完成后重新对比 Capacity 及 Allocatable 的值：<br> kubectl describe nodes k8s-m1</p> 
<pre><code class="prism language-bash"><span class="token punctuation">..</span>.
Capacity:
  cpu:                <span class="token number">8</span>
  ephemeral-storage:  206292644Ki
  hugepages-1Gi:      <span class="token number">0</span>
  hugepages-2Mi:      <span class="token number">0</span>
  memory:             15731900Ki
  pods:               <span class="token number">110</span>
Allocatable:
  cpu:                7500m
  ephemeral-storage:  <span class="token number">189045558572</span>
  hugepages-1Gi:      <span class="token number">0</span>
  hugepages-2Mi:      <span class="token number">0</span>
  memory:             13327548Ki
  pods:               <span class="token number">110</span>
<span class="token punctuation">..</span>.
</code></pre> 
<p>仔细对比可以发现其中的 Allocatable的值恰好是 Capacity 减去上面我们配置的预留资源的值：</p> 
<pre><code class="prism language-bash">allocatale <span class="token operator">=</span> capacity - kube_reserved - system_reserved - eviction_hard
</code></pre> 
<pre><code class="prism language-bash">内存： 13327548Ki <span class="token operator">=</span> 15731900Ki - <span class="token number">1</span>*1024*1024Ki - <span class="token number">1</span>*1024*1024Ki - <span class="token number">300</span>*1024Ki
CPU： 7500m <span class="token operator">=</span> 8000m - 500m （单位为m是以CPU的时间分片计量的，1C为1000m）
</code></pre> 
<p>再通过查看 kubepods.slice（systemd 驱动是以 .slice 结尾）cgroup 中对节点上所有 Pod 内存的限制，该值决定了 Node 上所有的 Pod 能使用的资源上限：</p> 
<pre><code class="prism language-bash"><span class="token function">cat</span> /sys/fs/cgroup/memory/kubepods.slice/memory.limit_in_bytes
<span class="token number">13961981952</span>      <span class="token comment">#单位是byte</span>
</code></pre> 
<p>得到的 Pod 资源使用上限为：</p> 
<pre><code class="prism language-bash"><span class="token number">13961981952</span> Bytes <span class="token operator">=</span> <span class="token number">13634748</span> Ki <span class="token operator">=</span> Allocatable<span class="token punctuation">(</span><span class="token number">13327548</span> Ki<span class="token punctuation">)</span> + eviction_hard<span class="token punctuation">(</span><span class="token number">300</span>*1024Ki<span class="token punctuation">)</span>   <span class="token comment"># 13961981952 / 1024 = 13634748</span>
</code></pre> 
<p>也可以通过计算验证我们的配置是正确的：</p> 
<pre><code class="prism language-bash">kubepods.slice/memory.limit_in_bytes <span class="token operator">=</span> capacity - kube_reserved - system_reserved
</code></pre> 
<h3><a id="Eviction__OOM_114"></a>Eviction 与 OOM</h3> 
<ol><li>eviction 是指 kubelet 对该节点上的 Pod 进行驱逐，OOM 是指 cgroup 对进程进行 kill。</li><li>kubelet 对 Pod 进行驱逐时，是根据 --eviction-hard 参数，比如该参数如果设置了 memory.available&lt;20%，那么当主机的内存使用率达到80%时，kubelet 便会对Pod进行驱逐。但是，–eviction-hard=memory.available&lt;20% 不会对<code>/sys/fs/cgroup/memory/kubepods.slice/memory.limit_in_bytes</code> 的值产生影响，因为<code>kubepods.slice/memory.limit_in_bytes = capacity - kube-reserved - system-reserved</code>，换句话说，Pod 的内存使用量总和是可以超过80%的，且不会被 OOM-kill，只会被 eviction。</li><li>kubernetes 对 Pod 的驱逐机制如下（<a href="https://blog.csdn.net/weixin_44729138/article/details/112602635">其实就是 QoS 章节的定义</a>）：<br> 首先驱逐没有设置资源限制的 Pod<br> 然后驱逐资源上限和资源下限不一样的 Pod<br> 最后驱逐资源上限等资源下限的Pod</li></ol> 
<h3><a id="_121"></a>可分配约束</h3> 
<p>前面我们在配置资源预留的时候，其中有一个 enforceNodeAllocatable 配置项（–enforce-node-allocatable），该配置项的帮助信息为：</p> 
<pre><code class="prism language-bash">--enforce-node-allocatable strings    
                
A comma separated list of levels of node allocatable enforcement to be enforced by kubelet. Acceptable options are
 <span class="token string">'none'</span>, <span class="token string">'pods'</span>, <span class="token string">'system-reserved'</span>, and <span class="token string">'kube-reserved'</span><span class="token builtin class-name">.</span> If the latter two options are specified, <span class="token string">'--system-reserved-
 cgroup'</span> and <span class="token string">'--kube-reserved-cgroup'</span> must also be set, respectively. If <span class="token string">'none'</span> is specified, no additional options 
 should be set. See https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ <span class="token keyword">for</span> <span class="token function">more</span> details. 
 <span class="token punctuation">(</span>default <span class="token punctuation">[</span>pods<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>DEPRECATED: This parameter should be <span class="token builtin class-name">set</span> via the config <span class="token function">file</span> specified by the Kubelet's --config 
 flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ <span class="token keyword">for</span> <span class="token function">more</span> information.<span class="token punctuation">)</span>
</code></pre> 
<p>kubelet 默认对 Pod 执行 Allocatable 可分配约束，如果所有 Pod 的总用量超过了 Allocatable，那么驱逐 Pod 的措施将被执行，我们可通过设置 kubelet --enforce-node-allocatable 标志值为 pods 控制这个措施。</p> 
<p>此外我们还可以通过该标志来同时指定 kube-reserved 和 system-reserved 值，可以让 kubelet 强制实施 kube-reserved 和 system-reserved 约束，不过需要注意，如果配置了 kube-reserved 或者 system-reserved 约束，那么需要对应设置 --kube-reserved-cgroup 或者 --system-reserved-cgroup 参数。</p> 
<p>如果设置了对应的 --system-reserved-cgroup 和 --kube-reserved-cgroup 参数，Pod 能实际使用的资源上限是不会改变，但系统进程与 kube 进程也会受到资源上限的限制，如果系统进程超过了预留资源，那么系统进程会被 cgroup 杀掉。但是如果不设这两个参数，那么系统进程就可以使用超过预留的资源上限。</p> 
<p>所以如果要为系统预留和 kube 预留配置 cgroup，则需要非常小心，如果执行了 kube-reserved 约束，那么 kubelet 就不能出现突发负载用光所有可用资源，不然就会被杀掉。system-reserved 可以用于为诸如 sshd、udev 等系统守护进程争取资源预留，但是如果执行 system-reserved 约束，那么可能因为某些原因导致节点上的关键系统服务 CPU 资源短缺或因为内存不足而被终止，所以如果不是自己非常清楚如何配置，最好别配置 cgroup 约束，如果需要自行配置，可以参考第一期的资源预留文档进行相关操作。</p> 
<p>因此，我们强烈建议用户使用 enforce-node-allocatable 默认配置的 pods 即可，并为系统和 kube 进程预留出适当的资源，以保持整体节点的可靠性，不需要进行 cgroup 约束，除非操作人员对系统非常了解。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0d9d9562acd4ef794835b94b6d3fda6a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">ESXI ovf部署虚拟机时，提示缺少所需的磁盘镜像</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/64bfd511cf87f11390090e738d80b924/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SpringMVC-文件上传-保姆级教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>