<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Scrapy爬虫框架，入门案例（非常详细） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Scrapy爬虫框架，入门案例（非常详细）" />
<meta property="og:description" content="目录
一、概述
二、Scrapy五大基本构成:
三、整体架构图
四、Scrapy安装以及生成项目
五、日志等级与日志保存
六、导出为json或scv格式
七、一个完整的案例
一、概述 Scrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试.
其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 后台也应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫.
Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，最新版本又提供了web2.0爬虫的支持.
二、Scrapy五大基本构成: Scrapy框架主要由五大组件组成，它们分别是调度器(Scheduler)、下载器(Downloader)、爬虫（Spider）和实体管道(Item Pipeline)、Scrapy引擎(Scrapy Engine)。下面我们分别介绍各个组件的作用。
(1)、调度器(Scheduler):
调度器，说白了把它假设成为一个URL（抓取网页的网址或者说是链接）的优先队列，由它来决定下一个要抓取的网址是 什么，同时去除重复的网址（不做无用功）。用户可以自己的需求定制调度器。
(2)、下载器(Downloader):
下载器，是所有组件中负担最大的，它用于高速地下载网络上的资源。Scrapy的下载器代码不会太复杂，但效率高，主要的原因是Scrapy下载器是建立在twisted这个高效的异步模型上的(其实整个框架都在建立在这个模型上的)。
(3)、 爬虫（Spider）:
爬虫，是用户最关心的部份。用户定制自己的爬虫(通过定制正则表达式等语法)，用于从特定的网页中提取自己需要的信息，即所谓的实体(Item)。 用户也可以从中提取出链接,让Scrapy继续抓取下一个页面。
(4)、 实体管道(Item Pipeline):
实体管道，用于处理爬虫(spider)提取的实体。主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。
(5)、Scrapy引擎(Scrapy Engine):
Scrapy引擎是整个框架的核心.它用来控制调试器、下载器、爬虫。实际上，引擎相当于计算机的CPU,它控制着整个流程。
三、整体架构图 本图按顺序说明整个程序执行时候发生的顺序。
注意在调用下载器时，往往有一个下载器中间件，使下载速度提速。
官网架构图 四、Scrapy安装以及生成项目 新建一个项目，该项目的结构如下：
执行命令,widows和ubuntu命令格式是一样的：
下载方式
ubuntu，打开一个终端，输入pip install scrapy(或pip3 install scrapy）
widows ，打开一个cmd，输入pip install scrapy，前提是你装了pip
详细安装请点这
scrapy startproject 项目名
scrapy genspider 爬虫名 域名
scrapy crawl 爬虫名" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/39e2ebe90b0ce2edb1fd339e18b82c05/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-03-21T15:44:49+08:00" />
<meta property="article:modified_time" content="2020-03-21T15:44:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Scrapy爬虫框架，入门案例（非常详细）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p> </p> 
<p><strong>目录</strong></p> 
<p id="main-toc-toc" style="margin-left:40px;"><a href="#main-toc" rel="nofollow">一、概述</a></p> 
<p id="%E4%BA%8C%E3%80%81Scrapy%E4%BA%94%E5%A4%A7%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%3A-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81Scrapy%E4%BA%94%E5%A4%A7%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%3A" rel="nofollow">二、Scrapy五大基本构成:</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE" rel="nofollow">三、整体架构图</a></p> 
<p id="%C2%A0%E5%9B%9B%E3%80%81Scrapy%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E7%94%9F%E6%88%90%E9%A1%B9%E7%9B%AE-toc" style="margin-left:40px;"><a href="#%C2%A0%E5%9B%9B%E3%80%81Scrapy%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E7%94%9F%E6%88%90%E9%A1%B9%E7%9B%AE" rel="nofollow">四、Scrapy安装以及生成项目</a></p> 
<p id="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7%E4%B8%8E%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7%E4%B8%8E%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98" rel="nofollow">五、日志等级与日志保存</a></p> 
<p id="%E4%BA%94%E3%80%81%E5%AF%BC%E5%87%BA%E4%B8%BAjson%E6%88%96scv%E6%A0%BC%E5%BC%8F-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81%E5%AF%BC%E5%87%BA%E4%B8%BAjson%E6%88%96scv%E6%A0%BC%E5%BC%8F" rel="nofollow">六、导出为json或scv格式</a></p> 
<p id="%E4%BA%94%E3%80%81%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A1%88%E4%BE%8B-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A1%88%E4%BE%8B" rel="nofollow">七、一个完整的案例</a></p> 
<hr id="hr-toc"> 
<h3>一、概述</h3> 
<p style="text-indent:33px;">Scrapy，Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，<span style="color:#f33b45;">可以用于数据挖掘、监测和自动化测试</span>.</p> 
<p style="text-indent:33px;">其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 后台也应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫.</p> 
<p style="text-indent:33px;"><span style="color:#f33b45;">Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改</span>。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，最新版本又提供了web2.0爬虫的支持.</p> 
<h3 id="%E4%BA%8C%E3%80%81Scrapy%E4%BA%94%E5%A4%A7%E5%9F%BA%E6%9C%AC%E6%9E%84%E6%88%90%3A">二、Scrapy五大基本构成:</h3> 
<p style="text-indent:33px;">Scrapy框架主要由五大组件组成，它们分别是调度器(Scheduler)、下载器(Downloader)、爬虫（Spider）和实体管道(Item Pipeline)、Scrapy引擎(Scrapy Engine)。下面我们分别介绍各个组件的作用。</p> 
<p><strong>(1)、调度器(Scheduler):</strong></p> 
<p style="text-indent:33px;">调度器，说白了把它假设成为一个URL（抓取网页的网址或者说是链接）的<span style="color:#f33b45;">优先队列</span>，由它来决定下一个要抓取的网址是 什么，同时<span style="color:#f33b45;">去除重复的网址</span>（不做无用功）。用户可以自己的需求定制调度器。</p> 
<p><strong>(2)、下载器(Downloader):</strong></p> 
<p style="text-indent:33px;">下载器，是所有组件中负担最大的，<span style="color:#f33b45;">它用于高速地下载网络上的资源</span>。Scrapy的下载器代码不会太复杂，但效率高，主要的原因是Scrapy下载器是建立在twisted这个高效的异步模型上的(其实整个框架都在建立在这个模型上的)。</p> 
<p><strong>(3)、 爬虫（Spider）:</strong></p> 
<p style="text-indent:33px;">爬虫，是用户最关心的部份。<span style="color:#f33b45;">用户定制自己的爬虫(通过定制正则表达式等语法)</span>，用于从特定的网页中提取自己需要的信息，即所谓的实体(Item)。 用户也可以从中提取出链接,让Scrapy继续抓取下一个页面。</p> 
<p><strong>(4)、 实体管道(Item Pipeline):</strong></p> 
<p style="text-indent:33px;">实体管道，<span style="color:#f33b45;">用于处理爬虫(spider)提取的实体</span>。主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。</p> 
<p><strong>(5)、Scrapy引擎(Scrapy Engine):</strong></p> 
<p style="text-indent:33px;"><span style="color:#f33b45;">Scrapy引擎是整个框架的核心.它用来控制调试器、下载器、爬虫</span>。实际上，引擎相当于计算机的CPU,它控制着整个流程。</p> 
<p> </p> 
<h3 id="%E4%B8%89%E3%80%81%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE">三、整体架构图</h3> 
<p style="text-indent:33px;">本图按顺序说明整个程序执行时候发生的顺序。</p> 
<p style="text-indent:33px;">注意在调用下载器时，往往有一个下载器中间件，使下载速度提速。</p> 
<p style="text-align:center;"><img alt="" height="829" src="https://images2.imgbox.com/d8/d4/t2NQvwhG_o.png" width="1152"></p> 
<p>官网架构图 </p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/d4/b9/XkwjOo3q_o.png"></p> 
<p> </p> 
<h3 id="%C2%A0%E5%9B%9B%E3%80%81Scrapy%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E7%94%9F%E6%88%90%E9%A1%B9%E7%9B%AE">四、Scrapy安装以及生成项目</h3> 
<p> </p> 
<p>新建一个项目，该项目的结构如下：</p> 
<p> </p> 
<p>执行命令,widows和ubuntu命令格式是一样的：</p> 
<p><strong>下载方式</strong></p> 
<p>ubuntu，打开一个终端，输入pip install scrapy(或pip3  install scrapy）</p> 
<p>widows ，打开一个cmd，输入pip install scrapy，前提是你装了pip</p> 
<p><a href="https://segmentfault.com/a/1190000010377113" rel="nofollow">详细安装请点这</a></p> 
<p style="text-align:center;"><img alt="" height="639" src="https://images2.imgbox.com/31/a6/TiEdw5MO_o.png" width="1023"></p> 
<blockquote> 
 <p> </p> 
 <p>scrapy startproject 项目名</p> 
 <p>scrapy genspider 爬虫名 域名</p> 
 <p>scrapy crawl 爬虫名</p> 
</blockquote> 
<p>我使用的是widows版本，下面演示创建项目的例子</p> 
<p>打开cmd，输入(默认是在C:\Users\Administrator&gt;这个目录下，你可以自行切换）</p> 
<blockquote> 
 <p>scrapy startproject myfirstPj</p> 
 <p>cd my firstPj</p> 
 <p>scrapy genspider baidu www.baidu.com</p> 
</blockquote> 
<p>创建后目录大致页如下</p> 
<p>|-ProjectName              #项目文件夹</p> 
<p>   |-ProjectName           #项目目录</p> 
<p>      |-items.py               #定义数据结构</p> 
<p>      |-middlewares.py    #中间件</p> 
<p>      |-pipelines.py          #数据处理</p> 
<p>      |-settings.py            #全局配置</p> 
<p>      |-spiders               </p> 
<p>          |-__init__.py       #爬虫文件</p> 
<p>          |-baidu.py</p> 
<p>   |-scrapy.cfg               #项目基本配置文件</p> 
<p> </p> 
<p>spiders下的baidu.py是scrapy自动为我们生成的</p> 
<p style="text-align:center;"><img alt="" height="429" src="https://images2.imgbox.com/28/b1/prhE5w4m_o.png" width="607"></p> 
<p> </p> 
<p>下面再看一下spdier项目的配置文件，打开文件settings.py </p> 
<p>BOT_NAME：项目名</p> 
<p>USER_AGENT：默认是注释的，这个东西非常重要，如果不写很容易被判断为电脑，简单点洗一个Mozilla/5.0即可</p> 
<p>ROBOTSTXT_OBEY：是否遵循机器人协议，默认是true，需要改为false，否则很多东西爬不了</p> 
<p> </p> 
<p style="text-align:center;"><img alt="" height="402" src="https://images2.imgbox.com/6b/19/6jRMY5PL_o.png" width="739"></p> 
<p> CONCURRENT_REQUESTS：最大并发数，很好理解，就是同时允许开启多少个爬虫线程</p> 
<p>DOWNLOAD_DELAY：下载延迟时间，单位是秒，控制爬虫爬取的频率，根据你的项目调整，不要太快也不要太慢，默认是3秒，即爬一个停3秒，设置为1秒性价比较高，如果要爬取的文件较多，写零点几秒也行</p> 
<p>COOKIES_ENABLED：是否保存COOKIES，默认关闭，开机可以记录爬取过程中的COKIE，非常好用的一个参数</p> 
<p>DEFAULT_REQUEST_HEADERS：默认请求头，上面写了一个USER_AGENT，其实这个东西就是放在请求头里面的，这个东西可以根据你爬取的内容做相应设置。</p> 
<p style="text-align:center;"><img alt="" height="558" src="https://images2.imgbox.com/62/b8/GM3C91Fm_o.png" width="757"></p> 
<p> </p> 
<p>ITEM_PIPELINES：项目管道，300为优先级，越低越爬取的优先度越高</p> 
<p style="text-align:center;"><img alt="" height="140" src="https://images2.imgbox.com/52/61/ZHTfgiim_o.png" width="612"></p> 
<p> 比如我的pipelines.py里面写了两个管道，一个爬取网页的管道，一个存数据库的管道，我调整了他们的优先级，如果有爬虫数据，优先执行存库操作。</p> 
<blockquote> 
 <pre><code class="language-html">ITEM_PIPELINES = {
    'scrapyP1.pipelines.BaiduPipeline': 300,
    'scrapyP1.pipelines.BaiduMysqlPipeline': 200,
}</code></pre> 
</blockquote> 
<p style="text-align:center;"><img alt="" height="284" src="https://images2.imgbox.com/45/b3/4BLW31io_o.png" width="509"></p> 
<p> </p> 
<p> 到这里我们尝试用scrapy做一下爬取，打开spider.py下的baidu.py(取决于你scrapy genspider 爬虫名 域名时输入的爬虫名）</p> 
<p>输入一下代码，我们使用xpath提取百度首页的标题title</p> 
<pre><code class="language-python">import scrapy


class BaiduSpider(scrapy.Spider):
    name = 'baidu'
    allowed_domains = ['www.baidu.com']
    start_urls = ['http://www.baidu.com/']

    def parse(self, response):
        tile=response.xpath('//html/head/title/text()')
        print(tile)</code></pre> 
<p> </p> 
<p>打开一个终端cmd，输入scrapy crawl baidu(爬虫名），就可以看到一大堆输出信息，而其中就包括我们要的内容</p> 
<p style="text-align:center;"><img alt="" height="639" src="https://images2.imgbox.com/46/82/bYsi1kT6_o.png" width="1200"></p> 
<p> </p> 
<p>使用终端运行太麻烦了，而且不能提取数据，我们一个写一个run文件作为程序的入口,splite是必须写的，目的是把字符串转为列表形式，第一个参数是scrapy,第二个crawl,第三个baidu</p> 
<pre><code class="language-python">from scrapy import cmdline

cmdline.execute('scrapy crawl baidu'.split())</code></pre> 
<p>可以在编辑器中输出了 </p> 
<p><img alt="" height="272" src="https://images2.imgbox.com/8c/e7/BfAWRzHb_o.png" width="1001"></p> 
<p> </p> 
<h3 id="%E4%BA%94%E3%80%81%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7%E4%B8%8E%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98">五、日志等级与日志保存</h3> 
<p>在setting.py里面可以设置日志的等级与日志存放的路径</p> 
<p><strong>相关变量</strong></p> 
<p><strong>LOG_LEVEL= ""</strong></p> 
<p><strong>LOG_FILE="日志名.log"</strong></p> 
<p><strong>日志等级分为</strong></p> 
<p>1.DEBUG 调试信息</p> 
<p>2.INFO 一般信息</p> 
<p>3.WARNING 警告</p> 
<p>4.ERROR 普通错误</p> 
<p>5.CRITICAL 严重错误</p> 
<p>如果设置</p> 
<p>LOG_LEVEL="WARNING"，就只会WARNING等级之下的ERROR和CRITICAL</p> 
<p>默认等级是1</p> 
<p> </p> 
<h3 id="%E4%BA%94%E3%80%81%E5%AF%BC%E5%87%BA%E4%B8%BAjson%E6%88%96scv%E6%A0%BC%E5%BC%8F">六、导出为json或scv格式</h3> 
<p>执行爬虫文件时添加-o选项即可</p> 
<p>scrapy crawl 项目名 -o *.csv</p> 
<p>scrapy crawl 项目名 -o *.json</p> 
<p>对于json文件，在setting.js文件里添加，设置编码格式，否则会乱码：</p> 
<p>FEED_EXPORT_ENCODING='utf-8'</p> 
<p>示例：</p> 
<pre><code class="language-python">from scrapy import cmdline

cmdline.execute('scrapy crawl baidu -o baidu.csv'.split())</code></pre> 
<p> </p> 
<h3 id="%E4%BA%94%E3%80%81%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A1%88%E4%BE%8B">七、一个完整的案例</h3> 
<p>这个项目我们的主题是爬腾讯视频的电影信息，包括电影名和描述</p> 
<p style="text-align:center;"><img alt="" height="579" src="https://images2.imgbox.com/35/b4/AljeI5VI_o.png" width="1192"></p> 
<p> </p> 
<p><strong>1.创建项目</strong></p> 
<p>打开一个终端输入（建议放到合适的路径下，默认是C盘）</p> 
<blockquote> 
 <p>scrapy startproject TXmovies</p> 
 <p>cd TXmovies</p> 
 <p>scrapy genspider txms v.qq.com</p> 
</blockquote> 
<p><strong>2.修改setting</strong></p> 
<p>修改三项内容，第一个是不遵循机器人协议，第二个是下载间隙，由于下面的程序要下载多个页面，所以需要给一个间隙（不给也可以，只是很容易被侦测到），第三个是请求头，添加一个User-Agent，第四个是打开一个管道</p> 
<blockquote> 
 <pre><code class="language-python">ROBOTSTXT_OBEY = False

DOWNLOAD_DELAY = 1

DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en',
  'User-Agent':'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'
}

ITEM_PIPELINES = {
   'TXmovies.pipelines.TxmoviesPipeline': 300,
}</code></pre> 
 <p> </p> 
</blockquote> 
<p><strong>3.确认要提取的数据，item项</strong></p> 
<p>item定义你要提取的内容（定义数据结构），比如我提取的内容为电影名和电影描述，我就创建两个变量。Field方法实际上的做法是创建一个字典，给字典添加一个建，暂时不赋值，等待提取数据后再赋值。下面item的结构可以表示为：{'name':'','descripition':''}。</p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class TxmoviesItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()
    description = scrapy.Field()
</code></pre> 
<p><strong>4.写爬虫程序</strong></p> 
<p>我们要写的部分是parse方法里的内容，重点在于如何写xpath，关于xpath我不多讲，有兴趣可以看看我另一篇文章，<a href="https://blog.csdn.net/ck784101777/article/details/104291634">XPATH教程</a></p> 
<p>引入刚刚写好的item,刚刚说了item里面创建的变量就是字典的键值，可以直接进行赋值。赋值后交给管道处理。</p> 
<p>简单讲一下这一段代码的思路，首先腾讯视频的url为<a href="https://v.qq.com/x/bu/pagesheet/list?append=1&amp;channel=cartoon&amp;iarea=1&amp;listpage=2&amp;offset=0&amp;pagesize=30" rel="nofollow">https://v.qq.com/x/bu/pagesheet/list?append=1&amp;channel=cartoon&amp;iarea=1&amp;listpage=2&amp;offset=0&amp;pagesize=30</a></p> 
<p>我们注意到offset这一项，第一页的offset为0，第二页为30，依次推列。在程序中这一项用于控制抓取第一页，但是也要给一个范围，不可能无限大，否则会报错，可以去看看腾讯一共有多少页视频，也可以写一个异常捕获机制，捕捉到请求出错则退出。我这里仅仅是示范，所以只给了120，也就是4页。</p> 
<p> </p> 
<p><strong>yield</strong></p> 
<p>程序里一共有两个yield，我比较喜欢叫它中断，当然中断只在CPU中发生，它的作用是移交控制权，在本程序中，我们对item封装数据后，就调用yield把控制权给管道，管道拿到处理后return返回，又回到该程序。这是对第一个yield的解释。</p> 
<p>第二个yield稍微复杂点，这条程序里利用了一个回调机制，即callback,回调的对象是parse,也就是当前方法，通过不断的回调，程序将陷入循环，如果不给程序加条件，就会陷入死循环，如本程序我把if去掉，那就是死循环了。</p> 
<p>yield scrapy.Request(url=url,callback=self.parse)</p> 
<p> </p> 
<p><strong>xpath</strong></p> 
<p>还有一个要注意的是如何提取xpathl里的数据，我们的写法有四种，第一种写法拿到selector选择器，也就是原数据，里面有一些我们用不到的东西。第二个extract()，将选择器序列号为字符串。第三个和第四个一样，拿到字符串里的第一个数据，也就是我们要的数据。</p> 
<p>items['name']=i.xpath('./a/@title')[0]</p> 
<p>items['name']=i.xpath('./a/@title').extract()</p> 
<p>items['name']=i.xpath('./a/@title').extract_first()</p> 
<p>items['name']=i.xpath('./a/@title').get()</p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-
import scrapy
from ..items import TxmoviesItem

class TxmsSpider(scrapy.Spider):
    name = 'txms'
    allowed_domains = ['v.qq.com']
    start_urls = ['https://v.qq.com/x/bu/pagesheet/list?append=1&amp;channel=cartoon&amp;iarea=1&amp;listpage=2&amp;offset=0&amp;pagesize=30']
    offset=0

    def parse(self, response):
        items=TxmoviesItem()
        lists=response.xpath('//div[@class="list_item"]')
        for i in lists:
            items['name']=i.xpath('./a/@title').get()
            items['description']=i.xpath('./div/div/@title').get()

            yield items

        if self.offset &lt; 120:
            self.offset += 30
            url = 'https://v.qq.com/x/bu/pagesheet/list?append=1&amp;channel=cartoon&amp;iarea=1&amp;listpage=2&amp;offset={}&amp;pagesize=30'.format(
                str(self.offset))

            yield scrapy.Request(url=url,callback=self.parse)
</code></pre> 
<p><strong>5.交给管道输出</strong></p> 
<p>管道可以处理提取的数据，如存数据库。我们这里仅输出。</p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


class TxmoviesPipeline(object):
    def process_item(self, item, spider):
        print(item)
        return item
</code></pre> 
<p><strong>6.run，执行项目</strong></p> 
<pre><code class="language-python">from scrapy import cmdline

cmdline.execute('scrapy crawl txms'.split())</code></pre> 
<p><strong>7.测试结果</strong></p> 
<p>白色的管道输出的结果，红色的调试信息</p> 
<p style="text-align:center;"><img alt="" height="313" src="https://images2.imgbox.com/ba/75/MxyKa70y_o.png" width="823"></p> 
<p> </p> 
<p><strong>8.流程梳理</strong></p> 
<p>新建项目-》进入项目-》新建爬虫文件-》明确抓取的内容，写item-》写爬虫程序，爬取数据-》交给管道处理数据-》调整全局配置setting-》执行爬虫程序，可以通过终端或者在程序里写一个run程序</p> 
<p> </p> 
<p id="%E5%85%AD%E3%80%81%E6%8F%90%E9%80%9F%EF%BC%9A%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E5%8F%96"><strong>9.提速：多线程爬取</strong></p> 
<p>如果你实现了上面的实验，不难发现其爬取速度是非常慢，根本的原因就是因为它是顺序执行的，你可以从结果中看出，总是前面一页的内容被输出，再输出后面的内容。不适合处理数据量较大的情况，一个好的方式是采用多线程的方法，这里的多线程是基于方法的多线程，并不是通过创建Thread对象来实现，是在一个方法中，一次性把请求交给调度器。</p> 
<p>我们通过重写start_requests方法来实现我们的想法（这个方法的源码在__init__.py下面，有兴趣可以看一下）</p> 
<pre><code class="language-python"># -*- coding: utf-8 -*-
import scrapy
from ..items import TxmoviesItem

class TxmsSpider(scrapy.Spider):
    name = 'txms'
    allowed_domains = ['v.qq.com']
    url='https://v.qq.com/x/bu/pagesheet/list?append=1&amp;channel=cartoon&amp;iarea=1&amp;listpage=2&amp;offset={}&amp;pagesize=30'
    offset=0

    def start_requests(self):
        for i in range(0,121,30):
            url=self.url.format(i)
            yield scrapy.Request(
                url=url,
                callback=self.parse
            )

    def parse(self, response):
        items=TxmoviesItem()
        lists=response.xpath('//div[@class="list_item"]')
        for i in lists:
            items['name']=i.xpath('./a/@title').get()
            items['description']=i.xpath('./div/div/@title').get()

            yield items
</code></pre> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/44120a0a482512ce8373da1aaa56a9f2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">安装win2003 R2 SP2 x64 可用密钥</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7fe84bf2de888dbb79a6592a382095d6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【STM32】HAL库 STM32CubeMX教程十一---DMA (串口DMA发送接收)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>