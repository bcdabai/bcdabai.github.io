<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>用 python 实现一个简单的神经网络 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="用 python 实现一个简单的神经网络" />
<meta property="og:description" content="用 python 实现一个简单的神经网络 1. 前言 让我们先看一个经典的双层神经网络模型，想必大家都很熟悉了。
在这个模型中，有一个输入层、一个输出层和一个隐藏层。但是单纯根据这个模型来看图说话，实现一个神经网络，还是比较困难的。
2. 另一种表示 现在看看这个双层神经网络的另外一种表示方法：
同前面的图相比，有几点区别：
除了输入（x）、输出（y_pred）和隐藏层（h 和 h_relu）这几个表征层之外，另外还有两个网络层 w1 和 w2。可以将 w1 看做输入表示到隐藏表示之间的映射，将 w2 看做隐藏表示到输出表示之间的映射。输入向量的维度为 D_in, 输出向量维度为 D_out，隐藏层向量维度为 H。可以批量执行，每批样本量为 N。在隐藏层内部，使用了 relu 激活函数 3. 反向传播 反向传播中，最重要的是求梯度。在矩阵乘这种场景下如何求梯度呢？
如
h_relu * w2 = y_pred
那么，有了 y_pred 的梯度（grad_y_pred），如何求 h_relu 和 w2 的梯度呢？
很简单：
grad_h_relu = grad_y_pred * w2.T
grad_w2 = h_relu.T * grad_y_pred
等式左边变量的梯度，依赖于等式右边变量的梯度和左边其他变量的现值。具体如何乘，是否需要做转置，可根据矩阵的维度确定。
除了矩阵乘法梯度计算之外，其他梯度计算根据相应情况确定。
4. 实现 有了上面的表示，我们用 numpy 手动实现一个神经网络就比较简单了
# -*- coding: utf-8 -*- import numpy as np N, D_in, H, D_out = 64, 1000, 100, 10 x = np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/4079ede30f72bdd807794c57e0bb6444/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-16T22:54:44+08:00" />
<meta property="article:modified_time" content="2019-03-16T22:54:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">用 python 实现一个简单的神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_python__1"></a>用 python 实现一个简单的神经网络</h3> 
<h4><a id="1__3"></a>1. 前言</h4> 
<p>让我们先看一个经典的双层神经网络模型，想必大家都很熟悉了。<br> <img src="https://images2.imgbox.com/ce/7c/BXzaYoGe_o.png" width="50%" height="50%"><br> 在这个模型中，有一个输入层、一个输出层和一个隐藏层。但是单纯根据这个模型来看图说话，实现一个神经网络，还是比较困难的。</p> 
<h4><a id="2__8"></a>2. 另一种表示</h4> 
<p>现在看看这个双层神经网络的另外一种表示方法：</p> 
<img src="https://images2.imgbox.com/16/5e/E734sXiQ_o.jpg" width="90%"> 
<p>同前面的图相比，有几点区别：</p> 
<ul><li>除了输入（x）、输出（y_pred）和隐藏层（h 和 h_relu）这几个表征层之外，另外还有两个网络层 w1 和 w2。可以将 w1 看做输入表示到隐藏表示之间的映射，将 w2 看做隐藏表示到输出表示之间的映射。</li><li>输入向量的维度为 D_in, 输出向量维度为 D_out，隐藏层向量维度为 H。可以批量执行，每批样本量为 N。</li><li>在隐藏层内部，使用了 relu 激活函数</li></ul> 
<h4><a id="3__18"></a>3. 反向传播</h4> 
<p>反向传播中，最重要的是求梯度。在矩阵乘这种场景下如何求梯度呢？<br> 如<br> h_relu * w2 = y_pred<br> 那么，有了 y_pred 的梯度（grad_y_pred），如何求 h_relu 和 w2 的梯度呢？<br> 很简单：<br> grad_h_relu = grad_y_pred * w2.T<br> grad_w2 = h_relu.T * grad_y_pred<br> 等式左边变量的梯度，依赖于等式右边变量的梯度和左边其他变量的现值。具体如何乘，是否需要做转置，可根据矩阵的维度确定。</p> 
<p>除了矩阵乘法梯度计算之外，其他梯度计算根据相应情况确定。</p> 
<h4><a id="4__30"></a>4. 实现</h4> 
<p>有了上面的表示，我们用 numpy 手动实现一个神经网络就比较简单了</p> 
<pre><code># -*- coding: utf-8 -*-
import numpy as np
N, D_in, H, D_out = 64, 1000, 100, 10
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)
learning_rate = 1e-6
for t in range(500):
    h = x.dot(w1)
    h_relu = np.maximum(h, 0)
    y_pred = h_relu.dot(w2)
    loss = np.square(y_pred - y).sum()
    print(t, loss)
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.T.dot(grad_y_pred)
    grad_h_relu = grad_y_pred.dot(w2.T)
    grad_h = grad_h_relu.copy()
    grad_h[h &lt; 0] = 0
    grad_w1 = x.T.dot(grad_h)
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/925b27fa7d2780f9e36c2965a5ae95e1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Altium Designer导出STEP文件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0e9f1275508438e550486f47631e5ad8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">k8s ingress 理解及配置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>