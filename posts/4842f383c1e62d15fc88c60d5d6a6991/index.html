<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>图像/视频超分之降质过程 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="图像/视频超分之降质过程" />
<meta property="og:description" content="点击上方“计算机视觉工坊”，选择“星标”
干货第一时间送达
图像/视频超分领域近期并无突破性的方法出现，故近期计划将图像/视频超分相关方法进行一次综述性汇总。计划从不同点出发对图像/视频超分进行一次“反思”之旅。本文是该旅程的第一站：图像降质过程。
尽管图像超分和视频超分在方法上或多或少会有一些关键性的区别，但是两者在训练数据对的制作方面其实并无非常大的区别。所以本文主要以图像超分为例进行介绍。
说到图像超分，大家可能会很自然的想到这样几个数据集：DIV2K，Set5，Set14等等。确实，这些数据集都是图像超分领域最常见的数据集。我们先来简单汇总一下图像/视频超分领域有哪些公开数据集。
NamePhaseSet5testSet14testB100testMenga109testUrban100testGeneral100testL20testDIV2Ktrain/valDIV8Ktrain/valFlickr2KtrainDF2K(DIV2K&#43;Flickr2K)trainCity100train/valDRealSRtrain/valRealSR(V1, V2, V3)train/valAogra(声网)trainVid4testSPMCstestUDM10testMM522trainREDStrain/valVimeotrain/valYoukutrain 注：上面所列出的仅是图像/视频超分的常用数据集以及部分竞赛数据集，除此之外还有一些Real-World数据集。其中DIV2K当属图像超分领域应用最多的一个数据集，它也是目前图像超分最常用的一个训练数据集(部分模型会考虑采用DF2K进一步提升模型性能，比如AIM2020-Efficient SuperResolution中的方法都采用了DF2K进行模型训练)；REDS与Vimeo是视频超分领域应用最多的两个训练数据集(REDS是NTIRE2019竞赛中引入的一个数据集)；RealSR、DRealSR是两个真实场景采集的图像超分数据集(具体怎样构建的训练数据对后期有空会介绍一下)。
尽管有了上述数据集，那么训练数据对LR-HR是如何构建的呢？
原理 对于图像超分而言，LR图像的获取过程一般可以描述为：
其中，k表示降质模糊核(它有多种选择，比如双三次核、高斯核等)， 表示下采样，n表示加性高斯白噪声。
可以看到上述降质过程包含模糊、下采样以及噪声，而图像超分则仅仅考虑了模糊核下采样。我们就来对可能的降质过程做一个简单的归纳，见下表。
类型说明BIbicubic-downBDblur-downBNbicubic-down&#43;noiseDNblur-down&#43;noise 注：BI表示降质过程仅包含双三次下采样；BD表示通过高斯模糊下采样；BN表示双三次插值下采样&#43;高斯白噪声；DN表示高斯模糊下采样&#43;高斯白噪声。
在这四种降质类型中，BI与BD是最常见的两种降质类型，而针对BN和DN的研究相对较少。而针对BI与BD两种降质的研究则属BI更多。
BI实现 接下来，我们将简单的介绍一下上述四种降质类型是如何实现的。首先，我们来看一下BI。做CV的同学对Bicubic应该非常熟悉，可以轻松的采用OpenCV或者PIL等库图像的双三次插值。但是，这里大家需要特别注意：BI一般特指matlab中的imresize。OpenCV与MATLAB在imresize的实现上是有区别的：matlab中的imresize具有抗锯齿功能，而OpenCV中的resize则不具备上述功能。关于matlab如何制作数据，可以参考BasicSR。这里附上关键性代码。
sclae = 4 image_path = &#34;butterfly.png&#34; image = imread(image_path) image = im2double(image) image = modcrop(image, scale) imgLR = imresize(image, 1/scale, &#39;bicubic&#39;) function image = modcrop(image, scale) if size(img, 3) == 1 sz = size(img); sz = sz - mod(sz, scale) image = image(1:sz(1), 1:sz(2)) else tmpsz = size(img); sz = tmpsz(1:2) sz = sz - mod(sz, scale) image = image(1:sz(1), 1:sz(2)) end end 也许有同学会说，都2020年了，谁还会用MATLAB啊，有没有Python版的呢？这里提供两个基于Pytorch与matlab相当的imresize。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/4842f383c1e62d15fc88c60d5d6a6991/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-30T21:14:59+08:00" />
<meta property="article:modified_time" content="2020-10-30T21:14:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图像/视频超分之降质过程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: center">点击上方“计算机视觉工坊”，选择“星标”</p> 
 <p style="text-align: center">干货第一时间送达</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/06/01/RxR3OPH7_o.png" width="100%"></p> 
 <blockquote> 
  <p>图像/视频超分领域近期并无突破性的方法出现，故近期计划将图像/视频超分相关方法进行一次综述性汇总。计划从不同点出发对图像/视频超分进行一次“反思”之旅。本文是该旅程的第一站：<strong>图像降质过程</strong>。</p> 
 </blockquote> 
 <p>尽管图像超分和视频超分在方法上或多或少会有一些关键性的区别，但是两者在训练数据对的制作方面其实并无非常大的区别。所以本文主要以图像超分为例进行介绍。</p> 
 <p>说到图像超分，大家可能会很自然的想到这样几个数据集：DIV2K，Set5，Set14等等。确实，这些数据集都是图像超分领域最常见的数据集。我们先来简单汇总一下图像/视频超分领域有哪些公开数据集。</p> 
 <p></p> 
 <table><thead><tr><th>Name</th><th>Phase</th></tr></thead><tbody><tr><td>Set5</td><td>test</td></tr><tr><td>Set14</td><td>test</td></tr><tr><td>B100</td><td>test</td></tr><tr><td>Menga109</td><td>test</td></tr><tr><td>Urban100</td><td>test</td></tr><tr><td>General100</td><td>test</td></tr><tr><td>L20</td><td>test</td></tr><tr><td>DIV2K</td><td>train/val</td></tr><tr><td>DIV8K</td><td>train/val</td></tr><tr><td>Flickr2K</td><td>train</td></tr><tr><td>DF2K(DIV2K+Flickr2K)</td><td>train</td></tr><tr><td>City100</td><td>train/val</td></tr><tr><td>DRealSR</td><td>train/val</td></tr><tr><td>RealSR(V1, V2, V3)</td><td>train/val</td></tr><tr><td>Aogra(声网)</td><td>train</td></tr><tr><td>Vid4</td><td>test</td></tr><tr><td>SPMCs</td><td>test</td></tr><tr><td>UDM10</td><td>test</td></tr><tr><td>MM522</td><td>train</td></tr><tr><td>REDS</td><td>train/val</td></tr><tr><td>Vimeo</td><td>train/val</td></tr><tr><td>Youku</td><td>train</td></tr></tbody></table> 
 <p></p> 
 <p>注：上面所列出的仅是图像/视频超分的常用数据集以及部分竞赛数据集，除此之外还有一些Real-World数据集。其中DIV2K当属图像超分领域应用最多的一个数据集，它也是目前图像超分最常用的一个训练数据集(部分模型会考虑采用DF2K进一步提升模型性能，比如AIM2020-Efficient SuperResolution中的方法都采用了DF2K进行模型训练)；REDS与Vimeo是视频超分领域应用最多的两个训练数据集(REDS是NTIRE2019竞赛中引入的一个数据集)；RealSR、DRealSR是两个真实场景采集的图像超分数据集(具体怎样构建的训练数据对后期有空会介绍一下)。</p> 
 <p>尽管有了上述数据集，那么训练数据对LR-HR是如何构建的呢？</p> 
 <h3>原理</h3> 
 <p>对于图像超分而言，LR图像的获取过程一般可以描述为：</p> 
 <p style="text-align: center"> 
   <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -750 7732.2 1000" style="-webkit-overflow-scrolling: touch;vertical-align: -0.566ex;width: 17.494ex;height: 2.262ex;max-width: 300% !important;"> 
    <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
     <g> 
      <g> 
       <path d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path> 
      </g> 
      <g transform="translate(767.8, 0)"> 
       <path d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path> 
      </g> 
      <g transform="translate(1823.6, 0)"> 
       <path d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path> 
      </g> 
      <g transform="translate(2212.6, 0)"> 
       <path d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path> 
      </g> 
      <g transform="translate(3006.8, 0)"> 
       <path d="M56 250Q56 394 156 488T384 583Q530 583 626 485T722 250Q722 110 625 14T390 -83Q249 -83 153 14T56 250ZM582 471Q531 510 496 523Q446 542 381 542Q324 542 272 519T196 471L389 278L485 375L582 471ZM167 442Q95 362 95 250Q95 137 167 58L359 250L167 442ZM610 58Q682 138 682 250Q682 363 610 442L418 250L610 58ZM196 29Q209 16 230 2T295 -27T388 -42Q409 -42 429 -40T465 -33T496 -23T522 -11T544 1T561 13T574 22T582 29L388 222L196 29Z"></path> 
      </g> 
      <g transform="translate(4007, 0)"> 
       <path d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path> 
      </g> 
      <g transform="translate(4528, 0)"> 
       <path d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path> 
      </g> 
      <g transform="translate(5194.8, 0)"> 
       <g> 
        <path d="M473 86Q483 86 483 67Q483 63 483 61T483 56T481 53T480 50T478 48T474 47T470 46T464 44Q428 35 391 14T316 -55T264 -168Q264 -170 263 -173T262 -180T261 -184Q259 -194 251 -194Q242 -194 238 -176T221 -121T180 -49Q169 -34 155 -21T125 2T95 20T67 33T44 42T27 47L21 49Q17 53 17 67Q17 87 28 87Q33 87 42 84Q158 52 223 -45L230 -55V312Q230 391 230 482T229 591Q229 662 231 676T243 693Q244 694 251 694Q264 692 270 679V-55L277 -45Q307 1 353 33T430 76T473 86Z"></path> 
       </g> 
       <g transform="translate(500, -150) scale(0.707)"> 
        <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
       </g> 
      </g> 
      <g transform="translate(6354.2, 0)"> 
       <path d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path> 
      </g> 
      <g transform="translate(7132.2, 0)"> 
       <path d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path> 
      </g> 
     </g> 
    </g> 
   </svg></p> 
 <p>其中，k表示降质模糊核(它有多种选择，比如双三次核、高斯核等)， 
    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 -694 881.6 888" style="vertical-align: -0.439ex;width: 1.995ex;height: 2.009ex;"> 
     <g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"> 
      <g> 
       <g> 
        <g> 
         <path d="M473 86Q483 86 483 67Q483 63 483 61T483 56T481 53T480 50T478 48T474 47T470 46T464 44Q428 35 391 14T316 -55T264 -168Q264 -170 263 -173T262 -180T261 -184Q259 -194 251 -194Q242 -194 238 -176T221 -121T180 -49Q169 -34 155 -21T125 2T95 20T67 33T44 42T27 47L21 49Q17 53 17 67Q17 87 28 87Q33 87 42 84Q158 52 223 -45L230 -55V312Q230 391 230 482T229 591Q229 662 231 676T243 693Q244 694 251 694Q264 692 270 679V-55L277 -45Q307 1 353 33T430 76T473 86Z"></path> 
        </g> 
        <g transform="translate(500, -150) scale(0.707)"> 
         <path d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path> 
        </g> 
       </g> 
      </g> 
     </g> 
    </svg>表示下采样，n表示加性高斯白噪声。</p> 
 <p>可以看到上述降质过程包含模糊、下采样以及噪声，而图像超分则仅仅考虑了模糊核下采样。我们就来对可能的降质过程做一个简单的归纳，见下表。</p> 
 <p></p> 
 <table><thead><tr><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>BI</td><td>bicubic-down</td></tr><tr><td>BD</td><td>blur-down</td></tr><tr><td>BN</td><td>bicubic-down+noise</td></tr><tr><td>DN</td><td>blur-down+noise</td></tr></tbody></table> 
 <p></p> 
 <p>注：BI表示降质过程仅包含双三次下采样；BD表示通过高斯模糊下采样；BN表示双三次插值下采样+高斯白噪声；DN表示高斯模糊下采样+高斯白噪声。</p> 
 <p>在这四种降质类型中，BI与BD是最常见的两种降质类型，而针对BN和DN的研究相对较少。而针对BI与BD两种降质的研究则属BI更多。</p> 
 <h3>BI实现</h3> 
 <p>接下来，我们将简单的介绍一下上述四种降质类型是如何实现的。首先，我们来看一下BI。做CV的同学对Bicubic应该非常熟悉，可以轻松的采用OpenCV或者PIL等库图像的双三次插值。<strong>但是</strong>，这里大家需要特别注意：BI一般特指matlab中的imresize。OpenCV与MATLAB在imresize的实现上是有区别的：matlab中的imresize具有抗锯齿功能，而OpenCV中的resize则不具备上述功能。关于matlab如何制作数据，可以参考BasicSR。这里附上关键性代码。</p> 
 <pre class="has"><code class="language-go">sclae = 4
image_path = "butterfly.png"
image = imread(image_path)
image = im2double(image)
image = modcrop(image, scale)
imgLR = imresize(image, 1/scale, 'bicubic')

function image = modcrop(image, scale)
if size(img, 3) == 1
    sz = size(img);
    sz = sz - mod(sz, scale)
    image = image(1:sz(1), 1:sz(2))
else
    tmpsz = size(img);
    sz = tmpsz(1:2)
    sz = sz - mod(sz, scale)
    image = image(1:sz(1), 1:sz(2))
end
end
</code></pre> 
 <p>也许有同学会说，都2020年了，谁还会用MATLAB啊，有没有Python版的呢？这里提供两个基于Pytorch与matlab相当的imresize。</p> 
 <ol><li><p>matlab_functiosn_verification</p></li><li><p>bicubic_pytorch</p></li></ol> 
 <p>但是，需要注意：尽管上述两个版本的imresize实现是参考MATLAB中的imresize进行的实现，但因为一些数据精度问题，最终的实现还是会有一点点的区别。区别有多大呢，见下表。</p> 
 <img src="https://images2.imgbox.com/aa/93/iIL7SUQa_o.png"> 
 <figcaption>
   Accuracy 
 </figcaption> 
 <p>注：表中数据来自XinTao大佬的测试，笔者也对Diff进行了测试，指标一致；不过与测试机器的性能区别，算法的耗时存在部分出入。</p> 
 <p>从上述表中结果来看，如果要进行X4超分，强烈建议各位同学采用bicubic_pytorch中的实现，因为它还可以通过GPU进行加速(就是这么优秀)；当然XinTao大佬提供的Pytorch实现更为精确(笔者就是资深受益者，哈哈)。</p> 
 <h3>BD实现</h3> 
 <p>上面介绍了BI的实现方法(matlab和python)，这里我们将介绍BD的实现方法。</p> 
 <p>在图像超分领域，BD中的高斯模糊参数为：kernelsize=7，sigma=1.6。相关实现可以参考如下链接中的代码：Prepare_TrainData_HR_LR_BD.m。这里列出核心代码：</p> 
 <pre class="has"><code class="language-go">kernelsize = 7;
sigma = 1.6;
kernel = fspecial('gaussian', kernelsize, sigma)

sclae = 4
image_path = "butterfly.png"
image = imread(image_path)
image = im2double(image)
image = modcrop(image, scale)

blur = imfilter(image, kernel, 'replicate')
imgLR = imresize(blur, 1/scale, 'nearest')
</code></pre> 
 <p>在视频超分领域，BD中的高斯模糊参数为：kernelsize=13, sigma=1.6。相关实现可以参考如下链接中的代码：DUFDown（这里这里是采用Tesorflow进行的实现）。考虑到不少同学对于Tensorflow不熟悉，我们这里提供一版Pytorch实现（XinTao大佬提供）。</p> 
 <pre class="has"><code class="language-go">def DUF_downsample(x, scale=4):
    """Downsamping with Gaussian kernel used in the DUF official code

    Args:
        x (Tensor, [B, T, C, H, W]): frames to be downsampled.
        scale (int): downsampling factor: 2 | 3 | 4.
    """

    assert scale in [2, 3, 4], 'Scale [{}] is not supported'.format(scale)

    def gkern(kernlen=13, nsig=1.6):
        import scipy.ndimage.filters as fi
        inp = np.zeros((kernlen, kernlen))
        # set element at the middle to one, a dirac delta
        inp[kernlen // 2, kernlen // 2] = 1
        # gaussian-smooth the dirac, resulting in a gaussian filter mask
        return fi.gaussian_filter(inp, nsig)

    B, T, C, H, W = x.size()
    x = x.view(-1, 1, H, W)
    pad_w, pad_h = 6 + scale * 2, 6 + scale * 2  # 6 is the pad of the gaussian filter
    r_h, r_w = 0, 0
    if scale == 3:
        r_h = 3 - (H % 3)
        r_w = 3 - (W % 3)
    x = F.pad(x, [pad_w, pad_w + r_w, pad_h, pad_h + r_h], 'reflect')

    gaussian_filter = torch.from_numpy(gkern(13, 0.4 * scale)).type_as(x).unsqueeze(0).unsqueeze(0)
    x = F.conv2d(x, gaussian_filter, stride=scale)
    x = x[:, :, 2:-2, 2:-2]
    x = x.view(B, T, C, x.size(2), x.size(3))
    return x
</code></pre> 
 <h3>BN/DN实现</h3> 
 <p>接下来，我们将要介绍一下DN的实现。它是在BD的基础上添加额外的高斯白噪声，它是RDN所提出。RDN的官方代码中也提供了响应的matlab实现：Prepare_TrainData_HR_LR_DN。但不知为何这里的DN其实是BI+Noise的实现，也就是应当是Noise。但无论如何，这里关键的是Noise的生成方式，看到code后，想必各位同学可以轻易根据改成所需要的code。这里列出关键性code。</p> 
 <pre class="has"><code class="language-go">sclae = 4;
sigma = 30; %噪声水平
image_path = "butterfly.png";
image = imread(image_path);
image = im2double(image);
image = modcrop(image, scale);
imgLR = imresize(image, 1/scale, 'bicubic');
imgLR = single(imgLR)
LRNoise = imgLR + single(sigma * randn(size(imgLR)));
LRNoise = uint8(LRNoise)
</code></pre> 
 <p>上面所提供的代码为matlab代码，但相比BI，BD，这里的关键仅在于Noise的生成部分。大家可以采用Pytorch、Numpy以及Tensorflow等轻易实现。比如，Pytorch的参考实现：</p> 
 <pre class="has"><code class="language-go">noise = torch.randn(B,C,H,W).mul_(noise_level).float()
</code></pre> 
 <h3>Others</h3> 
 <p>上面介绍了BI、BD、BN以及BD降质原理以及实现代码。那么除了上述降质外，还有其他类型的吗？有的！但基本与上述降质大同小异，对此感兴趣的同学可以去看一下KAIR中实现的几种降质：SRMD, DPSR, USRNet。这里就不再进行过多的介绍。</p> 
 <h3>注意事项</h3> 
 <p>前面对图像/视频超分中的降质方案进行了简单的梳理与总结。有一点需要各位同学牢记在心：<strong>在进行方法对比时，其降质过程一定要相同，否则对比就会不公平</strong>。为什么这样说呢？见下表的结果对比。</p> 
 <p></p> 
 <table><thead><tr><th>Degradation</th><th>Method</th><th>Scale</th><th>Dataset</th><th>PSNR</th><th>SSIM</th></tr></thead><tbody><tr><td>BI</td><td>RDN</td><td>X3</td><td>Set5</td><td>34.71</td><td>0.9296</td></tr><tr><td>BD</td><td>RDN</td><td>X3</td><td>Set5</td><td>34.58</td><td>0.9280</td></tr><tr><td>BI</td><td>RCAN</td><td>X3</td><td>Set5</td><td>34.74</td><td>0.9299</td></tr><tr><td>BD</td><td>RCAN</td><td>X3</td><td>Set5</td><td>34.70</td><td>0.9288</td></tr><tr><td>BI</td><td>TDAN</td><td>X4</td><td>Vid4</td><td>26.24</td><td>0.7800</td></tr><tr><td>BD</td><td>TDAN</td><td>X4</td><td>Vid4</td><td>26.58</td><td>0.8010</td></tr></tbody></table> 
 <p></p> 
 <p>从上表可以看到：对于图像超分而言，BI与BD两种降质制作的数据训练的模型指标基本相当；而对于视频超分而言，BI与BD两种降质方式训练的模型的指标差别较大(0.34dB)。</p> 
 <p>所以研究视频超分的小伙伴一定要特别注意新方法的指标是在BD降质所得，还是BI降质所得。如果强制的将BD模型指标与BI模型指标进行对比，那么有点"贻笑大方"了。</p> 
 <h3>参考</h3> 
 <ol><li><p>KAIR</p></li><li><p>RDN</p></li><li><p>EDVR</p></li><li><p>BasicSR</p></li><li><p>matlab_functions_verification</p></li><li><p>bicubic_pytorch</p></li><li><p>VSR-DUF</p></li></ol> 
 <p style="text-align: left">本文仅做学术分享，如有侵权，请联系删文。</p> 
 <p style="text-align: left"><strong>下载1</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>深度学习</strong>，即可下载深度学习算法、3D深度学习、深度学习框架、目标检测、GAN等相关内容近30本pdf书籍。</p> 
 <p style="text-align: left"><strong>下载2</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>计算机视觉</strong>，即可下载计算机视觉相关17本pdf书籍，包含计算机视觉算法、Python视觉实战、Opencv3.0学习等。</p> 
 <p style="text-align: left"><strong>下载3</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>SLAM</strong>，即可下载独家SLAM相关视频课程，包含视觉SLAM、激光SLAM精品课程。</p> 
 <h3></h3><p style="text-align: center"><strong>重磅！计算机视觉工坊</strong><strong>-学习</strong><strong>交流群</strong><strong>已成立</strong></p><p style="text-align: left">扫码添加小助手微信，可申请加入3D视觉工坊-学术论文写作与投稿 微信交流群，旨在<strong>交流顶会、顶刊、SCI、EI等写作与投稿事宜。</strong></p><p style="text-align: left"><strong>同时</strong>也可申请加入我们的细分方向交流群，目前主要有<strong>3D视觉</strong>、<strong>CV&amp;深度学习</strong>、<strong>SLAM</strong>、<strong>三维重建</strong>、<strong>点云后处理</strong>、<strong>自动驾驶、CV入门、三维测量、VR/AR、3D人脸识别、医疗影像、缺陷检测、行人重识别、目标跟踪、视觉产品落地、视觉竞赛、车牌识别、硬件选型、<strong>学术交流、</strong>求职交流</strong>等微信群，请扫描下面微信号加群，备注：”研究方向+学校/公司+昵称“，例如：”3D视觉 + 上海交大 + 静静“。请按照格式备注，否则不予通过。添加成功后会根据研究方向邀请进去相关微信群。<strong>原创投稿</strong>也请联系。</p><p><img title="3D视觉工坊小助理微信.jpg.jpg" src="https://images2.imgbox.com/00/cf/FEL9w1Yo_o.png"></p><p>▲长按加微信群或投稿</p><p><img src="https://images2.imgbox.com/d5/16/ZrrAbS3M_o.png"></p><p>▲长按关注公众号</p> 
 <h3></h3><p style="text-align: right"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>觉得有用，麻烦给个赞和在看~</strong><strong>  </strong><strong><img src="https://images2.imgbox.com/00/b0/WfD4hQ3y_o.gif"></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8d816f2a9912011f425b6405fed1f0d2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">LeetCode题库 104. 二叉树的最大深度</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/de0fac212977f253332425ae7447c85f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">STM32 FFT DMA ADC THD</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>