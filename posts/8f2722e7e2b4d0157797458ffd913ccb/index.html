<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch学习(4)——自然语言分类模型 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch学习(4)——自然语言分类模型" />
<meta property="og:description" content="视频链接：https://www.bilibili.com/video/BV1vz4y1R7Mm?p=4
预先下载 https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz
然后将其解压
下面的tokenizer_language设置为 解压路径 &#43; “en_core_web_sm-2.3.0\en_core_web_sm\en_core_web_sm-2.3.0”
预先使用迅雷下载 http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
然后将文件移动到同级目录 ./data/imdb 中。
使用“解压到当前文件夹”进行解压
预先使用迅雷下载 https://apache-mxnet.s3.cn-north-1.amazonaws.com.cn/gluon/embeddings/glove/glove.6B.zip
然后移动到同级目录 .vector_cache 中。
import torch from torchtext import data SEED = 1 # 固定随机数种子 确保可复现 torch.manual_seed(SEED) torch.cuda.manual_seed(SEED) torch.backends.cudnn.deterministic = True # ?? text = data.Field(tokenize=&#39;spacy&#39;, tokenizer_language=r&#39;F:\tmp\en_core_web_sm-2.3.0\en_core_web_sm\en_core_web_sm-2.3.0&#39;) label = data.LabelField(dtype=torch.float) from torchtext import datasets train_data_all, test_data = datasets.IMDB.splits(text, label) # 耗时较长 print(vars(train_data.examples[0])) {&#39;text&#39;: [&#39;Bromwell&#39;, &#39;High&#39;, &#39;is&#39;, &#39;a&#39;, &#39;cartoon&#39;, &#39;comedy&#39;, &#39;.&#39;, &#39;It&#39;, &#39;ran&#39;, &#39;at&#39;, &#39;the&#39;, &#39;same&#39;, &#39;time&#39;, &#39;as&#39;, &#39;some&#39;, &#39;other&#39;, &#39;programs&#39;, &#39;about&#39;, &#39;school&#39;, &#39;life&#39;, &#39;,&#39;, &#39;such&#39;, &#39;as&#39;, &#39;&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8f2722e7e2b4d0157797458ffd913ccb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-18T17:42:07+08:00" />
<meta property="article:modified_time" content="2020-06-18T17:42:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch学习(4)——自然语言分类模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>视频链接：<a href="https://www.bilibili.com/video/BV1vz4y1R7Mm?p=4" rel="nofollow">https://www.bilibili.com/video/BV1vz4y1R7Mm?p=4</a></p> 
<p><strong>预先下载</strong> https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.0/en_core_web_sm-2.3.0.tar.gz</p> 
<p>然后将其解压</p> 
<p>下面的tokenizer_language设置为 解压路径 + “en_core_web_sm-2.3.0\en_core_web_sm\en_core_web_sm-2.3.0”</p> 
<p><strong>预先使用迅雷下载</strong> http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</p> 
<p>然后将文件移动到同级目录 ./data/imdb 中。</p> 
<p>使用“解压到当前文件夹”进行解压</p> 
<p><strong>预先使用迅雷下载</strong> https://apache-mxnet.s3.cn-north-1.amazonaws.com.cn/gluon/embeddings/glove/glove.6B.zip</p> 
<p>然后移动到同级目录 .vector_cache 中。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchtext <span class="token keyword">import</span> data

SEED <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># 固定随机数种子 确保可复现</span>

torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>  <span class="token comment"># ??</span>

text <span class="token operator">=</span> data<span class="token punctuation">.</span>Field<span class="token punctuation">(</span>tokenize<span class="token operator">=</span><span class="token string">'spacy'</span><span class="token punctuation">,</span> tokenizer_language<span class="token operator">=</span>r<span class="token string">'F:\tmp\en_core_web_sm-2.3.0\en_core_web_sm\en_core_web_sm-2.3.0'</span><span class="token punctuation">)</span>
label <span class="token operator">=</span> data<span class="token punctuation">.</span>LabelField<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torchtext <span class="token keyword">import</span> datasets

train_data_all<span class="token punctuation">,</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>IMDB<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>text<span class="token punctuation">,</span> label<span class="token punctuation">)</span>  <span class="token comment"># 耗时较长</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">vars</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>examples<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '"', 'Teachers', '"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', "'s", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '"', 'Teachers', '"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', "'", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', "'m", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', "n't", '!'], 'label': 'pos'}
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">import</span> random
train_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_data_all<span class="token punctuation">.</span>split<span class="token punctuation">(</span>random_state<span class="token operator">=</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>SEED<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Number of training examples: {len(train_data)}'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Number of validation examples: {len(valid_data)}'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Number of testing examples: {len(test_data)}'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>Number of training examples: 17500
Number of validation examples: 7500
Number of testing examples: 25000
</code></pre> 
<p><strong>创建 vocabulary</strong></p> 
<p>把单词映射到 index</p> 
<pre><code class="prism language-python">text<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> max_size<span class="token operator">=</span><span class="token number">25000</span><span class="token punctuation">,</span> vectors<span class="token operator">=</span><span class="token string">'glove.6B.100d'</span><span class="token punctuation">)</span>  <span class="token comment"># glove是高质量的词向量</span>
label<span class="token punctuation">.</span>build_vocab<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span>  <span class="token comment"># train_data 里面本来就包含着 text 和 label</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Unique tokens is text vocabulary: {len(text.vocab)}'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Unique tokens is label vocabulary: {len(label.vocab)}'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>Unique tokens is text vocabulary: 25002
Unique tokens is label vocabulary: 2
</code></pre> 
<pre><code class="prism language-python">text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>itos<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span>
</code></pre> 
<pre><code>['&lt;unk&gt;', '&lt;pad&gt;', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']
</code></pre> 
<pre><code class="prism language-python">label<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>itos
</code></pre> 
<pre><code>['neg', 'pos']
</code></pre> 
<pre><code class="prism language-python">BATCH_SIZE <span class="token operator">=</span> <span class="token number">16</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

train_iter<span class="token punctuation">,</span> valid_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>BucketIterator<span class="token punctuation">.</span>splits<span class="token punctuation">(</span>  <span class="token comment"># BucketIterator会先对句子排序再PAD</span>
        <span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">,</span> 
        batch_size<span class="token operator">=</span>BATCH_SIZE<span class="token punctuation">,</span>
        device<span class="token operator">=</span>device<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">batch <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>valid_iter<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">batch<span class="token punctuation">.</span>text<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label
</code></pre> 
<pre><code>(tensor([[ 388,    6,    0,  ..., 3281,   11,   66],
         [ 371, 1127, 3306,  ...,    3,   63,   23],
         [1784,  666,  214,  ...,  407,   28,    9],
         ...,
         [  13,  466,   23,  ...,    1,    1,    1],
         [  68,   88,    4,  ...,    1,    1,    1],
         [   4,    1,    1,  ...,    1,    1,    1]]),
 tensor([0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
         1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
         1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1.,
         1., 1., 1., 0., 0., 0., 1., 0., 0., 0.]))
</code></pre> 
<h4><a id="Word_Averaging__149"></a>Word Averaging 模型</h4> 
<p>把每个单词都通过Embedding层投射成Word Embedding Vector，然后把一句话中的所有Word Vector做平均，得到整个句子的Vector表示。</p> 
<p>接下来把Vector传入Linear层，做分类即可。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">WordAVGModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>WordAVGModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>text<span class="token punctuation">)</span>  <span class="token comment"># [seq_len, batch_size, embed_dim]</span>
        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 交换第0和第1个维度, [batch_size, seq_len, embed_dim]</span>
        <span class="token comment"># or </span>
        <span class="token comment"># embedded = embedded.permute(1, 0, 2)  # 维度重排序，依次指定每个维度对应的原维度</span>
        <span class="token comment"># 使用 avg_pool 之前, batch_size 必须挪到前面</span>
        pooled <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> <span class="token punctuation">(</span>embedded<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># [batch_size, 1, embed_dim]</span>
        pooled <span class="token operator">=</span> pooled<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
embedding_dim <span class="token operator">=</span> <span class="token number">100</span>
output_size <span class="token operator">=</span> <span class="token number">1</span>
padding_idx <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>text<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> WordAVGModel<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">count_parameters</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""获取模型参数总数"""</span>
    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>

<span class="token comment"># numel 返回tensor的总数</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>count_parameters<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>WordAVGModel(
  (embed): Embedding(25002, 100, padding_idx=1)
  (linear): Linear(in_features=100, out_features=1, bias=True)
)
2500301
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># Embedding Weight 初始化</span>

pretrained_embedding <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>pretrained_embedding<span class="token punctuation">)</span>  <span class="token comment"># 带_的是replace函数</span>
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>padding_idx<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
unk_idx <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>text<span class="token punctuation">.</span>unk_token<span class="token punctuation">]</span>
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>unk_idx<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
</code></pre> 
<h4><a id="_218"></a>训练模型</h4> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
crit <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># binary cross entropy with logits (sigmoid 之前叫 logits)</span>

model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">binary_accuracy</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""计算准确率"""</span>
    round_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>preds<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 四舍五入</span>
    correct <span class="token operator">=</span> <span class="token punctuation">(</span>round_preds <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    accuracy <span class="token operator">=</span> correct<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
    <span class="token keyword">return</span> accuracy
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> crit<span class="token punctuation">)</span><span class="token punctuation">:</span>
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
    epoch_accu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
    total_len <span class="token operator">=</span> <span class="token number">0</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
        preds <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> crit<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># loss 里面存放所有变量的梯度</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        epoch_accu <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        total_len <span class="token operator">+=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> total_len<span class="token punctuation">,</span> epoch_accu <span class="token operator">/</span> total_len
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">evaluation</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> iterator<span class="token punctuation">,</span> crit<span class="token punctuation">)</span><span class="token punctuation">:</span>
    epoch_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
    epoch_accu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
    total_len <span class="token operator">=</span> <span class="token number">0</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> iterator<span class="token punctuation">:</span>
        preds <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> crit<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        acc <span class="token operator">=</span> binary_accuracy<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
        epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        epoch_accu <span class="token operator">+=</span> acc<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        total_len <span class="token operator">+=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">.</span>label<span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> epoch_loss <span class="token operator">/</span> total_len<span class="token punctuation">,</span> epoch_accu <span class="token operator">/</span> total_len
</code></pre> 
<pre><code class="prism language-python">n_epochs <span class="token operator">=</span> <span class="token number">10</span>
best_valid_accu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token punctuation">,</span> train_accu <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    eval_loss<span class="token punctuation">,</span> eval_accu <span class="token operator">=</span> evaluation<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iter<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> valid_accu <span class="token operator">&gt;</span> best_valid_accu<span class="token punctuation">:</span>
        best_valid_accu <span class="token operator">=</span> valid_acc
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"wordavg_model.pytorch_model.bin"</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tTrain loss {train_loss}\t acc {train_accu}"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tValid loss {eval_loss}\t acc {eval_accu}"</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"wordavg_model.pytorch_model.bin"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">import</span> spacy
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span>r<span class="token string">'F:\tmp\en_core_web_sm-2.3.0\en_core_web_sm\en_core_web_sm-2.3.0'</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tokenized <span class="token operator">=</span> <span class="token punctuation">[</span>tok<span class="token punctuation">.</span>text <span class="token keyword">for</span> tok <span class="token keyword">in</span> nlp<span class="token punctuation">.</span>tokenizer<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">]</span>
    indexed <span class="token operator">=</span> <span class="token punctuation">[</span>text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tokenized<span class="token punctuation">]</span>
    <span class="token comment"># Long Tensor是Long类型的Tensor</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment">#  shape: [seq_len]</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [seq_len, 1]</span>
    pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>model<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># prediction</span>
    <span class="token keyword">return</span> pred<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"GOOD:"</span><span class="token punctuation">,</span> predict_sentiment<span class="token punctuation">(</span><span class="token string">"This film is good."</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"BAD: "</span><span class="token punctuation">,</span> predict_sentiment<span class="token punctuation">(</span><span class="token string">"This film is realy bad."</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>GOOD: 0.379981130361557
BAD:  0.4407946467399597
</code></pre> 
<h4><a id="RNN_330"></a>RNN模型</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token keyword">class</span> <span class="token class-name">RNNModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> padding_idx<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RNNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>text<span class="token punctuation">)</span>  <span class="token comment"># [seq_len, batch_size, embed_dim]</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>
        output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>  
        <span class="token comment"># hidden: 4 * batch_size * hidden_size</span>
        <span class="token comment"># 其中 4 = 双向 * num_layers</span>
        
        <span class="token comment"># 为什么用 hidden 不用 output?</span>
        <span class="token comment"># output 的最后一个就是 hidden_state</span>
        <span class="token comment"># output 的 shape 是 (seq_len*2) * batch_size * hidden_size</span>
        <span class="token comment"># hidden 的 shape 是 (num_layers*2) * batch_size * hidden_size</span>
        
<span class="token comment">#         print(output.shape)</span>
<span class="token comment">#         print(hidden.shape)</span>
        
        hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
embedding_dim <span class="token operator">=</span> <span class="token number">100</span>
output_size <span class="token operator">=</span> <span class="token number">1</span>
padding_idx <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>text<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> RNNModel<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> 
                 embedding_size<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span> 
                 output_size<span class="token operator">=</span>output_size<span class="token punctuation">,</span> 
                 padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">,</span>
                 hidden_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> 
                 dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
crit <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># binary cross entropy with logits (sigmoid 之前叫 logits)</span>

model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">n_epochs <span class="token operator">=</span> <span class="token number">10</span>
best_valid_accu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token punctuation">,</span> train_accu <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    eval_loss<span class="token punctuation">,</span> eval_accu <span class="token operator">=</span> evaluation<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iter<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> valid_accu <span class="token operator">&gt;</span> best_valid_accu<span class="token punctuation">:</span>
        best_valid_accu <span class="token operator">=</span> valid_acc
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"lstm_model.pytorch_model.bin"</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tTrain loss {train_loss}\t acc {train_accu}"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tValid loss {eval_loss}\t acc {eval_accu}"</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="CNN_404"></a>CNN模型</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CNNModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> padding_idx<span class="token punctuation">,</span> num_filters<span class="token punctuation">,</span> filter_sizes<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CNNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>num_filters<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span>filter_size<span class="token punctuation">,</span> embedding_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">for</span> filter_size <span class="token keyword">in</span> filter_sizes
        <span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embedding_size))</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_filters <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>filter_sizes<span class="token punctuation">)</span><span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        text <span class="token operator">=</span> text<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># seq_len * batch_size -&gt; batch_size * seq_len</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>text<span class="token punctuation">)</span>  <span class="token comment"># batch_size * seq_len * embbedding_size</span>
        embedded <span class="token operator">=</span> embedded<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># batch_size * 1 * seq_len * embedding_size</span>
        
        <span class="token comment"># conved = nn.functional.relu(self.conv(embedded))  # batch_size * num_filters * seq_len-filter_size+1 * 1</span>
        <span class="token comment"># conved = conved.squeeze()  # batch_size * num_filters * seq_len-filter_size+1</span>
        <span class="token comment"># pooled = nn.functional.max_pool1d(conved, conved.shape[2]).squeeze(2)</span>
        
        conved <span class="token operator">=</span> <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">]</span>
        pooled <span class="token operator">=</span> <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>max_pool1d<span class="token punctuation">(</span>conv<span class="token punctuation">,</span> conv<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> conv <span class="token keyword">in</span> conved<span class="token punctuation">]</span>
        pooled <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pooled<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># batch_size, 3 * n_filters</span>
        pooled <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span>
    
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>pooled<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
embedding_dim <span class="token operator">=</span> <span class="token number">100</span>
output_size <span class="token operator">=</span> <span class="token number">1</span>
padding_idx <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>text<span class="token punctuation">.</span>pad_token<span class="token punctuation">]</span>

model <span class="token operator">=</span> CNNModel<span class="token punctuation">(</span>vocab_size<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span>
                 embedding_size<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span>
                 output_size<span class="token operator">=</span>output_size<span class="token punctuation">,</span>
                 padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">,</span>
                 num_filters<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
                 filter_sizes<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                 dropout<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># Embedding Weight 初始化</span>

pretrained_embedding <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>vectors
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>pretrained_embedding<span class="token punctuation">)</span>  <span class="token comment"># 带_的是replace函数</span>
unk_idx <span class="token operator">=</span> text<span class="token punctuation">.</span>vocab<span class="token punctuation">.</span>stoi<span class="token punctuation">[</span>text<span class="token punctuation">.</span>unk_token<span class="token punctuation">]</span>
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>padding_idx<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>unk_idx<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
crit <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">n_epochs <span class="token operator">=</span> <span class="token number">10</span>
best_valid_acc <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token punctuation">,</span> train_accu <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    eval_loss<span class="token punctuation">,</span> eval_accu <span class="token operator">=</span> evaluation<span class="token punctuation">(</span>model<span class="token punctuation">,</span> valid_iter<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> valid_accu <span class="token operator">&gt;</span> best_valid_accu<span class="token punctuation">:</span>
        best_valid_accu <span class="token operator">=</span> valid_acc
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"cnn_model.pytorch_model.bin"</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tTrain loss {train_loss}\t acc {train_accu}"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Epoch {epoch}\tValid loss {eval_loss}\t acc {eval_accu}"</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">test_loss<span class="token punctuation">,</span> test_accu <span class="token operator">=</span> evaluation<span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> crit<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"Test loss {test_loss}\t acc {test_accu}"</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1f368cf5d95ef592d3290137219031a4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">服务器查看server manager的类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f6f5f192e99beabba06e0c8ec79c89a1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Golang 获取时间格式化</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>