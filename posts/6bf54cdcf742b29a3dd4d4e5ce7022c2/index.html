<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>手部数据太难找？最全手部开源数据集分享 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="手部数据太难找？最全手部开源数据集分享" />
<meta property="og:description" content="本期将给大家介绍22个与手部检测、手势识别、手部图像分割等任务相关的公开数据集，包含第一人称、第三人称视角，可用于人机交互、手语翻译、3D建模等场景。
手部数据集清单一览：
​
1. NVGesture ● 发布方：英伟达
● 发布时间：2016
● 简介：NVGesture 数据集专注于非接触式驾驶员控制。它包含 1532 个动态手势，分为 25 个类别。它包括 1050 个用于训练的样本和 482 个用于测试的样本。视频以三种模式（RGB、深度和红外）录制。主要为第三人称视角。
​
​
● 下载地址：
https://opendatalab.org.cn/NVGesture
● 论文地址：
https://dl.acm.org/doi/abs/10.1145/1869790.1869829
2. HaGRID ● 发布方：SberDevices
● 发布时间：2022
●简介：
HaGRID (Hand Gesture Recognition Image Dataset)是一个大型图像数据集。可用于图像分类或图像检测任务，适用于视频会议、智能家居、智慧驾驶等场景。
HaGRID 大小为716GB，数据集包含552,992 个FullHD (1920 × 1080) RGB 图像，分为18类手势。数据分为 92% 的训练集和 8% 的测试集user_id，其中 509,323 幅图像用于训练，43,669 幅图像用于测试。
该数据集包含34,730个独特的人以及至少这个数量的独特场景。受试者为 18 至 65 岁的人。该数据集主要是在室内收集的，光照变化很大，包括人造光和自然光。该数据集还包括在极端条件下拍摄的图像，例如面对和背靠窗户。此外，受试者必须在距相机 0.5 到 4 米的距离处显示手势。
​
​
● 下载地址：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6bf54cdcf742b29a3dd4d4e5ce7022c2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-10T20:45:00+08:00" />
<meta property="article:modified_time" content="2022-11-10T20:45:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">手部数据太难找？最全手部开源数据集分享</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本期将给大家介绍22个与手部检测、手势识别、手部图像分割等任务相关的公开数据集，包含第一人称、第三人称视角，可用于人机交互、手语翻译、3D建模等场景。</p> 
<p><strong>手部数据集清单一览：</strong></p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/6e/26/0llHcuPl_o.png">​</p> 
</div> 
<h3 id="No.1"><em><strong>1. </strong></em> <strong><a class="link-info" href="https://opendatalab.org.cn/NVGesture?source=Y3Nkbg%3D%3D" rel="nofollow" title="NVGesture">NVGesture</a></strong></h3> 
<p>● <strong>发布方</strong>：英伟达</p> 
<p>● <strong>发布时间</strong>：2016</p> 
<p>● <strong>简介</strong>：<a class="link-info" href="https://opendatalab.org.cn/NVGesture?source=Y3Nkbg%3D%3D" rel="nofollow" title="NVGesture 数据集">NVGesture 数据集</a>专注于非接触式驾驶员控制。它包含 1532 个动态手势，分为 25 个类别。它包括 1050 个用于训练的样本和 482 个用于测试的样本。视频以三种模式（RGB、深度和红外）录制。主要为第三人称视角。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/00/9f/0oaOXi9o_o.png">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/aa/d0/VVPJT2ln_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/NVGesture?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/NVGesture">https://opendatalab.org.cn/NVGesture</a></p> 
<p>● <strong>论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//dl.acm.org/doi/abs/10.1145/1869790.1869829" rel="nofollow" title="https://dl.acm.org/doi/abs/10.1145/1869790.1869829">https://dl.acm.org/doi/abs/10.1145/1869790.1869829</a></p> 
<p></p> 
<h3 id="No.2"><em><strong>2.  </strong></em><strong>HaGRID</strong></h3> 
<p>● <strong>发布方</strong>：SberDevices</p> 
<p>● <strong>发布时间</strong>：2022</p> 
<p>●<strong>简介</strong>：</p> 
<p>HaGRID (Hand Gesture Recognition Image Dataset)是一个大型图像数据集。可用于图像分类或图像检测任务，适用于视频会议、智能家居、智慧驾驶等场景。</p> 
<p>HaGRID 大小为716GB，数据集包含552,992 个FullHD (1920 × 1080) RGB 图像，分为18类手势。数据分为 92% 的训练集和 8% 的测试集user_id，其中 509,323 幅图像用于训练，43,669 幅图像用于测试。</p> 
<p>该数据集包含34,730个独特的人以及至少这个数量的独特场景。受试者为 18 至 65 岁的人。该数据集主要是在室内收集的，光照变化很大，包括人造光和自然光。该数据集还包括在极端条件下拍摄的图像，例如面对和背靠窗户。此外，受试者必须在距相机 0.5 到 4 米的距离处显示手势。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/5f/0d/1LQKfHWA_o.png">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/9d/b4/2rLiHHIJ_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/hukenovs/hagrid" title="https://github.com/hukenovs/hagrid">https://github.com/hukenovs/hagrid</a></p> 
<p>● <strong>论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2206.08219v1.pdf" rel="nofollow" title="https://arxiv.org/pdf/2206.08219v1.pdf">https://arxiv.org/pdf/2206.08219v1.pdf</a></p> 
<p></p> 
<h3 id="No.3"><em><strong>3.  </strong></em><strong>Chalearn IsoGD 、Chalearn ConGD</strong></h3> 
<p>● <strong>发布方</strong>：中国科学院自动化研究所</p> 
<p><strong>● 发布时间</strong>：2016</p> 
<p><strong>● 简介：</strong></p> 
<p>这两个数据集都是从CGD 2011 数据集创建的。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/3b/6c/j0DMa0ut_o.png">​</p> 
</div> 
<p>ChalearnIsoGD（Chalearn LAP RGB-D Isolated Gesture Dataset）是一个大规模的独立手势数据集。每类手势是 200 多个 RGB 和深度视频，并且来自同一个人的训练样本不会出现在验证和测试集中。</p> 
<p>该数据集包括 47933 个 RGB-D 手势视频（约 9G）。每个 RGB-D 视频仅代表一个手势，共有 21 个不同的人执行的 249 个手势标签。</p> 
<p>为了使用方便，被分成了三个子数据集，这三个子集是互斥的。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/18/8f/7saOLOWI_o.png">​</p> 
</div> 
<p>Chalearn ConGD（the Continuous Gesture Dataset）是一个大规模的连续手势识别数据集。该数据库包含 22535 个 RGB-D 手势视频（约 4G）中的 47933 个 RGB-D 手势。每个 RGB-D 视频可能代表一个或多个手势，共有 249 个手势标签，由 21 个不同的人执行。</p> 
<p>为了使用方便，数据库被分成了三个子数据集，这三个子集是互斥的。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/11/9d/Jn2QKWA8_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//gesture.chalearn.org/2016-looking-at-people-cvpr-challenge/isogd-and-congd-datasets" rel="nofollow" title="https://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge/isogd-and-congd-datasets">https://gesture.chalearn.org/2016-looking-at-people-cvpr-challenge/isogd-and-congd-datasets</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=http%3A//www.cbsr.ia.ac.cn/users/jwan/papers/CVPRW2016_JunWan.pdf" rel="nofollow" title="http://www.cbsr.ia.ac.cn/users/jwan/papers/CVPRW2016_JunWan.pdf">http://www.cbsr.ia.ac.cn/users/jwan/papers/CVPRW2016_JunWan.pdf</a></p> 
<p></p> 
<h3 id="No.4"><em><strong>4</strong></em>  <strong><a class="link-info" href="https://opendatalab.org.cn/HandNet?source=Y3Nkbg%3D%3D" rel="nofollow" title="HandNet">HandNet</a></strong></h3> 
<p><strong>● 发布方：</strong>以色列理工学院计算机科学学院GIP实验室</p> 
<p><strong>● 发布时间</strong>：2015</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/HandNet?source=Y3Nkbg%3D%3D" rel="nofollow" title="HandNet 数据集">HandNet 数据集</a>，包含了 10 名参与者在RealSense RGB-D 相机前拍摄的非刚性变形的手部深度图像。其中还包含了 6D 数据，用于描述手的中心及指尖的位置方向信息。</p> 
<p>此数据集包含训练集：202198；测试集：10000；验证集：2773。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/68/05/dnffGNpH_o.jpg">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/5a/75/KL04VJ3T_o.png">​</p> 
</div> 
<p>HandNet 测试集检测示例。颜色代表正确定位和识别的指尖。白框表示错误检测，错误阈‍值选择为 1cm</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/HandNet?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/HandNet">https://opendatalab.org.cn/HandNet</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1507.05726v1.pdf" rel="nofollow" title="https://arxiv.org/pdf/1507.05726v1.pdf">https://arxiv.org/pdf/1507.05726v1.pdf</a></p> 
<p></p> 
<h3 id="No.5"><em><strong>5</strong></em>.  <strong><a class="link-info" href="https://opendatalab.org.cn/Rendered_Handpose_Dataset?source=Y3Nkbg%3D%3D" rel="nofollow" title="RHD (Rendered Hand Pose)">RHD (Rendered Hand Pose)</a></strong></h3> 
<p><strong>● 发布方：弗莱堡大学</strong></p> 
<p><strong>● 发布时间</strong>：2017</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/Rendered_Handpose_Dataset?source=Y3Nkbg%3D%3D" rel="nofollow" title="Rendered Handpose Dataset">Rendered Handpose Dataset</a> 包含 41258 个训练样本和 2728 个测试样本。</p> 
<p>每个样本提供：</p> 
<ul><li>RGB 图像（320x320 像素）；</li><li>深度图（320x320 像素） ；</li><li>类别的分割掩码（320x320 像素）：背景、人物、每个手指三个类别和每个手掌一个类别；</li><li>每只手的21个关键点，其uv 坐标位于图像框架、世界框架中的 xyz 坐标和可见性指示器；</li><li>相机内参矩阵 K。</li></ul> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/c2/ad/pj7QnDXD_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/Rendered_Handpose_Dataset?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/Rendered_Handpose_Dataset">https://opendatalab.org.cn/Rendered_Handpose_Dataset</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1705.01389v3.pdf" rel="nofollow" title="https://arxiv.org/pdf/1705.01389v3.pdf">https://arxiv.org/pdf/1705.01389v3.pdf</a></p> 
<p></p> 
<h3 id="No.6"><em><strong>6</strong></em>.  <strong><a class="link-info" href="https://opendatalab.org.cn/FreiHAND?source=Y3Nkbg%3D%3D" rel="nofollow" title="FreiHAND">FreiHAND</a></strong></h3> 
<p><strong>● 发布方：弗莱堡大学、Adobe 研究院</strong></p> 
<p><strong>● 发布时间</strong>：2019</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/FreiHAND?source=Y3Nkbg%3D%3D" rel="nofollow" title="FreiHand">FreiHand</a>是一个3D手部姿态数据集，记录了32个人进行的不同手部动作。对于每个手图像，提供基于Mano的3D手姿态标注。它目前包含32560个不同训练样本和3960个用于评估的不同样本。训练样本被记录在允许背景移除的绿屏背景下。</p> 
<p>此外，它还采用了三种不同的后处理策略对训练样本进行数据增强。但这些后处理策略并没有应用于评估样本。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/62/bd/06tAN4t5_o.png">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/1c/9e/c50ntqMG_o.png">​</p> 
</div> 
<p>数据集示例，显示图像（顶行）和手形注释（底行）。这个训练集包含来自绿屏记录的合成图像，而评价数据集包含记录的室内和室外图像</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/FreiHAND?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/FreiHAND">https://opendatalab.org.cn/FreiHAND</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.04349v3.pdf" rel="nofollow" title="https://arxiv.org/pdf/1909.04349v3.pdf">https://arxiv.org/pdf/1909.04349v3.pdf</a></p> 
<p></p> 
<h3 id="No.7"><em><strong>7.</strong></em>  <strong><a class="link-info" href="https://opendatalab.org.cn/MSRA_Hand?source=Y3Nkbg%3D%3D" rel="nofollow" title="MSRA Hand">MSRA Hand</a></strong></h3> 
<p><strong>● 发布方：微软研究院、香港中文大学</strong></p> 
<p><strong>● 发布时间</strong>：2014</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/MSRA_Hand?source=Y3Nkbg%3D%3D" rel="nofollow" title="MSRA Hands">MSRA Hands</a> 是用于手部跟踪的数据集。使用英特尔的创意交互式手势相机总共捕获了 6 个受试者的右手。每个受试者被要求在 400 帧的视频序列中做出各种快速手势。</p> 
<p>为了考虑不同的手尺寸，为每个主题指定了全局手模型比例：主题 1~6 分别为 1.1、1.0、0.9、0.95、1.1、1.0。相机内在参数为：主点=图像中心（160,120），焦距=241.42。深度图像为 320x240，每个 .bin 文件按行扫描顺序存储深度像素值，即 320240 个浮点数。单位是毫米。</p> 
<p>bin 文件是二进制文件，需要使用 std::ios::binary 标志打开。joint.txt 文件存储 400 帧 x 每帧 21 个手关节。每条线有 3 * 21 = 63 个浮点数，用于 (x, y, z) 坐标中的 21 个 3D 点。</p> 
<p>21 个手关节是：手腕、index_mcp、index_pip、index_dip、index_tip、middle_mcp、middle_pip、middle_dip、middle_tip、ring_mcp、ring_pip、ring_dip、ring_tip、little_mcp、little_pip、little_dip、little_tip、thumb_mcp、thumb_pip、thumb_dip、thumb_tip。对应的 *.jpg 文件仅用于深度和地面实况关节的可视化。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/83/f6/rHGIf5tI_o.png">​</p> 
</div> 
<p>第一个对象的示例跟踪结果。带有红框的那些包含明显的错误。颜色编码为了更好地可视化，还显示了每个结果的对应图</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/MSRA_Hand?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/MSRA_Hand">https://opendatalab.org.cn/MSRA_Hand</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf" rel="nofollow" title="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf</a></p> 
<p></p> 
<h3 id="No.8"><em><strong>No.8</strong></em></h3> 
<h3 id="MSRC-12%20(MSRC-12%20Kinect%20Gesture%20Dataset)"><strong><a class="link-info" href="https://opendatalab.org.cn/MSRC-12?source=Y3Nkbg%3D%3D" rel="nofollow" title="MSRC-12 (MSRC-12 Kinect Gesture Dataset)">MSRC-12 (MSRC-12 Kinect Gesture Dataset)</a></strong></h3> 
<p><strong>● 发布方：亚历山大大学、微软</strong></p> 
<p><strong>● 发布时间</strong>：2012</p> 
<p><strong>● 简介：</strong>Microsoft Research Cambridge-12 Kinect 手势数据集由人体运动序列组成，表示为身体部位位置，以及系统识别的相关手势。该数据集包括 594 个序列和 719,359 帧 - 大约 6 小时 40 分钟 - 从 30 个人执行 12 个手势收集。总共有 6,244 个手势实例。运动文件包含使用 Kinect 姿势估计管道估计的 20 个关节的轨迹。身体姿势以 30Hz 的采样率捕获，关节位置的精度约为 2 厘米。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/74/6b/jz0fqUtN_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/MSRC-12?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/MSRC-12">https://opendatalab.org.cn/MSRC-12</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=http%3A//www.nowozin.net/sebastian/papers/fothergill2012gestures.pdf" rel="nofollow" title="http://www.nowozin.net/sebastian/papers/fothergill2012gestures.pdf">http://www.nowozin.net/sebastian/papers/fothergill2012gestures.pdf</a></p> 
<p></p> 
<h3 id="No.9"><em><strong>9</strong></em>.  <strong><a class="link-info" href="https://opendatalab.org.cn/MuViHand?source=Y3Nkbg%3D%3D" rel="nofollow" title="MuViHand">MuViHand</a></strong></h3> 
<p><strong>● 发布方：皇后大学</strong></p> 
<p><strong>● 发布时间</strong>：2021</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/MuViHand?source=Y3Nkbg%3D%3D" rel="nofollow" title="MuViHand">MuViHand</a>是在 Mixamo 的帮助下创建的合成手部姿势数据集，由手的多视图视频和真实 3D 姿势标签组成。</p> 
<p>包括 4,560 个视频中提供的超过 402,000 张合成手部图像。这些视频是从六个不同角度同时拍摄的，具有复杂的背景和随机的动态照明水平。数据是从 10 个不同的动画对象中捕获的，使用 12 个摄像头在半圆形拓扑中，其中六个跟踪摄像头只聚焦在手上，其他六个固定摄像头捕获整个身体。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/b1/71/sYn41e4D_o.png">​</p> 
</div> 
<p>MuViHand 数据集的多视图相机取景示意</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/MuViHand?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/MuViHand">https://opendatalab.org.cn/MuViHand</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2109.11747v1.pdf" rel="nofollow" title="https://arxiv.org/pdf/2109.11747v1.pdf">https://arxiv.org/pdf/2109.11747v1.pdf</a></p> 
<p></p> 
<h3 id="No.10"><em><strong>10.</strong></em>  <strong><a class="link-info" href="https://opendatalab.org.cn/3D_Hand_Pose?source=Y3Nkbg%3D%3D" rel="nofollow" title="3D Hand Pose">3D Hand Pose</a></strong></h3> 
<p><strong>● 发布方：阿利坎特大学</strong></p> 
<p><strong>● 发布时间</strong>：2017</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/3D_Hand_Pose?source=Y3Nkbg%3D%3D" rel="nofollow" title="3D Hand Pose">3D Hand Pose</a> 是一个多视图手部姿势数据集，由手部的彩色图像和不同类型的注释组成：边界框以及手部关节上的 2D 和 3D 位置。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/63/47/6BT64RHS_o.png">​</p> 
</div> 
<p>相关模型检测流程</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/f0/43/GwEzmkg9_o.png">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/4f/22/ey0Rh6DP_o.png">​</p> 
</div> 
<p>样例示意</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/3D_Hand_Pose?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/3D_Hand_Pose">https://opendatalab.org.cn/3D_Hand_Pose</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1707.03742v3.pdf" rel="nofollow" title="https://arxiv.org/pdf/1707.03742v3.pdf">https://arxiv.org/pdf/1707.03742v3.pdf</a></p> 
<p></p> 
<h3 id="No.11"><em><strong>11.  </strong></em><strong>Jester</strong></h3> 
<p><strong>● 发布方：高通</strong></p> 
<p><strong>● 发布时间</strong>：2019</p> 
<p><strong>● 简介：</strong></p> 
<p>Jester 手势识别数据集包括 148,092 个带标签的人类在笔记本电脑摄像头或网络摄像头前执行基本的、预定义的手势的视频剪辑。它旨在训练机器学习模型以识别人类手势，例如向下滑动两根手指、向左或向右滑动以及敲击手指。</p> 
<p>这些剪辑涵盖了 27 种不同类别的人类手势，按照 8:1:1 的比例进行了训练、开发和测试。该数据集还包括两个“无手势”类，以帮助网络区分特定手势和未知手部动作。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/47/fa/gGDnBW8W_o.png">​</p> 
</div> 
<p>数据集样例示意</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//developer.qualcomm.com/software/ai-datasets/jester" rel="nofollow" title="https://developer.qualcomm.com/software/ai-datasets/jester">https://developer.qualcomm.com/software/ai-datasets/jester</a></p> 
<p><strong>● 论文地址</strong>：<a href="https://link.zhihu.com/?target=https%3A//openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf" rel="nofollow" title="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Materzynska_The_Jester_Dataset_A_Large-Scale_Video_Dataset_of_Human_Gestures_ICCVW_2019_paper.pdf</a></p> 
<p></p> 
<h3 id="No.12"><em><strong>12.  </strong></em><strong>IPN Hand</strong></h3> 
<p><strong>● 发布方：电气通信大学</strong></p> 
<p><strong>● 发布时间</strong>：2020</p> 
<p><strong>● 简介：</strong></p> 
<p>IPN Hand 数据集包含来自50 个主题的4,000 多个手势实例和 800,000 个帧。包含13 种静态和动态手势，用于与非触摸屏交互。与其他公开可用的手势数据集相比，IPN Hand 包含每个视频最多的连续手势，以及最大的类内变化速度。除了RGB 帧外，还提供实时光流和手部分割结果。</p> 
<p>数据收集的设计考虑了连续 HGR 的现实世界问题，包括在没有过渡状态的情况下执行的连续手势、作为非手势片段的自然运动、包括杂乱背景、极端光照条件以及静态和动态环境的场景。</p> 
<p>没有过渡状态的连续手势示例：</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/44/31/cjvkkNxV_o.jpg">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/c3/d3/xU7n6xIq_o.png">​</p> 
</div> 
<p>数据集中包含的 13 个手势类别的示例。语义分割掩码被混合到 RGB 图像中便于可视化</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//gibranbenitez.github.io/IPN_Hand/" rel="nofollow" title="https://gibranbenitez.github.io/IPN_Hand/">https://gibranbenitez.github.io/IPN_Hand/</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//gibranbenitez.github.io/2021_ICPR_IpnHand.pdf" rel="nofollow" title="https://gibranbenitez.github.io/2021_ICPR_IpnHand.pdf">https://gibranbenitez.github.io/2021_ICPR_IpnHand.pdf</a></p> 
<p></p> 
<h3 id="No.13"><em><strong>13.  </strong></em><strong>EgoGesture</strong></h3> 
<p><strong>● 发布方：</strong>中国科学院自动化研究所</p> 
<p><strong>● 发布时间</strong>：2017</p> 
<p><strong>● 简介：</strong>EgoGesture 是一个第一人称视角的手势识别的多模态大规模数据集，包含来自 50 个不同主题的 2,081 个 RGB-D 视频、24,161 个手势样本和 2,953,224 帧。包含了83 类静态或动态手势，专注于与可穿戴设备的交互，</p> 
<p>这些视频是从 6 个不同的室内和室外场景中收集的。还考虑了人们在走路时做手势的场景。</p> 
<p><strong>4 个室内场景：</strong></p> 
<p>● 对象处于静止状态，具有静态杂波背景；</p> 
<p>● 主体处于动态背景的静止状态；</p> 
<p>● 对象处于静止状态，面对阳光剧烈变化的窗户；</p> 
<p>● 处于行走状态的主体；</p> 
<p><strong>2个户外场景：</strong></p> 
<p>● 主体处于动态背景的静止状态；</p> 
<p>● 动态背景下处于行走状态的主体。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/15/b4/VwUAUzj5_o.png">​</p> 
</div> 
<p>6类场景示意</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/36/f2/cPhpcpGn_o.png">​</p> 
</div> 
<p>数据集中包含的 83 种手势类别的示例</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=http%3A//www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html" rel="nofollow" title="http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html">http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=http%3A//www.nlpr.ia.ac.cn/iva/yfzhang/datasets/EgoGesture.pdf" rel="nofollow" title="http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/EgoGesture.pdf">http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/EgoGesture.pdf</a></p> 
<h3></h3> 
<h3 id="No.14"><em><strong>14.  </strong></em><strong>FPHA</strong></h3> 
<p><strong>● 发布方：</strong>伦敦帝国理工学院</p> 
<p><strong>● 发布时间</strong>：2018</p> 
<p><strong>● 简介：</strong>First-Person Hand Action Benchmark是 RGB-D 视频序列的集合，由 45 个日常手部动作类别的 100K 帧组成，涉及多种手部配置中的 26 个不同对象。可用于 3D 手势估计、6D 物体姿势、机器人以及动作识别等任务。第一人称视角。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/bc/42/mV2oIVk1_o.png">​</p> 
</div> 
<p>第一人称“倒果汁”动作示意。使用了磁性传感器和逆运动学模型来捕捉手的姿势。在右侧是深度图像和手部姿势。也为手部动作的子集捕获 6D 姿态</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/db/03/5FQhtduq_o.png">​</p> 
</div> 
<p>涉及物体的手部动作分类。一些对象与多个动作相关联（例如，勺子、海绵、液体肥皂），而其他一些有只有一个链接动作（例如，计算器、笔、电池充电器）</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//guiggh.github.io/publications/first-person-hands/" rel="nofollow" title="https://guiggh.github.io/publications/first-person-hands/">https://guiggh.github.io/publications/first-person-hands/</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1704.02463.pdf" rel="nofollow" title="https://arxiv.org/pdf/1704.02463.pdf">https://arxiv.org/pdf/1704.02463.pdf</a></p> 
<p></p> 
<h3 id="No.15"><em><strong>15</strong></em>.  <strong><a class="link-info" href="https://opendatalab.org.cn/EYTH?source=Y3Nkbg%3D%3D" rel="nofollow" title="EYTH (EgoYouTubeHands)">EYTH (EgoYouTubeHands)</a></strong></h3> 
<p><strong>● 发布方：</strong>中佛罗里达大学</p> 
<p><strong>● 发布时间</strong>：2018</p> 
<p><strong>● 简介：</strong></p> 
<p><a class="link-info" href="https://opendatalab.org.cn/EYTH?source=Y3Nkbg%3D%3D" rel="nofollow" title="EYTH">EYTH</a>是一个以自我为中心的手分割数据集，由1290个来自YouTube视频的注释帧组成，这些视频记录在无约束的现实世界中。这些视频在环境、参与者数量和动作方面都有所不同。该数据集有助于研究无约束环境下的手分割问题。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/f2/88/nj8N0e4b_o.png">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/EYTH?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/EYTH">https://opendatalab.org.cn/EYTH</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.03317v2.pdf" rel="nofollow" title="https://arxiv.org/pdf/1803.03317v2.pdf">https://arxiv.org/pdf/1803.03317v2.pdf</a></p> 
<p></p> 
<h3 id="No.16"><em><strong>16.  </strong></em><strong>EgoHOS</strong></h3> 
<p><strong>● 发布方：</strong>宾夕法尼亚大学 · 丰田研究所（TRI）</p> 
<p><strong>● 发布时间</strong>：2022</p> 
<p><strong>● 简介：</strong>EgoHOS（Egocentric Hand-Object Segmentation）是一个由 11,243 个以自我为中心的图像组成的标记数据集，其中包含在各种日常活动中交互的手和物体的每像素分割标签。是目前最早的标记详细的手对象接触边界的数据集。</p> 
<p>作者引入了一种上下文感知的组合数据增强技术，以适应分发外的 YouTube 以自我为中心的视频。展示其手部对象分割模型和数据集可以作为基础工具来增强或启用多种下游视觉应用，包括手部状态分类、视频活动识别、手对象交互的 3D 网格重建，以及以自我为中心的视频中手对象前景的视频修复。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/7d/ef/Yd6kUlD1_o.png">​</p> 
</div> 
<p>标记示意</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/owenzlz/EgoHOS" title="https://github.com/owenzlz/EgoHOS">https://github.com/owenzlz/EgoHOS</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2208.03826.pdf" rel="nofollow" title="https://arxiv.org/pdf/2208.03826.pdf">https://arxiv.org/pdf/2208.03826.pdf</a></p> 
<p></p> 
<h3 id="No.17"><em><strong>17.  </strong></em><strong><a class="link-info" href="https://opendatalab.org.cn/HO-3D?source=Y3Nkbg%3D%3D" rel="nofollow" title="HO-3D">HO-3D</a></strong></h3> 
<p><strong>● 发布方：</strong>格拉茨技术大学、古斯塔夫·埃菲尔大学</p> 
<p><strong>● 发布时间</strong>：2020</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/HO-3D?source=Y3Nkbg%3D%3D" rel="nofollow" title="HO-3D">HO-3D</a>是一个具有手和对象的3D 姿势注释的手对象交互数据集。该数据集包含来自总共 68 个序列的 66,034 个训练图像和 11,524 个测试图像。这些序列在多摄像头和单摄像头设置中捕获，包含 10 个不同的主体，从 YCB 数据集中操纵 10 个不同的对象。</p> 
<p>注释是使用优化算法自动获得的。测试集的手势注释被保留，测试集上算法的准确性可以使用 CodaLab 挑战提交通过标准指标进行评估。测试集和训练集的对象姿态注释与数据集一起提供。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/2b/c6/Zm81rUFu_o.jpg">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/HO-3D?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/HO-3D">https://opendatalab.org.cn/HO-3D</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf" rel="nofollow" title="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf</a></p> 
<p></p> 
<h3 id="No.18"><em><strong>18.  </strong></em><strong>CSL</strong></h3> 
<p><strong>● 发布方：</strong>中国科学技术大学</p> 
<p><strong>● 发布时间</strong>：2015</p> 
<p><strong>● 简介：</strong>中国手语数据集(CSL)是由中国科学技术大学自2015年起利用Kinect采集的中国手语数据集，包含25K标记的视频实例，共有超过100个时长的视频，50个操作者拍摄，每个操作者重复5次，包含RGB、深度以及骨架关节点数据，分为孤立词和连续语句，其中单词有500类，每类含250个样例， 包含21个骨架关节点坐标序列；句子有100个，共 有5000个视频，每一个句子平均包含4～8个单词。</p> 
<p>每一个视频实例都由专业的中国手语老师进行标注。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/63/54/yxu3Jpe3_o.png">​</p> 
</div> 
<p>CSL数据集参数</p> 
<p></p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/bb/7c/jJI65bSX_o.png">​</p> 
</div> 
<p>CSL中国手语数据样例</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=http%3A//home.ustc.edu.cn/~pjh/openresources/cslr-dataset-2015/index.html" rel="nofollow" title="http://home.ustc.edu.cn/~pjh/openresources/cslr-dataset-2015/index.html">http://home.ustc.edu.cn/~pjh/openresources/cslr-dataset-2015/index.html</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<h3 id="Isolated%20SLR%EF%BC%9A">Isolated SLR：</h3> 
<p><a href="https://link.zhihu.com/?target=http%3A//home.ustc.edu.cn/~pjh/publications/ICME2016Chinese/paper.pdf" rel="nofollow" title="http://home.ustc.edu.cn/~pjh/publications/ICME2016Chinese/paper.pdf">http://home.ustc.edu.cn/~pjh/publications/ICME2016Chinese/paper.pdf</a></p> 
<h3 id="Continuous%20SLR%EF%BC%9A">Continuous SLR：</h3> 
<p><a href="https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/document/8954236" rel="nofollow" title="https://ieeexplore.ieee.org/document/8954236">https://ieeexplore.ieee.org/document/8954236</a></p> 
<p></p> 
<h3 id="No.19"><em><strong>19.</strong></em>  <strong><a class="link-info" href="https://opendatalab.org.cn/MFH?source=Y3Nkbg%3D%3D" rel="nofollow" title="MFH">MFH</a></strong></h3> 
<p><strong>● 发布方：</strong>Willogy、AIOZ · Blood Transfusion Hematology Hospital</p> 
<p><strong>● 发布时间</strong>：2021</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/MFH?source=Y3Nkbg%3D%3D" rel="nofollow" title="MFH 数据集">MFH 数据集</a>是一个多视点细粒度的手部卫生数据集。它总共包含 73,1147 个样本，这些样本由 6 个不同位置的 6 个摄像机视图收集。所有样本总共分为 7 个类别。MFH 数据集与现有数据集的区别在于三个方面：大的类内差异、细微的类间差异以及训练阶段和推理阶段之间的数据分布不匹配。因此，该数据集提供了更现实的基准。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/24/3f/3NSuIHxv_o.png">​</p> 
</div> 
<p><a class="link-info" href="https://opendatalab.org.cn/MFH?source=Y3Nkbg%3D%3D" rel="nofollow" title="MFH数据集">MFH数据集</a>中7个手部卫生步骤的6个不同视角的样例总览</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/f6/4a/YJWbwQH9_o.png">​</p> 
</div> 
<p>数据集样例</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/MFH?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/MFH">https://opendatalab.org.cn/MFH</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2109.02917.pdf" rel="nofollow" title="https://arxiv.org/pdf/2109.02917.pdf">https://arxiv.org/pdf/2109.02917.pdf</a></p> 
<p></p> 
<h3 id="No.20"><em><strong>20.</strong></em>  <strong><a class="link-info" href="https://opendatalab.org.cn/11k_Hands" rel="nofollow" title="11k Hands">11k Hands</a></strong></h3> 
<p><strong>● 发布方：</strong>约克大学、阿斯尤特大学</p> 
<p><strong>● 发布时间</strong>：2019</p> 
<p><strong>● 简介：</strong><a class="link-info" href="https://opendatalab.org.cn/11k_Hands" rel="nofollow" title="11k Hands">11k Hands</a>是一个用于性别识别和生物特征识别的详细真实信息的人类手部图像（背侧和手掌侧）的大型数据集。包含 190 位参与者的人手图片数据集，年龄在 18-75 岁之间。所有背景色均为白色，且拍照距离一致。</p> 
<p>对应的元数据包含：● 主题 ID● 性别● 年龄● 肤色● 手的信息，即右手或左手，手侧（背侧或手掌）和逻辑指示符，用于指示手部图像是否包含附件，指甲油或不规则物。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/d6/c1/ROqrIWPF_o.png">​</p> 
</div> 
<p>数据集样例示意，每行显示的是同一对象的8个手部图像</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a class="link-info" href="https://opendatalab.org.cn/11k_Hands?source=Y3Nkbg%3D%3D" rel="nofollow" title="https://opendatalab.org.cn/11k_Hands">https://opendatalab.org.cn/11k_Hands</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1711.04322v9.pdf" rel="nofollow" title="https://arxiv.org/pdf/1711.04322v9.pdf">https://arxiv.org/pdf/1711.04322v9.pdf</a></p> 
<p></p> 
<h3 id="No.21"><em><strong>21.  </strong></em><strong>LD-ConGR</strong></h3> 
<p><strong>● 发布方：</strong>中国科学院软件研究所</p> 
<p><strong>● 发布时间</strong>：2022</p> 
<p><strong>● 简介：</strong>LD ConGR中总共收集了542个视频和44887个手势实例。视频从5个不同场景中的30名受试者收集，并用Kinect V4以第三视角拍摄。每个视频包含一个颜色流和一个深度流。这两个流以30fps同步记录，分辨率分别为1280 x 720和640 x 576。</p> 
<p>数据集按受试者随机分为训练集和测试集（23名受试者用于训练，7名受试人用于测试）。手势实例的数量和手势持续时间的统计（以帧为单位测量）如下：</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/56/5e/w80PkbJM_o.png">​</p> 
</div> 
<p>手势实例数和手势持续时间统计</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/5f/2e/rqWVnB1b_o.png">​</p> 
</div> 
<p>LD ConGR数据集的十类手势，包括三个静态手势和七个动态手势手势。每列显示了同一类。红色箭头指示手的动作</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/97/50/hfkze82Q_o.png">​</p> 
</div> 
<p>会议场景手势识别示意</p> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/Diananini/LD-ConGR-CVPR2022" title="https://github.com/Diananini/LD-ConGR-CVPR2022">https://github.com/Diananini/LD-ConGR-CVPR2022</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//openaccess.thecvf.com/content/CVPR2022/papers/Liu_LD-ConGR_A_Large_RGB-D_Video_Dataset_for_Long-Distance_Continuous_Gesture_CVPR_2022_paper.pdf" rel="nofollow" title="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_LD-ConGR_A_Large_RGB-D_Video_Dataset_for_Long-Distance_Continuous_Gesture_CVPR_2022_paper.pdf">https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_LD-ConGR_A_Large_RGB-D_Video_Dataset_for_Long-Distance_Continuous_Gesture_CVPR_2022_paper.pdf</a></p> 
<p></p> 
<h3 id="No.22"><strong><em>22.</em></strong>  InterHand2.6M</h3> 
<p><strong>● 发布方：</strong>Facebook Reality Lab</p> 
<p><strong>● 发布时间</strong>：2020</p> 
<p><strong>● 简介：</strong>第一个具有准确 GT 3D 交互手部姿势的大规模实拍数据集。</p> 
<p>InterHand2.6M 数据集是一个大规模实拍数据集，具有准确的 GT 3D 交互手部姿势，用于 3D 手部姿势估计该数据集包含 2.6M 标记的单个和交互手部框架。InterHand2.6M 最初是在 4096×2668 分辨率下拍摄的，但为了保护指纹隐私，发布的数据集分辨率为512×334。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/10/e5/74jZuOAN_o.jpg">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/d5/8e/VJw994ON_o.jpg">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/0f/6b/vBp58mmD_o.jpg">​</p> 
</div> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/47/0a/UmKVUMUt_o.jpg">​</p> 
</div> 
<p>● <strong>下载地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//mks0601.github.io/InterHand2.6M/" rel="nofollow" title="https://mks0601.github.io/InterHand2.6M/">https://mks0601.github.io/InterHand2.6M/</a></p> 
<p><strong>● 论文地址</strong>：</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2008.09309.pdf" rel="nofollow" title="https://arxiv.org/pdf/2008.09309.pdf">https://arxiv.org/pdf/2008.09309.pdf</a></p> 
<p></p> 
<p>以上就是本次分享，获取海量数据集资源，请访问<a class="link-info" href="https://opendatalab.org.cn/?source=Y3Nkbg%3D%3D" rel="nofollow" title="OpenDataLab官网">OpenDataLab官网</a>；获取更多开源工具及项目，请访问<a class="link-info" href="https://github.com/opendatalab" title="OpenDataLab Github空间">OpenDataLab Github空间</a>。另外还有哪些想看的内容，快来告诉小助手吧。更多数据集上架动态、更全面的数据集内容解读、最牛大佬在线答疑、最活跃的同行圈子……欢迎添加微信opendatalab_yunying加入OpenDataLab官方交流群。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8ba010d103f70b9b2bbb9c44739f5d56/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">删除字符串中的重复字符（C语言）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/144fcffcb02c232301768f36a18221fe/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【nacos】com.alibaba.nacos.shaded.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>