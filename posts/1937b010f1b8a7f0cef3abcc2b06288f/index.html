<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>scrapy入门小案例--爬取电影天堂最新电影下载地址 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="scrapy入门小案例--爬取电影天堂最新电影下载地址" />
<meta property="og:description" content="本文开发环境：ubuntu16.04 &#43; scrapy1.5 &#43; python3.5 &#43; pycharm2017.03 scrapy学习也有段时间了，刚开始也是跟着视屏一点点学习，看着挺简单的，到了动手的时候就不知道如何下手。现在通过一个小案例来总结下如何使用，如果能帮到你那我就很欣慰了。
必备技能：
python基础 推荐廖雪峰python教程网页基础 W3Cschoolxpath 推荐W3Cschool Xpath教程 正文
我们来看看电影天堂的网站结构。我们进入最新电影。 这里可以看到这里有很多电影，我们看到总共有173页。 光有下载连接可不行，我们还要电影的名称和上传时间。 下面看看怎么提取。 先回到第一页，然后CTRL &#43; shift &#43; X打开xpath helper，在电影名称上面右键–检查—copy–copy xpath，粘贴入Query那一栏，单击，看看是不是取到了名称，哈哈。 但是只是取到了一个，我们要这一页的全部名称，而且这么长的xpath表达式看着一点也不优雅。我们我们来修改下。 通过观察，我们所需要的内容是在一个&lt;div class=&#34;co_content8&#34;&gt;里面的。 所以，我们的xpath规则可以写成（名称）
//div[@class=&#39;co_content8&#39;]/ul/table//a/text() 是不是都取到了电影名称。哈哈。同理上传日期：
//div[@class=&#39;co_content8&#39;]/ul/table//tbody//font/text() 名称和时间都取到了，让我们看看怎么取每部电影的下载地址。进入详情页。打开xpath helper，F12进入调试模式。可以看到有两个下载地址。既然这样，那就全要了。 FTP:
//tbody//td[@style=&#34;WORD-WRAP: break-word&#34;]//a/@href[0] 迅雷：
//tbody//td[@style=&#34;WORD-WRAP: break-word&#34;]//a/@href[1] 现在我们已经取到第一页的数据，那如何跟进取到后面所有页的数据呢？一种办法是在第一页取到下一页的链接，然后进入下一页取数据。当然有第二种方法。我们右键查看源代码的时候，鼠标拉到最下边，可以看到：
&lt;option value=&#39;list_23_1.html&#39; selected&gt;1&lt;/option&gt; &lt;option value=&#39;list_23_2.html&#39;&gt;2&lt;/option&gt; &lt;option value=&#39;list_23_3.html&#39;&gt;3&lt;/option&gt; 这里列出了全部页面的链接。只要我们取到value的值，就可以遍历所有的页面了，哈哈，是不是很方便？电影天堂详情页虽然乱七八糟，但一点还是很赞的。 那我们怎么取到value的值，一样的。
这里注意一下，有些xpath规则在xpath helper里面正常，但到了scrapy就不行。原因就是在浏览器加载页面的时候帮我们优化了，而scrapy取到的是网页源代码。这就是个坑。所以，提取规格的时候要在scrapy shell验证好。
取全部页面链接的xpath规则：
//table//b/a/@href 好了，现在我们就可以开始写代码了。
在命令行中：
scrapy startproject Dianying 然后使用scrapy创建默认spider文件：
scrapy genspider dytt &#34;www.ygdy8.net&#34; 我的目录结构是这样的： 1. 编写要爬取的内容items.py" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/1937b010f1b8a7f0cef3abcc2b06288f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-27T23:38:52+08:00" />
<meta property="article:modified_time" content="2018-03-27T23:38:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">scrapy入门小案例--爬取电影天堂最新电影下载地址</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>本文开发环境：ubuntu16.04 + scrapy1.5 + python3.5 + pycharm2017.03 </p> 
</blockquote> 
<p>scrapy学习也有段时间了，刚开始也是跟着视屏一点点学习，看着挺简单的，到了动手的时候就不知道如何下手。现在通过一个小案例来总结下如何使用，如果能帮到你那我就很欣慰了。</p> 
<blockquote> 
 <p>必备技能：</p> 
</blockquote> 
<ul><li>python基础 推荐<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/" rel="nofollow">廖雪峰python教程</a></li><li>网页基础 <a href="http://www.w3school.com.cn/index.html" rel="nofollow">W3Cschool</a></li><li>xpath 推荐<a href="http://www.w3school.com.cn/xpath/" rel="nofollow">W3Cschool Xpath教程</a></li></ul> 
<blockquote> 
 <p>正文</p> 
</blockquote> 
<p>我们来看看电影天堂的网站结构。我们进入最新电影。 <br> <img src="https://images2.imgbox.com/39/51/Asr8LN3N_o.png" alt="这里写图片描述" title=""> <br> 这里可以看到这里有很多电影，我们看到总共有173页。 <br> <img src="https://images2.imgbox.com/fc/36/9HTEh9ZQ_o.png" alt="这里写图片描述" title=""> <br> 光有下载连接可不行，我们还要电影的名称和上传时间。 <br> 下面看看怎么提取。 <br> 先回到第一页，然后CTRL + shift + X打开xpath helper，在电影名称上面右键–检查—copy–copy xpath，粘贴入Query那一栏，单击，看看是不是取到了名称，哈哈。 <br> <img src="https://images2.imgbox.com/5e/31/VIIFKVcH_o.png" alt="这里写图片描述" title=""> <br> 但是只是取到了一个，我们要这一页的全部名称，而且这么长的xpath表达式看着一点也不优雅。我们我们来修改下。 <br> 通过观察，我们所需要的内容是在一个<code>&lt;div class="co_content8"&gt;</code>里面的。 <br> <img src="https://images2.imgbox.com/0d/6a/Sx95JpNb_o.png" alt="这里写图片描述" title=""> <br> 所以，我们的xpath规则可以写成（<strong>名称</strong>）</p> 
<pre class="prettyprint"><code class=" hljs ruby">/<span class="hljs-regexp">/div[@class='co_content8']/ul</span><span class="hljs-regexp">/table/</span><span class="hljs-regexp">/a/text</span>()</code></pre> 
<p>是不是都取到了电影名称。哈哈。同理<strong>上传日期</strong>：</p> 
<pre class="prettyprint"><code class=" hljs ruby">/<span class="hljs-regexp">/div[@class='co_content8']/ul</span><span class="hljs-regexp">/table/</span><span class="hljs-regexp">/tbody/</span><span class="hljs-regexp">/font/text</span>()</code></pre> 
<p>名称和时间都取到了，让我们看看怎么取每部电影的下载地址。进入详情页。打开xpath helper，F12进入调试模式。可以看到有两个下载地址。既然这样，那就全要了。 <br> <strong>FTP:</strong></p> 
<pre class="prettyprint"><code class=" hljs ruby">/<span class="hljs-regexp">/tbody/</span><span class="hljs-regexp">/td[@style="WORD-WRAP: break-word"]/</span><span class="hljs-regexp">/a/</span><span class="hljs-variable">@href</span>[<span class="hljs-number">0</span>]</code></pre> 
<p><strong>迅雷：</strong></p> 
<pre class="prettyprint"><code class=" hljs ruby">/<span class="hljs-regexp">/tbody/</span><span class="hljs-regexp">/td[@style="WORD-WRAP: break-word"]/</span><span class="hljs-regexp">/a/</span><span class="hljs-variable">@href</span>[<span class="hljs-number">1</span>]</code></pre> 
<p>现在我们已经取到第一页的数据，那如何跟进取到后面所有页的数据呢？一种办法是在第一页取到下一页的链接，然后进入下一页取数据。当然有第二种方法。我们右键查看源代码的时候，鼠标拉到最下边，可以看到：</p> 
<pre class="prettyprint"><code class=" hljs ocaml">&lt;<span class="hljs-built_in">option</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'list_23_1.html'</span> selected&gt;<span class="hljs-number">1</span>&lt;/<span class="hljs-built_in">option</span>&gt;
&lt;<span class="hljs-built_in">option</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'list_23_2.html'</span>&gt;<span class="hljs-number">2</span>&lt;/<span class="hljs-built_in">option</span>&gt;
&lt;<span class="hljs-built_in">option</span> <span class="hljs-keyword">value</span>=<span class="hljs-string">'list_23_3.html'</span>&gt;<span class="hljs-number">3</span>&lt;/<span class="hljs-built_in">option</span>&gt;</code></pre> 
<p>这里列出了全部页面的链接。只要我们取到value的值，就可以遍历所有的页面了，哈哈，是不是很方便？电影天堂详情页虽然乱七八糟，但一点还是很赞的。 <br> 那我们怎么取到value的值，一样的。</p> 
<p><strong>这里注意一下，有些xpath规则在xpath helper里面正常，但到了scrapy就不行。原因就是在浏览器加载页面的时候帮我们优化了，而scrapy取到的是网页源代码。这就是个坑。所以，提取规格的时候要在scrapy shell验证好。</strong></p> 
<p>取全部页面链接的xpath规则：</p> 
<pre class="prettyprint"><code class=" hljs ruby">/<span class="hljs-regexp">/table/</span><span class="hljs-regexp">/b/a</span><span class="hljs-regexp">/@href</span></code></pre> 
<p>好了，现在我们就可以开始写代码了。</p> 
<p>在命令行中：</p> 
<pre class="prettyprint"><code class=" hljs ">scrapy startproject Dianying</code></pre> 
<p>然后使用scrapy创建默认spider文件：</p> 
<pre class="prettyprint"><code class=" hljs bash">scrapy genspider dytt <span class="hljs-string">"www.ygdy8.net"</span></code></pre> 
<p>我的目录结构是这样的： <br> <img src="https://images2.imgbox.com/5a/32/m53pctcf_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>1. 编写要爬取的内容items.py</strong></p> 
<pre class="prettyprint"><code class=" hljs avrasm">
import scrapy

class DianyingItem(scrapy<span class="hljs-preprocessor">.Item</span>): 
    <span class="hljs-preprocessor"># 电影标题</span>
    title = scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor"># 上传时间</span>
    release_date = scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor"># 电影详情页</span>
    url = scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor"># 电影下载链接FTP</span>
    download_link = scrapy<span class="hljs-preprocessor">.Field</span>()
    <span class="hljs-preprocessor"># 迅雷下载链接</span>
    thunder_download_link = scrapy<span class="hljs-preprocessor">.Field</span>()
</code></pre> 
<p><strong>2. 编写爬虫文件</strong></p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> scrapy
<span class="hljs-comment"># 导入item类</span>
<span class="hljs-keyword">from</span> Dianying.items <span class="hljs-keyword">import</span> DianyingItem


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DyttSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span>
    <span class="hljs-comment"># 爬虫名</span>
    name = <span class="hljs-string">'dytt'</span>
    <span class="hljs-comment"># 爬虫爬取的域</span>
    allowed_domains = [<span class="hljs-string">'www.ygdy8.net'</span>]
    <span class="hljs-comment"># 爬取页面链接</span>
    start_urls = [<span class="hljs-string">'http://www.dytt8.net/html/gndy/dyzz/list_23_1.html'</span>]

    <span class="hljs-comment"># 解析函数</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span>
        <span class="hljs-comment"># 定义一个列表存放items</span>
        items = []
        <span class="hljs-comment"># 取全部标题，转化为列表</span>
        title = response.xpath(<span class="hljs-string">'//table//b/a/text()'</span>).extract()
        <span class="hljs-comment"># 取全部上传时间，转化为列表</span>
        release_date = response.xpath(<span class="hljs-string">'//table//font/text()'</span>).extract()
        <span class="hljs-comment"># 取全部详情页的链接</span>
        url = response.xpath(<span class="hljs-string">'//table//b/a/@href'</span>).extract()

        <span class="hljs-comment"># 遍历列表，将结果存入items</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(title)):
            <span class="hljs-comment"># 实例化一个item类，然后将结果全部存入items</span>
            item = DianyingItem()
            item[<span class="hljs-string">'title'</span>] = title[i]
            item[<span class="hljs-string">'release_date'</span>] = release_date[i]
            <span class="hljs-comment"># 拼接URL，获取所有电影详情的URL，拼成完整的URL来访问地址</span>
            item[<span class="hljs-string">'url'</span>] = <span class="hljs-string">'http://www.dytt8.net'</span> + url[i]

            items.append(item)
        <span class="hljs-comment"># 如果parse函数没有解析完成，可以将结果存为字典春给meta变量，交给parse_download_link继续解析</span>
        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items:
            <span class="hljs-keyword">yield</span> scrapy.Request(url=item[<span class="hljs-string">'url'</span>], meta={<!-- --><span class="hljs-string">'meta'</span>: item}, callback=self.parse_download_link)
        <span class="hljs-comment"># 这个是下一页跟进，从第二页开始，因为第一页我们已经爬过了。</span>
        next_urls = response.xpath(<span class="hljs-string">'//select[@name="sldd"]/option/@value'</span>).extract()[<span class="hljs-number">2</span>:]
        <span class="hljs-keyword">for</span> next_url <span class="hljs-keyword">in</span> next_urls:
            <span class="hljs-keyword">yield</span> response.follow(next_url, callback=self.parse)

    <span class="hljs-comment"># 这个函数进入详情页获取下载地址</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse_download_link</span><span class="hljs-params">(self, response)</span>:</span>
        <span class="hljs-comment"># 先获取parse函数解析的结果</span>
        item = response.meta[<span class="hljs-string">'meta'</span>]
        <span class="hljs-comment"># 取到下载链接，实际上这里有两个</span>
        download_link = response.xpath(<span class="hljs-string">'//tbody//td[@style="WORD-WRAP: break-word"]//a/@href'</span>).extract()
        <span class="hljs-comment"># 有些电影是没有下载地址的，所以要判断一下，有的才继续下一步。</span>
        <span class="hljs-keyword">if</span> len(download_link) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            <span class="hljs-comment"># 首先获取到FTP的地址</span>
            item[<span class="hljs-string">'download_link'</span>] = download_link[<span class="hljs-number">0</span>]
            <span class="hljs-comment"># 有些是没有迅雷下载地址的，所以要判断一下，没有thunder_download_link就为空，不然会报错。</span>
            <span class="hljs-keyword">if</span> len(download_link) == <span class="hljs-number">2</span>:
                item[<span class="hljs-string">'thunder_download_link'</span>] = download_link[<span class="hljs-number">1</span>]
            <span class="hljs-keyword">else</span>:
                item[<span class="hljs-string">'thunder_download_link'</span>] = <span class="hljs-string">''</span>
        <span class="hljs-comment"># 现在我们已经拿到全部数据了，可以把item返回了。</span>
        <span class="hljs-keyword">yield</span> item
</code></pre> 
<ul><li>name = “” ：爬虫名称，必须唯一。</li><li>allow_domains = [ ] ：定义搜索域名范围。</li><li>start_urls = () ：爬取的URL元祖/列表。</li><li>parse(self, response) ：解析函数：提取数据&amp;生成下一页链接。 <br> 我们来看URL链接，刚进来是这样的：</li></ul> 
<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//www<span class="hljs-preprocessor">.ygdy</span>8<span class="hljs-preprocessor">.net</span>/html/gndy/dyzz/index<span class="hljs-preprocessor">.html</span></code></pre> 
<p>我们点击第二页，第三页：</p> 
<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//www<span class="hljs-preprocessor">.ygdy</span>8<span class="hljs-preprocessor">.net</span>/html/gndy/dyzz/list_23_2<span class="hljs-preprocessor">.html</span>
<span class="hljs-label">http:</span>//www<span class="hljs-preprocessor">.ygdy</span>8<span class="hljs-preprocessor">.net</span>/html/gndy/dyzz/list_23_3<span class="hljs-preprocessor">.html</span>
</code></pre> 
<p>发现规律了吧，实际上就是<code>list_23_xx</code>的xx数字在变，要爬第一页打就是：</p> 
<pre class="prettyprint"><code class=" hljs avrasm"><span class="hljs-label">http:</span>//www<span class="hljs-preprocessor">.ygdy</span>8<span class="hljs-preprocessor">.net</span>/html/gndy/dyzz/list_23_1<span class="hljs-preprocessor">.html</span></code></pre> 
<p>和刚进来的页面是一样的。只要在代码里面拼接URL就可以了。</p> 
<p><strong>3. 编写pipelines.py</strong> <br> 现在我们拿到数据了，可以将结果保存为各种形式。</p> 
<ul><li>保存为json文件：</li></ul> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">import</span> codecs
<span class="hljs-comment"># 导入json模块</span>
<span class="hljs-keyword">import</span> json

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DianyingPipeline</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-comment"># 初始化函数，创建json文件，指定编码格式</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        self.filename = codecs.open(<span class="hljs-string">'dytt.json'</span>, <span class="hljs-string">'w'</span>, encoding=<span class="hljs-string">'utf-8'</span>)

    <span class="hljs-comment"># 将item写入，在结尾增加换行符</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        content = json.dumps(dict(item), ensure_ascii=<span class="hljs-keyword">False</span>) + <span class="hljs-string">'\n'</span>
        self.filename.write(content)
        <span class="hljs-comment"># 解析一个存一个，返回item继续跟进</span>
        <span class="hljs-keyword">return</span> item

    <span class="hljs-comment"># 爬问后就把文件关了，释放资源</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">spider_close</span><span class="hljs-params">(self, spider)</span>:</span>
        self.filename.close()</code></pre> 
<p>写好pipelines.py后还要在setting.py文件注册一下才能起作用。 <br> 当然我们也要把<a href="https://baike.baidu.com/item/%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86/1471005?fromtitle=user-agent&amp;fromid=10574244&amp;fr=aladdin" rel="nofollow">USER-AGENT</a>设置下，还有爬取时间，尊重下网站维护人员，哈哈。 <br> <img src="https://images2.imgbox.com/b9/72/2lCfr5bI_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/0e/a9/34oFt0g6_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/c8/b4/aaEQAuh0_o.png" alt="这里写图片描述" title=""> <br> 好了，现在可以跑起我们的爬虫了，命令行输入：</p> 
<pre class="prettyprint"><code class=" hljs ">scrapy crawl dytt</code></pre> 
<p>如果没有报错，结果： <br> <img src="https://images2.imgbox.com/76/5e/4UezeBpR_o.png" alt="这里写图片描述" title=""> <br> 4000多行，哈哈。</p> 
<ul><li>保存为excel文件 <br> 保存excel要安装一个python库：</li></ul> 
<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> pip install OpenPyxl</code></pre> 
<p>新建Item_to_xlsx.py文件 <br> <img src="https://images2.imgbox.com/70/5c/YUCHpZwH_o.png" alt="这里写图片描述" title=""> <br> 代码如下：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#!/usr/bin/python  </span>
<span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-comment"># 导入模块</span>
<span class="hljs-keyword">from</span> openpyxl <span class="hljs-keyword">import</span> Workbook


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Item_to_xlsx</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment"># 先实例化一个class对象</span>
        self.wb = Workbook()
        <span class="hljs-comment"># 激活工作表</span>
        self.ws = self.wb.active
        <span class="hljs-comment"># 添加一行数据，就是excel表的第一行，标记这一列的作用</span>
        self.ws.append([<span class="hljs-string">'名称'</span>, <span class="hljs-string">'收入时间'</span>, <span class="hljs-string">'地址详情'</span>, <span class="hljs-string">'下载链接'</span>, <span class="hljs-string">'迅雷下载链接'</span>])

    <span class="hljs-comment"># 下面就是把数据写入表中啦</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        line = [item[<span class="hljs-string">'title'</span>], item[<span class="hljs-string">'release_date'</span>], item[<span class="hljs-string">'url'</span>], item[<span class="hljs-string">'download_link'</span>], item[<span class="hljs-string">'thunder_download_link'</span>]]
        self.ws.append(line)
        self.wb.save(<span class="hljs-string">'/home/lucky/dytt.xlsx'</span>)</code></pre> 
<p>写完后，还是要在setting.py文件中注册下，实际上上面的图中已经给出了，只是我把它注释掉了，将它打开就行。注意如果既要保存为json,又要保存excel，就把它后面的数字改一下顺序，数据越小越优先。 <br> 跑起来：</p> 
<pre class="prettyprint"><code class=" hljs ">scrapy crawl dytt</code></pre> 
<p>来看下成果： <br> <img src="https://images2.imgbox.com/1c/ab/OBuHcihw_o.png" alt="这里写图片描述" title=""> <br> 也是4000多行，首行也有了我们定义的名字。哈哈。</p> 
<ul><li>保存进MySQL数据库 <br> 首先得保证你的系统已经<a href="https://www.linuxidc.com/Linux/2017-05/143864.htm" rel="nofollow">安装了MySQL</a>，还要安装python相关库：</li></ul> 
<pre class="prettyprint"><code class=" hljs bash"><span class="hljs-built_in">sudo</span> pip install pymysql</code></pre> 
<p>在setting.py文件末尾定义MySQL相关配置项：</p> 
<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor"># 端口</span>
MYSQL_HOST = <span class="hljs-string">'localhost'</span>
<span class="hljs-preprocessor"># 数据库名称,根据自己的情况定义</span>
MYSQL_DBNAME = <span class="hljs-string">'xxx'</span>
<span class="hljs-preprocessor"># 用户,根据自己的情况定义</span>
MYSQL_USER = <span class="hljs-string">'xxx'</span>
<span class="hljs-preprocessor"># 密码,根据自己的情况定义</span>
MYSQL_PASSWD = <span class="hljs-string">'xxx'</span></code></pre> 
<p>新建DBinfo_pipeline.py文件：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#!/usr/bin/python  </span>
<span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-comment"># 导入模块</span>
<span class="hljs-keyword">import</span> pymysql
<span class="hljs-comment"># 导入setting文件</span>
<span class="hljs-keyword">from</span> Dianying <span class="hljs-keyword">import</span> settings


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DBinfo_pipeline</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-comment"># 获取seting文件的设置，并连接数据库</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        self.connect = pymysql.connect(
            host=settings.MYSQL_HOST,
            db=settings.MYSQL_DBNAME,
            user=settings.MYSQL_USER,
            passwd=settings.MYSQL_PASSWD,
            charset=<span class="hljs-string">'utf8'</span>,
            use_unicode=<span class="hljs-keyword">True</span>
        )
        self.cursor = self.connect.cursor()

    <span class="hljs-comment"># 下面就是和数据库插入有关的命令了，如果不熟悉，建议学习下mysql</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        <span class="hljs-keyword">try</span>:
            self.cursor.execute(
                <span class="hljs-string">'''insert into dytt_info(title, release_date, url, download_link, thunder_download_link)
                  value(%s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE download_link=VALUES(download_link), thunder_download_link=VALUES(thunder_download_link)'''</span>,
                (item[<span class="hljs-string">'title'</span>],
                 item[<span class="hljs-string">'release_date'</span>],
                 item[<span class="hljs-string">'url'</span>],
                 item[<span class="hljs-string">'download_link'</span>],
                 item[<span class="hljs-string">'thunder_download_link'</span>]))
            self.connect.commit()
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> error:
            print(error)
        <span class="hljs-keyword">return</span> item</code></pre> 
<p>好了，写完了，在setting文件中注册下，跑起来：</p> 
<pre class="prettyprint"><code class=" hljs ">scrapy crawl dytt</code></pre> 
<p>用navicat看下： <br> <img src="https://images2.imgbox.com/d8/8a/UNmDzYlT_o.png" alt="这里写图片描述" title=""></p> 
<p>收工。</p> 
<p><strong>总结：</strong> <br> scrapy是python网络爬虫框架。 <br> 使用它首先确定要爬取的内容，然后用xpath helper匹配出xpath规则，其实不只xpath,还可以用css选择器，beautiful soul等提取页面内容，这里就只用xpath，希望能够起到抛砖引玉的作用。 <br> 接下来就是数据的保存了，可以保存为各种格式。scrapy已经为我们提供好了，别打我，我只是想让你多学一点，哈哈。</p> 
<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor"># json格式，默认为Unicode编码</span>
scrapy crawl dytt -o dytt.json

<span class="hljs-preprocessor"># json lines格式，默认为Unicode编码</span>
scrapy crawl dytt -o dytt.jsonl

<span class="hljs-preprocessor"># csv 逗号表达式，可用Excel打开</span>
scrapy crawl dytt -o dytt.csv

<span class="hljs-preprocessor"># xml格式</span>
scrapy crawl dytt -o dytt.xml</code></pre> 
<p>还有就是setting.py文件中的配置，设置爬取间隔，设置USER-AGENT，还有设置IP池等，大家慢慢研究了。 <br> 所谓师傅带进门，修行靠个人。希望这篇文章对你有帮助。</p> 
<p>over。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e75b20c8717dce4476f6ece93bd27d37/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【MySQL】死锁案例之七</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/208e6f2667e4329d725890bbd1079d01/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【MySQL】死锁案例之八</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>