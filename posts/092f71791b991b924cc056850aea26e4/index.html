<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于VideoReTalking&#43;GFPGAN的AI数字人 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于VideoReTalking&#43;GFPGAN的AI数字人" />
<meta property="og:description" content="VideoReTalking 是一个新颖的系统，用于根据输入音频编辑真实世界中的说话头部视频，生成具有不同情感的高质量和唇部同步的输出视频。
该系统将此目标分解为三个顺序任务：
具有规范表情的人脸视频生成；音频驱动的唇部同步；用于提高照片真实感的人脸增强。 wav2lip和VideoReTalking超分后效果对比
文章目录 基本原理准备工作创建虚拟环境激活虚拟环境pip安装匹配版本 模型预测对口型必要的数据准备命令行预测参数解释其他使用方法WebUI预测 基本原理 人脸视频生成：使用表情编辑网络修改每一帧的表情，使其与相同的表情模板相匹配，从而生成具有规范表情的视频。音频驱动的唇部同步：将该视频与给定的音频一起输入唇部同步网络，生成唇部同步视频。人脸增强：通过身份感知的人脸增强网络和后处理来提高合成人脸的照片真实感。 所有这些步骤都使用基于ML和DL学习的方法，并且可以在没有任何用户干预的情况下按顺序进行。
准备工作 从github上下载源码，下载完毕之后下载模型 pre-trained models。
模型分这么多种类，将全部模型复制到 checkpoints 中。
创建虚拟环境 切记这个里面有编译的环境在里面，不要移动创建好的虚拟环境，除非你能自己修改源码修改对应的路径文件。
conda create -n VideoReTalking python=3.8 激活虚拟环境 在GIT环境目录下激活，这里是我的路径，修改成你自己的。
conda activate VideoReTalking pip安装匹配版本 确保安装了所有必要的依赖项，可以通过requirements.txt文件进行安装。
这里会有个问题要根据显卡选择torch版本，我的显卡是RTX4090，我选择这个安装命令。
pip install torch==2.0.0&#43;cu118 torchvision==0.15.1&#43;cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118 pip install -r requirements.txt 其他显卡根据情况自行安装。
pip install torch==1.9.0&#43;cu111 torchvision==0.10.0&#43;cu111 -f https://download.pytorch.org/whl/torch_stable.html pip install -r requirements.txt 模型预测对口型 必要的数据准备 事先还要准备好素材视频和音频文件，这里分别放置原始的图片，音频以及视频文件。
其中音频和视频文件放在项目目录下的./examples/audio和 ./examples/face中即可。
命令行预测 使用预训练模型进行快速推理，可以测试任何说话的人脸视频，无需手动对齐。
\VideoReTalking\python.exe inference.py --face examples/face/1.mp4 --audio examples/audio/1.wav --outfile results/1_1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/092f71791b991b924cc056850aea26e4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-04T15:28:44+08:00" />
<meta property="article:modified_time" content="2023-08-04T15:28:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于VideoReTalking&#43;GFPGAN的AI数字人</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>VideoReTalking 是一个新颖的系统，用于根据输入音频编辑真实世界中的说话头部视频，生成具有不同情感的高质量和唇部同步的输出视频。</p> 
<p>该系统将此目标分解为三个顺序任务：</p> 
<ol><li>具有规范表情的人脸视频生成；</li><li>音频驱动的唇部同步；</li><li>用于提高照片真实感的人脸增强。</li></ol> 
<p></p> 
<div class="csdn-video-box"> 
 <iframe id="xL1s65RH-1691132242841" frameborder="0" src="https://live.csdn.net/v/embed/316965" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
 <p>wav2lip和VideoReTalking超分后效果对比</p> 
</div> 
<p></p> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_14" rel="nofollow">基本原理</a></li><li><a href="#_22" rel="nofollow">准备工作</a></li><li><ul><li><a href="#_30" rel="nofollow">创建虚拟环境</a></li><li><a href="#_38" rel="nofollow">激活虚拟环境</a></li><li><a href="#pip_47" rel="nofollow">pip安装匹配版本</a></li></ul> 
  </li><li><a href="#_67" rel="nofollow">模型预测对口型</a></li><li><ul><li><a href="#_69" rel="nofollow">必要的数据准备</a></li><li><a href="#_77" rel="nofollow">命令行预测</a></li><li><a href="#_88" rel="nofollow">参数解释</a></li><li><a href="#_228" rel="nofollow">其他使用方法</a></li><li><a href="#WebUI_238" rel="nofollow">WebUI预测</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_14"></a>基本原理</h2> 
<ol><li><strong>人脸视频生成</strong>：使用表情编辑网络修改每一帧的表情，使其与相同的表情模板相匹配，从而生成具有规范表情的视频。</li><li><strong>音频驱动的唇部同步</strong>：将该视频与给定的音频一起输入唇部同步网络，生成唇部同步视频。</li><li><strong>人脸增强</strong>：通过身份感知的人脸增强网络和后处理来提高合成人脸的照片真实感。</li></ol> 
<p>所有这些步骤都使用基于ML和DL学习的方法，并且可以在没有任何用户干预的情况下按顺序进行。</p> 
<h2><a id="_22"></a>准备工作</h2> 
<p><a href="https://github.com/OpenTalker/video-retalking">从github上下载源码</a>，下载完毕之后下载模型 <a href="https://drive.google.com/drive/folders/18rhjMpxK8LVVxf7PI6XwOidt8Vouv_H0?usp=share_link" rel="nofollow">pre-trained models</a>。</p> 
<p><img src="https://images2.imgbox.com/4d/27/JsXLbv1T_o.png" alt="在这里插入图片描述"><br> 模型分这么多种类，将全部模型复制到 checkpoints 中。</p> 
<h3><a id="_30"></a>创建虚拟环境</h3> 
<p>切记这个里面有编译的环境在里面，不要移动创建好的虚拟环境，除非你能自己修改源码修改对应的路径文件。</p> 
<pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> VideoReTalking <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.8</span>
</code></pre> 
<h3><a id="_38"></a>激活虚拟环境</h3> 
<p>在GIT环境目录下激活，这里是我的路径，修改成你自己的。</p> 
<pre><code class="prism language-bash">conda activate VideoReTalking 
</code></pre> 
<h3><a id="pip_47"></a>pip安装匹配版本</h3> 
<p>确保安装了所有必要的依赖项，可以通过<code>requirements.txt</code>文件进行安装。</p> 
<p>这里会有个问题要根据显卡选择torch版本，我的显卡是RTX4090，我选择这个安装命令。</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token assign-left variable">torch</span><span class="token operator">==</span><span class="token number">2.0</span>.0+cu118 <span class="token assign-left variable">torchvision</span><span class="token operator">==</span><span class="token number">0.15</span>.1+cu118 <span class="token assign-left variable">torchaudio</span><span class="token operator">==</span><span class="token number">2.0</span>.1 --index-url https://download.pytorch.org/whl/cu118

pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
</code></pre> 
<p>其他显卡根据情况自行安装。</p> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token assign-left variable">torch</span><span class="token operator">==</span><span class="token number">1.9</span>.0+cu111 <span class="token assign-left variable">torchvision</span><span class="token operator">==</span><span class="token number">0.10</span>.0+cu111 <span class="token parameter variable">-f</span> https://download.pytorch.org/whl/torch_stable.html

pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txt
</code></pre> 
<h2><a id="_67"></a>模型预测对口型</h2> 
<h3><a id="_69"></a>必要的数据准备</h3> 
<p>事先还要准备好素材视频和音频文件，这里分别放置原始的图片，音频以及视频文件。</p> 
<p>其中音频和视频文件放在项目目录下的<code>./examples/audio</code>和 <code>./examples/face</code>中即可。</p> 
<p><img src="https://images2.imgbox.com/ac/cd/pjyWvEAp_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_77"></a>命令行预测</h3> 
<p>使用预训练模型进行快速推理，可以测试任何说话的人脸视频，无需手动对齐。</p> 
<pre><code class="prism language-bash"><span class="token punctuation">\</span>VideoReTalking<span class="token punctuation">\</span>python.exe inference.py <span class="token parameter variable">--face</span> examples/face/1.mp4 <span class="token parameter variable">--audio</span> examples/audio/1.wav <span class="token parameter variable">--outfile</span> results/1_1.mp4

python inference.py <span class="token parameter variable">--face</span> examples/face/1.mp4 <span class="token parameter variable">--audio</span> examples/audio/1.wav <span class="token parameter variable">--outfile</span> results/1_1.mp4
</code></pre> 
<p>推理总共分6步，依次执行后会在result文件夹下生成对应的视频文件。<img src="https://images2.imgbox.com/ac/20/eqr2qwM8_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_88"></a>参数解释</h3> 
<p>基础参数设置<code>base_options.py</code></p> 
<table><thead><tr><th>参数</th><th>类型</th><th>默认值</th><th>解释</th></tr></thead><tbody><tr><td>–name</td><td>str</td><td>‘face_recon’</td><td>实验名称，决定样本和模型存储的位置</td></tr><tr><td>–gpu_ids</td><td>str</td><td>‘0’</td><td>GPU的ID，例如：0、0,1,2、0,2。使用-1表示CPU</td></tr><tr><td>–checkpoints_dir</td><td>str</td><td>‘./checkpoints’</td><td>模型存储的目录</td></tr><tr><td>–vis_batch_nums</td><td>float</td><td>1</td><td>用于可视化的图像批次数</td></tr><tr><td>–eval_batch_nums</td><td>float</td><td>inf</td><td>用于评估的图像批次数，设置为inf表示所有图像都参与评估</td></tr><tr><td>–use_ddp</td><td>bool</td><td>True</td><td>是否使用分布式数据并行</td></tr><tr><td>–ddp_port</td><td>str</td><td>‘12355’</td><td>DDP端口</td></tr><tr><td>–display_per_batch</td><td>bool</td><td>True</td><td>是否使用批次显示损失</td></tr><tr><td>–add_image</td><td>bool</td><td>True</td><td>是否将图像添加到Tensorboard中</td></tr><tr><td>–world_size</td><td>int</td><td>1</td><td>分布式数据并行的总批次数</td></tr><tr><td>–model</td><td>str</td><td>‘facerecon’</td><td>选择要使用的模型</td></tr><tr><td>–epoch</td><td>str</td><td>‘latest’</td><td>要加载的模型的训练轮数，设置为’latest’表示使用最新的缓存模型</td></tr><tr><td>–verbose</td><td>bool</td><td></td><td>如果指定，则打印更多调试信息</td></tr><tr><td>–suffix</td><td>str</td><td>‘’</td><td>自定义后缀，将添加到opt.name中，例如：{model}_{netG}_size{load_size}</td></tr></tbody></table> 
<p>推理工具参数设置<code>inference_utils.py</code></p> 
<table><thead><tr><th>参数名</th><th>类型</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>DNet_path</td><td>str</td><td>‘checkpoints/DNet.pt’</td><td>DNet模型的路径</td></tr><tr><td>LNet_path</td><td>str</td><td>‘checkpoints/LNet.pth’</td><td>LNet模型的路径</td></tr><tr><td>ENet_path</td><td>str</td><td>‘checkpoints/ENet.pth’</td><td>ENet模型的路径</td></tr><tr><td>face3d_net_path</td><td>str</td><td>‘checkpoints/face3d_pretrain_epoch_20.pth’</td><td>face3d模型的路径</td></tr><tr><td>face</td><td>str</td><td>None</td><td>包含要使用的人脸的视频/图像的文件路径，此参数必填</td></tr><tr><td>audio</td><td>str</td><td>None</td><td>要用作原始音频源的视频/音频文件的文件路径，此参数必填</td></tr><tr><td>exp_img</td><td>str</td><td>‘neutral’</td><td>表情模板。可以是’neutral’，‘smile’或图像路径。默认为’neutral’</td></tr><tr><td>outfile</td><td>str</td><td>None</td><td>要保存结果视频的路径</td></tr><tr><td>fps</td><td>float</td><td>25.0</td><td>只有当输入为静态图像时可以指定的帧率，默认为25.0</td></tr><tr><td>pads</td><td>list</td><td>[0, 20, 0, 0]</td><td>填充（上、下、左、右）。请确保至少包含下巴区域</td></tr><tr><td>face_det_batch_size</td><td>int</td><td>4</td><td>人脸检测的批处理大小</td></tr><tr><td>LNet_batch_size</td><td>int</td><td>16</td><td>LNet的批处理大小</td></tr><tr><td>img_size</td><td>int</td><td>384</td><td>图像的大小（宽度和高度相等）</td></tr><tr><td>crop</td><td>list</td><td>[0, -1, 0, -1]</td><td>将视频裁剪为较小的区域（上、下、左、右）。在resize_factor和rotate参数之后应用。如果有多个人脸，这很有用。 -1表示根据高度、宽度自动推断值</td></tr><tr><td>box</td><td>list</td><td>[-1, -1, -1, -1]</td><td>为人脸指定一个固定的边界框。如果人脸检测失败，请仅在万不得已时使用此选项。仅在人脸几乎不移动时有效。 语法：(上、下、左、右)</td></tr><tr><td>nosmooth</td><td>bool</td><td>False</td><td>在短时间窗口内阻止平滑人脸检测</td></tr><tr><td>static</td><td>bool</td><td>False</td><td>指定输入是否为静态图像</td></tr><tr><td>up_face</td><td>str</td><td>‘original’</td><td>人脸朝向的方向。可以是’original’或其他用户指定的方向</td></tr><tr><td>one_shot</td><td>bool</td><td>False</td><td>一次处理整个视频而不是逐帧处理</td></tr><tr><td>without_rl1</td><td>bool</td><td>False</td><td>不使用相对l1损失</td></tr><tr><td>tmp_dir</td><td>str</td><td>‘temp’</td><td>保存临时结果的文件夹路径</td></tr><tr><td>re_preprocess</td><td>bool</td><td>False</td><td>重新预处理视频（例如，检测新的人脸）</td></tr></tbody></table> 
<p>模型训练参数设置<code>train_options.py</code>，训练模型根据实际情况调整。</p> 
<table><thead><tr><th>训练数据参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>data_root</td><td>str</td><td>./</td><td>数据集根目录</td></tr><tr><td>flist</td><td>str</td><td>datalist/train/masks.txt</td><td>训练集掩膜文件名列表</td></tr><tr><td>batch_size</td><td>int</td><td>32</td><td>批处理大小</td></tr><tr><td>dataset_mode</td><td>str</td><td>flist</td><td>选择数据集加载方式。[None</td></tr><tr><td>serial_batches</td><td>bool</td><td></td><td>如果为True，按顺序获取图像以形成批次；否则随机获取图像。</td></tr><tr><td>num_threads</td><td>int</td><td>4</td><td>加载数据的线程数</td></tr><tr><td>max_dataset_size</td><td>int</td><td>inf</td><td>数据集允许的最大样本数。如果数据集目录包含的样本数超过max_dataset_size，则仅加载子集。</td></tr><tr><td>preprocess</td><td>str</td><td>shift_scale_rot_flip</td><td>加载时图像的缩放和裁剪方式。[shift_scale_rot_flip</td></tr><tr><td>use_aug</td><td>bool</td><td>True</td><td>是否使用数据增强</td></tr></tbody></table> 
<table><thead><tr><th>验证参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>flist_val</td><td>str</td><td>datalist/val/masks.txt</td><td>验证集掩膜文件名列表</td></tr><tr><td>batch_size_val</td><td>int</td><td>32</td><td>验证集的批处理大小</td></tr></tbody></table> 
<table><thead><tr><th>可视化参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>display_freq</td><td>int</td><td>1000</td><td>在屏幕上显示训练结果的频率</td></tr><tr><td>print_freq</td><td>int</td><td>100</td><td>在控制台上显示训练结果的频率</td></tr></tbody></table> 
<table><thead><tr><th>网络保存和加载参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>save_latest_freq</td><td>int</td><td>5000</td><td>保存最新结果的频率</td></tr><tr><td>save_epoch_freq</td><td>int</td><td>1</td><td>在每个epoch结束时保存检查点的频率</td></tr><tr><td>evaluation_freq</td><td>int</td><td>5000</td><td>评估的频率</td></tr><tr><td>save_by_iter</td><td>bool</td><td></td><td>是否按迭代保存模型</td></tr><tr><td>continue_train</td><td>bool</td><td></td><td>继续训练：加载最新模型</td></tr><tr><td>epoch_count</td><td>int</td><td>1</td><td>起始epoch计数，我们按&lt;epoch_count&gt;，&lt;epoch_count&gt;+&lt;save_latest_freq&gt;，…保存模型</td></tr><tr><td>phase</td><td>str</td><td>train</td><td>训练、验证、测试等</td></tr><tr><td>pretrained_name</td><td>str</td><td>None</td><td>从其他检查点继续训练</td></tr></tbody></table> 
<table><thead><tr><th>训练参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>n_epochs</td><td>int</td><td>20</td><td>初始学习率的epoch数</td></tr><tr><td>lr</td><td>float</td><td>0.0001</td><td>Adam的初始学习率</td></tr><tr><td>lr_policy</td><td>str</td><td>step</td><td>学习率策略。[linear</td></tr><tr><td>lr_decay_epochs</td><td>int</td><td>10</td><td>每lr_decay_epochs个epoch乘以一个gamma</td></tr></tbody></table> 
<p>脸部对焦参数配置<code>facerecon_model.py</code>，这些参数默认即可。</p> 
<table><thead><tr><th>网络结构参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>net_recon</td><td>str</td><td>‘resnet50’</td><td>网络结构</td></tr><tr><td>init_path</td><td>str</td><td>‘checkpoints/init_model/resnet50-0676ba61.pth’</td><td>初始化路径</td></tr><tr><td>use_last_fc</td><td>bool</td><td>False</td><td>是否对最后一个全连接层进行零初始化</td></tr><tr><td>bfm_folder</td><td>str</td><td>‘BFM’</td><td>BFM文件夹路径</td></tr><tr><td>bfm_model</td><td>str</td><td>‘BFM_model_front.mat’</td><td>BFM模型</td></tr></tbody></table> 
<table><thead><tr><th>渲染器参数参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>focal</td><td>float</td><td>1015.</td><td>焦距</td></tr><tr><td>center</td><td>float</td><td>112.</td><td>中心点</td></tr><tr><td>camera_d</td><td>float</td><td>10.</td><td>相机参数d</td></tr><tr><td>z_near</td><td>float</td><td>5.</td><td>近截面</td></tr><tr><td>z_far</td><td>float</td><td>15.</td><td>远截面</td></tr></tbody></table> 
<table><thead><tr><th>训练参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>net_recog</td><td>str</td><td>‘r50’</td><td>人脸识别网络结构</td></tr><tr><td>net_recog_path</td><td>str</td><td>‘checkpoints/recog_model/ms1mv3_arcface_r50_fp16/backbone.pth’</td><td>人脸识别网络的权重文件路径</td></tr><tr><td>use_crop_face</td><td>bool</td><td>False</td><td>是否使用裁剪掩码来计算照片损失</td></tr><tr><td>use_predef_M</td><td>bool</td><td>False</td><td>是否使用预定义的M矩阵来处理预测的人脸特征 (M矩阵用于三维形状预测)</td></tr></tbody></table> 
<table><thead><tr><th>数据增强参数参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>shift_pixs</td><td>float</td><td>10.0</td><td>像素平移大小</td></tr><tr><td>scale_delta</td><td>float</td><td>0.1</td><td>尺度缩放因子的变化范围</td></tr><tr><td>rot_angle</td><td>float</td><td>10.0</td><td>旋转角度的变化范围 (单位：度)</td></tr></tbody></table> 
<table><thead><tr><th>损失权重参数</th><th>数据类型</th><th>默认值</th><th>解释说明</th></tr></thead><tbody><tr><td>w_feat</td><td>float</td><td>0.2</td><td>特征损失权重</td></tr><tr><td>w_color</td><td>float</td><td>1.92</td><td>颜色损失权重</td></tr><tr><td>w_reg</td><td>float</td><td>3.0e-4</td><td>形状正则化损失权重</td></tr><tr><td>w_id</td><td>float</td><td>1.0</td><td>身份正则化损失权重</td></tr><tr><td>w_exp</td><td>float</td><td>0.8</td><td>表情正则化损失权重</td></tr><tr><td>w_tex</td><td>float</td><td>1.7e-2</td><td>纹理正则化损失权重</td></tr><tr><td>w_gamma</td><td>float</td><td>10.0</td><td>Gamma矫正损失权重</td></tr><tr><td>w_lm</td><td>float</td><td>1.6e-3</td><td>关键点坐标损失权重</td></tr><tr><td>w_reflc</td><td>float</td><td>5.0</td><td>反照率损失权重</td></tr></tbody></table> 
<h3><a id="_228"></a>其他使用方法</h3> 
<p>表情控制参数操作，可以通过添加以下参数来控制表情：</p> 
<table><thead><tr><th>参数</th><th>解释</th></tr></thead><tbody><tr><td>–exp_img</td><td>预定义的表情模板。默认为"neutral"（中性表情）。可以选择"smile"（微笑）或提供一个图片路径。</td></tr><tr><td>–up_face</td><td>可以选择"surprise"（惊讶）或"angry"（愤怒）来使用 GANimation 修改上半部分脸部的表情。</td></tr></tbody></table> 
<h3><a id="WebUI_238"></a>WebUI预测</h3> 
<p>项目中自带WebUI启动文件，是基于gradio开发的页面。</p> 
<p>由于代码中素材文件是写死的如果想更换素材并且在页面中显示必须进行一些修改。</p> 
<p>添加音频和视频文件的路径目录。</p> 
<pre><code class="prism language-python">audio_list <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span><span class="token string">"./examples/audio"</span><span class="token punctuation">)</span>
video_list <span class="token operator">=</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span><span class="token string">"./examples/face"</span><span class="token punctuation">)</span>
</code></pre> 
<p>添加路径并遍历文件到对应的文件位置。</p> 
<pre><code class="prism language-python">            <span class="token keyword">with</span> gradio<span class="token punctuation">.</span>Row<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                gradio<span class="token punctuation">.</span>Examples<span class="token punctuation">(</span>
                    label<span class="token operator">=</span><span class="token string">"Face Examples"</span><span class="token punctuation">,</span>
                    examples<span class="token operator">=</span><span class="token punctuation">[</span>
                        os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"examples/face/"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> video_list
                    <span class="token punctuation">]</span><span class="token punctuation">,</span>
                    inputs<span class="token operator">=</span><span class="token punctuation">[</span>v<span class="token punctuation">]</span><span class="token punctuation">,</span>
                    fn<span class="token operator">=</span>convert<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token keyword">with</span> gradio<span class="token punctuation">.</span>Row<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                gradio<span class="token punctuation">.</span>Examples<span class="token punctuation">(</span>
                    label<span class="token operator">=</span><span class="token string">"Audio Examples"</span><span class="token punctuation">,</span>
                    examples<span class="token operator">=</span><span class="token punctuation">[</span>
                        os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"examples/audio/"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> audio_list
                    <span class="token punctuation">]</span><span class="token punctuation">,</span>
                    inputs<span class="token operator">=</span><span class="token punctuation">[</span>a<span class="token punctuation">]</span><span class="token punctuation">,</span>
                    fn<span class="token operator">=</span>convert<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
</code></pre> 
<p>然后启动<code>webUI.py</code>文件，然后在浏览器中就可以看到自己添加好的对应文件啦。</p> 
<p><img src="https://images2.imgbox.com/93/fd/5NbSu583_o.png" alt="在这里插入图片描述"></p> 
<p>最终出来的结果是这样的，视频效果呢个人感觉比wav2lip强一些，如果视频分辨率不大的话无所谓，如果原始分辨率大的话需要进行超分操作。</p> 
<p><img src="https://images2.imgbox.com/3d/3c/Q276rGIa_o.png" alt="在这里插入图片描述"></p> 
<p>来看一下和wav2lip的对比吧。虽然处理效率慢了一点淡出出来的效果更佳清楚。<br> <img src="https://images2.imgbox.com/7d/03/IHl3Pghy_o.jpg" alt="请添加图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7f575377b88cb96c5effc29dd0143bb4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">SQL多表查询的注意点,以及 join on 、where 执行的顺序</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c6a1bd51a9aeef6edb10b8c3b5bf957f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">太好了！终于可以离线安装浏览器扩展了~</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>