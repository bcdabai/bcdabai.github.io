<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Segment Anything论文翻译，SAM模型，SAM论文，SAM论文翻译；一个用于图像分割的新任务、模型和数据集；SA-1B数据集 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Segment Anything论文翻译，SAM模型，SAM论文，SAM论文翻译；一个用于图像分割的新任务、模型和数据集；SA-1B数据集" />
<meta property="og:description" content="【论文翻译】- Segment Anything / Model / SAM论文 论文链接：
https://arxiv.org/pdf/2304.02643.pdfhttps://ai.facebook.com/research/publications/segment-anything/ 代码连接：https://github.com/facebookresearch/segment-anything
论文翻译：
http://t.csdn.cn/nnqs8https://blog.csdn.net/leiduifan6944/article/details/130080159 文章目录 【论文翻译】- Segment Anything / Model / SAM论文摘要1、简介2、分割任何物体任务3、分割任意物体模型4、分割任何数据引擎5、数据集6、RAI分析7、零样本迁移实验7.1、零样本单点有效掩码评估7.2、零样本边缘检测7.3、零样本目标建议7.4、零样本实例分割7.5、Zero-Shot Text-to-Mask7.6、消融研究 8、讨论参考引用链接： 摘要 本文提出Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在1100万张授权和尊重隐私的图像上有超过10亿个掩码。该模型被设计和训练为可提示的，因此它可以将零样本迁移到新的图像分布和任务。评估了其在许多任务上的能力，发现其零样本性能令人印象深刻——通常与之前的完全监督结果竞争，甚至优于。我们将在https://segment-anything.com&#34;上发布Segment Anything模型(SAM)和相应的数据集(SA-1B)，其中包含1B个掩码和1100万张图像，以促进对计算机视觉基础模型的研究。
1、简介 在网络规模的数据集上预训练的大型语言模型正在以强大的零样本和少样本泛化[10]彻底改变NLP。这些“基础模型”[8]可以泛化到训练期间看到的任务和数据分布之外。这种能力通常通过提示工程实现，其中使用手工制作的文本来提示语言模型为手头的任务生成有效的文本响应。当使用来自web的丰富文本语料库进行缩放和训练时，这些模型的零次和少次性能与微调模型(在某些情况下甚至匹配)相比惊人地好[10,21]。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算量的增加而改善[56,10,21,51]。
计算机视觉领域也对基础模型进行了探索，尽管程度较低。也许最突出的插图对齐了来自网络的成对文本和图像。例如，CLIP[82]和ALIGN[55]使用对比学习来训练对齐两种模态的文本和图像编码器。经过训练，工程文本提示可以零样本泛化到新的视觉概念和数据分布。这种编码器还可以与其他模块有效组合，以实现下游任务，如图像生成(如DALL·E[83])。虽然在视觉和语言编码器方面已经取得了很大进展，但计算机视觉包括了超出这个范围的广泛问题，对于其中许多问题，并不存在丰富的训练数据。
本文的目标是建立一个图像分割的基础模型。也就是说，本文试图开发一个可提示的模型，并使用一个能实现强大泛化的任务在广泛的数据集上对其进行预训练。使用该模型，旨在使用prompt工程解决新数据分布上的一系列下游分割问题。
这个计划的成功取决于三个部分:任务、模型和数据。为发展它们，本文解决有关图像分割的以下问题。
1.什么任务将实现零样本泛化?
2.对应的模型架构是什么?
3.什么数据可以支持这个任务和模型?
这些问题错综复杂，需要一个全面的解决方案。首先定义一个可提示的分割任务，足够通用，可以提供一个强大的预训练目标，并实现广泛的下游应用。此任务需要一个支持灵活提示的模型，并可以在提示时实时输出分割掩码，以允许交互使用。为了训练我们的模型，我们需要一个多样化的、大规模的数据源。不幸的是，没有web规模的分割数据源;为了解决这个问题，我们构建了一个“数据引擎”，也就是说，我们在使用高效的模型来帮助收集数据和使用新收集的数据来改进模型之间进行迭代。接下来介绍每个相互关联的组件，然后是创建的数据集和证明方法有效性的实验。
任务(§2)。在NLP和最近的计算机视觉中，基础模型是一个有希望的发展，可以通过使用&#34;提示&#34;技术对新数据集和任务进行零样本和少样本学习。受这一行工作的启发，本文提出了promptable分割任务，目标是在给定任何分割提示时返回有效的分割掩码(见图1a)。提示符只是指定要在图像中分割的内容，例如，提示符可以包括识别对象的空间或文本信息。有效输出掩码的要求意味着，即使提示是模糊的，并且可能指向多个对象(例如，衬衫上的一个点可能表示衬衫或穿着它的人)，输出也应该是其中至少一个对象的合理掩码。将提示分割任务作为预训练目标，并通过提示工程解决一般的下游分割任务。
模型(§3)。可提示的分割任务和现实世界使用的目标对模型架构施加了约束。特别是，该模型必须支持灵活的提示，需要实时摊销计算掩码以允许交互使用，并且必须能够感知歧义。一个简单的设计满足了所有三个约束:一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后将两个信息源组合在一个预测分割掩码的轻量级掩码解码器中。我们把这个模型称为Segment Anything模型，简称SAM(见图1b)。通过将SAM分离为图像编码器和提示符快速编码器/掩码解码器，相同的图像嵌入可以在不同的提示符中重用(及其成本分摊)。给定图像嵌入，提示编码器和掩码解码器在web浏览器中从提示符预测掩码的时间为50ms。重点关注点、框和掩码提示，还用自由形式的文本提示呈现初步结果。为使SAM具有歧义性，设计了它来为单个提示预测多个面具，使SAM能够自然地处理歧义，如衬衫和人的例子。
数据引擎(§4)。为了对新的数据分布实现强大的泛化，有必要在一个大型和多样化的掩码集上训练SAM，而不是现有的任何分割数据集。虽然基础模型的典型方法是在线获取数据[82]，但掩码本身并不丰富，因此我们需要另一种策略。我们的解决方案是建立一个“数据引擎”，即我们与在环模型数据集注释共同开发我们的模型(见图1c)。我们的数据引擎有3个阶段:辅助手动、半自动和全自动。在第一阶段，SAM帮助标注者注释掩码，类似于经典的交互式分割设置。在第二阶段，SAM可以通过提示它可能的对象位置，自动为一个对象子集生成掩码，注释器专注于注释剩余的对象，帮助增加掩码多样性。在最后阶段，用规则的前景点网格提示SAM，平均每张图像产生100个高质量掩模。
数据集(§5)。最终数据集SA-1B包括来自11M授权和保护隐私图像的超过1B个掩码(见图2)。使用数据引擎的最后阶段完全自动收集的SA-1B的掩码比任何现有的分割数据集[66,44,117,60]多400倍，并且经过广泛验证，掩码具有高质量和多样性。除了用于训练SAM使其健壮和通用外，我们希望SA-1B成为旨在建立新的基础模型的研究的宝贵资源。
负责任的AI(§6)。研究和报告了使用SA-1B和SAM时潜在的公平问题和偏见。SA-1B中的图像涵盖了一组地理和经济上不同的国家，我们发现SAM在不同人群中的表现相似。总之，我们希望这将使我们的工作对现实世界的用例更加公平。我们在附录中提供了模型卡和数据集卡。
实验(§7)。我们对SAM进行了广泛的评估。首先，使用一套不同的23个分割数据集，SAM从单个前景点产生高质量的掩码，通常只略低于手动标注的地面真实值。在使用prompt工程的零样本迁移协议下，对各种下游任务取得了一致强大的定量和定性结果，包括边缘检测、目标建议框生成、实例分割，以及对文本到掩码预测的初步探索。这些结果表明，SAM可以与prompt engineering一起开箱即用，以解决涉及SAM训练数据之外的物体和图像分布的各种任务。然而，正如我们在§8中所讨论的，改进的空间仍然存在。
Release。为了研究目的，我们发布了SA-1B数据集，并使SAM在一个许可的开放许可证(Apache 2.0)下可在https://segment-anything.com上使用。我们还通过一个在线演示展示了SAM的功能。
2、分割任何物体任务 从NLP中获得灵感，下一个token预测任务用于基础模型预训练，并通过提示工程[10]解决各种下游任务。为了建立一个分割的基础模型，本文旨在定义一个具有类似能力的任务。
任务。首先，我们将提示的想法从NLP转换为分割，其中提示可以是一组前景/背景点，一个粗略的框或掩码，自由形式的文本，或者一般来说，任何表示图像中要分割的信息。那么，promptable切分任务就是在给定任何提示时返回一个有效的切分掩码。“有效”掩模的要求只是意味着，即使提示是模糊的，并且可能指向多个对象(例如，回想一下衬衫和人的例子，参见图3)，输出也应该是这些对象中的至少一个的合理掩模。这个要求类似于期望语言模型对有歧义的提示输出连贯的响应。选择这项任务，是因为它导致了一种自然的预训练算法和一种通过提示将零样本迁移到下游分割任务的通用方法。
预训练。promptable segmentation任务提出了一种自然的预训练算法，为每个训练样本模拟一系列提示(例如，点、框、掩码)，并将模型的掩码预测与基本事实进行比较。本文从交互式分割[109,70]中采用这种方法，尽管与交互式分割的目的是在足够的用户输入后最终预测有效掩码不同，本文的目标是始终为任何提示预测有效掩码，即使提示是模糊的。这确保了预训练模型在涉及歧义的用例中是有效的，包括我们的数据引擎要求的自动注释§4。我们注意到，在这项任务中表现良好具有挑战性，需要专门的建模和训练损失选择，我们在§3中讨论。
Zero-shot迁移。预训练任务赋予了模型在推理时适当响应任何提示的能力，因此下游任务可以通过工程适当的提示来解决。例如，如果有一个猫的边界框检测器，则可以通过提供检测器的框输出作为我们模型的提示来解决猫实例分割。一般来说，一系列实际的分割任务都可以作为提示。除了自动数据集标注，在§7的实验中探索了五个不同的示例任务。
相关的任务。分割是一个广泛的领域:有交互式分割[57,109]、边缘检测[3]、超像素化[85]、目标建议生成[2]、前景分割[94]、语义分割[90]、实例分割[66]、全景分割[59]等。提示分割任务的目标是产生一个功能广泛的模型，通过提示工程可以适应许多(尽管不是全部)现有的和新的分割任务。这种能力是任务泛化[26]的一种形式。请注意，这与之前在多任务分割系统上的工作不同。在多任务系统中，单个模型执行一组固定的任务，例如联合语义、实例和全景分割[114,19,54]，但训练和测试任务是相同的。本文工作中的一个重要区别是，为提示分割训练的模型可以作为更大系统中的一个组件，在推理时执行新的、不同的任务，例如，为了执行实例分割，将提示分割模型与现有的目标检测器相结合。
讨论。提示和组合是强大的工具，使单个模型能够以可扩展的方式使用，可能完成模型设计时未知的任务。这种方法类似于其他基础模型的使用方式，例如CLIP[82]是DALL·E[83]图像生成系统的文本-图像对齐组件。我们预计，由prompt engineering等技术驱动的可组合系统设计，将比专门为固定任务集训练的系统实现更广泛的应用。通过组合的角度来比较提示分割和交互分割也是很有趣的:虽然交互分割模型是为人类用户设计的，但为提示分割训练的模型也可以组合成一个更大的算法系统，我们将演示。
3、分割任意物体模型 接下来，我们描述了分段任意模型(SAM)，以实现快速分割。SAM有三个组件，如图4所示:图像编码器，灵活的提示编码器和快速掩码解码器。建立了Transformer视觉模型[14,33,20,62]，并对(平摊)实时性能进行了特定的权衡。我们在这里高层次地描述这些组件，详情见§A。
图像编码器。受可扩展性和强大的预训练方法的激励，本文使用MAE[47]预训练视觉Transformer (ViT)[33]最小适应于处理高分辨率输入[62]。图像编码器对每张图像运行一次，可以在提示模型之前应用。
提示编码器。考虑两组提示:稀疏(点、框、文本)和密集(掩码)。我们通过位置编码[95]表示点和框，并对每种提示类型的学习嵌入和自由形式的文本使用CLIP[82]的现成文本编码器进行求和。密集提示(即掩码)使用卷积嵌入，并与图像嵌入元素相加。
掩码译码器。掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。该设计受[14,20]的启发，采用了对Transformer解码器块[103]的修改，然后是动态掩码预测头。修改后的解码器块使用两个方向上的提示自注意力和交叉注意力(提示到图像嵌入，反之亦然)来更新所有嵌入。在运行两个块后，我们对图像嵌入进行上采样，MLP将输出标记映射到动态线性分类器，然后计算每个图像位置的掩码前景概率。
解决歧义。使用一个输出，如果给出模糊的提示，模型将平均多个有效掩码。为解决这个问题，我们修改了模型，以为单个提示预测多个输出掩码(见图3)。我们发现3个掩码输出足以解决大多数常见的情况(嵌套掩码通常最多有3个深度:整体、部分和子部分)。在训练过程中，我们只对掩模的最小损失[15,45,64]进行反向传播。为了对口罩进行排名，该模型预测每个口罩的置信度分数(即估计的IoU)。
效率。整体模型设计很大程度上是由效率驱动的。给定一个预先计算的图像嵌入，prompt编码器和mask解码器在web浏览器上运行，在CPU上，大约需要50毫秒。这种运行时性能使我们的模型能够无缝、实时的交互提示。
损失和训练。我们用[14]中使用的focal loss[65]和dice loss[73]的线性组合来监督掩膜预测。使用几何提示的混合来训练可提示的分割任务(关于文本提示，见§7.5)。在[92,37]之后，通过在每个掩码中11轮随机采样提示来模拟交互式设置，允许SAM无缝集成到我们的数据引擎中。
4、分割任何数据引擎 由于分割掩码在互联网上并不丰富，我们构建了一个数据引擎，以收集我们的11 b掩码数据集SA-1B。该数据引擎有三个阶段:(1)模型辅助的手动标注阶段，(2)半自动阶段，其中混合了自动预测的掩码和模型辅助的标注，以及(3)全自动阶段，在该阶段中，我们的模型在没有标注者输入的情况下生成掩码。我们接下来会详细介绍每一种。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/aee4d91f02e81adb31f9838117148475/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-12T23:12:43+08:00" />
<meta property="article:modified_time" content="2023-04-12T23:12:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Segment Anything论文翻译，SAM模型，SAM论文，SAM论文翻译；一个用于图像分割的新任务、模型和数据集；SA-1B数据集</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_Segment_Anything__Model__SAM_0"></a>【论文翻译】- Segment Anything / Model / SAM论文</h2> 
<p>论文链接：</p> 
<ul><li><a href="https://arxiv.org/pdf/2304.02643.pdf" rel="nofollow">https://arxiv.org/pdf/2304.02643.pdf</a></li><li><a href="https://ai.facebook.com/research/publications/segment-anything/" rel="nofollow">https://ai.facebook.com/research/publications/segment-anything/</a></li></ul> 
<p>代码连接：<a href="https://github.com/facebookresearch/segment-anything">https://github.com/facebookresearch/segment-anything</a></p> 
<p>论文翻译：</p> 
<ul><li><a href="http://t.csdn.cn/nnqs8" rel="nofollow">http://t.csdn.cn/nnqs8</a></li><li><a href="https://blog.csdn.net/leiduifan6944/article/details/130080159">https://blog.csdn.net/leiduifan6944/article/details/130080159</a></li></ul> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_Segment_Anything__Model__SAM_0" rel="nofollow">【论文翻译】- Segment Anything / Model / SAM论文</a></li><li><ul><li><a href="#_14" rel="nofollow">摘要</a></li><li><a href="#1_19" rel="nofollow">1、简介</a></li><li><a href="#2_59" rel="nofollow">2、分割任何物体任务</a></li><li><a href="#3_78" rel="nofollow">3、分割任意物体模型</a></li><li><a href="#4_99" rel="nofollow">4、分割任何数据引擎</a></li><li><a href="#5_115" rel="nofollow">5、数据集</a></li><li><a href="#6RAI_133" rel="nofollow">6、RAI分析</a></li><li><a href="#7_146" rel="nofollow">7、零样本迁移实验</a></li><li><ul><li><a href="#71_156" rel="nofollow">7.1、零样本单点有效掩码评估</a></li><li><a href="#72_173" rel="nofollow">7.2、零样本边缘检测</a></li><li><a href="#73_185" rel="nofollow">7.3、零样本目标建议</a></li><li><a href="#74_195" rel="nofollow">7.4、零样本实例分割</a></li><li><a href="#75ZeroShot_TexttoMask_209" rel="nofollow">7.5、Zero-Shot Text-to-Mask</a></li><li><a href="#76_217" rel="nofollow">7.6、消融研究</a></li></ul> 
   </li><li><a href="#8_230" rel="nofollow">8、讨论</a></li><li><a href="#_240" rel="nofollow">参考引用链接：</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_14"></a>摘要</h3> 
<p>本文提出Segment Anything (SA)项目:一个用于图像分割的新任务、模型和数据集。在数据收集循环中使用我们的高效模型，我们构建了迄今为止(到目前为止)最大的分割数据集，在1100万张授权和尊重隐私的图像上有超过10亿个掩码。该模型被设计和训练为可提示的，因此它可以将零样本迁移到新的图像分布和任务。评估了其在许多任务上的能力，发现其零样本性能令人印象深刻——通常与之前的完全监督结果竞争，甚至优于。我们将在<a href="https://segment-anything.com" rel="nofollow">https://segment-anything.com</a>"上发布Segment Anything模型(SAM)和相应的数据集(SA-1B)，其中包含1B个掩码和1100万张图像，以促进对计算机视觉基础模型的研究。<br> <img src="https://images2.imgbox.com/3a/62/09ApOh3X_o.png" alt="1-18.PNG"></p> 
<h3><a id="1_19"></a>1、简介</h3> 
<p>在网络规模的数据集上预训练的大型语言模型正在以强大的零样本和少样本泛化[10]彻底改变NLP。这些“基础模型”[8]可以泛化到训练期间看到的任务和数据分布之外。这种能力通常通过提示工程实现，其中使用手工制作的文本来提示语言模型为手头的任务生成有效的文本响应。当使用来自web的丰富文本语料库进行缩放和训练时，这些模型的零次和少次性能与微调模型(在某些情况下甚至匹配)相比惊人地好[10,21]。经验趋势表明，这种行为随着模型规模、数据集大小和总训练计算量的增加而改善[56,10,21,51]。</p> 
<p>计算机视觉领域也对基础模型进行了探索，尽管程度较低。也许最突出的插图对齐了来自网络的成对文本和图像。例如，CLIP[82]和ALIGN[55]使用对比学习来训练对齐两种模态的文本和图像编码器。经过训练，工程文本提示可以零样本泛化到新的视觉概念和数据分布。这种编码器还可以与其他模块有效组合，以实现下游任务，如图像生成(如DALL·E[83])。虽然在视觉和语言编码器方面已经取得了很大进展，但计算机视觉包括了超出这个范围的广泛问题，对于其中许多问题，并不存在丰富的训练数据。</p> 
<p>本文的目标是建立一个图像分割的基础模型。也就是说，本文试图开发一个可提示的模型，并使用一个能实现强大泛化的任务在广泛的数据集上对其进行预训练。使用该模型，旨在使用prompt工程解决新数据分布上的一系列下游分割问题。</p> 
<p>这个计划的成功取决于三个部分:任务、模型和数据。为发展它们，本文解决有关图像分割的以下问题。</p> 
<p>1.什么任务将实现零样本泛化?</p> 
<p>2.对应的模型架构是什么?</p> 
<p>3.什么数据可以支持这个任务和模型?</p> 
<p>这些问题错综复杂，需要一个全面的解决方案。首先定义一个可提示的分割任务，足够通用，可以提供一个强大的预训练目标，并实现广泛的下游应用。此任务需要一个支持灵活提示的模型，并可以在提示时实时输出分割掩码，以允许交互使用。为了训练我们的模型，我们需要一个多样化的、大规模的数据源。不幸的是，没有web规模的分割数据源;为了解决这个问题，我们构建了一个“数据引擎”，也就是说，我们在使用高效的模型来帮助收集数据和使用新收集的数据来改进模型之间进行迭代。接下来介绍每个相互关联的组件，然后是创建的数据集和证明方法有效性的实验。</p> 
<p>任务(§2)。在NLP和最近的计算机视觉中，基础模型是一个有希望的发展，可以通过使用"提示"技术对新数据集和任务进行零样本和少样本学习。受这一行工作的启发，本文提出了promptable分割任务，目标是在给定任何分割提示时返回有效的分割掩码(见图1a)。提示符只是指定要在图像中分割的内容，例如，提示符可以包括识别对象的空间或文本信息。有效输出掩码的要求意味着，即使提示是模糊的，并且可能指向多个对象(例如，衬衫上的一个点可能表示衬衫或穿着它的人)，输出也应该是其中至少一个对象的合理掩码。将提示分割任务作为预训练目标，并通过提示工程解决一般的下游分割任务。</p> 
<p>模型(§3)。可提示的分割任务和现实世界使用的目标对模型架构施加了约束。特别是，该模型必须支持灵活的提示，需要实时摊销计算掩码以允许交互使用，并且必须能够感知歧义。一个简单的设计满足了所有三个约束:一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后将两个信息源组合在一个预测分割掩码的轻量级掩码解码器中。我们把这个模型称为Segment Anything模型，简称SAM(见图1b)。通过将SAM分离为图像编码器和提示符快速编码器/掩码解码器，相同的图像嵌入可以在不同的提示符中重用(及其成本分摊)。给定图像嵌入，提示编码器和掩码解码器在web浏览器中从提示符预测掩码的时间为50ms。重点关注点、框和掩码提示，还用自由形式的文本提示呈现初步结果。为使SAM具有歧义性，设计了它来为单个提示预测多个面具，使SAM能够自然地处理歧义，如衬衫和人的例子。</p> 
<p><img src="https://images2.imgbox.com/f5/c1/t53in7QY_o.png" alt="02-18.PNG"></p> 
<p>数据引擎(§4)。为了对新的数据分布实现强大的泛化，有必要在一个大型和多样化的掩码集上训练SAM，而不是现有的任何分割数据集。虽然基础模型的典型方法是在线获取数据[82]，但掩码本身并不丰富，因此我们需要另一种策略。我们的解决方案是建立一个“数据引擎”，即我们与在环模型数据集注释共同开发我们的模型(见图1c)。我们的数据引擎有3个阶段:辅助手动、半自动和全自动。在第一阶段，SAM帮助标注者注释掩码，类似于经典的交互式分割设置。在第二阶段，SAM可以通过提示它可能的对象位置，自动为一个对象子集生成掩码，注释器专注于注释剩余的对象，帮助增加掩码多样性。在最后阶段，用规则的前景点网格提示SAM，平均每张图像产生100个高质量掩模。</p> 
<p><img src="https://images2.imgbox.com/41/7f/rpSzbiTI_o.png" alt="03-18.PNG"></p> 
<p>数据集(§5)。最终数据集SA-1B包括来自11M授权和保护隐私图像的超过1B个掩码(见图2)。使用数据引擎的最后阶段完全自动收集的SA-1B的掩码比任何现有的分割数据集[66,44,117,60]多400倍，并且经过广泛验证，掩码具有高质量和多样性。除了用于训练SAM使其健壮和通用外，我们希望SA-1B成为旨在建立新的基础模型的研究的宝贵资源。</p> 
<p>负责任的AI(§6)。研究和报告了使用SA-1B和SAM时潜在的公平问题和偏见。SA-1B中的图像涵盖了一组地理和经济上不同的国家，我们发现SAM在不同人群中的表现相似。总之，我们希望这将使我们的工作对现实世界的用例更加公平。我们在附录中提供了模型卡和数据集卡。</p> 
<p>实验(§7)。我们对SAM进行了广泛的评估。首先，使用一套不同的23个分割数据集，SAM从单个前景点产生高质量的掩码，通常只略低于手动标注的地面真实值。在使用prompt工程的零样本迁移协议下，对各种下游任务取得了一致强大的定量和定性结果，包括边缘检测、目标建议框生成、实例分割，以及对文本到掩码预测的初步探索。这些结果表明，SAM可以与prompt engineering一起开箱即用，以解决涉及SAM训练数据之外的物体和图像分布的各种任务。然而，正如我们在§8中所讨论的，改进的空间仍然存在。</p> 
<p>Release。为了研究目的，我们发布了SA-1B数据集，并使SAM在一个许可的开放许可证(Apache 2.0)下可在https://segment-anything.com上使用。我们还通过一个在线演示展示了SAM的功能。</p> 
<h3><a id="2_59"></a>2、分割任何物体任务</h3> 
<p>从NLP中获得灵感，下一个token预测任务用于基础模型预训练，并通过提示工程[10]解决各种下游任务。为了建立一个分割的基础模型，本文旨在定义一个具有类似能力的任务。</p> 
<p><img src="https://images2.imgbox.com/93/f9/yFJSYbFR_o.png" alt="04-18.png"></p> 
<p>任务。首先，我们将提示的想法从NLP转换为分割，其中提示可以是一组前景/背景点，一个粗略的框或掩码，自由形式的文本，或者一般来说，任何表示图像中要分割的信息。那么，promptable切分任务就是在给定任何提示时返回一个有效的切分掩码。“有效”掩模的要求只是意味着，即使提示是模糊的，并且可能指向多个对象(例如，回想一下衬衫和人的例子，参见图3)，输出也应该是这些对象中的至少一个的合理掩模。这个要求类似于期望语言模型对有歧义的提示输出连贯的响应。选择这项任务，是因为它导致了一种自然的预训练算法和一种通过提示将零样本迁移到下游分割任务的通用方法。</p> 
<p>预训练。promptable segmentation任务提出了一种自然的预训练算法，为每个训练样本模拟一系列提示(例如，点、框、掩码)，并将模型的掩码预测与基本事实进行比较。本文从交互式分割[109,70]中采用这种方法，尽管与交互式分割的目的是在足够的用户输入后最终预测有效掩码不同，本文的目标是始终为任何提示预测有效掩码，即使提示是模糊的。这确保了预训练模型在涉及歧义的用例中是有效的，包括我们的数据引擎要求的自动注释§4。我们注意到，在这项任务中表现良好具有挑战性，需要专门的建模和训练损失选择，我们在§3中讨论。</p> 
<p>Zero-shot迁移。预训练任务赋予了模型在推理时适当响应任何提示的能力，因此下游任务可以通过工程适当的提示来解决。例如，如果有一个猫的边界框检测器，则可以通过提供检测器的框输出作为我们模型的提示来解决猫实例分割。一般来说，一系列实际的分割任务都可以作为提示。除了自动数据集标注，在§7的实验中探索了五个不同的示例任务。</p> 
<p>相关的任务。分割是一个广泛的领域:有交互式分割[57,109]、边缘检测[3]、超像素化[85]、目标建议生成[2]、前景分割[94]、语义分割[90]、实例分割[66]、全景分割[59]等。提示分割任务的目标是产生一个功能广泛的模型，通过提示工程可以适应许多(尽管不是全部)现有的和新的分割任务。这种能力是任务泛化[26]的一种形式。请注意，这与之前在多任务分割系统上的工作不同。在多任务系统中，单个模型执行一组固定的任务，例如联合语义、实例和全景分割[114,19,54]，但训练和测试任务是相同的。本文工作中的一个重要区别是，为提示分割训练的模型可以作为更大系统中的一个组件，在推理时执行新的、不同的任务，例如，为了执行实例分割，将提示分割模型与现有的目标检测器相结合。</p> 
<p>讨论。提示和组合是强大的工具，使单个模型能够以可扩展的方式使用，可能完成模型设计时未知的任务。这种方法类似于其他基础模型的使用方式，例如CLIP[82]是DALL·E[83]图像生成系统的文本-图像对齐组件。我们预计，由prompt engineering等技术驱动的可组合系统设计，将比专门为固定任务集训练的系统实现更广泛的应用。通过组合的角度来比较提示分割和交互分割也是很有趣的:虽然交互分割模型是为人类用户设计的，但为提示分割训练的模型也可以组合成一个更大的算法系统，我们将演示。</p> 
<h3><a id="3_78"></a>3、分割任意物体模型</h3> 
<p>接下来，我们描述了分段任意模型(SAM)，以实现快速分割。SAM有三个组件，如图4所示:图像编码器，灵活的提示编码器和快速掩码解码器。建立了Transformer视觉模型[14,33,20,62]，并对(平摊)实时性能进行了特定的权衡。我们在这里高层次地描述这些组件，详情见§A。</p> 
<p><img src="https://images2.imgbox.com/19/4a/HggQAATb_o.png" alt="05-18.PNG"></p> 
<p>图像编码器。受可扩展性和强大的预训练方法的激励，本文使用MAE[47]预训练视觉Transformer (ViT)[33]最小适应于处理高分辨率输入[62]。图像编码器对每张图像运行一次，可以在提示模型之前应用。</p> 
<p>提示编码器。考虑两组提示:稀疏(点、框、文本)和密集(掩码)。我们通过位置编码[95]表示点和框，并对每种提示类型的学习嵌入和自由形式的文本使用CLIP[82]的现成文本编码器进行求和。密集提示(即掩码)使用卷积嵌入，并与图像嵌入元素相加。</p> 
<p>掩码译码器。掩码解码器有效地将图像嵌入、提示嵌入和输出标记映射到掩码。该设计受[14,20]的启发，采用了对Transformer解码器块[103]的修改，然后是动态掩码预测头。修改后的解码器块使用两个方向上的提示自注意力和交叉注意力(提示到图像嵌入，反之亦然)来更新所有嵌入。在运行两个块后，我们对图像嵌入进行上采样，MLP将输出标记映射到动态线性分类器，然后计算每个图像位置的掩码前景概率。</p> 
<p>解决歧义。使用一个输出，如果给出模糊的提示，模型将平均多个有效掩码。为解决这个问题，我们修改了模型，以为单个提示预测多个输出掩码(见图3)。我们发现3个掩码输出足以解决大多数常见的情况(嵌套掩码通常最多有3个深度:整体、部分和子部分)。在训练过程中，我们只对掩模的最小损失[15,45,64]进行反向传播。为了对口罩进行排名，该模型预测每个口罩的置信度分数(即估计的IoU)。</p> 
<p>效率。整体模型设计很大程度上是由效率驱动的。给定一个预先计算的图像嵌入，prompt编码器和mask解码器在web浏览器上运行，在CPU上，大约需要50毫秒。这种运行时性能使我们的模型能够无缝、实时的交互提示。</p> 
<p>损失和训练。我们用[14]中使用的focal loss[65]和dice loss[73]的线性组合来监督掩膜预测。使用几何提示的混合来训练可提示的分割任务(关于文本提示，见§7.5)。在[92,37]之后，通过在每个掩码中11轮随机采样提示来模拟交互式设置，允许SAM无缝集成到我们的数据引擎中。</p> 
<h3><a id="4_99"></a>4、分割任何数据引擎</h3> 
<p>由于分割掩码在互联网上并不丰富，我们构建了一个数据引擎，以收集我们的11 b掩码数据集SA-1B。该数据引擎有三个阶段:(1)模型辅助的手动标注阶段，(2)半自动阶段，其中混合了自动预测的掩码和模型辅助的标注，以及(3)全自动阶段，在该阶段中，我们的模型在没有标注者输入的情况下生成掩码。我们接下来会详细介绍每一种。</p> 
<p>Assisted-manual阶段。在第一阶段，类似于经典的交互式分割，一组专业标注人员通过使用SAM支持的基于浏览器的交互式分割工具点击前景/背景对象点来标记掩模。蒙版可以使用pixelprecise“刷”和“橡皮”工具进行细化。我们的模型辅助注释直接在浏览器中实时运行(使用预先计算的图像嵌入)，从而实现真正的交互体验。我们没有对标注对象施加语义约束，标注者自由地标记“东西”和“东西”[1]。我们建议标注者标记他们可以命名或描述的对象，但不收集这些名称或描述。标注者被要求按突出程度标记对象，并被鼓励在掩码注释超过30秒时继续处理下一个图像。</p> 
<p>在这一阶段的开始，SAM使用常见的公共分割数据集进行训练。在充分的数据注释后，仅使用新注释的掩码对SAM进行重新训练。随着收集到更多的掩码，图像编码器从ViT-B扩展到ViT-H，其他架构细节也得到了发展;我们总共重新训练了我们的模型6次。随着模型的改进，每个掩码的平均注释时间从34秒减少到14秒。我们注意到14秒比COCO的mask annotation[66]快6.5倍，只比极值点的边界框标记慢2倍[76,71]。随着SAM的改进，每个图像的平均掩码数量从20个掩码增加到44个掩码。总的来说，我们在这个阶段从12万张图像中收集了430万个掩模。</p> 
<p>半自动的阶段。在这一阶段，我们的目标是增加掩模的多样性，以提高我们的模型分割任何东西的能力。为了使标注者专注于不太突出的对象，我们首先自动检测自信掩码。然后，我们向标注者展示了预填充这些掩码的图像，并要求他们注释任何其他未注释的对象。为了检测置信掩码，我们使用通用“对象”类别在所有第一阶段掩码上训练了一个边界框检测器[84]。在此阶段，我们在180k图像中收集了额外的590万个掩码(总共1020万个掩码)。与第一阶段一样，我们定期在新收集的数据上重新训练模型(5次)。每个掩码的平均注释时间回到了34秒(不包括自动掩码)，因为这些对象更具有挑战性。每个图像的平均掩码数量从44个增加到72个(包括自动掩码)。</p> 
<p>全自动阶段。在最后阶段，标注是全自动的。这是可行的，因为我们的模型有两个主要的增强。首先，在这一阶段的开始，我们收集了足够的掩码来大大改进模型，包括上一阶段的不同掩码。其次，到此阶段，我们已经开发了模糊感知模型，它允许我们在有歧义的情况下预测有效的掩码。具体来说，我们用32×32规则的点网格来提示模型，并为每个点预测一组可能对应于有效对象的掩码。在模糊感知模型中，如果一个点位于某个部分或子部分上，我们的模型将返回子部分、局部和整个对象。该模型的IoU预测模块用于选择置信掩码;此外，我们只识别和选择了稳定的掩码(如果对0:5−δ和0:5 + δ的概率图进行阈值化，会得到相似的掩码，则认为掩码是稳定的)。最后，在选取置信度和稳定性掩码后，采用非极大值抑制(NMS)算法进行重复数据过滤。为了进一步提高较小掩模的质量，我们还处理了多个重叠的放大图像作物。关于这一阶段的更多细节，见§B。我们将全自动掩码生成应用于数据集中的所有1100万张图像，产生了总共11亿个高质量掩码。接下来，我们描述和分析结果数据集SA-1B。</p> 
<h3><a id="5_115"></a>5、数据集</h3> 
<p>所提出的数据集SA-1B由11M不同的、高分辨率的、授权的、保护隐私的图像和用数据引擎收集的11亿个高质量分割掩码组成。将SA-1B与现有数据集进行比较，并分析掩码的质量和属性。我们正在发布SA-1B，以帮助计算机视觉基础模型的未来发展。我们注意到SA-1B将在一个有利的许可协议下发布，用于某些研究用途，并保护研究人员。</p> 
<p>图像。我们从一家直接与摄影师合作的提供商那里获得了一套1100万张图片的新授权。这些图像是高分辨率的(平均3300×4950像素)，由此产生的数据大小可能会带来可访问性和存储挑战。因此，我们将发布最短边设置为1500像素的下采样图像。即使在降采样后，我们的图像的分辨率也明显高于许多现有的视觉数据集(例如，COCO[66]图像是~ 480×640像素)。请注意，目前大多数模型都是在分辨率低得多的输入上运行的。在公布的照片中，人脸和车牌都被模糊处理。</p> 
<p>掩码。我们的数据引擎生产了11亿个掩码，其中99.1%是完全自动生成的。因此，自动掩码的质量至关重要。我们直接将它们与专业注释进行比较，并研究各种掩码属性如何与突出的分割数据集进行比较。我们的主要结论(如下面的分析和§7中的实验所证明)是，我们的自动掩码质量高，对训练模型有效。在这些发现的推动下，SA-1B只包括自动生成的掩码。</p> 
<p>掩码的质量。为了估计掩模质量，我们随机抽样了500张图像(~ 50k掩模)，并要求我们的专业标注者提高这些图像中所有掩模的质量。注释器使用我们的模型和像素精确的“画笔”和“橡皮擦”编辑工具来做到这一点。这一过程产生了成对的自动预测和专业校正的掩码。我们计算了每对组合之间的欠条，发现94%的组合的欠条大于90%(97%的组合的欠条大于75%)。作为比较，之前的工作估计标注者之间的一致性在85-91% IoU[44,60]。我们在§7中的实验通过人工评分证实，相对于各种数据集，掩码的质量都很高，在自动掩码上训练我们的模型几乎与使用数据引擎产生的所有掩码一样好。<br> <img src="https://images2.imgbox.com/67/cf/F2VWPrJF_o.png" alt="06-18.PNG"></p> 
<p>掩码的属性。在图5中，我们绘制了SA-1B中对象中心的空间分布，并与现有的最大分割数据集进行了比较。常见的摄影师偏差存在于所有数据集中。我们观察到，与LVIS v1[44]和ADE20K[117]这两个分布最相似的数据集相比，SA-1B具有更大的图像角点覆盖率，而COCO[66]和Open Images V5[60]具有更突出的中心偏差。在图6(图例)中，我们按大小比较了这些数据集。SA-1B比第二大开放图像多11倍的图像和400倍的掩码。平均而言，每个图像的掩码比打开的图像多36倍。在这方面最接近的数据集ADE20K，每张图像的掩码仍然少3.5倍。图6(左)绘制了掩模-围像分布。接下来，我们查看图6(中间)中相对图像的掩膜大小(掩膜区域除以图像区域的平方根)。不出所料，由于我们的数据集每张图像有更多的掩码，它也倾向于包括更大比例的中小型相对尺寸的掩码。最后，为了分析形状的复杂性，我们查看图6(右)中的掩模凹凸性(1减去掩模面积除以掩模凸包面积)。由于形状复杂度与掩码大小相关，通过首先从分箱掩码大小中进行分层抽样来控制数据集的掩码大小分布。掩码的凹凸分布与其他数据集的大致相似。</p> 
<p><img src="https://images2.imgbox.com/03/4f/gFrDuAgI_o.png" alt="07-18.PNG"></p> 
<h3><a id="6RAI_133"></a>6、RAI分析</h3> 
<p>通过调查使用SA-1B和SAM时潜在的公平性问题和偏差，对工作进行负责任的AI (RAI)分析。我们关注SA-1B的地理和收入分配以及SAM跨受保护属性的公平性。我们还在§F中提供数据集、数据注释和模型卡。</p> 
<p>地理和收入代表。我们推断这些国家的图像是使用标准方法拍摄的(见§C)。在图7中，我们可视化了SA-1B中每个国家的图像数量(左)和图像最多的50个国家(右)。我们注意到，排名前三的国家来自世界不同地区。接下来，在表1中，我们比较了SA-1B、COCO[66]和Open Images[60]的地理和收入代表性。SA-1B在欧洲、亚洲和大洋洲以及中等收入国家拥有更高的图像比例。所有数据集对非洲和低收入国家的代表性不足。我们注意到，在SA-1B中，包括非洲在内的所有地区都至少有2800万个口罩，比以往任何数据集的口罩总数多10倍。每幅图像的平均掩码数量(未显示)在不同地区和收入中相当一致(每张图像94-108)。</p> 
<p><img src="https://images2.imgbox.com/5d/7b/iXRO89oi_o.png" alt="08-18.PNG"></p> 
<p>划分人群的公平性。通过测量组间SAM的表现差异，研究了感知性别呈现、感知年龄组和感知肤色之间的潜在公平性问题。我们使用更具包容性的人物注释(MIAP)[87]数据集用于性别表示和年龄，以及用于肤色的专有数据集(见§C)。我们的评估使用随机抽样1点和3点的模拟交互式分割(见§D)。表2(左上)显示了感知性别呈现的结果。我们注意到，女性在检测和分割数据集中被证明是代表性不足的[115]，但观察到SAM在组间的表现类似。我们重复表2(左下)中感知年龄的分析，注意到那些被认为更年轻和更年长的人在大规模数据集中被证明代表性不足[110]。SAM在那些被认为年龄较大的人身上表现最好(尽管置信区间很大)。最后，我们重复了表2(右)中对感知肤色的分析，注意到在大规模数据集中，明显肤色较浅的人被证明被过度代表，而肤色较深的人被低估[110]。由于MIAP不包含感知的肤色注释，我们使用了一个专有数据集，其中包含感知的Fitzpatrick皮肤类型[36]的注释，范围从1(最浅的肤色)到6(最暗的肤色)。虽然平均值略有不同，但我们没有发现组间的显著差异。我们相信我们的发现源于任务的性质，并承认当SAM被用作更大系统中的一个组件时，可能会出现偏差。最后，在§C中，我们将分析扩展到服装的分割，发现了感知到的性别呈现存在偏见的迹象。<br> <img src="https://images2.imgbox.com/57/43/yPloagWx_o.png" alt="09-18.PNG"></p> 
<h3><a id="7_146"></a>7、零样本迁移实验</h3> 
<p>在本节中，我们将介绍SAM (Segment Anything模型)的零样本迁移实验。我们考虑了5个任务，其中4个与用于训练SAM的promptable segmentation任务有显著不同。这些实验在训练期间未见过的数据集和任务上评估了SAM(我们对“零样本迁移”的使用遵循了CLIP[82]中的使用)。数据集可能包括新的图像分布，如水下或以自我为中心的图像(如图8)，据我们所知，这些图像在SA-1B中没有出现。<br> <img src="https://images2.imgbox.com/17/37/1TocfMHI_o.png" alt="10-18.PNG"></p> 
<p>实验从测试可提示分割的核心目标开始:从任何提示生成有效的掩码。本文强调单个前景点提示的挑战性场景，因为它比其他更具体的提示更可能是模糊的。提出了一系列实验，遍历低、中、高层图像理解，并大致平行该领域的历史发展。(2)分割一切，即目标建议生成，(3)分割检测到的目标，即实例分割，以及(4)，作为概念证明，从自由形式的文本中分割目标。这四个任务与SAM所训练并通过prompt工程实现的promptable segmentation任务有显著不同。我们的实验以消融研究结束。</p> 
<p>实现。除非另有规定:(1)SAM使用MAE[47]预训练的ViT-H[33]图像编码器，(2)SAM在SA-1B上进行训练，注意此数据集只包括从我们的数据引擎的最后阶段自动生成的掩码。对于所有其他模型和训练细节，如超参数，请参见§A。</p> 
<h4><a id="71_156"></a>7.1、零样本单点有效掩码评估</h4> 
<p>任务。本文评估了从单个前景点分割物体。这个任务是病态的，因为一个点可以指向多个对象。大多数数据集中的基本真值掩码没有枚举所有可能的掩码，这可能使自动度量不可靠。因此，本文用一项人类研究补充了标准的mIoU指标(即预测掩码和真实掩码之间所有借据的平均值)，在这项研究中，标注者对掩码质量的评分从1(无意义)到10(像素完美)。看到§D。1，§E和§G的额外细节。</p> 
<p>默认情况下，我们从真实掩码的“中心”(在掩码内部距离变换的最大值)采样点，遵循交互式分割中的标准评估协议[92]。由于SAM能够预测多个掩码，因此默认情况下我们只评估模型中最自信的掩码。基线都是单掩码方法。我们主要与RITM[92]进行比较，RITM是一种强大的交互式分割器，与其他强基线[67,18]相比，它在我们的基准上表现最好。</p> 
<p>数据集。我们使用了一套新编译的23个具有不同图像分布的数据集。图8列出了这些数据集，并展示了每个数据集的一个示例(更多细节请参见附录表7)。我们使用所有23个数据集进行mIoU评估。对于人体研究，我们使用图9b中列出的子集(由于此类研究的资源需求)。这个子集包括SAM在自动指标上优于和低于RITM的两个数据集。</p> 
<p>结果。首先，我们研究了使用mIoU对一整套23个数据集的自动评估。我们将图9a中每个数据集的结果与RITM进行了比较。SAM在23个数据集中的16个上获得了更高的结果，高出了47 IoU。本文还提出了一个" oracle "结果，通过将SAM的3个面具与基本事实进行比较，选择出最相关的面具，而不是选择最自信的面具。这揭示了歧义性对自动评测的影响。特别地，在oracle进行二义性解析时，SAM在所有数据集上的表现都优于RITM。</p> 
<p>人体研究结果见图9b。误差条为平均掩膜评分的95%置信区间(所有差异均显著;详见§E)。标注者对SAM的掩模质量的评价始终大大高于最强的基线RITM。带有单一输出掩码的减弱版SAM的评分始终较低，但仍然高于RITM。SAM的平均评级在7到9之间，这与定性评级指南相对应:“一个高分(7-9):对象是可识别的，错误很小且很少(例如，错过一个小的、严重遮挡的不连接组件，……)。”这些结果表明，SAM已经学会了从单点分割有效掩码。请注意，对于像DRAM和IBD这样的数据集，SAM在自动指标上较差，但在人工研究中始终获得较高的评分。</p> 
<p><img src="https://images2.imgbox.com/cd/ac/kPagAfNv_o.png" alt="11-18.PNG"></p> 
<p>图9c显示了其他基线SimpleClick[67]和FocalClick[18]，它们的单点性能低于RITM和SAM。随着点数从1增加到9，方法之间的差距减小。随着任务变得更容易，这是意料之中的。此外，SAM并没有针对非常高的欠条制度进行优化。最后，在图9d中，我们将默认的中心点采样替换为随机点采样。我们观察到SAM和基线之间的差距在增长，SAM能够在两种采样方法下取得可比的结果。</p> 
<h4><a id="72_173"></a>7.2、零样本边缘检测</h4> 
<p>方法。我们使用BSDS500[72,3]在经典的低层次边缘检测任务上评估SAM。我们使用自动掩码生成管道的简化版本。用16×16规则的前景点网格提示SAM，产生768个预测掩模(每个点3个)。网管删除冗余的掩码。然后，使用非阈值掩码概率图的Sobel滤波和标准的轻量级后处理来计算边缘图，包括边缘NMS(见§D。2)。<br> <img src="https://images2.imgbox.com/c7/88/XA1XHOQY_o.png" alt="12-18.PNG"></p> 
<p>结果。我们在图10中可视化了代表性的边缘图(更多信息请参见图15)。从质量上说，即使SAM没有经过边缘检测的训练，它也能产生合理的边缘图。与真实值相比，SAM预测了更多的边，包括BSDS500中没有标注的合理边。这种偏差在表3中定量地反映出来:50%精度(R50)下的召回率很高，这是以精度为代价的。SAM自然落后于学习BSDS500偏差的最先进方法，即要抑制哪些边缘。然而，与先驱的深度学习方法(如HED[108] (也在BSDS500上训练))相比，SAM表现良好，并明显优于之前的零样本迁移方法，尽管不可否认已过时。</p> 
<p><img src="https://images2.imgbox.com/3c/c3/QkbDvVPl_o.png" alt="13-18.PNG"></p> 
<h4><a id="73_185"></a>7.3、零样本目标建议</h4> 
<p>方法。接下来，在目标建议生成的中层任务上评估SAM[2,102]。该任务在目标检测研究中发挥了重要作用，作为开拓性系统(如[102,41,84])的中间步骤。为了生成目标建议，我们运行了自动掩码生成管道的一个稍微修改的版本，并将掩码作为建议输出(见§D。3)。</p> 
<p>我们在LVIS v1[44]上计算标准平均召回率(AR)指标。我们关注LVIS，因为它的大量类别提出了一个具有挑战性的测试。将其与作为ViTDet[62]检测器实现的强基线(使用级联Mask R-CNN [48,11] ViT-H)进行比较。我们注意到，这个“基线”对应于向游戏AR展示的“探测器伪装为建议生成器”(DMP)方法[16]，使其成为一个真正要求很高的比较。<br> <img src="https://images2.imgbox.com/c8/ae/4YSCYIQn_o.png" alt="14-18.PNG"></p> 
<p>结果。在表4中，我们不出所料地看到，使用ViTDet-H的检测作为目标建议(即游戏AR的DMP方法[16])总体表现最好。然而，SAM在几个指标上表现非常好。值得注意的是，它在中型和大型对象以及稀有和常见对象上的性能优于ViTDet-H。事实上，SAM只在小物体和频繁物体上表现不如ViTDet-H，其中ViTDet-H可以很容易地学习特定于LVIS的标注偏差，因为与SAM不同，它是在LVIS上训练的。还与SAM的一个消除了歧义的版本(" single out. ")进行了比较，该版本在所有AR指标上的表现都明显比SAM差。</p> 
<h4><a id="74_195"></a>7.4、零样本实例分割</h4> 
<p>方法。说到更高层次的视觉，我们使用SAM作为实例分割器的分割模块。实现很简单:我们运行一个对象检测器(之前使用的ViTDet)并用其输出框提示SAM。这说明了如何在一个更大的系统中组合SAM。</p> 
<p><img src="https://images2.imgbox.com/6e/1a/qCGaZ6yf_o.png" alt="15-18.PNG"></p> 
<p>结果。我们在表5中比较了SAM和ViTDet对COCO和LVIS预测的面具。观察掩码AP指标，我们观察到两个数据集上的差距，SAM相当接近，但肯定落后于ViTDet。通过可视化输出，我们观察到SAM掩模通常在质量上优于ViTDet的掩模，具有更清晰的边界(见§D.4、图16)。为了调查这一观察结果，我们进行了一项额外的人类研究，要求注释者根据之前使用的1到10的质量量表对ViTDet口罩和SAM口罩进行评分。在图11中，我们观察到SAM在人体研究中始终优于ViTDet。</p> 
<p><img src="https://images2.imgbox.com/a9/f1/Wv74uU2y_o.png" alt="16-18.PNG"></p> 
<p>假设在COCO上，面具AP差距较大，真实值质量相对较低(由人类研究证实)，ViTDet了解到COCO面具的特定偏差。SAM是一种零样本方法，无法利用这些(通常不受欢迎的)偏差。LVIS数据集具有更高质量的基本事实，但仍然有特定的特性(例如，掩码不包含孔，它们从构造上来说是简单的多边形)和模态掩码与模态掩码的偏差。同样，SAM没有经过学习这些偏见的训练，而ViTDet可以利用它们。</p> 
<h4><a id="75ZeroShot_TexttoMask_209"></a>7.5、Zero-Shot Text-to-Mask</h4> 
<p>方法。最后，考虑一个更高层次的任务:从自由形式的文本中分割对象。这个实验是SAM处理文本提示的能力的概念验证。虽然我们在之前的所有实验中都使用了完全相同的SAM，但对于这个SAM的训练过程进行了修改，使其具有文本感知能力，但以一种不需要新的文本注释的方式。对于每个人工收集的面积大于1002的掩模，提取CLIP图像嵌入。然后，在训练过程中，将提取的CLIP图像嵌入作为SAM的第一次交互。这里的关键观察是，因为CLIP的图像嵌入被训练成与其文本嵌入对齐，我们可以用图像嵌入进行训练，但使用文本嵌入进行推理。也就是说，在推理时，我们通过CLIP的文本编码器运行文本，然后将得到的文本嵌入作为提示给SAM(见§D.5详细信息)。<br> <img src="https://images2.imgbox.com/7c/55/beLcfOCx_o.png" alt="17-18.PNG"></p> 
<p>结果。我们在图12中显示了定性结果。SAM可以基于简单的文本提示(如“一个轮子”)以及短语(如“海狸牙齿格栅”)来分割物体。当SAM无法仅从文本提示中选择正确的对象时，一个额外的点通常会修复预测，类似于[31]。</p> 
<h4><a id="76_217"></a>7.6、消融研究</h4> 
<p>我们使用单中心点提示协议对23个数据集套件进行了几次消融。回想一下，单个点可能是模棱两可的，而且这种模棱两可可能不会在基本真值中表示，每个点只包含一个掩码。由于SAM是在零样本传输环境中运行的，因此SAM的顶级掩码与数据注释指南产生的掩码之间可能存在系统偏差。因此，我们另外报告了关于基本事实的最佳掩码(“oracle”)。</p> 
<p><img src="https://images2.imgbox.com/74/4b/30QmYMeD_o.png" alt="18-18.PNG"></p> 
<p>图13(左)绘制了SAM在数据引擎阶段累积数据上训练时的性能。我们观察到每个阶段mIoU都在增加。当使用所有三个阶段进行训练时，自动口罩的数量大大超过手动和半自动口罩。为了解决这个问题，我们发现在训练过程中对手动和半自动掩模进行10倍的过采样可以得到最好的结果。这种设置使训练变得复杂。因此，我们测试了第四个设置，它只使用自动生成的掩码。使用该数据时，SAM的性能只比使用所有数据时略低(约0.5 mIoU)。因此，默认情况下，我们只使用自动生成的掩码来简化训练设置。</p> 
<p>在图13(中间)中，我们看看数据量的影响。完整的SA-1B包含1100万张图像，我们将其均匀采样到1M和0.1M进行消融。在0.1万张图像中，我们观察到在所有设置下mIoU都有很大的下降。然而，有100万张图像，约占整个数据集的10%，我们观察到的结果与使用整个数据集的结果相当。这种数据机制仍然包括大约1亿个掩码，对于许多用例来说可能是一个实用的设置。</p> 
<p>最后，图13(右)显示了ViT-B、ViT-L和ViT-H图像编码器的结果。ViT-H比ViT-B有显著改善，但比ViT-L只有边际收益。进一步的图像编码器缩放似乎没有丰硕的成果，目前。</p> 
<h3><a id="8_230"></a>8、讨论</h3> 
<p>基础模型。自机器学习的早期[99]以来，预训练模型就已经适应下游任务。近年来，随着对规模的日益重视，这种范式变得越来越重要，这种模型最近被(重新)称为“基础模型”:即“在大规模数据上训练的模型，并适应广泛的下游任务”[8]。本文工作与这个定义很好地相关，但注意到图像分割的基础模型是一个内在有限的范围，因为它代表了计算机视觉的一个重要但分数阶的子集。还将该方法的一个方面与[8]进行了对比，后者强调了自监督学习在基础模型中的作用。虽然该模型用自监督技术(MAE[47])初始化，但其绝大多数能力来自大规模的监督训练。在数据引擎可以扩展可用注释的情况下，如我们的例子，监督训练提供了一个有效的解决方案。</p> 
<p>组合性。预训练模型可以提供甚至超出训练时想象的新功能。一个突出的例子是CLIP[82]如何在更大的系统中作为组件使用，如DALL·E[83]。我们的目标是用SAM直接实现这种组合。本文旨在通过要求SAM为广泛的分割提示预测有效的掩码来实现这一目标。其效果是在SAM和其他组件之间创建一个可靠的接口。例如，MCC[106]可以轻松地使用SAM分割感兴趣的对象，并实现对未见过对象的强泛化，以便从单个RGB-D图像进行3D重建。在另一个例子中，SAM可以通过可穿戴设备检测到的注视点来提示，从而启用新的应用程序。由于SAM具有泛化到以自我为中心的图像等新领域的能力，这样的系统无需额外的训练即可工作。</p> 
<p>局限性。虽然SAM总体上表现良好，但它并不完美。它可能会错过精细的结构，有时会产生小的不连接组件的幻觉，并且不像“放大”的更计算密集的方法(如[18])那样产生清晰的边界。通常，当提供许多点时，我们期望专用的交互式分割方法优于SAM，例如[67]。与这些方法不同的是，SAM被设计为通用性和使用广度，而不是高IoU交互式分割。此外，SAM可以实时处理提示信息，但当使用大型图像编码器时，SAM的整体性能不实时。我们对文本到掩码任务的尝试是探索性的，并不完全健壮，尽管我们相信可以通过更多的努力来改进。虽然SAM可以执行许多任务，但尚不清楚如何设计简单的提示来实现语义和全景分割。最后，还有一些特定于领域的工具，如[7]，我们希望它们在各自的领域中比SAM表现更好。</p> 
<p>结论。Segment Anything项目试图将图像分割提升到基础模型时代。本文的主要贡献是一个新的任务(promptable segmentation)、模型(SAM)和数据集(SA-1B)，使这一飞跃成为可能。SAM是否达到了基础模型的地位还有待于它在社区中如何使用，但无论如何，我们期待这项工作的前景，超过1B个口罩的发布和我们的promptable分割模型将有助于铺平前进的道路。</p> 
<h3><a id="_240"></a>参考引用链接：</h3> 
<ul><li>论文：<a href="https://arxiv.org/pdf/2304.02643.pdf" rel="nofollow">https://arxiv.org/pdf/2304.02643.pdf</a></li><li>代码：<a href="https://github.com/facebookresearch/segment-anything">https://github.com/facebookresearch/segment-anything</a></li><li>Demo：<a href="https://segment-anything.com" rel="nofollow">https://segment-anything.com</a></li><li>论文官方下载：<a href="https://ai.facebook.com/research/publications/segment-anything/" rel="nofollow">https://ai.facebook.com/research/publications/segment-anything/</a></li><li>官方博客：<a href="https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/" rel="nofollow">https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/</a></li><li>SA-1B数据集：<a href="https://ai.facebook.com/datasets/segment-anything/" rel="nofollow">https://ai.facebook.com/datasets/segment-anything/</a></li><li><a href="" rel="nofollow">https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/129997240</a></li><li><a href="" rel="nofollow">https://emoumcwvfx.feishu.cn/docx/D971dWcuMoyMJUxE1Mfc1fwPn7c</a></li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/40d675a1a7a7ece65f8a953bcc499e5b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux内核的 反向路由检查机制rp_filter</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e1bdfec5675528ba9af540b436b56ba6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">stata数据处理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>