<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>android hwcomposer 在视频播放中的应用 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="android hwcomposer 在视频播放中的应用" />
<meta property="og:description" content="之前写了一篇博客，分析了视频如何显示的
http://blog.csdn.net/wan8180192/article/details/50269405
以及gralloc的内存管理
http://blog.csdn.net/wan8180192/article/details/50513895
这里结合hwcomposer模块，以及视频播放的场景，对其中有一些细节，在这里再做补充一下 。 android中，多个surface layer要显示到屏幕上，就要合成到一起，合成方式有两种： 离线合成 先将所有图层画到一个最终层（FrameBuffer）上，再将FrameBuffer送到LCD显示。由于合成FrameBuffer与送LCD显示一般是异步的（线下生成FrameBuffer，需要时线上的LCD去取），因此叫离线合成。 在线合成 不使用FrameBuffer，在LCD需要显示某一行的像素时，用显示控制器将所有图层与该行相关的数据取出，合成一行像素送过去。只有一个图层时，又叫Overlay技术。 由于省去合成FrameBuffer时读图层，写FrameBuffer的步骤，大幅降低了内存传输量，减少了功耗，但这个需要硬件支持。 对于这两种方式，各有优缺点， 离线合成充分利用GPU，更加灵活，不受win layer数量限制。但是功耗大，不利于移动设备。GPU如果性能不强，复杂应用场景下会出现卡顿，实时性不好 在线合成，功耗小，没有性能瓶颈，没有时延。但是不够灵活。UI layer一旦变多，需要重新借助于GPU的离线合成。 一般来说，优先使用overlay.实在不行就用GPU 在实际代码中，可以看到SurfaceFlinger::doComposeSurfaces中， 有以下处理， switch (cur-&gt;getCompositionType()) { case HWC_OVERLAY: { overlay 方式，采用HWC硬件来合成 const Layer::State&amp; state(layer-&gt;getDrawingState()); if ((cur-&gt;getHints() &amp; HWC_HINT_CLEAR_FB) &amp;&amp; i &amp;&amp; layer-&gt;isOpaque() &amp;&amp; (state.alpha == 0xFF) &amp;&amp; hasGlesComposition) { // never clear the very first layer since we&#39;re // guaranteed the FB is already cleared layer-&gt;clearWithOpenGL(hw, clip); } break; } case HWC_FRAMEBUFFER: { ///非overlay方式，采用GPU来合成，后续调用的是openGL layer-&gt;draw(hw, clip); break; } case HWC_FRAMEBUFFER_TARGET: { /异常处理。FRAMEBUFFER_TARGET不应该出现在这个流程里。FRAMEBUFFER_TARGET是合成是使用的目标buffer。 // this should not happen as the iterator shouldn&#39;t // let us get there." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a466bc5b57147737d78bdd43e9f5318b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-01-13T22:59:48+08:00" />
<meta property="article:modified_time" content="2016-01-13T22:59:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">android hwcomposer 在视频播放中的应用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <span style="white-space:pre"></span> 
<span style="white-space:pre"></span> 
<br> 
<p>之前写了一篇博客，分析了视频如何显示的</p> 
<p>http://blog.csdn.net/wan8180192/article/details/50269405</p> 
<p>以及gralloc的内存管理</p> 
<p>http://blog.csdn.net/wan8180192/article/details/50513895</p> 
<p><br> </p> 这里结合hwcomposer模块，以及视频播放的场景，对其中有一些细节，在这里再做补充一下 
<span style="white-space:pre"> </span> 。 
<br> 
<br> android中，多个surface layer要显示到屏幕上，就要合成到一起，合成方式有两种： 
<br> 离线合成 
<br> 先将所有图层画到一个最终层（FrameBuffer）上，再将FrameBuffer送到LCD显示。由于合成FrameBuffer与送LCD显示一般是异步的（线下生成FrameBuffer，需要时线上的LCD去取），因此叫离线合成。 
<br> 在线合成 
<br> 不使用FrameBuffer，在LCD需要显示某一行的像素时，用显示控制器将所有图层与该行相关的数据取出，合成一行像素送过去。只有一个图层时，又叫Overlay技术。  
<br> 由于省去合成FrameBuffer时读图层，写FrameBuffer的步骤，大幅降低了内存传输量，减少了功耗，但这个需要硬件支持。 
<br> 
<span style="white-space:pre"></span> 
<br> 对于这两种方式，各有优缺点， 
<br> 离线合成充分利用GPU，更加灵活，不受win layer数量限制。但是功耗大，不利于移动设备。GPU如果性能不强，复杂应用场景下会出现卡顿，实时性不好 
<br> 在线合成，功耗小，没有性能瓶颈，没有时延。但是不够灵活。UI layer一旦变多，需要重新借助于GPU的离线合成。  
<br> 
<span style="white-space:pre"></span> 
<br> 一般来说，优先使用overlay.实在不行就用GPU 
<br> 
<br> 在实际代码中，可以看到SurfaceFlinger::doComposeSurfaces中， 有以下处理， 
<br> 
<pre><code class="language-cpp">switch (cur-&gt;getCompositionType()) {
                    case HWC_OVERLAY: {   overlay 方式，采用HWC硬件来合成
                        const Layer::State&amp; state(layer-&gt;getDrawingState());
                        if ((cur-&gt;getHints() &amp; HWC_HINT_CLEAR_FB)
                                &amp;&amp; i
                                &amp;&amp; layer-&gt;isOpaque() &amp;&amp; (state.alpha == 0xFF)
                                &amp;&amp; hasGlesComposition) {
                            // never clear the very first layer since we're
                            // guaranteed the FB is already cleared
                            layer-&gt;clearWithOpenGL(hw, clip);
                        }
                        break;
                    }
                    case HWC_FRAMEBUFFER: {   ///非overlay方式，采用GPU来合成，后续调用的是openGL
                        layer-&gt;draw(hw, clip);
                        break;
                    }
                    case HWC_FRAMEBUFFER_TARGET: {   /异常处理。FRAMEBUFFER_TARGET不应该出现在这个流程里。FRAMEBUFFER_TARGET是合成是使用的目标buffer。
                        // this should not happen as the iterator shouldn't
                        // let us get there.
                        ALOGW("HWC_FRAMEBUFFER_TARGET found in hwc list (index=%d)", i);
                        break;
                    }
                }
</code></pre> 
<br> 其中： 
<br> HWC_FRAMEBUFFER_TARGET：该Layer是3D合成的目标Layer 
<br> HWC_FRAMEBUFFER：hwcomposer无法处理此Layer，该Layer需要走GPU合成流程，用OpenGL绘制 
<br> HWC_OVERLAY：该Layer为硬件合成器所处理，不需要OpenGLES去渲染 
<br> 
<br> 结合视频播放的场景，这里打印了部分主要的log, 
<span style="white-space:pre"> </span>打出来的基本都是函数名。 
<span style="white-space:pre"></span>在log的基础上分析一下。 
<br> 显示视频帧，exynos上主要有两个路子： 
<br> 1，利用GPU。 
<br>   采用GPU做渲染以实现缩放，色彩空间转换，以及与其他APP UI layer的composer合成。 
<br>   这里的合成，实际上就是offline离线合成. 
<br>    
<br> 2，采用display controller 模块。 
<br>   display controller会利用exynos的FIMC模块来完成缩放，色彩空间转换， 
<br>   利用exynos 4412的5个win layer, 将视频单独一个win layer,与其他win layer的APP UI合成。 
<br>   这实际上是online在线合成。 这种合成方式，也叫做overlay方式 
<br>   exynos的display controller实际上相当于高通的MSM平台上的MDP，这两个模块，通俗意义上可以被称作hwcomposer/HWC 
<br>   
<br>   
<br> 1，先介绍下离线合成场景下的显示过程：   
<br> ##########################tiled 和linear 转换 ：解码完毕之后，exynos解出来的是很变态的YUV420 tiled格式。后面要显示，就要转成常见的linear格式，目前的实现采用了 NEON软件方法实现 
<br> E/libcsc  ( 1660):  csc_convert 
<br> E/libcsc  ( 1660):  conv_sw 
<br> E/libcsc  ( 1660):  HAL_PIXEL_FORMAT_YCbCr_420_SP 
<br> E/libcsc  ( 1660):  HAL_PIXEL_FORMAT_YCbCr_420_SP_TILED 
<br> 
<br> 
<br> ##########################OMX 把YUV数据queuebuffer 
<br> E/AwesomePlayer( 1660): status_t err = mNativeWindow-&gt;queueBuffer //这里queuebuffer的操作是gralloc利用MALI的UMP/ION内存管理机制分配得到的图形缓冲区，不是framebuffer 
<br> E/Surface ( 1660): Surface::hook_queueBuffer 
<br> E/SurfaceFlinger( 1652): Layer::onFrameAvailable 
<br> E/SurfaceFlinger( 1652): handlePageFlip 
<br> E/SurfaceFlinger( 1652): latchBuffer //把YUV数据绑定到openGL纹理， 
<br> 
<br> 
<br> ##########################VSYNC到来，开始用GPU合成，主要是在doComposeSurfaces里面，实际上就是GPU读取YUV数据作为纹理,进行渲染。渲染完了放在了framebuffer里面 
<br> E/SurfaceFlinger( 1652): handleMessageRefresh 
<br> E/SurfaceFlinger( 1652): doComposition 
<br> E/SurfaceFlinger( 1652): doDisplayComposition 
<br> E/SurfaceFlinger( 1652): doComposeSurfaces 
<br> E/SurfaceFlinger( 1652): hasGlesComposition 
<br> E/Surface ( 1652): Surface::hook_dequeueBuffer    
<br> E/Surface ( 1652): Surface::dequeueBuffer   //这里dequeuebuffer的操作是gralloc申请到的framebuffer 
<br> E/SurfaceFlinger( 1652): count 2 
<br> E/SurfaceFlinger( 1652): else clip.isEmpty 
<br> E/SurfaceFlinger( 1652): onDraw 
<br> E/SurfaceFlinger( 1652): drawWithOpenGL 0,0,1280,720,1280,720,  //进行纹理渲染。 
<br> E/SurfaceFlinger( 1652): count 2 
<br> E/SurfaceFlinger( 1652): else clip.isEmpty 
<br> E/SurfaceFlinger( 1652): onDraw 
<br> E/SurfaceFlinger( 1652): drawWithOpenGL 0,0,800,480,800,480, //这是干啥？哪位大侠知道？暂且认为是在做composer 
<br> 
<br> 
<br> ##########################渲染，合成之后调用swapBuffers，把渲染完的数据hook_queueBuffer，这里的buffer都是framebuffer了。然后调用mFbDev-&gt;post，也就是framebuffer HAL的fb_post,画到屏幕上。 
<br> E/SurfaceFlinger( 1652): hw-&gt;swapBuffers 
<br> E/SurfaceFlinger( 1652): DisplayDevice::swapBuffers 
<br> E/SurfaceFlinger( 1652): eglSwapBuffers  //eglSwapBuffers最终调用了openGL,maliDDK中的代码，也就是调用了调用了libmali.so,其内部调用了NativeWindow-&gt;queueBuffer，这部分代码没有开源，不做分析 
<br> E/Surface ( 1652): Surface::hook_queueBuffer 
<br> E/SurfaceFlinger( 1652): FramebufferSurface::onFrameAvailable 
<br> E/SurfaceFlinger( 1652): HWComposer::fbPost 
<br> E/SurfaceFlinger( 1652): mFbDev-&gt;post 
<br> 
<br> 
<br> ##########################下面好像都没有实质工作。都是在处理fence之类的 
<br> E/SurfaceFlinger( 1652): mDisplaySurface-&gt;advanceFrame 
<br> E/SurfaceFlinger( 1652): DisplayDevice::flip 
<br> E/SurfaceFlinger( 1652): SurfaceFlinger::postFramebuffer 
<br> E/SurfaceFlinger( 1652): hw-&gt;onSwapBuffersCompleted 
<br> E/SurfaceFlinger( 1652): else currentLayers[i]-&gt;onLayerDisplayed 
<br> E/SurfaceFlinger( 1652): Layer::onLayerDisplayed 
<br> E/SurfaceFlinger( 1652): else currentLayers[i]-&gt;onLayerDisplayed 
<br> E/SurfaceFlinger( 1652): Layer::onLayerDisplayed 
<br> 
<br> 
<br> 综上：离线合成的方法实际上就是：  
<br> decoder----&gt;UMP/ION graphicbuffer 1 
<br> 
<span style="white-space:pre"></span>                                                                    --------&gt;GPU---------&gt;framebuffer-----&gt;screen 
<br> APP UI ----&gt;UMP/ION graphicbuffer 2 
<br> 
<br> 2，在线合成的尚未调试完毕,具体细节后面会补充： 
<br> 
<span style="white-space:pre"></span>其主要思路就是 
<br> decoder--&gt;FIMC----------------------------------&gt;framebuffer for win1 
<br> 
<span style="white-space:pre"></span>                                                                                                                --------&gt;display controller-----&gt;screen 
<br> APP UI --&gt;ION graphicbuffer ----&gt;GPU-----&gt;framebuffer for win2 
<br> 可以看到HWC在合成时，overlay方式的视频不需要直接和非Overlay方式的APP UI layer同步，它只要和这些非Overlay层合成的最终结果同步就可以了。 
<br> 非Overlay层合成的最终结果放在了FramebufferTarget中。 
<br> GPU渲染完非Overlay的层后，通过queueBuffer()将GraphicBuffer放入FramebufferSurface对应的BufferQueue，然后FramebufferSurface::onFrameAvailable()被调用。它先会通过nextBuffer()-&gt;acquireBufferLocked()从BufferQueue中拿一个GraphicBuffer，接着调用HWComposer::fbPost()-&gt;setFramebufferTarget()，其中会把刚才acquire的GraphicBuffer设到HWC的Layer list中的FramebufferTarget slot中，然后进行HWC合成 
<br> 
<br>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/58c8792cca7a40a6775607579148d432/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">android 显示系统初步总结</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a363095d9d1ca9022ef41c7babfacb00/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">android gralloc 小结</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>