<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【Hive 01】简介、安装部署、高级函数使用 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【Hive 01】简介、安装部署、高级函数使用" />
<meta property="og:description" content="1 Hive简介 1.1 Hive系统架构 Hive是建立在 Hadoop上的数据仓库基础构架，它提供了一系列的工具，可以进行数据提取、转化、加载（ ETL ）Hive定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据Hive包含SQL解析引擎，它会将SQL语句转译成MR Job，然后在Hadoop中执行Hive的数据存储基于Hadoop的HDFSHive没有专门的数据存储格式，默认可以直接加载文本文件TextFile，还支持SequenceFile、RCFile等 1.2 Metastore Metastore是Hive元数据的集中存放地元数据包括表的名字，表的列和分区及其属性，表的数据所在目录等Metastore默认使用内嵌的Derby数据库作为存储引擎，推荐使用Mysql数据库作为外置存储引擎 1.3 Hive与MySQL对比 HiveMySQL数据存储位置HDFS本地磁盘数据格式用户决定系统决定数据更新不支持支持索引有，但较弱有执行MapReduceExecutor执行延迟高低可扩展性高低数据规模大小 2 Hive安装部署 访问Hive官方网站，下载apache-hive-3.1.2-bin.tar.gz安装包，在/data/soft目录下解压文件。
2.1 配置hive-env.sh cd apache-hive-3.1.2-bin/conf/ mv hive-env.sh.template hive-env.sh 在文件末尾添加以下内容：
export JAVA_HOME=/home/gdan/data/jdk-8u131-linux-x64/jdk1.8.0_131 export HIVE_HOME=/home/gdan/data/soft/apache-hive-3.1.2-bin export HADOOP_HOME=/home/gdan/data/soft/hadoop-3.2.0 再/etc/profile也需要设置hive的环境变量
export HIVE_HOME=/home/gdan/data/soft/apache-hive-3.1.2-bin export PATH=$HIVE_HOME/bin:$PATH 2.2 安装mysql数据库，并创建hive库 1.安装MySQL 在终端中输入以下命令来安装MySQL：
sudo apt-get update sudo apt-get install mysql-server 安装过程中会提示您设置MySQL的root用户密码，请根据提示进行设置。
安装时没有提示输入root账户密码，默认是空，可以执行以下命令设置密码为xxxx：
sudo mysql -u root -p #密码按Enter即可进入mysql shell，空格也可以，普通用户一定sudo 2. 创建hive元数据数据库
create database hive; 2.3 配置hive-site.xml cd apache-hive-3.1.2-bin/conf/ mv hive-default.xml.template hive-site.xml 在文件中添加以下内容：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/65ef24cc017b70afc74e3a7a2dc1b98c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-24T22:16:28+08:00" />
<meta property="article:modified_time" content="2023-07-24T22:16:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Hive 01】简介、安装部署、高级函数使用</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>1 Hive简介</h2> 
<h3>1.1 Hive系统架构</h3> 
<p class="img-center"><img alt="Hive系统架构" height="905" src="https://images2.imgbox.com/b0/a9/KU9lIxly_o.png" width="1200"></p> 
<p></p> 
<ul><li>Hive是建立在 Hadoop上的数据仓库基础构架，它提供了一系列的工具，可以进行数据提取、转化、加载（ ETL ）</li><li>Hive定义了简单的类SQL查询语言，称为HQL，它允许熟悉SQL的用户直接查询Hadoop中的数据</li><li>Hive包含SQL解析引擎，它会将SQL语句转译成MR Job，然后在Hadoop中执行</li><li>Hive的数据存储基于Hadoop的HDFS</li><li>Hive没有专门的数据存储格式，默认可以直接加载文本文件TextFile，还支持SequenceFile、RCFile等</li></ul> 
<p></p> 
<h3>1.2 Metastore</h3> 
<ul><li>Metastore是Hive元数据的集中存放地</li><li>元数据包括表的名字，表的列和分区及其属性，表的数据所在目录等</li><li>Metastore默认使用内嵌的Derby数据库作为存储引擎，推荐使用Mysql数据库作为外置存储引擎</li></ul> 
<p></p> 
<h3>1.3 Hive与MySQL对比</h3> 
<p></p> 
<table><thead><tr><th></th><th>Hive</th><th>MySQL</th></tr></thead><tbody><tr><td><strong>数据存储位置</strong></td><td>HDFS</td><td>本地磁盘</td></tr><tr><td><strong>数据格式</strong></td><td>用户决定</td><td>系统决定</td></tr><tr><td><strong>数据更新</strong></td><td>不支持</td><td>支持</td></tr><tr><td><strong>索引</strong></td><td>有，但较弱</td><td>有</td></tr><tr><td><strong>执行</strong></td><td>MapReduce</td><td>Executor</td></tr><tr><td><strong>执行延迟</strong></td><td>高</td><td>低</td></tr><tr><td><strong>可扩展性</strong></td><td>高</td><td>低</td></tr><tr><td><strong>数据规模</strong></td><td>大</td><td>小</td></tr></tbody></table> 
<p></p> 
<p></p> 
<h2>2 Hive安装部署</h2> 
<p>访问<a href="https://dlcdn.apache.org/hive/hive-3.1.2/" rel="nofollow" title="Hive官方网站">Hive官方网站</a>，下载apache-hive-3.1.2-bin.tar.gz安装包，在<code>/data/soft</code>目录下解压文件。</p> 
<h3>2.1 配置hive-env.sh</h3> 
<pre><code>cd apache-hive-3.1.2-bin/conf/
mv hive-env.sh.template hive-env.sh
</code></pre> 
<p>在文件末尾添加以下内容：</p> 
<pre><code>export JAVA_HOME=/home/gdan/data/jdk-8u131-linux-x64/jdk1.8.0_131
export HIVE_HOME=/home/gdan/data/soft/apache-hive-3.1.2-bin
export HADOOP_HOME=/home/gdan/data/soft/hadoop-3.2.0
</code></pre> 
<p>再/etc/profile也需要设置hive的环境变量</p> 
<pre><code>export HIVE_HOME=/home/gdan/data/soft/apache-hive-3.1.2-bin
export PATH=$HIVE_HOME/bin:$PATH
</code></pre> 
<p><img alt="" height="909" src="https://images2.imgbox.com/31/a4/gL6YqUgc_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<p></p> 
<h3>2.2 安装mysql数据库，并创建hive库</h3> 
<p>1.安装MySQL 在终端中输入以下命令来安装MySQL：</p> 
<pre><code>sudo apt-get update
sudo apt-get install mysql-server
</code></pre> 
<p>        安装过程中会提示您设置MySQL的root用户密码，请根据提示进行设置。</p> 
<p>安装时没有提示输入root账户密码，默认是空，可以执行以下命令设置密码为<code>xxxx</code>：</p> 
<pre><code>sudo mysql -u root -p  #密码按Enter即可进入mysql shell，空格也可以，普通用户一定sudo
</code></pre> 
<p></p> 
<p>2. 创建hive元数据数据库</p> 
<pre><code>create database hive;
</code></pre> 
<p><img alt="" height="555" src="https://images2.imgbox.com/9a/7f/HS7K36eP_o.png" width="448"></p> 
<p></p> 
<p></p> 
<h3>2.3 配置hive-site.xml</h3> 
<pre><code>cd apache-hive-3.1.2-bin/conf/
mv hive-default.xml.template hive-site.xml
</code></pre> 
<p> 在文件中添加以下内容：</p> 
<pre><code>&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://localhost:3306/hive?serverTimezone=Asia/Shanghai&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
    &lt;value&gt;admin&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hive.querylog.location&lt;/name&gt;
    &lt;value&gt;/data/hive_repo/querylog&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
    &lt;value&gt;/data/hive_repo/scratchdir&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
    &lt;value&gt;/data/hive_repo/resources&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
<blockquote> 
 <p>给定的代码片段是Hive的配置文件，用于配置连接到MySQL数据库的相关属性。具体解释如下：</p> 
 <ol><li> <p><code>javax.jdo.option.ConnectionURL</code>：指定连接到MySQL数据库的URL。在这个例子中，URL为<code>jdbc:mysql://localhost:3306/hive?serverTimezone=Asia/Shanghai</code>，表示连接到本地的MySQL数据库，端口为3306，数据库名为hive，使用Asia/Shanghai时区。</p> </li><li> <p><code>javax.jdo.option.ConnectionDriverName</code>：指定用于连接到Hive数据库的JDBC驱动程序的类名。在这个例子中，驱动程序的类名为<code>com.mysql.cj.jdbc.Driver</code>，表示使用MySQL的JDBC驱动程序。</p> </li><li> <p><code>javax.jdo.option.ConnectionUserName</code>：指定连接到Hive数据库时使用的用户名。在这个例子中，用户名为<code>root</code>。</p> </li><li> <p><code>javax.jdo.option.ConnectionPassword</code>：指定连接到Hive数据库时使用的密码。在这个例子中，密码为<code>admin</code>。</p> </li><li> <p><code>hive.querylog.location</code>：指定查询日志的存储位置。在这个例子中，查询日志存储在<code>/data/hive_repo/querylog</code>目录中。</p> </li><li> <p><code>hive.exec.local.scratchdir</code>：指定本地临时文件的存储位置。在这个例子中，临时文件存储在<code>/data/hive_repo/scratchdir</code>目录中。</p> </li><li> <p><code>hive.downloaded.resources.dir</code>：指定下载资源的存储位置。在这个例子中，下载的资源存储在<code>/data/hive_repo/resources</code>目录中。</p> </li></ol> 
 <p>这些属性用于配置连接到Hive数据库的相关参数，并定义了查询日志、临时文件和下载资源的存储位置。</p> 
</blockquote> 
<p></p> 
<p></p> 
<h3> 2.4 修改Hadoop的etc/hadoop/core-site.xml文件</h3> 
<pre><code>&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
<blockquote> 
 <p>给定的代码片段是Hadoop的配置文件，用于配置代理用户的相关属性。具体解释如下：</p> 
 <ol><li> <p><code>hadoop.proxyuser.root.hosts</code>：指定可以通过代理用户<code>root</code>进行访问的主机列表。在这个例子中，<code>*</code>表示允许所有主机通过<code>root</code>用户进行访问。</p> </li><li> <p><code>hadoop.proxyuser.root.groups</code>：指定可以通过代理用户<code>root</code>进行访问的用户组列表。在这个例子中，<code>*</code>表示允许所有用户组通过<code>root</code>用户进行访问。</p> </li></ol> 
 <p>这些属性用于配置Hadoop代理用户的访问权限，允许指定的主机和用户组通过指定的代理用户进行访问。</p> 
</blockquote> 
<p></p> 
<h3>  2.5 初始化Hive的Metastore</h3> 
<pre><code>cd /data/soft/apache-hive-3.1.2-bin
bin/schematool -dbType mysql -initSchema
</code></pre> 
<p> 一直出现这种问题</p> 
<p><img alt="" height="519" src="https://images2.imgbox.com/f5/8b/tmq3Y3ST_o.png" width="1200"></p> 
<p>后来发现是环境问题，回去更改 hive-env.sh 文件</p> 
<p>再继续执行，又报错了</p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/e6/5c/zeBRJ7kt_o.png" width="1200"></p> 
<p>at [row,col,system-id]: [3215,96</p> 
<p>需要删除3215行里面的description，把hive.txn.xlock.iow对应的description标签内容删掉。</p> 
<p></p> 
<p>又开始报错</p> 
<p><img alt="" height="334" src="https://images2.imgbox.com/81/3c/Z8usS59i_o.png" width="1200"></p> 
<p> 此外还需要下载mysql jdbc 的jar包</p> 
<p>去官网下载，把jar包放进hive的lib文件夹下</p> 
<p><img alt="" height="238" src="https://images2.imgbox.com/4e/36/WaSZM5zM_o.png" width="1200"></p> 
<p></p> 
<p> 去mysql创建用户<img alt="" height="447" src="https://images2.imgbox.com/76/f6/EC192pyv_o.png" width="919"></p> 
<p> 使用的是mysql8.0版本，所以正确代码是：</p> 
<pre><code>create user 'dw'@'localhost' identified by '123456'; -- 创建用户
grant all on *.* to 'dw'@'localhost'; -- 将所有数据库的所有表的所有权限赋给datawhale
flush privileges;  -- 刷新mysql系统权限关系表
</code></pre> 
<p> 注意不要use hive</p> 
<p><img alt="" height="126" src="https://images2.imgbox.com/cb/67/Akp7u7yK_o.png" width="856"></p> 
<p> 检查hive是否成功部署</p> 
<p><img alt="" height="563" src="https://images2.imgbox.com/05/91/3hjiFT8f_o.png" width="1200"></p> 
<p></p> 
<p></p> 
<h2>3 Hive使用</h2> 
<p> 创建数据库</p> 
<pre><code>hive
create database hive;
</code></pre> 
<h3>3.1 创建数据表</h3> 
<pre><code>create table t3_new(
  id int comment 'ID',
  stu_name string comment 'name' ,
  stu_birthday date comment 'birthday',
  online boolean comment 'is onlone'
)row format delimited 
fields terminated by '\t' 
lines terminated by '\n';
</code></pre> 
<p><img alt="" height="730" src="https://images2.imgbox.com/bb/a7/0KLNxcwf_o.png" width="1129"></p> 
<p> hdfs文件系统中的存储位置如下：</p> 
<p> <img alt="" height="409" src="https://images2.imgbox.com/67/1f/QcDCYYDe_o.png" width="1125"></p> 
<blockquote> 
 <p> "hadoop fs -ls /" 是一个 Hadoop 命令，用于列出 Hadoop 分布式文件系统（HDFS）根目录下的文件和目录。</p> 
 <p>具体解释如下：</p> 
 <ul><li>"hadoop": 这是 Hadoop 命令行工具的名称。</li><li>"fs": 表示要执行与文件系统相关的操作。</li><li>"-ls": 是一个选项，表示要列出指定路径下的文件和目录。</li><li>"/": 这是 HDFS 的根目录路径。</li></ul> 
 <p>因此，运行 "hadoop fs -ls /" 命令将返回 HDFS 根目录下的所有文件和目录的列表。</p> 
</blockquote> 
<p></p> 
<h3>3.2 创建带Array的表</h3> 
<pre><code>create table stu(
  id int,
  name string,
  favors array&lt;string&gt;
)row format delimited 
fields terminated by '\t'
collection items terminated by ','
lines terminated by '\n';
</code></pre> 
<p></p> 
<h3>3.3 创建带Map的表</h3> 
<pre><code>create table stu2(
  id int,
  name string,
  scores map&lt;string,int&gt;
)row format delimited 
fields terminated by '\t'
collection items terminated by ','
map keys terminated by ':'
lines terminated by '\n';
</code></pre> 
<p></p> 
<h3>3.4 加载数据到表中</h3> 
<p>加载数据到表中属于<code>DML</code>操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中的命令，命令如下：</p> 
<pre><code>load data local inpath '/home/gdan/data/soft/hivedata/stu3.data' into table emp;
</code></pre> 
<p>其中<code>stu3.data</code>的内容如下：</p> 
<pre><code>7369	SMITH	CLERK	7902	1980-12-17 00:00:00	800.00		20
7499	ALLEN	SALESMAN	7698	1981-02-20 00:00:00	1600.00	300.00	30
7521	WARD	SALESMAN	7698	1981-02-22 00:00:00	1250.00	500.00	30
7566	JONES	MANAGER	7839	1981-04-02 00:00:00	2975.00		20
7654	MARTIN	SALESMAN	7698	1981-09-28 00:00:00	1250.00	1400.00	30
7698	BLAKE	MANAGER	7839	1981-05-01 00:00:00	2850.00		30
7782	CLARK	MANAGER	7839	1981-06-09 00:00:00	2450.00		10
7788	SCOTT	ANALYST	7566	1987-04-19 00:00:00	1500.00		20
7839	KING	PRESIDENT		1981-11-17 00:00:00	5000.00		10
7844	TURNER	SALESMAN	7698	1981-09-08 00:00:00	1500.00	0.00	30
7876	ADAMS	CLERK	7788	1987-05-23 00:00:00	1100.00		20
7900	JAMES	CLERK	7698	1981-12-03 00:00:00	950.00		30
7902	FORD	ANALYST	7566	1981-12-03 00:00:00	3000.00		20
7934	MILLER	CLERK	7782	1982-01-23 00:00:00	1300.00		10</code></pre> 
<p> <img alt="" height="200" src="https://images2.imgbox.com/c1/ba/wK8CaNlW_o.png" width="977"></p> 
<p></p> 
<p> <img alt="" height="88" src="https://images2.imgbox.com/d6/08/PatehNKY_o.png" width="1066"></p> 
<h3></h3> 
<h3>3.5 创建桶表</h3> 
<pre><code>create table bucket_tb(
   id int
)clustered by (id) into 4 buckets;
</code></pre> 
<p> 注意：需要从普通表中将数据插入到桶表中</p> 
<p></p> 
<h3>3.6 创建视图</h3> 
<p></p> 
<pre><code>create view v1 as select id,stu_name from t3_new;
</code></pre> 
<p>又开始报错</p> 
<p><img alt="" height="809" src="https://images2.imgbox.com/fa/4a/Yr92E4ux_o.png" width="1200"></p> 
<p><a href="http://t.csdn.cn/TBkLa" rel="nofollow" title="http://t.csdn.cn/TBkLa">http://t.csdn.cn/TBkLa</a></p> 
<p>网上的教程也是很多坑，之前说把Hadoop的一个jar包放进Hive里面，其实不需要这个操作。</p> 
<p>改回来就可以了</p> 
<p><img alt="" height="817" src="https://images2.imgbox.com/83/42/YIeKnJY1_o.png" width="999"></p> 
<p></p> 
<h3>3.7 综合案例</h3> 
<p><strong>案例需求：</strong>通过Flume按天将日志数据采集到HDFS中对应目录，使用SQL按天统计每天的相关指标，数据样例如下：</p> 
<pre><code>{
	"uid": "861848974414839801",
	"nickname": "mick",
	"usign": "",
	"sex": 1,
	"birthday": "",
	"face": "",
	"big_face": "",
	"email": "abc@qq.com",
	"mobile": "",
	"reg_type": "102",
	"last_login_time": "1494344580",
	"reg_time": "1494344580",
	"last_update_time": "1494344580",
	"status": "5",
	"is_verified": "0",
	"verified_info": "",
	"is_seller": "0",
	"level": 1,
	"exp": 0,
	"anchor_level": 0,
	"anchor_exp": 0,
	"os": "android",
	"timestamp": 1494344580,
	"type": "user_info"
}</code></pre> 
<p><strong>解决方法：</strong></p> 
<ol><li>针对Flume的Source可以使用Exec Source，Channel可以使用基于文件的或者内存的，Sink使用HDFS Sink，在HDFS Sink的Path路径中需要使用<code>%Y%m%d</code>获取日期，将每天的日志数据采集到指定的HDFS目录中。</li><li>需要对按天采集的日志数据建表，由于这份数据可能会被多种计算引擎使用，所以建议使用外部 表。</li><li>在实际工作中，离线计算的需求大部分都是按天计算的，所以在这里最好在表中增加日期这个分区字段，最终决定使用外部分区表。</li><li>针对Json数据，先创建外部分区表，再创建视图，解析Json数据中的字段</li></ol> 
<p></p> 
<p>如果你要重新启动hive的话，需要把hadoop重新启动一下：</p> 
<p><img alt="" height="318" src="https://images2.imgbox.com/bf/87/d73KglG9_o.png" width="1020"></p> 
<p></p> 
<p>1. 创建分区表</p> 
<pre><code class="language-sql">CREATE EXTERNAL TABLE ex_par_more_type (
  log STRING
)
PARTITIONED BY (dt STRING, d_type STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/moreType';
</code></pre> 
<blockquote> 
 <p>这是一个创建外部表的SQL语句，用于创建名为ex_par_more_type的外部表。该表包含一个名为log的列，并照dt和d_type两个分区进行分区。</p> 
 <p>以下是对每个部分的解释：</p> 
 <ul><li><code>CREATE EXTERNAL TABLE ex_par_more_type</code>: 创建一个名为ex_par_more_type的外部表。</li><li><code>(log string)</code>: 定义了一个名为log的列，其数据类型为string。</li><li><code>PARTITIONED (dt string, d_type string)</code>: 按dt和d_type两个列进行分区。</li><li><code>ROW FORMAT DELIMITED</code>: 指定行格式为分隔符格式。</li><li><code>FIELDS TERMINATED '\t'</code>: 指定字段之间的隔符为制表符（'\t'）。</li><li><code>LOCATION '/moreType'</code>: 指定外部表存储位置为/mType。</li></ul> 
 <p>通过执行这个SQL语句，你可以在指定的存储位置中创建一个外部表ex_par_more_type，该表含一个log列，并按照dt和d_type进行分区。</p> 
</blockquote> 
<p></p> 
<p>2. 添加分区</p> 
<pre><code class="language-sql">alter table ex_par_more_type add  partition(dt='20200504',d_type='giftRecord') location '/moreType/20200504/giftRecord';
</code></pre> 
<blockquote> 
 <p>这个命令是用于在表 <code>ex_par_more_type</code> 中添加一个分区。</p> 
 <p></p> 
 <p>分区的条件是 <code>dt='20200504'</code> 和 <code>d_type='giftRecord'</code>，并且该分区的数据将存储在路径 <code>/moreType/20200504/giftRecord</code> 中。</p> 
</blockquote> 
<p></p> 
<p>3. 创建视图</p> 
<pre><code class="language-sql">create view gift_record_view as 
select 
    get_json_object(log,'$.send_id') as send_id,
    get_json_object(log,'$.good_id') as good_id,
    get_json_object(log,'$.video_id') as video_id,
    get_json_object(log,'$.gold') as gold,
dt
from ex_par_more_type
where d_type='giftRecord';
</code></pre> 
<p></p> 
<p>4. 创建脚本</p> 
<pre><code class="language-bash">#!/bin/bash
# 每天凌晨1点定时添加当天日期的分区
if [ "a$1" = "a" ]
then
 dt=`date +%Y%m%d`
else
 dt=$1
fi
# 指定添加分区操作
hive -e "
alter table ex_par_more_type add if not exists  partition(dt='${dt}',d_type='giftRecord') location '/moreType/${dt}/giftRecord';
alter table ex_par_more_type add if not exists partition(dt='${dt}',d_type='userInfo') location '/moreType/${dt}/userInfo';
alter table ex_par_more_type add if not exists  partition(dt='${dt}',d_type='videoInfo') location '/moreType/${dt}/videoInfo';
"
</code></pre> 
<p>这是一个Bash脚本，用于在每天凌晨1点定时添加当天日期的分区。</p> 
<p>脚本首先检查是否提了命令行参数如果没有提供参数，则使用当前日期作为分区的日期。如果提供了参数，则使用该参数作为分区的日期。</p> 
<p>接下来，脚本使用Hive命令执行三个ALTER TABLE语句，用于添加分区。这些语句将在名为"ex_par_more_type"表中添加三个分区，分别对应不同的"d_type"值giftRecord、userInfovideoInfo）。每个分区的位置（location）都基于日期动态生成。</p> 
<p> 这些操作将在H中的"ex_par_more_type"表中添加当天日期的个分区，并指定相应的位置。</p> 
<p><img alt="" height="921" src="https://images2.imgbox.com/b7/f3/oJJ0tuRx_o.png" width="1200"></p> 
<p></p> 
<p>5. 配置crontab，每天执行一次</p> 
<pre><code class="language-bash">00 01 * * * root /bin/bash /data/soft/hivedata/addPartion.sh &gt;&gt; /data/soft/hivedata/addPartion.log
</code></pre> 
<p></p> 
<h2 id="_4-hive高级函数应用">4 Hive高级函数应用</h2> 
<h3>4.1 分组排序取TopN</h3> 
<p>使用<code>row_number()</code>（对数据编号，从1开始）和<code>over()</code>（把数据划分到一个窗口，加上<code>partition by</code>，按照字段对数据进行分组，使用<code>order by</code>按照字段进行排序）</p> 
<pre><code class="language-sql">SELECT *
FROM (
  SELECT *,
    ROW_NUMBER() OVER (PARTITION BY sub ORDER BY score DESC) AS num
  FROM student_score
) s
WHERE s.num &lt;= 3;
</code></pre> 
<p>这个查询语句的目的是从名为 "student_score" 的表中选择每个科目（""）的前三名学生。使用窗口函数 <code>ROW_NUMBER()</code> 对每个科目的成绩进行降序排列，并为每个分组分配一个行号，外部查询选择行号小等于3的记录即每个科的前三名学生。</p> 
<ul><li>可以使用<code>rank()</code>替代<code>row_number()</code>，表示上下两条记录的score相等时，记录的行号是一样的，但下一个score值的行号递增N</li><li>可以使用<code>dense_rank()</code>替代<code>row_number()</code>，表示上下两条记录的score相等时，下一个score值的行号递增1</li></ul> 
<p></p> 
<h3>4.2 行转列</h3> 
<p><code>concat_ws()</code>（根据指定的分隔符拼接多个字段的值）、<br><code>collect_set()</code>（返回一个set集合，集合汇中的元素不重复） 、<br><code>collect_list()</code>（返回一个list集合，集合中的元素会重复）</p> 
<pre><code class="language-sql">select name,collect_list(favor) as favor_list from student_favors group by name;
</code></pre> 
<p></p> 
<h3>4.3 列转行</h3> 
<p><code>split()</code>（接受一个字符串和切割规则，就类似于java中的split函数，使用切割规则对字符串中的数据进行切割，最终返回一个array数组）<br><code>explode()</code>（表示把数组中的每个元素转成一行）<br><code>lateral view</code>（可以对数据产生一个支持别名的虚拟表）</p> 
<pre><code class="language-sql">select name,favor_new from student_favors_2 lateral view explode(split(favorlist,',')) table1 as favor_new;
</code></pre> 
<p></p> 
<h3>4.4 distribute by</h3> 
<ul><li><code>ditribute by</code>：只会根据指定的key对数据进行分区，但是不会排序。</li><li>一般情况下可以和<code>sort by</code>结合使用，先对数据分区，再进行排序，两者结合使用的时候，<code>distribute by</code>必须要写在<code>sort by</code>之前。</li></ul> 
<p></p> 
<h3>4.5 cluster by</h3> 
<ul><li><code>cluster by</code>：是<code>distribute by</code>和<code>sort by</code>的简写形式，即<code>cluster by id</code>等于<code>distribute by id sort by id</code></li></ul> 
<p></p> 
<h2>5 应用示例</h2> 
<h3>5.1 解决数据倾斜问题的SQL语句</h3> 
<p>数据倾斜的根本原因是数据的key分布不均，个别key数据很多，超出了计算节点的计算能力的结果</p> 
<pre><code class="language-sql">SELECT a.Key, SUM(a.Cnt) AS Cnt
FROM (
 SELECT Key, COUNT(*) AS Cnt
 FROM TableName GROUP BY Key,
 CASE
 WHEN Key = 'KEY001' THEN Hash(Random()) % 50
 ELSE 0
 END
) a GROUP BY a.Key;
</code></pre> 
<p>其中<code>CASE WHEN</code>将数据打散，然后再进行<code>GROUP BY</code></p> 
<p></p> 
<h3>5.2 解决数据倾斜方案</h3> 
<ul><li> <p>参数调节：hive.groupby.skewindata=true 有数据倾斜的时候进行负载均衡，当选项设定为true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果，按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p> </li><li> <p>SQL语句调节：</p> 
  <ol><li>大小表Join：使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</li><li>大表Join大表：把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。</li><li>count distinct大量相同特殊值：在count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</li><li>去重求和：采用sum() group by的方式来替换count(distinct)完成计算。</li></ol></li></ul> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/38880fd3ff31d0039d0ae89311227bd0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">XILINX ZYNQ 7000 AXI总线 (三) AXI GPIO</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4f4b17fe7cc23de3abd9b271025e8fe3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Spring基础知识讲解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>