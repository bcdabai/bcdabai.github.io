<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【大数据】Hive函数&#43;分区表和分桶表＋文件格式和压缩 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【大数据】Hive函数&#43;分区表和分桶表＋文件格式和压缩" />
<meta property="og:description" content="文章目录 一、函数1. 炸裂函数1.1 常用的UDTF1.2 Lateral View 2. 窗口函数（开窗函数）2.1 概述和语法2.2 常用窗口函数 3. 自定义函数4. 自定义UDF函数 二、分区表和分桶表1. 分区表1.1 分区表基本语法1.2 二级分区表 2. 分桶表2.1 分桶表基本语法2.2 分桶排序表 三、文件格式和压缩1. Hadoop压缩概述2. Hive文件格式2.1 Text File2.2 ORC2.3 Parquet 3. 压缩3.1 Hive表数据进行压缩3.2 计算过程中使用压缩 一、函数 1. 炸裂函数 UDTF（Table-Generating Functions），接收一行数据，输出一行或多行数据。
1.1 常用的UDTF explode(ARRAY&lt;T&gt; a) select explode(array(“a”,”b”,”c”)) as item; explode(MAP&lt;K,V&gt; m) select explode(map(“a”,1,”b”,2,”c”,3)) as (key,value); posexplode(ARRAY&lt;T&gt; a) select posexplode(array(“a”,”b”,”c”)) as (pos,item); inline(ARRAY&lt;STRUCT&lt;f1:T1,…,fn:Tn&gt;&gt; a) select inline(array&lt;named_struct(“id”,1,”name”,”zs”), named_struct(“id”,2,”name”,”ls”), named_struct(“id”,3,”name”,”ww”)) as (id,name); 1.2 Lateral View Lateral View通常与UDTF配合使用。Lateral View可以将UDTF应用到源表的每行数据，将每行数据转换伟一行或多行，并将源表钟每行的输出结果与该行连接起来，形成一个虚拟表。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/97fdab36eaa4087f2b9c6be3069e0cf6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-06T15:22:48+08:00" />
<meta property="article:modified_time" content="2023-10-06T15:22:48+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【大数据】Hive函数&#43;分区表和分桶表＋文件格式和压缩</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p></p> 
 <div class="toc"> 
  <h4>文章目录</h4> 
  <ul><li><a href="#_2" rel="nofollow">一、函数</a></li><li><ul><li><a href="#1__3" rel="nofollow">1. 炸裂函数</a></li><li><ul><li><a href="#11_UDTF_6" rel="nofollow">1.1 常用的UDTF</a></li><li><a href="#12_Lateral_View_38" rel="nofollow">1.2 Lateral View</a></li></ul> 
    </li><li><a href="#2__42" rel="nofollow">2. 窗口函数（开窗函数）</a></li><li><ul><li><a href="#21__43" rel="nofollow">2.1 概述和语法</a></li><li><a href="#22__61" rel="nofollow">2.2 常用窗口函数</a></li></ul> 
    </li><li><a href="#3__89" rel="nofollow">3. 自定义函数</a></li><li><a href="#4_UDF_124" rel="nofollow">4. 自定义UDF函数</a></li></ul> 
   </li><li><a href="#_246" rel="nofollow">二、分区表和分桶表</a></li><li><ul><li><a href="#1__247" rel="nofollow">1. 分区表</a></li><li><ul><li><a href="#11__249" rel="nofollow">1.1 分区表基本语法</a></li><li><a href="#12__333" rel="nofollow">1.2 二级分区表</a></li></ul> 
    </li><li><a href="#2__407" rel="nofollow">2. 分桶表</a></li><li><ul><li><a href="#21__411" rel="nofollow">2.1 分桶表基本语法</a></li><li><a href="#22__459" rel="nofollow">2.2 分桶排序表</a></li></ul> 
   </li></ul> 
   </li><li><a href="#_484" rel="nofollow">三、文件格式和压缩</a></li><li><ul><li><a href="#1_Hadoop_485" rel="nofollow">1. Hadoop压缩概述</a></li><li><a href="#2_Hive_513" rel="nofollow">2. Hive文件格式</a></li><li><ul><li><a href="#21_Text_File_515" rel="nofollow">2.1 Text File</a></li><li><a href="#22_ORC_527" rel="nofollow">2.2 ORC</a></li><li><a href="#23_Parquet_578" rel="nofollow">2.3 Parquet</a></li></ul> 
    </li><li><a href="#3__612" rel="nofollow">3. 压缩</a></li><li><ul><li><a href="#31_Hive_614" rel="nofollow">3.1 Hive表数据进行压缩</a></li><li><a href="#32__651" rel="nofollow">3.2 计算过程中使用压缩</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </div> 
 <p></p> 
</blockquote> 
<h2><a id="_2"></a>一、函数</h2> 
<h3><a id="1__3"></a>1. 炸裂函数</h3> 
<p>UDTF（Table-Generating Functions），接收一行数据，输出一行或多行数据。<br> <img src="https://images2.imgbox.com/63/4a/kuB5blYL_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="11_UDTF_6"></a>1.1 常用的UDTF</h4> 
<ul><li><code>explode(ARRAY&lt;T&gt; a)</code></li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> explode<span class="token punctuation">(</span>array<span class="token punctuation">(</span>“a”<span class="token punctuation">,</span>”b”<span class="token punctuation">,</span>”c”<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> item<span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ad/7d/9IHX0uif_o.png" alt="在这里插入图片描述"></p> 
<ul><li><code>explode(MAP&lt;K,V&gt; m)</code></li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> explode<span class="token punctuation">(</span>map<span class="token punctuation">(</span>“a”<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>”b”<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>”c”<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token punctuation">(</span><span class="token keyword">key</span><span class="token punctuation">,</span><span class="token keyword">value</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/0a/7a/p7zWJfWJ_o.png" alt="在这里插入图片描述"></p> 
<ul><li><code>posexplode(ARRAY&lt;T&gt; a)</code></li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> posexplode<span class="token punctuation">(</span>array<span class="token punctuation">(</span>“a”<span class="token punctuation">,</span>”b”<span class="token punctuation">,</span>”c”<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token punctuation">(</span>pos<span class="token punctuation">,</span>item<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a2/c1/Ta569xDO_o.png" alt="在这里插入图片描述"></p> 
<ul><li><code>inline(ARRAY&lt;STRUCT&lt;f1:T1,…,fn:Tn&gt;&gt; a)</code></li></ul> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> inline<span class="token punctuation">(</span>array<span class="token operator">&lt;</span>named_struct<span class="token punctuation">(</span>“id”<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>”name”<span class="token punctuation">,</span>”zs”<span class="token punctuation">)</span><span class="token punctuation">,</span>
					named_struct<span class="token punctuation">(</span>“id”<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>”name”<span class="token punctuation">,</span>”ls”<span class="token punctuation">)</span><span class="token punctuation">,</span>
					named_struct<span class="token punctuation">(</span>“id”<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>”name”<span class="token punctuation">,</span>”ww”<span class="token punctuation">)</span><span class="token punctuation">)</span> 
<span class="token keyword">as</span> <span class="token punctuation">(</span>id<span class="token punctuation">,</span>name<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/9d/b8/J4E0LWh1_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="12_Lateral_View_38"></a>1.2 Lateral View</h4> 
<p>Lateral View通常与UDTF配合使用。Lateral View可以将UDTF应用到源表的每行数据，将每行数据转换伟一行或多行，并将源表钟每行的输出结果与该行连接起来，形成一个虚拟表。</p> 
<p><img src="https://images2.imgbox.com/4b/db/uOCaIfx1_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2__42"></a>2. 窗口函数（开窗函数）</h3> 
<h4><a id="21__43"></a>2.1 概述和语法</h4> 
<p><img src="https://images2.imgbox.com/c2/1c/6bHkqWWv_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8d/ef/1ufFvOxl_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/ad/f9/nWWkLaKM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/dd/4a/w2hgSFI8_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/6b/51/pERKHfwh_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/2f/40/lnNFL4XB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/4e/65/FxhHzXjS_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ee/ed/ec7dC6T2_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/4f/00/yWoVOHTQ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/d0/px8OgRxJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/63/e6/zQlNQnWq_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="22__61"></a>2.2 常用窗口函数</h4> 
<p>按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数。<br> （1）聚合函数</p> 
<blockquote> 
 <p>max：最大值。<br> min：最小值。<br> sum：求和。<br> avg：平均值。<br> count：计数。</p> 
</blockquote> 
<p>（2）跨行取值函数</p> 
<p><strong>lead和lag</strong></p> 
<p><img src="https://images2.imgbox.com/65/fe/fN1CwoL0_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>lag和lead函数不支持自定义窗口。</p> 
</blockquote> 
<p><strong>first_value和last_value</strong></p> 
<p><img src="https://images2.imgbox.com/b6/e7/nmCNH5CD_o.png" alt="在这里插入图片描述"></p> 
<p>(3）排名函数<br> <img src="https://images2.imgbox.com/5b/6c/FPk2IYPw_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>rank 、dense_rank、row_number不支持自定义窗口。</p> 
</blockquote> 
<p><a href="https://www.bilibili.com/video/BV1g84y147sX?p=82" rel="nofollow">案例演示及练习题</a></p> 
<h3><a id="3__89"></a>3. 自定义函数</h3> 
<blockquote> 
 <ul><li>Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。</li><li>当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</li></ul> 
</blockquote> 
<p>根据用户自定义函数类别分为以下三种：</p> 
<ul><li>UDF（User-Defined-Function）<br> 一进一出。</li><li>UDAF（User-Defined Aggregation Function）<br> 用户自定义聚合函数，多进一出。<br> 类似于：count/max/min</li><li>UDTF（User-Defined Table-Generating Functions）<br> 用户自定义表生成函数，一进多出。<br> 如lateral view explode()</li></ul> 
<p><strong>编程步骤</strong><br> （1）继承Hive提供的类</p> 
<pre><code class="prism language-sql">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>generic<span class="token punctuation">.</span>GenericUDF
org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>generic<span class="token punctuation">.</span>GenericUDTF<span class="token punctuation">;</span>
</code></pre> 
<p>（2）实现类中的抽象方法<br> （3）在hive的命令行窗口创建函数</p> 
<pre><code class="prism language-sql"><span class="token comment">--添加jar。</span>
<span class="token keyword">add</span> jar linux_jar_path
<span class="token comment">--创建function。</span>
<span class="token keyword">create</span> <span class="token punctuation">[</span><span class="token keyword">temporary</span><span class="token punctuation">]</span> <span class="token keyword">function</span> <span class="token punctuation">[</span>dbname<span class="token punctuation">.</span><span class="token punctuation">]</span>function_name <span class="token keyword">AS</span> class_name<span class="token punctuation">;</span>
<span class="token comment">--在hive的命令行窗口删除函数</span>
<span class="token keyword">drop</span> <span class="token punctuation">[</span><span class="token keyword">temporary</span><span class="token punctuation">]</span> <span class="token keyword">function</span> <span class="token punctuation">[</span><span class="token keyword">if</span> <span class="token keyword">exists</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>dbname<span class="token punctuation">.</span><span class="token punctuation">]</span>function_name<span class="token punctuation">;</span>
</code></pre> 
<h3><a id="4_UDF_124"></a>4. 自定义UDF函数</h3> 
<p>需求：自定义一个UDF实现计算给定基本数据类型的长度，例如：</p> 
<pre><code class="prism language-sql">hive<span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token keyword">select</span> my_len<span class="token punctuation">(</span><span class="token string">"abcd"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token number">4</span>
</code></pre> 
<p>（1）创建一个Maven工程Hive，导入依赖</p> 
<pre><code class="prism language-sql"><span class="token operator">&lt;</span>dependencies<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span>dependency<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>groupId<span class="token operator">&gt;</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hive<span class="token operator">&lt;</span><span class="token operator">/</span>groupId<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>artifactId<span class="token operator">&gt;</span>hive<span class="token operator">-</span><span class="token keyword">exec</span><span class="token operator">&lt;</span><span class="token operator">/</span>artifactId<span class="token operator">&gt;</span>
		<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span><span class="token number">3.1</span><span class="token number">.3</span><span class="token operator">&lt;</span><span class="token operator">/</span>version<span class="token operator">&gt;</span>
	<span class="token operator">&lt;</span><span class="token operator">/</span>dependency<span class="token operator">&gt;</span>
<span class="token operator">&lt;</span><span class="token operator">/</span>dependencies<span class="token operator">&gt;</span>
</code></pre> 
<p>（3）创建一个类</p> 
<pre><code class="prism language-java"><span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span></span><span class="token class-name">UDFArgumentException</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span></span><span class="token class-name">UDFArgumentLengthException</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>exec<span class="token punctuation">.</span></span><span class="token class-name">UDFArgumentTypeException</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>metadata<span class="token punctuation">.</span></span><span class="token class-name">HiveException</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>ql<span class="token punctuation">.</span>udf<span class="token punctuation">.</span>generic<span class="token punctuation">.</span></span><span class="token class-name">GenericUDF</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>serde2<span class="token punctuation">.</span>objectinspector<span class="token punctuation">.</span></span><span class="token class-name">ObjectInspector</span></span><span class="token punctuation">;</span>
<span class="token keyword">import</span> <span class="token import"><span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>serde2<span class="token punctuation">.</span>objectinspector<span class="token punctuation">.</span>primitive<span class="token punctuation">.</span></span><span class="token class-name">PrimitiveObjectInspectorFactory</span></span><span class="token punctuation">;</span>

<span class="token comment">/**
 * 我们需计算一个要给定基本数据类型的长度
 */</span>
<span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyUDF</span> <span class="token keyword">extends</span> <span class="token class-name">GenericUDF</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">/**
     * 判断传进来的参数的类型和长度
     * 约定返回的数据类型
     */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token class-name">ObjectInspector</span> <span class="token function">initialize</span><span class="token punctuation">(</span><span class="token class-name">ObjectInspector</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arguments<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">UDFArgumentException</span> <span class="token punctuation">{<!-- --></span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span>arguments<span class="token punctuation">.</span>length <span class="token operator">!=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
            <span class="token keyword">throw</span>  <span class="token keyword">new</span> <span class="token class-name">UDFArgumentLengthException</span><span class="token punctuation">(</span><span class="token string">"please give me  only one arg"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>arguments<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">getCategory</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token class-name">ObjectInspector<span class="token punctuation">.</span>Category</span><span class="token punctuation">.</span><span class="token constant">PRIMITIVE</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token keyword">throw</span>  <span class="token keyword">new</span> <span class="token class-name">UDFArgumentTypeException</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">"i need primitive type arg"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token keyword">return</span> <span class="token class-name">PrimitiveObjectInspectorFactory</span><span class="token punctuation">.</span>javaIntObjectInspector<span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token comment">/**
     * 解决具体逻辑
     */</span>
    <span class="token annotation punctuation">@Override</span>
    <span class="token keyword">public</span> <span class="token class-name">Object</span> <span class="token function">evaluate</span><span class="token punctuation">(</span><span class="token class-name">DeferredObject</span><span class="token punctuation">[</span><span class="token punctuation">]</span> arguments<span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">HiveException</span> <span class="token punctuation">{<!-- --></span>

        <span class="token class-name">Object</span> o <span class="token operator">=</span> arguments<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token keyword">if</span><span class="token punctuation">(</span>o<span class="token operator">==</span><span class="token keyword">null</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
            <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
        <span class="token punctuation">}</span>

        <span class="token keyword">return</span> o<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>

    <span class="token annotation punctuation">@Override</span>
    <span class="token comment">// 用于获取解释的字符串</span>
    <span class="token keyword">public</span> <span class="token class-name">String</span> <span class="token function">getDisplayString</span><span class="token punctuation">(</span><span class="token class-name">String</span><span class="token punctuation">[</span><span class="token punctuation">]</span> children<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
        <span class="token keyword">return</span> <span class="token string">""</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>（4）创建临时函数</p> 
<pre><code class="prism language-sql"><span class="token comment">--打成jar包上传到服务器</span>
<span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>hive<span class="token operator">/</span>datas<span class="token operator">/</span>myudf<span class="token punctuation">.</span>jar
<span class="token comment">--将jar包添加到hive的classpath，临时生效</span>
<span class="token keyword">add</span> jar <span class="token operator">/</span>opt<span class="token operator">/</span>module<span class="token operator">/</span>hive<span class="token operator">/</span>datas<span class="token operator">/</span>myudf<span class="token punctuation">.</span>jar<span class="token punctuation">;</span>
<span class="token comment">--创建临时函数与开发好的java class关联</span>
<span class="token keyword">create</span> <span class="token keyword">temporary</span> <span class="token keyword">function</span> my_len 
<span class="token keyword">as</span> <span class="token string">"com.atguigu.hive.udf.MyUDF"</span><span class="token punctuation">;</span>

<span class="token comment">--即可在hql中使用自定义的临时函数</span>
<span class="token keyword">select</span> 
    ename<span class="token punctuation">,</span>
    my_len<span class="token punctuation">(</span>ename<span class="token punctuation">)</span> ename_len 
<span class="token keyword">from</span> emp<span class="token punctuation">;</span>

<span class="token comment">--删除临时函数</span>
<span class="token keyword">drop</span> <span class="token keyword">temporary</span> <span class="token keyword">function</span> my_len<span class="token punctuation">;</span>
</code></pre> 
<blockquote> 
 <p>临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。</p> 
</blockquote> 
<p>（5）创建永久函数</p> 
<blockquote> 
 <p>因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）。</p> 
</blockquote> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">function</span> my_len2 
<span class="token keyword">as</span> <span class="token string">"com.huahua.hive.udf.MyUDF"</span> 
<span class="token keyword">using</span> jar <span class="token string">"hdfs://hadoop102:8020/udf/myudf.jar"</span><span class="token punctuation">;</span>

<span class="token comment">--即可在hql中使用自定义的永久函数 </span>
<span class="token keyword">select</span> 
    ename<span class="token punctuation">,</span>
    my_len2<span class="token punctuation">(</span>ename<span class="token punctuation">)</span> ename_len 
<span class="token keyword">from</span> emp<span class="token punctuation">;</span>

<span class="token comment">--删除永久函数 </span>
hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token keyword">drop</span> <span class="token keyword">function</span> my_len2<span class="token punctuation">;</span>
</code></pre> 
<blockquote> 
 <p>注意：</p> 
 <ul><li>永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。</li><li>永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。</li><li>永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。</li></ul> 
</blockquote> 
<h2><a id="_246"></a>二、分区表和分桶表</h2> 
<h3><a id="1__247"></a>1. 分区表</h3> 
<p>Hiv中的分区就是把一张大表的数据按照业务需要分散的存储到多个目录，每个目录就称为该表的一个分区。在查询时通过where子句中的表达式选择查询所需要的分区，这样的查询效率会提高很多。</p> 
<h4><a id="11__249"></a>1.1 分区表基本语法</h4> 
<p>（1）创建分区表</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition
<span class="token punctuation">(</span>
    deptno <span class="token keyword">int</span><span class="token punctuation">,</span>    <span class="token comment">--部门编号</span>
    dname  string<span class="token punctuation">,</span> <span class="token comment">--部门名称</span>
    loc    string  <span class="token comment">--部门位置</span>
<span class="token punctuation">)</span>
    partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span><span class="token keyword">day</span> string<span class="token punctuation">)</span>
    <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<p>（2）分区表读写数据<br> <strong>写数据</strong></p> 
<pre><code class="prism language-sql"><span class="token comment">-- load</span>
<span class="token comment">--数据准备</span>
vim dept_20220401<span class="token punctuation">.</span>log
<span class="token number">10</span>	行政部	<span class="token number">1700</span>
<span class="token number">20</span>	财务部	<span class="token number">1800</span>
<span class="token comment">--装载语句</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/module/hive/datas/dept_20220401.log'</span> 
<span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition 
<span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220401'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--insert</span>
<span class="token comment">--将day='20220401'分区的数据插入到day='20220402'分区，可执行如下装载语句</span>
<span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> dept_partition <span class="token keyword">partition</span> <span class="token punctuation">(</span><span class="token keyword">day</span> <span class="token operator">=</span> <span class="token string">'20220402'</span><span class="token punctuation">)</span>
<span class="token keyword">select</span> deptno<span class="token punctuation">,</span> dname<span class="token punctuation">,</span> loc
<span class="token keyword">from</span> dept_partition
<span class="token keyword">where</span> <span class="token keyword">day</span> <span class="token operator">=</span> <span class="token string">'2020-04-01'</span><span class="token punctuation">;</span>
</code></pre> 
<p>查询分区表数据时，可以将分区字段看作表的伪列，可像使用其他字段一样使用分区字段。</p> 
<pre><code class="prism language-sql"><span class="token keyword">select</span> deptno<span class="token punctuation">,</span> dname<span class="token punctuation">,</span> loc <span class="token punctuation">,</span><span class="token keyword">day</span>
<span class="token keyword">from</span> dept_partition
<span class="token keyword">where</span> <span class="token keyword">day</span> <span class="token operator">=</span> <span class="token string">'2020-04-01'</span><span class="token punctuation">;</span>
</code></pre> 
<p>（3）分区表基本操作</p> 
<pre><code class="prism language-sql"><span class="token comment">--查看所有分区信息</span>
<span class="token keyword">show</span> partitions dept_partition<span class="token punctuation">;</span>

<span class="token comment">--增加分区</span>
<span class="token comment">--创建单个分区</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition 
<span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220403'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">--同时创建多个分区（分区之间不能有逗号）</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition 
<span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220404'</span><span class="token punctuation">)</span> <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220405'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--删除分区</span>
<span class="token comment">--删除单个分区</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition 
<span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220403'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">--同时删除多个分区（分区之间必须有逗号）</span>
<span class="token keyword">alter</span> <span class="token keyword">table</span> dept_partition 
<span class="token keyword">drop</span> <span class="token keyword">partition</span> <span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220404'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220405'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（4）修复分区</p> 
<blockquote> 
 <p>Hive将分区表的所有分区信息都保存在了元数据中，只有元数据与HDFS上的分区路径一致时，分区表才能正常读写数据。若用户手动创建/删除分区路径，Hive都是感知不到的，这样就会导致Hive的元数据和HDFS的分区路径不一致。再比如，若分区表为外部表，用户执行drop partition命令后，分区元数据会被删除，而HDFS的分区路径不会被删除，同样会导致Hive的元数据和HDFS的分区路径不一致。</p> 
</blockquote> 
<p>若出现元数据和HDFS路径不一致的情况，可通过如下几种手段进行修复。</p> 
<ul><li><code>add partition</code><br> 若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致。</li><li><code>drop partition</code><br> 若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致。</li><li><code>msck</code><br> 若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，以下是该命令的用法说明。<br> <code>msck repair table table_name [add/drop/sync partitions];</code><br> 说明：<br> <code>msck repair table table_name add partitions</code>：该命令会增加HDFS路径存在但元数据缺失的分区信息。<br> <code>msck repair table table_name drop partitions</code>：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息。<br> <code>msck repair table table_name sync partitions</code>：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令。<br> <code>msck repair table table_name</code>：等价于<code>msck repair table table_name add partitions</code>命令。</li></ul> 
<h4><a id="12__333"></a>1.2 二级分区表</h4> 
<blockquote> 
 <p>如果一天内的日志数据量也很大，如何再将数据拆分?答案是二级分区表，例如可以在按天分区的基础上，再对每天的数据按小时进行分区。</p> 
</blockquote> 
<pre><code class="prism language-sql"><span class="token comment">--二级分区表建表语句</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition2<span class="token punctuation">(</span>
    deptno <span class="token keyword">int</span><span class="token punctuation">,</span>    <span class="token comment">-- 部门编号</span>
    dname string<span class="token punctuation">,</span> <span class="token comment">-- 部门名称</span>
    loc string     <span class="token comment">-- 部门位置</span>
<span class="token punctuation">)</span>
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span><span class="token keyword">day</span> string<span class="token punctuation">,</span> <span class="token keyword">hour</span> string<span class="token punctuation">)</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>

<span class="token comment">--数据装载语句</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/module/hive/datas/dept_20220401.log'</span> 
<span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition2 
<span class="token keyword">partition</span><span class="token punctuation">(</span><span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220401'</span><span class="token punctuation">,</span> <span class="token keyword">hour</span><span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">--查询分区数据</span>
<span class="token keyword">select</span> 
    <span class="token operator">*</span> 
<span class="token keyword">from</span> dept_partition2 
<span class="token keyword">where</span> <span class="token keyword">day</span><span class="token operator">=</span><span class="token string">'20220401'</span> <span class="token operator">and</span> <span class="token keyword">hour</span><span class="token operator">=</span><span class="token string">'12'</span><span class="token punctuation">;</span>
</code></pre> 
<p>1.3 动态分区</p> 
<blockquote> 
 <p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。使用动态分区，可只用一个insert语句将数据写入多个分区。</p> 
</blockquote> 
<p>（1）动态分区相关参数</p> 
<pre><code class="prism language-sql"><span class="token comment">--动态分区功能总开关（默认true，开启）</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">true</span>
<span class="token comment">--严格模式和非严格模式</span>
<span class="token comment">--动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区。</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span><span class="token keyword">mode</span><span class="token operator">=</span>nonstrict
<span class="token comment">--一条insert语句可同时创建的最大的分区个数，默认为1000。</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token operator">=</span><span class="token number">1000</span>
<span class="token comment">--单个Mapper或者Reducer可同时创建的最大的分区个数，默认为100。</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token punctuation">.</span>pernode<span class="token operator">=</span><span class="token number">100</span>
<span class="token comment">--一条insert语句可以创建的最大的文件个数，默认100000。</span>
hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>created<span class="token punctuation">.</span>files<span class="token operator">=</span><span class="token number">100000</span>
<span class="token comment">--当查询结果为空时且进行动态分区时，是否抛出异常，默认false。</span>
hive<span class="token punctuation">.</span>error<span class="token punctuation">.</span><span class="token keyword">on</span><span class="token punctuation">.</span>empty<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">false</span>
</code></pre> 
<p>（2）案例实操</p> 
<blockquote> 
 <p>需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition_dynamic的相应分区中。</p> 
</blockquote> 
<pre><code class="prism language-sql"><span class="token comment">--创建目标分区表</span>
<span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition_dynamic<span class="token punctuation">(</span>
    id <span class="token keyword">int</span><span class="token punctuation">,</span> 
    name string
<span class="token punctuation">)</span> 
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span>loc <span class="token keyword">int</span><span class="token punctuation">)</span> 
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>

<span class="token comment">--设置动态分区</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span><span class="token keyword">mode</span> <span class="token operator">=</span> nonstrict<span class="token punctuation">;</span>
<span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition_dynamic 
<span class="token keyword">partition</span><span class="token punctuation">(</span>loc<span class="token punctuation">)</span> 
<span class="token keyword">select</span> 
    deptno<span class="token punctuation">,</span> 
    dname<span class="token punctuation">,</span> 
    loc 
<span class="token keyword">from</span> dept<span class="token punctuation">;</span>
<span class="token comment">--查看目标分区表的分区情况</span>
hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token keyword">show</span> partitions dept_partition_dynamic<span class="token punctuation">;</span>
</code></pre> 
<h3><a id="2__407"></a>2. 分桶表</h3> 
<p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分，分区针对的是数据的存储路径，分桶针对的是数据文件。</p> 
<p>分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）。</p> 
<h4><a id="21__411"></a>2.1 分桶表基本语法</h4> 
<p>（1）建表语句</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">(</span>
    id <span class="token keyword">int</span><span class="token punctuation">,</span> 
    name string
<span class="token punctuation">)</span>
<span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span> 
<span class="token keyword">into</span> <span class="token number">4</span> buckets
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<p>（2）数据装载<br> 数据准备<br> 在/opt/module/hive/datas/路径上创建student.txt文件，并输入如下内容。</p> 
<pre><code class="prism language-sql"><span class="token number">1001</span>	student1
<span class="token number">1002</span>	student2
<span class="token number">1003</span>	student3
<span class="token number">1004</span>	student4
<span class="token number">1005</span>	student5
<span class="token number">1006</span>	student6
<span class="token number">1007</span>	student7
<span class="token number">1008</span>	student8
<span class="token number">1009</span>	student9
<span class="token number">1010</span>	student10
<span class="token number">1011</span>	student11
<span class="token number">1012</span>	student12
<span class="token number">1013</span>	student13
<span class="token number">1014</span>	student14
<span class="token number">1015</span>	student15
<span class="token number">1016</span>	student16
</code></pre> 
<blockquote> 
 <p>说明：Hive新版本load数据可以直接跑MapReduce，老版的Hive需要将数据传到一张表里，再通过查询的方式导入到分桶表里面。</p> 
</blockquote> 
<pre><code class="prism language-sql"><span class="token comment">--导入数据到分桶表中</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/module/hive/datas/student.txt'</span> 
<span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck<span class="token punctuation">;</span>
</code></pre> 
<p>查看创建的分桶表中是否分成4个桶，观察每个分桶中的数据<br> <img src="https://images2.imgbox.com/ac/a7/HfHoV2MS_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="22__459"></a>2.2 分桶排序表</h4> 
<p>（1）建表语句</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> stu_buck_sort<span class="token punctuation">(</span>
    id <span class="token keyword">int</span><span class="token punctuation">,</span> 
    name string
<span class="token punctuation">)</span>
<span class="token keyword">clustered</span> <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span> sorted <span class="token keyword">by</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>
<span class="token keyword">into</span> <span class="token number">4</span> buckets
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
<p>（2）数据装载</p> 
<pre><code class="prism language-sql"><span class="token comment">--导入数据到分桶表中</span>
<span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'/opt/module/hive/datas/student.txt'</span> 
<span class="token keyword">into</span> <span class="token keyword">table</span> stu_buck_sort<span class="token punctuation">;</span>
</code></pre> 
<p>查看创建的分桶表中是否分成4个桶，观察每个分桶中的数据<br> <img src="https://images2.imgbox.com/51/7c/j45vBxFs_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_484"></a>三、文件格式和压缩</h2> 
<h3><a id="1_Hadoop_485"></a>1. Hadoop压缩概述</h3> 
<table><thead><tr><th>压缩格式</th><th>算法</th><th>文件扩展名</th><th>是否可切分</th></tr></thead><tbody><tr><td>DEFLATE</td><td>DEFLATE</td><td>.deflate</td><td>否</td></tr><tr><td>Gzip</td><td>DEFLATE</td><td>.gz</td><td>否</td></tr><tr><td>bzip2</td><td>bzip2</td><td>.bz2</td><td>是</td></tr><tr><td>LZO</td><td>LZO</td><td>.lzo</td><td>是</td></tr><tr><td>Snappy</td><td>Snappy</td><td>.snappy</td><td>否</td></tr></tbody></table> 
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：<br> Hadoop查看支持压缩的方式<code>hadoop checknative</code>。<br> Hadoop在<code>driver</code>端设置压缩。</p> 
<table><thead><tr><th>压缩格式</th><th>对应的编码/解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table> 
<p>压缩性能的比较：</p> 
<table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB/s</td><td>58MB/s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB/s</td><td>9.5MB/s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB/s</td><td>74.6MB/s</td></tr></tbody></table> 
<h3><a id="2_Hive_513"></a>2. Hive文件格式</h3> 
<p>为Hive表中的数据选择一个合适的文件格式，对提高查询性能的提高是十分有益的。Hive表数据的存储格式，可以选择text file、orc、parquet、sequence file等。</p> 
<h4><a id="21_Text_File_515"></a>2.1 Text File</h4> 
<blockquote> 
 <p>文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录。</p> 
</blockquote> 
<p>可通过以下建表语句指定文件格式为文本文件:</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> textfile_table
<span class="token punctuation">(</span>column_specs<span class="token punctuation">)</span>
stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span>
</code></pre> 
<h4><a id="22_ORC_527"></a>2.2 ORC</h4> 
<p>(1）文件格式</p> 
<blockquote> 
 <p>ORC（Optimized Row Columnar）file format是Hive 0.11版里引入的一种列式存储的文件格式。ORC文件能够提高Hive读写数据和处理数据的性能。</p> 
</blockquote> 
<p>与列式存储相对的是行式存储，下图是两者的对比：<br> <img src="https://images2.imgbox.com/85/24/NRsh01z7_o.png" alt="在这里插入图片描述"></p> 
<p>如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p> 
<blockquote> 
 <ul><li>行存储的特点<br> 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</li><li>列存储的特点<br> 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。<br> text file和sequence file都是基于行存储的，orc和parquet是基于列式存储的。</li></ul> 
</blockquote> 
<p>orc文件的具体结构如下图所示：<br> <img src="https://images2.imgbox.com/96/e6/clzh7AfI_o.png" alt="在这里插入图片描述"></p> 
<p>每个Orc文件由Header、Body和Tail三部分组成。</p> 
<ol><li>其中Header内容为ORC，用于表示文件类型。</li><li>Body由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，每个stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer。</li></ol> 
<blockquote> 
 <ul><li>Index Data：一个轻量级的index，默认是为各列每隔1W行做一个索引。每个索引会记录第n万行的位置，和最近一万行的最大值和最小值等信息。</li><li>Row Data：存的是具体的数据，按列进行存储，并对每个列进行编码，分成多个Stream来存储。</li><li>Stripe Footer：存放的是各个Stream的位置以及各column的编码信息。</li></ul> 
</blockquote> 
<ol start="3"><li>Tail由File Footer和PostScript组成。</li></ol> 
<blockquote> 
 <ul><li>File Footer中保存了各Stripe的其实位置、索引长度、数据长度等信息，各Column的统计信息等</li><li>PostScript记录了整个文件的压缩类型以及File Footer的长度信息等。</li></ul> 
</blockquote> 
<p>在读取ORC文件时，会先从最后一个字节读取PostScript长度，进而读取到PostScript，从里面解析到File Footer长度，进而读取FileFooter，从中解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p> 
<p>(2）建表语句</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> orc_table
<span class="token punctuation">(</span>column_specs<span class="token punctuation">)</span>
stored <span class="token keyword">as</span> orc
tblproperties <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>ORC文件格式支持的参数如下：</p> 
<table><thead><tr><th>参数</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>orc.compress</td><td>ZLIB</td><td>压缩格式，可选项：NONE、ZLIB,、SNAPPY</td></tr><tr><td>orc.compress.size</td><td>262,144</td><td>每个压缩块的大小（ORC文件是分块压缩的）</td></tr><tr><td>orc.stripe.size</td><td>67,108,864</td><td>每个stripe的大小</td></tr><tr><td>orc.row.index.stride</td><td>10,000</td><td>索引步长（每隔多少行数据建一条索引）</td></tr></tbody></table> 
<h4><a id="23_Parquet_578"></a>2.3 Parquet</h4> 
<p>（1）文件格式</p> 
<blockquote> 
 <p>Parquet文件是Hadoop生态中的一个通用的文件格式，它也是一个列式存储的文件格式。</p> 
</blockquote> 
<p>Parquet文件的格式如下图所示：<br> <img src="https://images2.imgbox.com/3f/37/1uiexDzr_o.png" alt="在这里插入图片描述"></p> 
<p>上图展示了一个Parquet文件的基本结构，文件的首尾都是该文件的Magic Code，用于校验它是否是一个Parquet文件。</p> 
<ul><li>首尾中间由若干个Row Group和一个Footer（File Meta Data）组成。</li><li>每个Row Group包含多个Column Chunk，每个Column Chunk包含多个Page。</li></ul> 
<blockquote> 
 <p>以下是Row Group、Column Chunk和Page三个概念的说明：</p> 
 <ul><li>行组（Row Group）：一个行组对应逻辑表中的若干行。</li><li>列块（Column Chunk）：一个行组中的一列保存在一个列块中。</li><li>页（Page）：一个列块的数据会划分为若干个页。</li></ul> 
</blockquote> 
<ul><li>Footer（File Meta Data）中存储了每个行组（Row Group）中的每个列快（Column Chunk）的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的Data Page位置等信息。</li></ul> 
<p>（2）建表语句</p> 
<pre><code class="prism language-sql"><span class="token keyword">Create</span> <span class="token keyword">table</span> parquet_table
<span class="token punctuation">(</span>column_specs<span class="token punctuation">)</span>
stored <span class="token keyword">as</span> parquet
tblproperties <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>支持的参数如下：</p> 
<table><thead><tr><th>参数</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>parquet.compression</td><td>uncompressed</td><td>压缩格式，可选项：uncompressed，snappy，gzip，lzo，brotli，lz4</td></tr><tr><td>parquet.block.size</td><td>134217728</td><td>行组大小，通常与HDFS块大小保持一致</td></tr><tr><td>parquet.page.size</td><td>1048576</td><td>页大小</td></tr></tbody></table> 
<h3><a id="3__612"></a>3. 压缩</h3> 
<p>在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的。</p> 
<h4><a id="31_Hive_614"></a>3.1 Hive表数据进行压缩</h4> 
<blockquote> 
 <p>在Hive中，不同文件类型的表，声明数据压缩的方式是不同的。</p> 
</blockquote> 
<p>（1）TextFile</p> 
<blockquote> 
 <p>若一张表的文件类型为TextFile，若需要对该表中的数据进行压缩，多数情况下，无需在建表语句做出声明。直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压。</p> 
</blockquote> 
<p>需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。</p> 
<pre><code class="prism language-sql"><span class="token comment">--SQL语句的最终输出结果是否压缩</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>output<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">--输出结果的压缩格式（以下示例为snappy）</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>output<span class="token punctuation">.</span>fileoutputformat<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec <span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>SnappyCodec<span class="token punctuation">;</span>
</code></pre> 
<p>（2）ORC<br> 若一张表的文件类型为ORC，若需要对该表数据进行压缩，需在建表语句中声明压缩格式如下：</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> orc_table
<span class="token punctuation">(</span>column_specs<span class="token punctuation">)</span>
stored <span class="token keyword">as</span> orc
tblproperties <span class="token punctuation">(</span><span class="token string">"orc.compress"</span><span class="token operator">=</span><span class="token string">"snappy"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>（3）Parquet<br> 若一张表的文件类型为Parquet，若需要对该表数据进行压缩，需在建表语句中声明压缩格式如下：</p> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> orc_table
<span class="token punctuation">(</span>column_specs<span class="token punctuation">)</span>
stored <span class="token keyword">as</span> parquet
tblproperties <span class="token punctuation">(</span><span class="token string">"parquet.compression"</span><span class="token operator">=</span><span class="token string">"snappy"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<h4><a id="32__651"></a>3.2 计算过程中使用压缩</h4> 
<p>（1）单个MR的中间结果进行压缩<br> 单个MR的中间结果是指Mapper输出的数据，对其进行压缩可降低shuffle阶段的网络IO，可通过以下参数进行配置：</p> 
<pre><code class="prism language-sql"><span class="token comment">--开启MapReduce中间数据压缩功能</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">--设置MapReduce中间数据数据的压缩方式（以下示例为snappy）</span>
<span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>map<span class="token punctuation">.</span>output<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>codec<span class="token operator">=</span>org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>SnappyCodec<span class="token punctuation">;</span>
</code></pre> 
<p>（2）单条SQL语句的中间结果进行压缩<br> 单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据，可通过以下参数进行配置：</p> 
<pre><code class="prism language-sql"><span class="token comment">--是否对两个MR之间的临时数据进行压缩</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>compress<span class="token punctuation">.</span>intermediate<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span>
<span class="token comment">--压缩格式（以下示例为snappy）</span>
<span class="token keyword">set</span> hive<span class="token punctuation">.</span>intermediate<span class="token punctuation">.</span>compression<span class="token punctuation">.</span>codec<span class="token operator">=</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>hadoop<span class="token punctuation">.</span>io<span class="token punctuation">.</span>compress<span class="token punctuation">.</span>SnappyCodec<span class="token punctuation">;</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9c456f12524f1f4189e625bf20b0d038/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">简单两步实现离线部署ChatGPT，ChatGPT平替版，无需GPU离线搭建ChatGPT</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/34670a2a27b88294458ae57ce85286ef/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">10.4：输入两个正整数m和n，求其最大公约数和最小公倍数</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>