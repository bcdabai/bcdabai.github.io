<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【pytorch】计算向量相似度 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【pytorch】计算向量相似度" />
<meta property="og:description" content="原文链接：https://zhuanlan.zhihu.com/p/442092801
本文提供几个pytorch中常用的向量相似度评估方法，并给出其源码实现，供大家参考。分别为以下六个。
CosineSimilarity
DotProductSimilarity
ProjectedDotProductSimilarity
BiLinearSimilarity
TriLinearSimilarity
MultiHeadedSimilarity
（其中第一个pytoch自带有库函数）
1.CosineSimilarity 余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，称为&#34;余弦相似性&#34;
class CosineSimilarity(nn.Module): def forward(self, tensor_1, tensor_2): normalized_tensor_1 = tensor_1 / tensor_1.norm(dim=-1, keepdim=True) normalized_tensor_2 = tensor_2 / tensor_2.norm(dim=-1, keepdim=True) return (normalized_tensor_1 * normalized_tensor_2).sum(dim=-1) 2.DotProductSimilarity 这个相似度函数简单地计算每对向量之间的点积，并使用可选的缩放来减少输出的方差。
class DotProductSimilarity(nn.Module): def __init__(self, scale_output=False): super(DotProductSimilarity, self).__init__() self.scale_output = scale_output def forward(self, tensor_1, tensor_2): result = (tensor_1 * tensor_2).sum(dim=-1) if self.scale_output: # TODO why allennlp do multiplication at here ? result /= math.sqrt(tensor_1.size(-1)) return result 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e6db0292586e18f80fa1494ee31706c3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-14T20:57:58+08:00" />
<meta property="article:modified_time" content="2022-11-14T20:57:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【pytorch】计算向量相似度</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>原文链接：<a href="https://zhuanlan.zhihu.com/p/442092801" rel="nofollow">https://zhuanlan.zhihu.com/p/442092801</a></p> 
<p>本文提供几个pytorch中常用的向量相似度评估方法，并给出其源码实现，供大家参考。分别为以下六个。</p> 
<ol><li> <p>CosineSimilarity</p> </li><li> <p>DotProductSimilarity</p> </li><li> <p>ProjectedDotProductSimilarity</p> </li><li> <p>BiLinearSimilarity</p> </li><li> <p>TriLinearSimilarity</p> </li><li> <p>MultiHeadedSimilarity</p> </li></ol> 
<p>（其中第一个pytoch自带有库函数）</p> 
<h5><a id="1CosineSimilarity_17"></a>1.CosineSimilarity</h5> 
<p>余弦相似度用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，称为"余弦相似性"</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CosineSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        normalized_tensor_1 <span class="token operator">=</span> tensor_1 <span class="token operator">/</span> tensor_1<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        normalized_tensor_2 <span class="token operator">=</span> tensor_2 <span class="token operator">/</span> tensor_2<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>normalized_tensor_1 <span class="token operator">*</span> normalized_tensor_2<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="2DotProductSimilarity_27"></a>2.DotProductSimilarity</h5> 
<p>这个相似度函数简单地计算每对向量之间的点积，并使用可选的缩放来减少输出的方差。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DotProductSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scale_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DotProductSimilarity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scale_output <span class="token operator">=</span> scale_output
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> <span class="token punctuation">(</span>tensor_1 <span class="token operator">*</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>scale_output<span class="token punctuation">:</span>
            <span class="token comment"># TODO why allennlp do multiplication at here ?</span>
            result <span class="token operator">/=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>tensor_1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> result
</code></pre> 
<h5><a id="3ProjectedDotProductSimilarity_44"></a>3.ProjectedDotProductSimilarity</h5> 
<p>这个相似度函数做一个投影，然后计算点积，计算公式为：<img src="https://images2.imgbox.com/8b/53/GKhfNps6_o.jpg" alt="请添加图片描述"><br> 计算后的激活函数。默认为不激活。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ProjectedDotProductSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1_dim<span class="token punctuation">,</span> tensor_2_dim<span class="token punctuation">,</span> projected_dim<span class="token punctuation">,</span>
                 reuse_weight<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ProjectedDotProductSimilarity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reuse_weight <span class="token operator">=</span> reuse_weight
        self<span class="token punctuation">.</span>projecting_weight_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>tensor_1_dim<span class="token punctuation">,</span> projected_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>reuse_weight<span class="token punctuation">:</span>
            <span class="token keyword">if</span> tensor_1_dim <span class="token operator">!=</span> tensor_2_dim<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'if reuse_weight=True, tensor_1_dim must equal tensor_2_dim'</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>projecting_weight_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>tensor_2_dim<span class="token punctuation">,</span> projected_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> bias <span class="token keyword">else</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> activation
 
    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projecting_weight_1<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>reuse_weight<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>projecting_weight_2<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        projected_tensor_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>projecting_weight_1<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>reuse_weight<span class="token punctuation">:</span>
            projected_tensor_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>projecting_weight_1<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            projected_tensor_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>projecting_weight_2<span class="token punctuation">)</span>
        result <span class="token operator">=</span> <span class="token punctuation">(</span>projected_tensor_1 <span class="token operator">*</span> projected_tensor_2<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> result <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>result<span class="token punctuation">)</span>
        <span class="token keyword">return</span> result
</code></pre> 
<h5><a id="4BiLinearSimilarity_84"></a>4.BiLinearSimilarity</h5> 
<p>此相似度函数执行两个输入向量的双线性变换。这个函数有一个权重矩阵“W”和一个偏差“b”，以及两个向量之间的相似度，计算公式为：<br> <img src="https://images2.imgbox.com/0e/72/ngFBDRzX_o.jpg" alt="请添加图片描述"><br> 计算后的激活函数。 默认为不激活。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BiLinearSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1_dim<span class="token punctuation">,</span> tensor_2_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BiLinearSimilarity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight_matrix <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>tensor_1_dim<span class="token punctuation">,</span> tensor_2_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> activation
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight_matrix<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        intermediate <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight_matrix<span class="token punctuation">)</span>
        result <span class="token operator">=</span> <span class="token punctuation">(</span>intermediate <span class="token operator">*</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>result<span class="token punctuation">)</span>
        <span class="token keyword">return</span> result
</code></pre> 
<h5><a id="5TriLinearSimilarity_110"></a>5.TriLinearSimilarity</h5> 
<p>此相似度函数执行两个输入向量的三线性变换，计算公式为：<br> <img src="https://images2.imgbox.com/e2/03/MuB2rAfK_o.jpg" alt="请添加图片描述"><br> 计算后的激活函数。 默认为不激活。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TriLinearSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TriLinearSimilarity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight_vector <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">3</span> <span class="token operator">*</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> activation
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        std <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">6</span> <span class="token operator">/</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight_vector<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight_vector<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span>std<span class="token punctuation">,</span> std<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>fill_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        combined_tensors <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">,</span> tensor_1 <span class="token operator">*</span> tensor_2<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        result <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>combined_tensors<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight_vector<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>activation <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            result <span class="token operator">=</span> self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>result<span class="token punctuation">)</span>
        <span class="token keyword">return</span> result
       
</code></pre> 
<h5><a id="6MultiHeadedSimilarity_137"></a>6.MultiHeadedSimilarity</h5> 
<p>这个相似度函数使用多个“头”来计算相似度。也就是说，我们将输入张量投影到多个新张量中，并分别计算每个投影张量的相似度。这里的结果比典型的相似度函数多一个维度。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadedSimilarity</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
 
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 num_heads<span class="token punctuation">,</span>
                 tensor_1_dim<span class="token punctuation">,</span>
                 tensor_1_projected_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 tensor_2_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 tensor_2_projected_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 internal_similarity<span class="token operator">=</span>DotProductSimilarity<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadedSimilarity<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>internal_similarity <span class="token operator">=</span> internal_similarity
        tensor_1_projected_dim <span class="token operator">=</span> tensor_1_projected_dim <span class="token keyword">or</span> tensor_1_dim
        tensor_2_dim <span class="token operator">=</span> tensor_2_dim <span class="token keyword">or</span> tensor_1_dim
        tensor_2_projected_dim <span class="token operator">=</span> tensor_2_projected_dim <span class="token keyword">or</span> tensor_2_dim
        <span class="token keyword">if</span> tensor_1_projected_dim <span class="token operator">%</span> num_heads <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Projected dimension not divisible by number of heads: %d, %d"</span>
                             <span class="token operator">%</span> <span class="token punctuation">(</span>tensor_1_projected_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> tensor_2_projected_dim <span class="token operator">%</span> num_heads <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Projected dimension not divisible by number of heads: %d, %d"</span>
                             <span class="token operator">%</span> <span class="token punctuation">(</span>tensor_2_projected_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tensor_1_projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>tensor_1_dim<span class="token punctuation">,</span> tensor_1_projected_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tensor_2_projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>tensor_2_dim<span class="token punctuation">,</span> tensor_2_projected_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reset_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">reset_parameters</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tensor_1_projection<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tensor_2_projection<span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor_1<span class="token punctuation">,</span> tensor_2<span class="token punctuation">)</span><span class="token punctuation">:</span>
        projected_tensor_1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tensor_1_projection<span class="token punctuation">)</span>
        projected_tensor_2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor_2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>tensor_2_projection<span class="token punctuation">)</span>
 
        <span class="token comment"># Here we split the last dimension of the tensors from (..., projected_dim) to</span>
        <span class="token comment"># (..., num_heads, projected_dim / num_heads), using tensor.view().</span>
        last_dim_size <span class="token operator">=</span> projected_tensor_1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        new_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>projected_tensor_1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> last_dim_size<span class="token punctuation">]</span>
        split_tensor_1 <span class="token operator">=</span> projected_tensor_1<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_shape<span class="token punctuation">)</span>
        last_dim_size <span class="token operator">=</span> projected_tensor_2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        new_shape <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>projected_tensor_2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> last_dim_size<span class="token punctuation">]</span>
        split_tensor_2 <span class="token operator">=</span> projected_tensor_2<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">*</span>new_shape<span class="token punctuation">)</span>
 
        <span class="token comment"># And then we pass this off to our internal similarity function. Because the similarity</span>
        <span class="token comment"># functions don't care what dimension their input has, and only look at the last dimension,</span>
        <span class="token comment"># we don't need to do anything special here. It will just compute similarity on the</span>
        <span class="token comment"># projection dimension for each head, returning a tensor of shape (..., num_heads).</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>internal_similarity<span class="token punctuation">(</span>split_tensor_1<span class="token punctuation">,</span> split_tensor_2<span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4a4a2c7d7ca72635b89f4e2cb0f86291/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">垃圾回收机制——GC详讲</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c7196ef0ce7d4bcd25433d5226c1c784/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C语言实现扫雷游戏(超详细讲解&#43;全部源码)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>