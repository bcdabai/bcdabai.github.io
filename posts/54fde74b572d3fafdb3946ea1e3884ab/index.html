<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>张量的连续性、contiguous函数 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="张量的连续性、contiguous函数" />
<meta property="og:description" content="在pytorch中，tensor的实际数据以一维数组（storage）的形式存储于某个连续的内存中，以“行优先”进行存储。
tensor的连续性 tensor连续（contiguous）是指tensor的storage元素排列顺序与其按行优先时的元素排列顺序相同。如下图所示：
上图中，tensor b是tensor a经过转置而来的，即使用了 tensor.t() 方法。
出现不连续现象，本质上是由于pytorch中不同tensor可能共用同一个storage导致的。
pytorch的很多操作都会导致tensor不连续，如tensor.transpose()（tensor.t()）、tensor.narrow()、tensor.expand()。
以转置为例，因为转置操作前后共用同一个storage，但显然转置后的tensor按照行优先排列成1维后与原storage不同了，因此转置后结果属于不连续（见下例）。
2. tensor.is_contiguous() is_contiguous直观的解释是Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致。
如果想要变得连续使用contiguous方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的；如果Tensor是连续的，则contiguous无操作。
tensor.is_contiguous()用于判断tensor是否连续，以转置为例说明：
import torch a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(a) print(a.storage()) print(a.is_contiguous()) # a是连续的 &#34;&#34;&#34; tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 1 2 3 4 5 6 7 8 9 [torch.LongStorage of size 9] True &#34;&#34;&#34; b = a.t() # b是a的转置 print(b) print(b.storage()) print(b.is_contiguous()) # b是不连续的 &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/54fde74b572d3fafdb3946ea1e3884ab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-11T14:11:29+08:00" />
<meta property="article:modified_time" content="2023-09-11T14:11:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">张量的连续性、contiguous函数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>        在pytorch中，tensor的实际数据以<strong>一维数组（storage）的形式存储于某个连续的内存中，</strong>以<strong>“行优先”</strong>进行存储。</p> 
<h3>tensor的连续性</h3> 
<p>         tensor连续（<a href="https://so.csdn.net/so/search?q=contiguous&amp;spm=1001.2101.3001.7020" title="contiguous">contiguous</a>）是指<strong>tensor的storage元素排列顺序与其按行优先时的元素排列顺序相同</strong>。如下图所示：</p> 
<p><img alt="" height="381" src="https://images2.imgbox.com/e4/f1/WN8iS2lW_o.png" width="829"></p> 
<p style="text-align:justify;">        上图中，tensor b是tensor a经过转置而来的，即使用了 tensor.t() 方法。</p> 
<p style="text-align:justify;">        出现不连续现象，本质上是由于pytorch中不同tensor可能共用同一个storage导致的。<br>     pytorch的很多操作都会导致tensor不连续，如tensor.transpose()（tensor.t()）、tensor.narrow()、tensor.expand()。<br>         以转置为例，因为转置操作前后共用同一个storage，但显然转置后的tensor按照行优先排列成1维后与原storage不同了，因此转置后结果属于不连续（见下例）。</p> 
<h3>2. tensor.is_contiguous()</h3> 
<p style="text-align:justify;"><strong><code>   is_contiguous</code></strong>直观的解释是<strong>Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致</strong>。</p> 
<p style="text-align:justify;">        如果想要变得连续使用<code><strong>contiguous</strong></code>方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的；如果Tensor是连续的，则<code><strong>contiguous</strong></code>无操作。</p> 
<p>        tensor.is_contiguous()用于判断tensor是否连续，以转置为例说明：</p> 
<pre><code>import torch
a = torch.tensor([[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]])
print(a)
print(a.storage())
print(a.is_contiguous())  # a是连续的
"""
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
 1
 2
 3
 4
 5
 6
 7
 8
 9
[torch.LongStorage of size 9]
True
"""

b = a.t()  # b是a的转置
print(b)
print(b.storage())
print(b.is_contiguous())  # b是不连续的
"""
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
 1
 2
 3
 4
 5
 6
 7
 8
 9
[torch.LongStorage of size 9]
False
"""</code></pre> 
<h3><strong>3. tensor不连续的后果</strong></h3> 
<p><strong>        tensor不连续会导致某些操作无法进行，比如view()就无法进行</strong>。在上面的例子中：由于 b 是不连续的，所以对其进行view()操作会报错；b.view(3,3)没报错，因为b本身的shape就是(3,3)。</p> 
<pre><code>print(b.view(3, 3))
"""
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
"""
print(b.view(1, 9))# 报错
print(b.view(-1))# 报错</code></pre> 
<h3> <strong>4. tensor.contiguous()</strong></h3> 
<p><br>         tensor.contiguous()返回一个与原始tensor有相同元素的 “连续”tensor，如果原始tensor本身就是连续的，则返回原始tensor。<br>         注意：tensor.contiguous()函数不会对原始数据做任何修改，他不仅返回一个新tensor，还为这个新tensor创建了一个新的storage，在这个storage上，该新的tensor是连续的。<br> 继续使用上面的例子：</p> 
<pre><code>c = b.contiguous()
print(b)
print(c)
print(b.storage())
print(c.storage())</code></pre> 
<p> 输出结果：</p> 
<pre><code># b
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
# c
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
# b.storage
 1
 2
 3
 4
 5
 6
 7
 8
 9
[torch.LongStorage of size 9]

#c.storage 
 1
 4
 7
 2
 5
 8
 3
 6
 9
[torch.LongStorage of size 9]

</code></pre> 
<p>接着运行如下代码： </p> 
<pre><code>print(b.is_contiguous()) # False
print(c.is_contiguous()) # True
print(c.view(1, 9)) # tensor([[1, 4, 7, 2, 5, 8, 3, 6, 9]])</code></pre> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p><br> 参考自：https://blog.csdn.net/baidu_41774120/article/details/128666944</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d7e7d748bf71350e8b24473f50ab7948/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Modbus RTU/Modbus TCP协议详细、区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e0da51486615788b153cc9c81f7ac5e6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux系统编程：文件编程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>