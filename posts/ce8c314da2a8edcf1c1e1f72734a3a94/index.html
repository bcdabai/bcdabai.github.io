<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>NLP学习路线（其实就是个汇集了所有经典模型的自然语言处理学习笔记） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="NLP学习路线（其实就是个汇集了所有经典模型的自然语言处理学习笔记）" />
<meta property="og:description" content="按照该知乎文章：从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史里罗列的发展史，对自然语言处理各个模型进行学习。
同步观看斯坦福大学的经典NLP课程打基础。
但说实话，概念是讲得很好，但不足以完全理解，所以也同时自己断断续续找论文或者什么来进行理解吧。
这里也会将所有的链接放上来，加上一些学习体会。
开始学习 词向量（Word2Vec） 1、按理来说从知乎文章的标题看，应该是要从word embedding开始学起，但是我在看文章的时候，又发现第一个概念是语言模型，所以也自己去搜索了一些相关资料来看，说实话，看的不是很懂，只有一点模模糊糊的概念吧——语言模型分不同种的语言模型，现在最经典的就是神经网络语言模型。
语言模型找到的文章我没觉得有什么合适的，这里暂时不放。
下一个主要是需要理解word2vec吧，它是一种词嵌入方法。我找到的一篇文章word2vec介绍，感觉写得很棒。
词嵌入（word embedding）：将高维度的词表示转换为低维度的词表示的方法。
按照我的理解，其实就是将onehot编码的长度浪费改善，变成能够控制在30维度的表示方法。即“句子中每个单词以Onehot形式作为输入，乘以学好的Word Embedding矩阵Q，直接取出单词对应的Word Embedding”
2、此处再补上我在斯坦福大学视频里看到的另一种词向量转换方法——GloVe，按照视频里的讲解来说，如果只是单拎出word2vec和GloVe，GloVe的效率会更好些。但是视频我并没有看得很懂，因此也去找了相关文章，GloVe模型理解此文章中也引用了视频中的PPT。
遗留下的问题：word2vec最大的缺陷是无法解决多义词问题，即如果将一个词对应的向量定下来后，即使出现其他语义情况也无法再改变。
RNN 老规矩，依旧是找了个介绍通俗易懂的RNN来看。我觉得概念上是很好的，但是它的图画的不是很清楚，可能一时难以看懂。
这是斯坦福视频上截图下来的图片，我觉得更准确些，可以结合着看下。
RNN：时序神经网络，拥有时间上的顺序性。能结合上下文考虑，弥补了全连接神经网络的不足。
补充一个那篇博客中没谈到的信息：第一个h(0)一般设为零。
RNN通过结合前向传播信息的方式，初步为多义词问题提供了解决方法——结合上文信息。本质上就是不割裂地将每个词提取出来理解，而是能够通过匹配前文关键词来进行查找，使得多义词能在不同语境下得到不同解释。
遗留下的问题：会存在梯度消失，即模型难以学习到远距离的依赖关系。
LSTM 我感觉这篇LSTM详解已经把LSTM原理解释得很清楚了，而且很生动形象。
LSTM的输入输出其实和RNN都是一样的，但是通过细胞层的设置解决了梯度消失的问题，即将各个部分的重要性依次使用公式计算，这样就不会出现远距离的依赖关系低的问题。
遗留下的问题：表面上似乎没有什么问题了，但其实这个结合的方法是死板的，只结合前文而没有结合后文，依旧会出现考虑不全面的问题。
其实如果按照斯坦福课程的顺序，后面就直接到transformer了，即解决LSTM遗留下这个问题的关键模型。但是知乎上那篇文章的顺序，则是按照历史轨迹，讲了所有的发展史，但都是致力于解决多义词（无法动态联系上下文判断词语意义）的问题。
可以按照知乎这篇文章的顺序看下ELMo等相关的模型，会更全面点，它的介绍也差不多将各个模型的精髓都讲到了。但直接跳到“transformer”看，其实也完全没问题，因为历史上这些模型，大体上的原理没有太多差别，不过细节不一样而已。
Seq2Seq（Attention） seq2seq是在RNN的基础上加以改进的，最大的区别就是对输入输出的数量能够随意进行改动。而在这里将它强调一下，是为了突出Seq2Seq和attention结合的用法，这是后面的transformer最重要的一部分。
Seq2Seq 模型详解这篇文章是一个整体的介绍，我觉得理解是挺好理解的，但其实对于attention的机制介绍的不是很清楚。
注意力机制的话，其实类似于人，当我们看文章或者图片等信息时，肯定会对其中某个部分投入更多的注意力，比如一段话中“这是一个桌子”，注意力肯定也是放在“桌子”这个词上的。至于使用方法是分别建立K、V、Q，然后找到其中注意力最大的词。
想要详细了解可以看李宏毅老师的这篇强烈推荐！台大李宏毅自注意力机制和Transformer详解！，我看了很多介绍的文章视频，只有这个是让我看懂了的。下面的transformer李宏毅老师也在这里讲解了，可以一起学习下。
transformer transformer中有两篇很著名的文章“Attention Is All You Need” 和 “The Illustrated Transformer”，不过直接看的话，可能因为基础知识不够和语言不通容易有点困难，因此我在在csdn上找到了解析“Attention Is All You Need”这篇论文的文章Transformer 模型详解，再加上看李宏毅老师的视频才算对transformer勉强有了个大概理解。transformer的原理和attention息息相关，具体的能看懂文章和听懂视频应该就差不多了。
Bert bert是现在nlp里面独占鳌头的模型了，nlp大部分领域用bert模型做训练应该都能能得到很好的效果，也有许多人在它的基础上不断改进。bert是基于transformer的encoder模块做出的，其中关于mask的部分是比较特殊的使用方法，却在提升效率上有很大进展。
关于bert的文章其实挺多的，但我觉得这篇什么是BERT？解释的很清楚。
补充 开头的斯坦福教学视频资源是2017年的，虽然被很多人奉为经典，但不可否认，里面很多信息都没有进行更新，也就是说最近的语言模型都是没用提到的（我也是看了几集才发现的……但这个视频的价值还是很高，有时间最好是全部过一遍）
我又去B站再次找资源，斯坦福CS224n这个似乎是2021年录制的视频。
有了LSTM，Seq2Seq等内容。但是，这个版本好像没有中文翻译……
B站有同步字幕，但不是所有的都能够翻译，所以再带着练练听力吧……
总结 这是我整理的关于NLP较为详细的一个学习路线笔记，大方向的发展应该就是这样，但想要真正能够理解这些模型也是很困难的事情。其实现在bert已经占了nlp的半壁江山了，所以如果真的只是为了运用，又想要高效率的话，似乎把bert搞懂就差不多了。而且越到后面，其实有些思路反而简单了起来，没有之前那么晦涩难懂了。
如有不足请批评指正，我也是个刚踏入这行的菜鸟，望共同进步。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/ce8c314da2a8edcf1c1e1f72734a3a94/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-17T20:46:58+08:00" />
<meta property="article:modified_time" content="2023-02-17T20:46:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP学习路线（其实就是个汇集了所有经典模型的自然语言处理学习笔记）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>按照该知乎文章：<a href="https://zhuanlan.zhihu.com/p/49271699" rel="nofollow">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a>里罗列的发展史，对自然语言处理各个模型进行学习。<br> 同步观看<a href="https://www.bilibili.com/video/BV1b34y1B7zR" rel="nofollow">斯坦福大学的经典NLP课程</a>打基础。<br> 但说实话，概念是讲得很好，但不足以完全理解，所以也同时自己断断续续找论文或者什么来进行理解吧。<br> 这里也会将所有的链接放上来，加上一些学习体会。</p> 
<h2><a id="_4"></a>开始学习</h2> 
<h3><a id="Word2Vec_5"></a>词向量（Word2Vec）</h3> 
<p>1、按理来说从知乎文章的标题看，应该是要从word embedding开始学起，但是我在看文章的时候，又发现第一个概念是<strong>语言模型</strong>，所以也自己去搜索了一些相关资料来看，说实话，看的不是很懂，只有一点模模糊糊的概念吧——语言模型分不同种的语言模型，现在最经典的就是神经网络语言模型。<br> <strong>语言模型</strong>找到的文章我没觉得有什么合适的，这里暂时不放。<br> 下一个主要是需要理解word2vec吧，它是一种词嵌入方法。我找到的一篇文章<a href="https://blog.csdn.net/vincent_duan/article/details/117967110?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166583504016782412512945%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166583504016782412512945&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-117967110-null-null.142%5Ev56%5Ejs_top,201%5Ev3%5Eadd_ask&amp;utm_term=word2vec&amp;spm=1018.2226.3001.4187">word2vec介绍</a>，感觉写得很棒。</p> 
<blockquote> 
 <p>词嵌入（word embedding）：将高维度的词表示转换为低维度的词表示的方法。<br> 按照我的理解，其实就是将onehot编码的长度浪费改善，变成能够控制在30维度的表示方法。<em><strong>即“句子中每个单词以Onehot形式作为输入，乘以学好的Word Embedding矩阵Q，直接取出单词对应的Word Embedding”</strong></em></p> 
</blockquote> 
<p>2、此处再补上我在斯坦福大学视频里看到的另一种词向量转换方法——GloVe，按照视频里的讲解来说，如果只是单拎出word2vec和GloVe，GloVe的效率会更好些。但是视频我并没有看得很懂，因此也去找了相关文章，<a href="https://blog.csdn.net/yjw123456/article/details/118163934?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=GloVe%E6%A8%A1%E5%9E%8B%E7%90%86%E8%A7%A3&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-118163934.142%5Ev56%5Ejs_top,201%5Ev3%5Eadd_ask&amp;spm=1018.2226.3001.4187">GloVe模型理解</a>此文章中也引用了视频中的PPT。</p> 
<blockquote> 
 <p>遗留下的问题：word2vec最大的缺陷是无法解决多义词问题，即如果将一个词对应的向量定下来后，即使出现其他语义情况也无法再改变。</p> 
</blockquote> 
<h3><a id="RNN_19"></a>RNN</h3> 
<p>老规矩，依旧是找了个介绍<a href="https://blog.csdn.net/qq_39439006/article/details/121554808">通俗易懂的RNN</a>来看。我觉得概念上是很好的，但是它的图画的不是很清楚，可能一时难以看懂。<br> 这是斯坦福视频上截图下来的图片，我觉得更准确些，可以结合着看下。<br> <img src="https://images2.imgbox.com/16/28/qW3SERBT_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>RNN：时序神经网络，拥有时间上的顺序性。能结合上下文考虑，弥补了全连接神经网络的不足。</p> 
</blockquote> 
<p>补充一个那篇博客中没谈到的信息：第一个h(0)一般设为零。<br> RNN通过结合前向传播信息的方式，初步为多义词问题提供了解决方法——结合上文信息。本质上就是不割裂地将每个词提取出来理解，而是能够通过匹配前文关键词来进行查找，使得多义词能在不同语境下得到不同解释。</p> 
<blockquote> 
 <p>遗留下的问题：会存在梯度消失，即模型难以学习到远距离的依赖关系。</p> 
</blockquote> 
<h3><a id="LSTM_31"></a>LSTM</h3> 
<p>我感觉这篇<a href="https://blog.csdn.net/qian99/article/details/88628383">LSTM详解</a>已经把LSTM原理解释得很清楚了，而且很生动形象。<br> LSTM的输入输出其实和RNN都是一样的，但是通过细胞层的设置解决了梯度消失的问题，即将各个部分的重要性依次使用公式计算，这样就不会出现远距离的依赖关系低的问题。</p> 
<blockquote> 
 <p>遗留下的问题：表面上似乎没有什么问题了，但其实这个结合的方法是死板的，只结合前文而没有结合后文，依旧会出现考虑不全面的问题。</p> 
</blockquote> 
<p>其实如果按照斯坦福课程的顺序，后面就直接到transformer了，即解决LSTM遗留下这个问题的关键模型。但是知乎上那篇文章的顺序，则是按照历史轨迹，讲了所有的发展史，但都是致力于解决多义词（无法动态联系上下文判断词语意义）的问题。<br> 可以按照<a href="https://zhuanlan.zhihu.com/p/49271699" rel="nofollow">知乎这篇文章</a>的顺序看下ELMo等相关的模型，会更全面点，它的介绍也差不多将各个模型的精髓都讲到了。<strong>但直接跳到“transformer”看，其实也完全没问题</strong>，因为历史上这些模型，大体上的原理没有太多差别，不过细节不一样而已。</p> 
<h3><a id="Seq2SeqAttention_40"></a>Seq2Seq（Attention）</h3> 
<p>seq2seq是在RNN的基础上加以改进的，最大的区别就是对输入输出的数量能够随意进行改动。而在这里将它强调一下，是为了突出Seq2Seq和attention结合的用法，这是后面的transformer最重要的一部分。<br> <a href="https://blog.csdn.net/angus_huang_xu/article/details/115873866">Seq2Seq 模型详解</a>这篇文章是一个整体的介绍，我觉得理解是挺好理解的，但其实对于attention的机制介绍的不是很清楚。<br> 注意力机制的话，其实类似于人，当我们看文章或者图片等信息时，肯定会对其中某个部分投入更多的注意力，比如一段话中“这是一个桌子”，注意力肯定也是放在“桌子”这个词上的。至于使用方法是分别建立K、V、Q，然后找到其中注意力最大的词。<br> 想要详细了解可以看李宏毅老师的这篇<a href="https://www.bilibili.com/video/BV1v3411r78R" rel="nofollow">强烈推荐！台大李宏毅自注意力机制和Transformer详解！</a>，我看了很多介绍的文章视频，只有这个是让我看懂了的。下面的transformer李宏毅老师也在这里讲解了，可以一起学习下。</p> 
<h3><a id="transformer_46"></a>transformer</h3> 
<p>transformer中有两篇很著名的文章“Attention Is All You Need” 和 “The Illustrated Transformer”，不过直接看的话，可能因为基础知识不够和语言不通容易有点困难，因此我在在csdn上找到了解析“Attention Is All You Need”这篇论文的文章<a href="https://blog.csdn.net/benzhujie1245com/article/details/117173090">Transformer 模型详解</a>，再加上看李宏毅老师的视频才算对transformer勉强有了个大概理解。transformer的原理和attention息息相关，具体的能看懂文章和听懂视频应该就差不多了。</p> 
<h3><a id="Bert_49"></a>Bert</h3> 
<p>bert是现在nlp里面独占鳌头的模型了，nlp大部分领域用bert模型做训练应该都能能得到很好的效果，也有许多人在它的基础上不断改进。bert是基于transformer的encoder模块做出的，其中关于mask的部分是比较特殊的使用方法，却在提升效率上有很大进展。<br> 关于bert的文章其实挺多的，但我觉得这篇<a href="https://zhuanlan.zhihu.com/p/98855346" rel="nofollow">什么是BERT？</a>解释的很清楚。</p> 
<h2><a id="_53"></a>补充</h2> 
<p>开头的斯坦福教学视频资源是2017年的，虽然被很多人奉为经典，但不可否认，里面很多信息都没有进行更新，也就是说最近的语言模型都是没用提到的（我也是看了几集才发现的……但这个视频的价值还是很高，有时间最好是全部过一遍）<br> 我又去B站再次找资源，<a href="https://www.bilibili.com/video/BV12D4y117hQ?p=1&amp;vd_source=ce57e82224d3f9498db6d04ea195bed7" rel="nofollow">斯坦福CS224n</a>这个似乎是2021年录制的视频。<br> 有了LSTM，Seq2Seq等内容。但是，这个版本好像没有中文翻译……<br> B站有同步字幕，但不是所有的都能够翻译，所以再带着练练听力吧……</p> 
<h2><a id="_59"></a>总结</h2> 
<p>这是我整理的关于NLP较为详细的一个学习路线笔记，大方向的发展应该就是这样，但想要真正能够理解这些模型也是很困难的事情。其实现在bert已经占了nlp的半壁江山了，所以如果真的只是为了运用，又想要高效率的话，似乎把bert搞懂就差不多了。而且越到后面，其实有些思路反而简单了起来，没有之前那么晦涩难懂了。<br> 如有不足请批评指正，我也是个刚踏入这行的菜鸟，望共同进步。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9c91bf42cc6b2d04006c955032f97ea5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Springboot多环境配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f20aec401e9a0f90c714752bcfc6b8fa/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Elasticsearch基础学习（Java API 实现增删查改）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>