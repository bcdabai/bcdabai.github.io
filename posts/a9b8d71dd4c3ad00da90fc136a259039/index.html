<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ES文档索引、查询、分片、文档评分和分析器技术原理 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ES文档索引、查询、分片、文档评分和分析器技术原理" />
<meta property="og:description" content="技术原理 索引文档 索引文档分为单个文档和多个文档。
单个文档 新建单个文档所需要的步骤顺序：
客户端向 Node 1 发送新建、索引或者删除请求。节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 多个文档 使用 bulk 修改多个文档步骤顺序：
客户端向 Node 1 发送 bulk 请求。Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。 索引文档的实现机制 写操作的关键点
在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑：
可靠性：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。一致性：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。原子性：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。隔离性：多个写入操作相互不影响。实时性：写入后是否可以立即被查询到。性能：写入性能，吞吐量到底怎么样。 Elasticsearch作为分布式系统，通过如何保证的呢？
可靠性：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。一致性：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现最终一致性。原子性：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。隔离性：仍然采用Version和局部锁来保证更新的是特定版本的数据。实时性：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。性能：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。 Elasticsearch的写
Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。
此外，Elasticsearch整体架构上采用了一主多副的方式：
每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。
请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。
这种模式下，写入操作的延时就等于latency = Latency(Primary Write) &#43; Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a9b8d71dd4c3ad00da90fc136a259039/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-26T10:41:42+08:00" />
<meta property="article:modified_time" content="2024-01-26T10:41:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ES文档索引、查询、分片、文档评分和分析器技术原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_2"></a>技术原理</h3> 
<h4><a id="_3"></a>索引文档</h4> 
<p>索引文档分为单个文档和多个文档。</p> 
<h5><a id="_6"></a>单个文档</h5> 
<p>新建单个文档所需要的步骤顺序：<br> <img src="https://images2.imgbox.com/17/5c/I5tmlja7_o.png" alt="在这里插入图片描述"></p> 
<ol><li>客户端向 Node 1 发送新建、索引或者删除请求。</li><li>节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。</li><li>Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。</li></ol> 
<h5><a id="_12"></a>多个文档</h5> 
<p>使用 bulk 修改多个文档步骤顺序：<br> <img src="https://images2.imgbox.com/31/28/DNtvSvuH_o.png" alt="在这里插入图片描述"></p> 
<ol><li>客户端向 Node 1 发送 bulk 请求。</li><li>Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。</li><li>主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。</li></ol> 
<h4><a id="_18"></a>索引文档的实现机制</h4> 
<p>写操作的关键点<br> 在考虑或分析一个分布式系统的写操作时，一般需要从下面几个方面考虑：</p> 
<ol><li><strong>可靠性</strong>：或者是持久性，数据写入系统成功后，数据不会被回滚或丢失。</li><li><strong>一致性</strong>：数据写入成功后，再次查询时必须能保证读取到最新版本的数据，不能读取到旧数据。</li><li><strong>原子性</strong>：一个写入或者更新操作，要么完全成功，要么完全失败，不允许出现中间状态。</li><li><strong>隔离性</strong>：多个写入操作相互不影响。</li><li><strong>实时性</strong>：写入后是否可以立即被查询到。</li><li><strong>性能</strong>：写入性能，吞吐量到底怎么样。</li></ol> 
<p>Elasticsearch作为分布式系统，通过如何保证的呢？</p> 
<ol><li><strong>可靠性</strong>：由于Lucene的设计中不考虑可靠性，在Elasticsearch中通过Replica和TransLog两套机制保证数据的可靠性。</li><li><strong>一致性</strong>：Lucene中的Flush锁只保证Update接口里面Delete和Add中间不会Flush，但是Add完成后仍然有可能立即发生Flush，导致Segment可读。这样就没法保证Primary和所有其他Replica可以同一时间Flush，就会出现查询不稳定的情况，这里只能实现<strong>最终一致性</strong>。</li><li><strong>原子性</strong>：Add和Delete都是直接调用Lucene的接口，是原子的。当部分更新时，使用Version和锁保证更新是原子的。</li><li><strong>隔离性</strong>：仍然采用Version和局部锁来保证更新的是特定版本的数据。</li><li><strong>实时性</strong>：使用定期Refresh Segment到内存，并且Reopen Segment方式保证搜索可以在较短时间（比如1秒）内被搜索到。通过将未刷新到磁盘数据记入TransLog，保证对未提交数据可以通过ID实时访问到。</li><li><strong>性能</strong>：性能是一个系统性工程，所有环节都要考虑对性能的影响，在Elasticsearch中，在很多地方的设计都考虑到了性能，一是不需要所有Replica都返回后才能返回给用户，只需要返回特定数目的就行；二是生成的Segment现在内存中提供服务，等一段时间后才刷新到磁盘，Segment在内存这段时间的可靠性由TransLog保证；三是TransLog可以配置为周期性的Flush，但这个会给可靠性带来伤害；四是每个线程持有一个Segment，多线程时相互不影响，相互独立，性能更好；五是系统的写入流程对版本依赖较重，读取频率较高，因此采用了versionMap，减少热点数据的多次磁盘IO开销。Lucene中针对性能做了大量的优化。</li></ol> 
<p>Elasticsearch的写<br> Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集，每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送给特定Shard中建立索引。这样就能实现分布式了。</p> 
<p>此外，Elasticsearch整体架构上采用了一主多副的方式：<br> <img src="https://images2.imgbox.com/11/d1/pYhwLwbU_o.png" alt="在这里插入图片描述"><br> 每个Index由多个Shard组成，每个Shard有一个主节点和多个副本节点，副本个数可配。但每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个Filed的值作为路由参数，如果没有设置，则使用Mapping中的配置，如果mapping中也没有配置，则使用_id作为路由参数，然后通过_routing的Hash值选择出Shard（在OperationRouting类中），最后从集群的Meta中找出出该Shard的Primary节点。</p> 
<p>请求接着会发送给Primary Shard，在Primary Shard上执行成功后，再从Primary Shard上将请求同时发送给多个Replica Shard，请求在多个Replica Shard上执行成功并返回给Primary Shard后，写入请求执行成功，返回结果给客户端。</p> 
<p>这种模式下，写入操作的延时就等于latency = Latency(Primary Write) + Max(Replicas Write)。只要有副本在，写入延时最小也是两次单Shard的写入时延总和，写入效率会较低，但是这样的好处也很明显，避免写入后，单机或磁盘故障导致数据丢失，在数据重要性和性能方面，一般都是优先选择数据，除非一些允许丢数据的特殊场景。</p> 
<p>采用多个副本后，避免了单机或磁盘故障发生时，对已经持久化后的数据造成损害，但是Elasticsearch里为了减少磁盘IO保证读写性能，一般是每隔一段时间（比如5分钟）才会把Lucene的Segment写入磁盘持久化，对于写入内存，但还未Flush到磁盘的Lucene数据，如果发生机器宕机或者掉电，那么内存中的数据也会丢失，这时候如何保证？</p> 
<p>对于这种问题，Elasticsearch学习了数据库中的处理方式：增加CommitLog模块，Elasticsearch中叫<code>TransLog</code>。</p> 
<p><img src="https://images2.imgbox.com/2e/c4/msXfLf5q_o.jpg" alt="在这里插入图片描述"><br> 在每一个Shard中，写入流程分为两部分，<strong>先写入Lucene，再写入TransLog</strong>。</p> 
<p>写入请求到达Shard后，先写Lucene文件，创建好索引，此时索引还在内存里面，接着去写TransLog，写完TransLog后，刷新TransLog数据到磁盘上，写磁盘成功后，请求返回给用户。这里有几个关键点:</p> 
<ol><li>一是和数据库不同，数据库是先写CommitLog，然后再写内存，而Elasticsearch是先写内存，最后才写TransLog，一种可能的原因是<strong>Lucene的内存写入会有很复杂的逻辑，很容易失败，比如分词，字段长度超过限制等，比较重，为了避免TransLog中有大量无效记录，减少recover的复杂度和提高速度，所以就把写Lucene放在了最前面</strong>。</li><li>二是<strong>写Lucene内存后，并不是可被搜索</strong>的，需要通过<strong>Refresh把内存的对象转成完整的Segment后，然后再次reopen后才能被搜索</strong>，一般这个时间设置为1秒钟，导致写入Elasticsearch的文档，最快要1秒钟才可被从搜索到，所以<strong>Elasticsearch在搜索方面是NRT（Near Real Time）近实时的系统</strong>。</li><li>三是当Elasticsearch作为NoSQL数据库时，查询方式是GetById，这种查询可以直接从TransLog中查询，这时候就成了RT（Real Time）实时系统。</li><li>四是每隔一段比较长的时间，比如30分钟后，Lucene会把内存中生成的新Segment刷新到磁盘上，刷新后索引文件已经持久化了，历史的TransLog就没用了，会清空掉旧的TransLog。</li></ol> 
<h4><a id="_62"></a>数据更新</h4> 
<p>Update流程：<br> <img src="https://images2.imgbox.com/e2/88/gxZBMKZI_o.jpg" alt="在这里插入图片描述"><br> Lucene中不支持部分字段的Update，所以需要在Elasticsearch中实现该功能，具体流程如下：</p> 
<ol><li>收到Update请求后，从Segment或者TransLog中读取同id的完整Doc，记录版本号为V1。</li><li>将版本V1的全量Doc和请求中的部分字段Doc合并为一个完整的Doc，同时更新内存中的VersionMap。获取到完整Doc后，Update请求就变成了Index请求。 加锁。</li><li>再次从VersionMap中读取该id的最大版本号V2，如果VersionMap中没有，则从Segment或者TransLog中读取，这里基本都会从VersionMap中获取到。<br> 检查版本是否冲突(V1==V2)，如果冲突，则回退到开始的“Update doc”阶段，重新执行。如果不冲突，则执行最新的Add请求。</li><li>在Index Doc阶段，首先将Version + 1得到V3，再将Doc加入到Lucene中去，Lucene中会先删同id下的已存在doc id，然后再增加新Doc。写入Lucene成功后，将当前V3更新到VersionMap中。</li><li>释放锁，部分更新的流程就结束了。</li></ol> 
<h4><a id="_73"></a>查询文档</h4> 
<h5><a id="_74"></a>单个文档</h5> 
<p>从主分片或者副本分片检索文档的步骤顺序：<br> <img src="https://images2.imgbox.com/d3/55/AMgxmFvR_o.png" alt="在这里插入图片描述"></p> 
<ol><li>客户端向 Node 1（Master） 发送获取请求。</li><li>节点使用文档的 _id 来确定文档属于分片 0（根据_routering确认文档所在的Shard） 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。</li><li>Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。</li></ol> 
<p>在处理读取请求时，协调结点在每次请求的时候都会通过<strong>轮询所有的副本分片来达到负载均衡</strong>。</p> 
<p>在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。</p> 
<h5><a id="_85"></a>多个文档</h5> 
<p>使用 mget 取回多个文档的步骤顺序：<br> <img src="https://images2.imgbox.com/cc/aa/98G7GeH5_o.png" alt="在这里插入图片描述"><br> 使用单个 mget 请求取回多个文档所需的步骤顺序：</p> 
<ol><li>客户端向 Node 1（Master） 发送 mget 请求。</li><li>Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， Node 1 构建响应并将其返回给客户端。</li></ol> 
<h4><a id="_94"></a>文档读取过程详解</h4> 
<p>所有的搜索系统一般都是两阶段查询：</p> 
<ol><li>第一阶段查询到匹配的DocID，</li><li>第二阶段再查询DocID对应的完整文档，这种在Elasticsearch中称为query_then_fetch。</li></ol> 
<p>除了一阶段，两阶段外，还有一种三阶段查询的情况。搜索里面有一种算分逻辑是根据TF（Term Frequency）和DF（Document Frequency）计算基础分，但是Elasticsearch中查询的时候，是在每个Shard中独立查询的，每个Shard中的TF和DF也是独立的，虽然在写入的时候通过_routing保证Doc分布均匀，但是没法保证TF和DF均匀，那么就有会导致局部的TF和DF不准的情况出现，这个时候基于TF、DF的算分就不准。</p> 
<p>为了解决这个问题，Elasticsearch中引入了DFS查询，比如DFS_query_then_fetch，会先收集所有Shard中的TF和DF值，然后将这些值带入请求中，再次执行query_then_fetch，这样算分的时候TF和DF就是准确的，类似的有DFS_query_and_fetch。这种查询的优势是算分更加精准，但是效率会变差。</p> 
<p>另一种选择是用BM25代替TF/DF模型。</p> 
<h5><a id="_105"></a>第一阶段</h5> 
<p>todo</p> 
<h5><a id="_107"></a>第二阶段</h5> 
<p><img src="https://images2.imgbox.com/70/70/4AAcklf9_o.jpg" alt="在这里插入图片描述"></p> 
<ol><li>在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。</li><li>每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li><li>接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并丰富文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。</li></ol> 
<p>二阶段查询为例来介绍查询流程：<img src="https://images2.imgbox.com/b2/83/0pjCNoRF_o.jpg" alt="在这里插入图片描述"></p> 
<h6><a id="Client_Node_116"></a>Client Node</h6> 
<p><strong>1.Get Remove Cluster Shard</strong><br> 判断是否需要跨集群访问，如果需要，则获取到要访问的Shard列表。</p> 
<p><strong>2.Get Search Shard Iterator</strong><br> 获取当前Cluster中要访问的Shard，和上一步中的Remove Cluster Shard合并，构建出最终要访问的完整Shard列表。</p> 
<p>这一步中，会根据Request请求中的参数从Primary Node和多个Replica Node中选择出一个要访问的Shard。</p> 
<p><strong>3.For Every Shard:Perform</strong><br> 遍历每个Shard，对每个Shard执行后面逻辑。</p> 
<p><strong>4.Send Request To Query Shard</strong><br> 将查询阶段请求发送给相应的Shard。</p> 
<p><strong>5.Merge Docs</strong><br> 上一步将请求发送给多个Shard后，这一步就是异步等待返回结果，然后对结果合并。这里的合并策略是维护一个Top N大小的优先级队列，每当收到一个shard的返回，就把结果放入优先级队列做一次排序，直到所有的Shard都返回。</p> 
<p>翻页逻辑也是在这里，如果需要取Top 30~ Top 40的结果，这个的意思是所有Shard查询结果中的第30到40的结果，那么在每个Shard中无法确定最终的结果，每个Shard需要返回Top 40的结果给Client Node，然后Client Node中在merge docs的时候，计算出Top 40的结果，最后再去除掉Top 30，剩余的10个结果就是需要的Top 30~ Top 40的结果。</p> 
<p>上述翻页逻辑有一个明显的缺点就是每次Shard返回的数据中包括了已经翻过的历史结果，如果翻页很深，则在这里需要排序的Docs会很多，比如Shard有1000，取第9990到10000的结果，那么这次查询，Shard总共需要返回1000 * 10000，也就是一千万Doc，这种情况很容易导致OOM。</p> 
<p>另一种翻页方式是使用search_after，这种方式会更轻量级，如果每次只需要返回10条结构，则每个Shard只需要返回search_after之后的10个结果即可，返回的总数据量只是和Shard个数以及本次需要的个数有关，和历史已读取的个数无关。这种方式更安全一些，推荐使用这种。</p> 
<p>如果有aggregate，也会在这里做聚合，但是不同的aggregate类型的merge策略不一样，具体的可以在后面的aggregate文章中再介绍。</p> 
<p><strong>6.Send Request To Fetch Shard</strong><br> 选出Top N个Doc ID后发送给这些Doc ID所在的Shard执行Fetch Phase，最后会返回Top N的Doc的内容。</p> 
<h6><a id="Query_Phase_145"></a>Query Phase</h6> 
<p>接下来我们看第一阶段查询的步骤：<br> <strong>1.Create Search Context</strong><br> 创建Search Context，之后Search过程中的所有中间状态都会存在Context中，这些状态总共有50多个，具体可以查看DefaultSearchContext或者其他SearchContext的子类。</p> 
<p><strong>2.Parse Query</strong><br> 解析Query的Source，将结果存入Search Context。这里会根据请求中Query类型的不同创建不同的Query对象，比如TermQuery、FuzzyQuery等，最终真正执行TermQuery、FuzzyQuery等语义的地方是在Lucene中。</p> 
<p>这里包括了dfsPhase、queryPhase和fetchPhase三个阶段的preProcess部分，只有queryPhase的preProcess中有执行逻辑，其他两个都是空逻辑，执行完preProcess后，所有需要的参数都会设置完成。</p> 
<p>由于Elasticsearch中有些请求之间是相互关联的，并非独立的，比如scroll请求，所以这里同时会设置Context的生命周期。</p> 
<p>同时会设置lowLevelCancellation是否打开，这个参数是集群级别配置，同时也能动态开关，打开后会在后面执行时做更多的检测，检测是否需要停止后续逻辑直接返回。</p> 
<p><strong>3.Get From Cache</strong><br> 判断请求是否允许被Cache，如果允许，则检查Cache中是否已经有结果，如果有则直接读取Cache，如果没有则继续执行后续步骤，执行完后，再将结果加入Cache。</p> 
<p><strong>4.Add Collectors</strong><br> Collector主要目标是收集查询结果，实现排序，对自定义结果集过滤和收集等。这一步会增加多个Collectors，多个Collector组成一个List。</p> 
<ol><li>FilteredCollector：先判断请求中是否有Post Filter，Post Filter用于Search，Agg等结束后再次对结果做Filter，希望Filter不影响Agg结果。如果有Post Filter则创建一个FilteredCollector，加入Collector List中。</li><li>PluginInMultiCollector：判断请求中是否制定了自定义的一些Collector，如果有，则创建后加入Collector List。</li><li>MinimumScoreCollector：判断请求中是否制定了最小分数阈值，如果指定了，则创建MinimumScoreCollector加入Collector List中，在后续收集结果时，会过滤掉得分小于最小分数的Doc。</li><li>EarlyTerminatingCollector：判断请求中是否提前结束Doc的Seek，如果是则创建EarlyTerminatingCollector，加入Collector List中。在后续Seek和收集Doc的过程中，当Seek的Doc数达到Early Terminating后会停止Seek后续倒排链。</li><li>CancellableCollector：判断当前操作是否可以被中断结束，比如是否已经超时等，如果是会抛出一个TaskCancelledException异常。该功能一般用来提前结束较长的查询请求，可以用来保护系统。</li><li>EarlyTerminatingSortingCollector：如果Index是排序的，那么可以提前结束对倒排链的Seek，相当于在一个排序递减链表上返回最大的N个值，只需要直接返回前N个值就可以了。这个Collector会加到Collector List的头部。EarlyTerminatingSorting和EarlyTerminating的区别是，EarlyTerminatingSorting是一种对结果无损伤的优化，而EarlyTerminating是有损的，人为掐断执行的优化。</li><li>TopDocsCollector：这个是最核心的Top N结果选择器，会加入到Collector List的头部。TopScoreDocCollector和TopFieldCollector都是TopDocsCollector的子类，TopScoreDocCollector会按照固定的方式算分，排序会按照分数+doc id的方式排列，如果多个doc的分数一样，先选择doc id小的文档。而TopFieldCollector则是根据用户指定的Field的值排序。</li></ol> 
<p><strong>5.lucene::search</strong><br> 这一步会调用Lucene中IndexSearch的search接口，执行真正的搜索逻辑。每个Shard中会有多个Segment，每个Segment对应一个LeafReaderContext，这里会遍历每个Segment，到每个Segment中去Search结果，然后计算分数。</p> 
<p>搜索里面一般有两阶段算分，第一阶段是在这里算的，会对每个Seek到的Doc都计算分数，为了减少CPU消耗，一般是算一个基本分数。这一阶段完成后，会有个排序。然后在第二阶段，再对Top 的结果做一次二阶段算分，在二阶段算分的时候会考虑更多的因子。二阶段算分在后续操作中。</p> 
<p>具体请求，比如TermQuery、WildcardQuery的查询逻辑都在Lucene中，后面会有专门文章介绍。</p> 
<p><strong>6.rescore</strong><br> 根据Request中是否包含rescore配置决定是否进行二阶段排序，如果有则执行二阶段算分逻辑，会考虑更多的算分因子。二阶段算分也是一种计算机中常见的多层设计，是一种资源消耗和效率的折中。</p> 
<p>Elasticsearch中支持配置多个Rescore，这些rescore逻辑会顺序遍历执行。每个rescore内部会先按照请求参数window选择出Top window的doc，然后对这些doc排序，排完后再合并回原有的Top 结果顺序中。</p> 
<p><strong>7.suggest::execute()</strong><br> 如果有推荐请求，则在这里执行推荐请求。如果请求中只包含了推荐的部分，则很多地方可以优化。推荐不是今天的重点，这里就不介绍了，后面有机会再介绍。</p> 
<p><strong>8.aggregation::execute()</strong><br> 如果含有聚合统计请求，则在这里执行。Elasticsearch中的aggregate的处理逻辑也类似于Search，通过多个Collector来实现。在Client Node中也需要对aggregation做合并。aggregate逻辑更复杂一些，就不在这里赘述了，后面有需要就再单独开文章介绍。</p> 
<p>上述逻辑都执行完成后，如果当前查询请求只需要查询一个Shard，那么会直接在当前Node执行Fetch Phase。</p> 
<ol><li>ExplainFetchSubPhase</li><li>DocValueFieldsFetchSubPhase</li><li>ScriptFieldsFetchSubPhase</li><li>FetchSourceSubPhase</li><li>VersionFetchSubPhase</li><li>MatchedQueriesFetchSubPhase</li><li>HighlightPhase</li><li>ParentFieldSubFetchPhase</li></ol> 
<p>除了系统默认的8种外，还有通过插件的形式注册自定义的功能，这些SubPhase中最重要的是Source和Highlight，Source是加载原文，Highlight是计算高亮显示的内容片断。</p> 
<p>上述多个SubPhase会针对每个Doc顺序执行，可能会产生多次的随机IO，这里会有一些优化方案，但是都是针对特定场景的，不具有通用性。</p> 
<p>Fetch Phase执行完后，整个查询流程就结束了。</p> 
<h6><a id="Fetch_Phase_211"></a>Fetch Phase</h6> 
<p>Elasticsearch作为搜索系统时，或者任何搜索系统中，除了Query阶段外，还会有一个Fetch阶段，这个Fetch阶段在数据库类系统中是没有的，是搜索系统中额外增加的阶段。搜索系统中额外增加Fetch阶段的原因是搜索系统中数据分布导致的，在搜索中，数据通过routing分Shard的时候，只能根据一个主字段值来决定，但是查询的时候可能会根据其他非主字段查询，那么这个时候所有Shard中都可能会存在相同非主字段值的Doc，所以需要查询所有Shard才能不会出现结果遗漏。同时如果查询主字段，那么这个时候就能直接定位到Shard，就只需要查询特定Shard即可，这个时候就类似于数据库系统了。另外，数据库中的二级索引又是另外一种情况，但类似于查主字段的情况，这里就不多说了。</p> 
<p>基于上述原因，第一阶段查询的时候并不知道最终结果会在哪个Shard上，所以每个Shard中管都需要查询完整结果，比如需要Top 10，那么每个Shard都需要查询当前Shard的所有数据，找出当前Shard的Top 10，然后返回给Client Node。如果有100个Shard，那么就需要返回100 * 10 = 1000个结果，而Fetch Doc内容的操作比较耗费IO和CPU，如果在第一阶段就Fetch Doc，那么这个资源开销就会非常大。所以，一般是当Client Node选择出最终Top N的结果后，再对最终的Top N读取Doc内容。通过增加一点网络开销而避免大量IO和CPU操作，这个折中是非常划算的。</p> 
<p>Fetch阶段的目的是通过DocID获取到用户需要的完整Doc内容。这些内容包括了DocValues，Store，Source，Script和Highlight等，具体的功能点是在SearchModule中注册的，系统默认注册的有：</p> 
<h5><a id="_217"></a>第三阶段</h5> 
<p>todo</p> 
<h4><a id="_219"></a>使用相关性进行搜索</h4> 
<p>在进行相关性搜索前先了解下面问题：</p> 
<ol><li>Lucene和EIasticsearch内部打分是如何运作的</li><li>提升特定查询或字段的得分</li><li>使用解释的API接口来理解词频、逆文档频率、相关性得分</li><li>通过重新计算文档子集的得分来减少评分操作的性能影响</li><li>使用function—score查询，获取终极的打分能力</li><li>字段数据的缓存，以及它是如何影响实例的</li></ol> 
<p>文档的评分使用的是TF-IDF或者BM25。</p> 
<p>其他评分算法：</p> 
<ol><li>随机性分歧（Divergencefromrandomness），即DFR相似度；</li><li>基于信息的（lnformation），即IB相似度；</li><li>LMDirichIet相似度；</li><li>LMJeIinekMercer相似度。</li></ol> 
<h5><a id="ElasticSearch_238"></a>ElasticSearch如何给文档评分</h5> 
<p>有两种配置评分：</p> 
<ol><li>索引字段评分</li><li>ElasticSearch配置评分</li></ol> 
<p>索引字段评分-使用内置评分算法</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"similarity"</span><span class="token operator">:</span> <span class="token string">"BM25"</span><span class="token punctuation">,</span> 
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>在字段similarity配置内置的评分算法。</p> 
<p>索引字段评分-自定义算法：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"similarity"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		  <span class="token string">"customSimilarity"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"BM25"</span><span class="token punctuation">,</span>
		    <span class="token string">"k1"</span><span class="token operator">:</span><span class="token number">1.2</span><span class="token punctuation">,</span>
		    <span class="token string">"b"</span><span class="token operator">:</span><span class="token number">0.75</span><span class="token punctuation">,</span>
		    <span class="token string">"discount_overlaps"</span><span class="token operator">:</span><span class="token boolean">false</span>
		  <span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"similarity"</span><span class="token operator">:</span> <span class="token string">"customSimilarity"</span><span class="token punctuation">,</span> 
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>在setting.similarity参数定义评分算法，并且在字段的similarity指定自定义的评分算法。</p> 
<p>BM25有3种主要的设置，即k1、b和discount_overlaps:</p> 
<ol><li>k1:k1控制对于得分而言词频（词条出现在文档里的频繁程度，或者是TF）的重要性。</li><li>b:b是介于0到1之间的数值，它控制了文档篇幅对于得分的影响程度。</li><li>discount_overlaps:discount_overlaps的设置告知Elasticsearch，在某个字段中，多个分词出现在同一个位置，是否应该影响长度的标准化。默认值是true.</li></ol> 
<p>默认情况下，k1被设置为1.2，而b被设置为0.75。</p> 
<p>如果希望全局修改评分算法，在elasticsearch.yml配置文件中配置：</p> 
<pre><code class="prism language-yaml"><span class="token key atrule">index.similarity.default.type</span><span class="token punctuation">:</span> BM25
</code></pre> 
<h5><a id="boosting_326"></a>boosting</h5> 
<p>boosting是一个可以用来修改文档的相关性的程序。</p> 
<p>boosting有两种类型，当索引或者查询文档的时候，可以提升一篇文档的得分。在索引期间修改的文档boosting是存储在索引中的，修改boosting值唯一的方法是重新索引这篇文档。鉴于此，我们当然建议用户使用查询期间的boosting，因为这样更为灵活，并允许用户改变主意，在不重新索引数据的前提下改变字段或者词条的重要性。</p> 
<p>在索引期间boost文档是不推荐的。</p> 
<p>1.在索引期间配置boost</p> 
<pre><code class="prism language-haskell"><span class="token constant">PUT</span> <span class="token operator">/</span><span class="token hvariable">mall_user</span>
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"boost"</span><span class="token operator">:</span> <span class="token number">3.0</span><span class="token punctuation">,</span> 
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>在设置该索引的映射后，任何自动索引的文档就拥有一个boost值，运用于name字段的词条中。再次强调一下，请记住这个boost的值是固定的(fixed），也就是说如果决定修改这个值，你必须重新索引文档。</p> 
<p>在索引期间boost文档是不推荐的，原因：</p> 
<ol><li><strong>精度丢失</strong>：boost的值是以低精度的数值存储在Lucene的内部索引结构中。只有一个字节用于存储浮点型数值，所以计算文档的最终得分时可能会丢失精度。</li><li>boost是运用于一个词条的。因此，在被boost的字段中如果匹配上了多个词条，就意味着多次的boost，将会进一步增加字段的权重。</li></ol> 
<p>2.查询期间boosting<br> 当进行搜索的时候，有几种方法进行boosting。如果使用基本的match、multi_match、simple_query_string或query_string查询，就可以基于每个词条或者每个字段来控制boost.几乎所有的Elasticsearch查询类型都支持boosting.如果这个还不够灵活，那么可以通过function_score查询，以更为精细的方式来控制boosting.</p> 
<p>开始查询前先创建一个索引：</p> 
<pre><code class="prism language-haskell"><span class="token constant">PUT</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span>
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"title"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"attribute"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"text"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"desc"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"text"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"price"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"double"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>


<span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_doc</span><span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"title"</span><span class="token operator">:</span><span class="token string">"Apple iPhone 15 Pro Max - 全新旗舰智能手机"</span><span class="token punctuation">,</span>
  <span class="token string">"attribute"</span><span class="token operator">:</span><span class="token string">"256GB Pro"</span><span class="token punctuation">,</span>
  <span class="token string">"desc"</span><span class="token operator">:</span><span class="token string">"iPhone 15 Pro Max采用了全新的设计语言，机身采用航空级铝合金打造，正面是一块6.7英寸的超视网膜XDR显示屏，分辨率高达4K，显示效果惊艳。同时，这款手机还支持120Hz的高刷新率，让您在浏览网页、玩游戏时更加流畅。"</span><span class="token punctuation">,</span>
  <span class="token string">"price"</span><span class="token operator">:</span> <span class="token number">4999</span>
<span class="token punctuation">}</span>

<span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_doc</span><span class="token operator">/</span><span class="token number">2</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"title"</span><span class="token operator">:</span><span class="token string">"Apple iPhone 15 Pro Max - 全新旗舰智能手机"</span><span class="token punctuation">,</span>
  <span class="token string">"attribute"</span><span class="token operator">:</span><span class="token string">"512GB Pro Max"</span><span class="token punctuation">,</span>
  <span class="token string">"desc"</span><span class="token operator">:</span><span class="token string">"iPhone 15 Pro Max采用了全新的设计语言，机身采用航空级铝合金打造，正面是一块6.7英寸的超视网膜XDR显示屏，分辨率高达4K，显示效果惊艳。同时，这款手机还支持120Hz的高刷新率，让您在浏览网页、玩游戏时更加流畅。"</span><span class="token punctuation">,</span>
  <span class="token string">"price"</span><span class="token operator">:</span> <span class="token number">6999</span>
<span class="token punctuation">}</span>

<span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_doc</span><span class="token operator">/</span><span class="token number">3</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"title"</span><span class="token operator">:</span><span class="token string">"Apple iPhone 15 Pro Max - 全新旗舰智能手机"</span><span class="token punctuation">,</span>
  <span class="token string">"attribute"</span><span class="token operator">:</span><span class="token string">"1TB Pro Max"</span><span class="token punctuation">,</span>
  <span class="token string">"desc"</span><span class="token operator">:</span><span class="token string">"iPhone 15 Pro Max采用了全新的设计语言，机身采用航空级铝合金打造，正面是一块6.7英寸的超视网膜XDR显示屏，分辨率高达4K，显示效果惊艳。同时，这款手机还支持120Hz的高刷新率，让您在浏览网页、玩游戏时更加流畅。"</span><span class="token punctuation">,</span>
  <span class="token string">"price"</span><span class="token operator">:</span> <span class="token number">12999</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>match查询：</p> 
<pre><code class="prism language-haskell"><span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_search</span>
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"bool"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"should"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>
					<span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"title"</span><span class="token operator">:</span> <span class="token string">"iPhone 1TB"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span><span class="token punctuation">,</span>
				<span class="token punctuation">{<!-- --></span>
					<span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"attribute"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
							<span class="token string">"query"</span><span class="token operator">:</span> <span class="token string">"iPhone 1TB"</span><span class="token punctuation">,</span>
							<span class="token string">"boost"</span><span class="token operator">:</span> <span class="token number">3.0</span>
						<span class="token punctuation">}</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">]</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>增加boost参数，默认1.0。在搜索中，我们认为attribute 的描述更加符合搜索的需求，通过设置boost参数实现。</p> 
<p>当使用bool或and/or/not组合多个查询时，boost查询才有意义。</p> 
<p>multi_match查询：<br> multi_match和match类似，多个字段指定同一个boost值。</p> 
<pre><code class="prism language-haskell"><span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_search</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"multi_match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"query"</span><span class="token operator">:</span> <span class="token string">"iPhone 1TB"</span><span class="token punctuation">,</span>
      <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">,</span><span class="token string">"attribute"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
      <span class="token string">"boost"</span><span class="token operator">:</span> <span class="token number">3.0</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>或者使用^为各个字段指定boost值：</p> 
<pre><code class="prism language-haskell"><span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_search</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"multi_match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"query"</span><span class="token operator">:</span> <span class="token string">"iPhone 1TB"</span><span class="token punctuation">,</span>
      <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"title^0.75"</span><span class="token punctuation">,</span><span class="token string">"attribute^3"</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>title：设置boost为0.75<br> attribute：设置boost为3</p> 
<p>query_string 查询：<br> match、multi_match查询是为字段指定boost值，query_string可以为搜索文本中的某个词设置boost。</p> 
<pre><code class="prism language-haskell"><span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_search</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"query_string"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"query"</span><span class="token operator">:</span> <span class="token string">"iPhone 256GB^3"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>认为256比iPhone关键词相关性更加高，为256设置更高的boost。</p> 
<p>explain查看评分的详细信息：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_goods_test<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"query_string"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"query"</span><span class="token operator">:</span> <span class="token string">"iPhone 256GB^3"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string">"explain"</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ff/52/cNwaCo0b_o.png" alt="在这里插入图片描述"><br> 256GB boost对评分的贡献比较大。</p> 
<p>_explain查看为何没有匹配文档：</p> 
<pre><code class="prism language-haskell"><span class="token constant">POST</span> <span class="token operator">/</span><span class="token hvariable">mall_goods_test</span><span class="token operator">/</span><span class="token hvariable">_doc</span><span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span><span class="token hvariable">_explain</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"title"</span><span class="token operator">:</span> <span class="token string">"Huawei"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-haskell"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"_index"</span> <span class="token operator">:</span> <span class="token string">"mall_goods_test"</span><span class="token punctuation">,</span>
  <span class="token string">"_type"</span> <span class="token operator">:</span> <span class="token string">"_doc"</span><span class="token punctuation">,</span>
  <span class="token string">"_id"</span> <span class="token operator">:</span> <span class="token string">"1"</span><span class="token punctuation">,</span>
  <span class="token string">"matched"</span> <span class="token operator">:</span> <span class="token hvariable">false</span><span class="token punctuation">,</span>
  <span class="token string">"explanation"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"value"</span> <span class="token operator">:</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
    <span class="token string">"description"</span> <span class="token operator">:</span> <span class="token string">"no matching term"</span><span class="token punctuation">,</span>
    <span class="token string">"details"</span> <span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token punctuation">]</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>查看_id=1的文档为何没有匹配“Huawei”，从结果明显看出是不匹配。</p> 
<p>function_score 打分，更加自由的打分。todo<br> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-function-score-query.html" rel="nofollow">Elasticsearch Function Score</a></p> 
<h5><a id="1LuceneEIasticsearch_550"></a>1.Lucene和EIasticsearch内部打分是如何运作的？</h5> 
<p>确定文档和查询相关程度的过程被称为打分（scoring）。</p> 
<h4><a id="Bulk_553"></a>Bulk请求</h4> 
<p>Elasticsearch写入请求类型<br> Elasticsearch中的写入请求类型，主要包括下列几个：Index(Create)，Update，Delete和Bulk，其中前3个是单文档操作，后一个Bulk是多文档操作，其中Bulk中可以包括Index(Create)，Update和Delete。</p> 
<p>在6.0.0及其之后的版本中，前3个单文档操作的实现基本都和Bulk操作一致，甚至有些就是通过调用Bulk的接口实现的。估计接下来几个版本后，Index(Create)，Update，Delete都会被当做Bulk的一种特例化操作被处理。这样，代码和逻辑都会更清晰一些。</p> 
<p>下面，我们就以Bulk请求为例来介绍写入流程。<br> <img src="https://images2.imgbox.com/1a/c8/Tw3VDiua_o.jpg" alt="在这里插入图片描述"><br> 红色：Client Node。<br> 绿色：Primary Node。<br> 蓝色：Replica Node。</p> 
<h5><a id="Client_Node_565"></a>Client Node</h5> 
<p><strong>1.Ingest Pipeline</strong><br> 在这一步可以对原始文档做一些处理，比如HTML解析，自定义的处理，具体处理逻辑可以通过插件来实现。在Elasticsearch中，由于Ingest Pipeline会比较耗费CPU等资源，可以设置专门的Ingest Node，专门用来处理Ingest Pipeline逻辑。<br> 如果当前Node不能执行Ingest Pipeline，则会将请求发给另一台可以执行Ingest Pipeline的Node。</p> 
<p><strong>2.Auto Create Index</strong><br> 判断当前Index是否存在，如果不存在，则需要自动创建Index，这里需要和Master交互。也可以通过配置关闭自动创建Index的功能。</p> 
<p><strong>3.Set Routing</strong><br> 设置路由条件，如果Request中指定了路由条件，则直接使用Request中的Routing，否则使用Mapping中配置的，如果Mapping中无配置，则使用默认的_id字段值。<br> 在这一步中，如果没有指定id字段，则会自动生成一个唯一的_id字段，目前使用的是UUID。</p> 
<p><strong>4.Construct BulkShardRequest</strong><br> 由于Bulk Request中会包括多个(Index/Update/Delete)请求，这些请求根据routing可能会落在多个Shard上执行，这一步会按Shard挑拣Single Write Request，同一个Shard中的请求聚集在一起，构建BulkShardRequest，每个BulkShardRequest对应一个Shard。</p> 
<p><strong>5.Send Request To Primary</strong><br> 这一步会将每一个BulkShardRequest请求发送给相应Shard的Primary Node。</p> 
<h5><a id="Primary_Node_583"></a>Primary Node</h5> 
<p>Primary 请求的入口是在PrimaryOperationTransportHandler的messageReceived.</p> 
<p><strong>1.Index or Update or Delete</strong><br> 循环执行每个Single Write Request，对于每个Request，根据操作类型(CREATE/INDEX/UPDATE/DELETE)选择不同的处理逻辑。<br> 其中，Create/Index是直接新增Doc，Delete是直接根据_id删除Doc，Update会稍微复杂些，我们下面就以Update为例来介绍。</p> 
<p><strong>2.Translate Update To Index or Delete</strong><br> 这一步是Update操作的特有步骤，在这里，会<strong>将Update请求转换为Index或者Delete请求</strong>。首先，会通过GetRequest查询到已经存在的同_id Doc（如果有）的完整字段和值（依赖_source字段），然后和请求中的Doc合并。同时，这里会获取到读到的Doc版本号，记做V1。</p> 
<p><strong>3.Parse Doc</strong><br> 这里会解析Doc中各个字段。生成ParsedDocument对象，同时会生成uid Term。在Elasticsearch中，_uid = type # _id，对用户，_Id可见，而Elasticsearch中存储的是_uid。这一部分生成的ParsedDocument中也有Elasticsearch的系统字段，大部分会根据当前内容填充，部分未知的会在后面继续填充ParsedDocument。</p> 
<p><strong>4.Update Mapping</strong><br> Elasticsearch中有个自动更新Mapping的功能，就在这一步生效。会先挑选出Mapping中未包含的新Field，然后判断是否运行自动更新Mapping，如果允许，则更新Mapping。</p> 
<p><strong>5.Get Sequence Id and Version</strong><br> 由于当前是Primary Shard，则会从SequenceNumber Service获取一个sequenceID和Version。SequenceID在Shard级别每次递增1，SequenceID在写入Doc成功后，会用来初始化LocalCheckpoint。Version则是根据当前Doc的最大Version递增1。</p> 
<p><strong>6.Add Doc To Lucene</strong><br> 这一步开始的时候会给特定_uid加锁，然后判断该_uid对应的Version是否等于之前Translate Update To Index步骤里获取到的Version，如果不相等，则说明刚才读取Doc后，该Doc发生了变化，出现了版本冲突，这时候会抛出一个VersionConflict的异常，该异常会在Primary Node最开始处捕获，重新从“Translate Update To Index or Delete”开始执行。</p> 
<p>如果Version相等，则继续执行，如果已经存在同id的Doc，则会调用Lucene的UpdateDocument(uid, doc)接口，先根据uid删除Doc，然后再Index新Doc。如果是首次写入，则直接调用Lucene的AddDocument接口完成Doc的Index，AddDocument也是通过UpdateDocument实现。</p> 
<p>这一步中有个问题是，如何保证Delete-Then-Add的原子性，怎么避免中间状态时被Refresh？答案是在开始Delete之前，会加一个Refresh Lock，禁止被Refresh，只有等Add完后释放了Refresh Lock后才能被Refresh，这样就保证了Delete-Then-Add的原子性。</p> 
<p>Lucene的UpdateDocument接口中就只是处理多个Field，会遍历每个Field逐个处理，处理顺序是invert index，store field，doc values，point dimension，后续会有文章专门介绍Lucene中的写入。</p> 
<p><strong>7.Write Translog</strong><br> 写完Lucene的Segment后，会以key-value的形式写TransLog，Key是_id，Value是Doc内容。当查询的时候，如果请求是GetDocByID，则可以直接根据_id从TransLog中读取到，满足NoSQL场景下的实时性要去。</p> 
<p>需要注意的是，这里只是写入到内存的TransLog，是否Sync到磁盘的逻辑还在后面。</p> 
<p>这一步的最后，会标记当前SequenceID已经成功执行，接着会更新当前Shard的LocalCheckPoint。</p> 
<p><strong>8.Renew Bulk Request</strong><br> 这里会重新构造Bulk Request，原因是前面已经将UpdateRequest翻译成了Index或Delete请求，则后续所有Replica中只需要执行Index或Delete请求就可以了，不需要再执行Update逻辑，一是保证Replica中逻辑更简单，性能更好，二是保证同一个请求在Primary和Replica中的执行结果一样。</p> 
<p><strong>9.Flush Translog</strong><br> 这里会根据TransLog的策略，选择不同的执行方式，要么是立即Flush到磁盘，要么是等到以后再Flush。Flush的频率越高，可靠性越高，对写入性能影响越大。</p> 
<p><strong>10.Send Requests To Replicas</strong><br> 这里会将刚才构造的新的Bulk Request并行发送给多个Replica，然后等待Replica的返回，这里需要等待所有Replica返回后（可能有成功，也有可能失败），Primary Node才会返回用户。如果某个Replica失败了，则Primary会给Master发送一个Remove Shard请求，要求Master将该Replica Shard从可用节点中移除。</p> 
<p>这里，同时会将SequenceID，PrimaryTerm，GlobalCheckPoint等传递给Replica。</p> 
<p>发送给Replica的请求中，Action Name等于原始ActionName + [R]，这里的R表示Replica。通过这个[R]的不同，可以找到处理Replica请求的Handler。</p> 
<p><strong>11.Receive Response From Replicas</strong><br> Replica中请求都处理完后，会更新Primary Node的LocalCheckPoint。</p> 
<h5><a id="Replica_Node_635"></a>Replica Node</h5> 
<p>Replica 请求的入口是在ReplicaOperationTransportHandler的messageReceived.</p> 
<p><strong>1.Index or Delete</strong><br> 根据请求类型是Index还是Delete，选择不同的执行逻辑。这里没有Update，是因为在Primary Node中已经将Update转换成了Index或Delete请求了。</p> 
<p><strong>2.Parse Doc</strong><br> <strong>3.Update Mapping</strong><br> 以上都和Primary Node中逻辑一致。</p> 
<p><strong>4.Get Sequence Id and Version</strong><br> Primary Node中会生成Sequence ID和Version，然后放入ReplicaRequest中，这里只需要从Request中获取到就行。</p> 
<p><strong>5.Add Doc To Lucene</strong><br> 由于已经在Primary Node中将部分Update请求转换成了Index或Delete请求，这里只需要处理Index和Delete两种请求，不再需要处理Update请求了。比Primary Node会更简单一些。</p> 
<p><strong>6.Write Translog</strong><br> <strong>7.Flush Translog</strong><br> 以上都和Primary Node中逻辑一致。</p> 
<h4><a id="_656"></a>数据分片</h4> 
<h3><a id="_657"></a>查询</h3> 
<p>Elasticsearch查询的详细过程是复杂的，而它的核心部分就是倒排索引。这里将概述查询的过程，包括分片的运用、主从节点的交互以及数据排序和结果汇总。</p> 
<ol><li> <p><strong>查询分发</strong></p> <p>当一个查询到达Elasticsearch时，它首先被发送到一个协调节点（coordinating node）。这个节点负责解析查询，确定需要从哪些分片（shards）检索数据，并将查询请求转发到这些分片所在的节点。</p> </li><li> <p><strong>主从节点交互</strong></p> <p>在Elasticsearch中，每个索引被分为多个分片，并且每个分片可以有一个或多个副本。副本分为主副本（primary shard）和从副本（replica shard）。查询可以在主分片或任何一个从分片上执行，这有助于分散读取负载并提高查询性能。协调节点通常会根据分片的当前负载情况来决定发送查询请求给主分片或某个从分片。</p> </li><li> <p><strong>本地查询处理</strong></p> <p>查询被发送到负责存储相关分片的节点后，每个节点上的Elasticsearch进程会执行实际的查询操作。查询操作通常分为两个阶段：</p> 
  <ul><li> <p><strong>查询阶段（Query Phase）</strong>：在这一阶段中，Elasticsearch会检查查询条件并从倒排索引中查找匹配的文档ID。由于Elasticsearch使用的是倒排索引，它可以非常高效地进行文本搜索。</p> </li><li> <p><strong>取回阶段（Fetch Phase）</strong>：一旦文档ID被查找到，Elasticsearch就会进入取回阶段，此时会根据需要从存储中检索文档的完整内容。如果查询包括了<strong>排序或聚合</strong>，还可能需要在这个阶段对结果进行进一步处理。</p> </li></ul> </li><li> <p><strong>结果排序与剪枝</strong></p> <p>如果查询请求指定了排序条件，各个节点需要在本地对查询结果进行排序。每个节点只返回最顶端的结果（例如，如果请求指定了<code>size: 10</code>，则每个节点只返回排名前10的结果）给协调节点，这样可以避免回传大量不必要的数据，提高效率。</p> </li><li> <p><strong>结果汇总</strong></p> <p>协调节点收到所有涉及的节点返回的结果后，会进行最终的排序和汇总。然后协调节点会将最终的查询结果返回给客户端。</p> </li><li> <p><strong>缓存</strong></p> <p>Elasticsearch还有一个查询缓存机制，它会缓存经常被执行的过滤器查询的结果。这能够使得相同或相似的后续查询更快地返回结果。</p> </li></ol> 
<p>查询复杂性可以根据查询中涉及的文档数量、查询类型（如文本搜索、范围查询等）、是否有聚合操作等因素有很大的不同。尽管Elasticsearch旨在使查询尽可能快速，但一些复杂的查询还是会因为涉及大量的数据处理和传输而耗费较多时间。对于真正的性能要求，通常需要结合监控、索引优化、查询优化等一系列措施来确保系统的查询响应效率。</p> 
<h3><a id="_692"></a>分析器</h3> 
<h4><a id="_693"></a>什么是分析</h4> 
<p>分析(analysis)是在文档被发送并加入倒排索引之前,Elasticsearch在其主体上进行的操作。在文档被加入索引之前,Elasticsearch让每个被分析字段经过一系列的处理步骤。</p> 
<ol><li><strong>字符过滤</strong>：使用字符过滤器转变字符。</li><li><strong>文本切分为分词</strong>：将文本切分为单个或多个分词。</li><li><strong>分词过滤</strong>：使用分词过滤器转变每个分词。</li><li><strong>分词索引</strong>：将这些分词存储到索引中。</li></ol> 
<p>使用到ElasticSearch的<strong>字符过滤器</strong>、<strong>分词器</strong>、<strong>分词过滤器</strong>和<strong>分词索引</strong>，它们便组成了分析器（analyzer）。</p> 
<p><strong>字符过滤器</strong><br> 字符过滤器将特定的字符序列转变为其他的字符序列。可以用于将HTML从文本中剥离，或者是将任意数量的字符转化为其他字符（也许是将"i love u 2”缩写的短消息纠正为"i love you too”），或者将“&amp;”替换为“and”等。</p> 
<p><strong>分词器</strong><br> 分词器是从文本片段生成的，可能会产生任意数量（甚至是0）的分词(token)。例如，在英文中一个通用的分词是标准分词器，它根据空格、换行和破折号等其他字符，将文本分割为分词。</p> 
<p>例如将字符串“share your experience with NoSql and big data technologies"分解为分词share、your、experience、with、NoSql、and、big、data 和 technologies。</p> 
<p><strong>分词过滤器</strong><br> 分词器文本块被转换为分词，将会对每个分词运用分词过滤器（tokenfilter）。分词过滤器可以将一个分词作为输人，然后根据需要进行修改，添加或者是删除。</p> 
<p>最为有用的和常用的分词过滤器是<strong>小写分词过滤器</strong>，它将输人的分词变为小写，确保在搜索词条"nosql"的时候，可以发现关于“nosql"的聚会。分词可以经过多于1个的分词过滤器，每个过滤器对分词进行不同的操作，将数据塑造为最佳的形式，便于之后的索引。</p> 
<p><strong>分词索引</strong><br> 当经历了零个或者多个分词过滤器，它们将被发送到Lucene进行文档的索引。这些分词组成了<strong>倒排索引</strong>。</p> 
<h4><a id="_719"></a>配置分析器</h4> 
<p>配置分析器方式：</p> 
<ol><li>当创建索引的时候，为特定的索引进行设置。</li><li>在Elasticsearch的配置文件中，设置全局的分析器。</li></ol> 
<p>通常来说，出于灵活性的考虑，在创建索引或者是配置映射的时候，指定分析器是更简单的。</p> 
<h5><a id="_727"></a>创建索引配置分析器</h5> 
<p>分为两种，一种是索引全局分析器，另一种某个字段自定分析器</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_order_test
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"number_of_shards"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token string">"number_of_replicas"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token string">"index"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
	  # 分析器
      <span class="token string">"analysis"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          # 设置分析器名称
          <span class="token string">"myCustomAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
            #定制化类型
            <span class="token string">"token"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
            # 字符过滤器
            <span class="token string">"char_filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"myCustomCharFilter"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            # 分词器
            <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"myCustomTokenizer"</span><span class="token punctuation">,</span>
            # 分词过滤器
            <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"myCustomFilter1"</span><span class="token punctuation">,</span><span class="token string">"myCustomFilter2"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
          <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      # 定义字符过滤器
      <span class="token string">"char_filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"myCustomCharFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
          # 类型为映射（把字符转成其他字符）
          <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"mapping"</span><span class="token punctuation">,</span>
          <span class="token string">"mappings"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"ph=&gt;f"</span><span class="token punctuation">,</span><span class="token string">"u=&gt;you"</span><span class="token punctuation">]</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
      # 分词器
      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"myCustomTokenizer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
	      # 分词器为 letter
          <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"letter"</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      # 分词过滤器
      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"myCustomFilter1"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
          # 小写分词过滤器
          <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"lowercase"</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"myCustomFilter2"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
          # kstem就行词干处理
          <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"kstem"</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span> 
	<span class="token string">"mappings"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		  <span class="token string">"orderTile"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"text"</span><span class="token punctuation">,</span>
		    #指定分词器，es内置有多种analyzer<span class="token operator">:</span>whitespace、standard、simple、stop
		    <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"myCustomAnalyzer"</span>     
		  <span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>如果想一个字段不被任何分析处理，需要增加not_analyzed（7.x之前） 或者 type=keyword（7.x之后）：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_order_test
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"mappings"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		  <span class="token string">"orderTile"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
		    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"keyword"</span>
		  <span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><strong>用多字段类型来存储分析方式不同的文本</strong><br> 通常情况下，可以同时搜索字段分析后的文本和原始、未经分析的文本，是非常有用的。</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span> 
        <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
        <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"raw"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
	         # keyword 即不分词
            <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"keyword"</span>
          <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="ElasticSearch_829"></a>ElasticSearch配置分析器</h5> 
<pre><code class="prism language-yaml"><span class="token key atrule">index</span><span class="token punctuation">:</span>
	<span class="token key atrule">analysis</span><span class="token punctuation">:</span>
		<span class="token key atrule">analyer</span><span class="token punctuation">:</span>
			<span class="token key atrule">myCustomAnalyzer</span><span class="token punctuation">:</span>
				<span class="token key atrule">type</span><span class="token punctuation">:</span> custom
				<span class="token key atrule">char_filter</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>customCharFilter<span class="token punctuation">]</span>
				<span class="token key atrule">tokenizer</span><span class="token punctuation">:</span> customTokenizer
				<span class="token key atrule">filter</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>customFilter1<span class="token punctuation">,</span>customFilter2<span class="token punctuation">]</span>
			<span class="token key atrule">char_filter</span><span class="token punctuation">:</span>
				<span class="token key atrule">customCharFilter</span><span class="token punctuation">:</span>
					<span class="token key atrule">type</span><span class="token punctuation">:</span> mapping
					<span class="token key atrule">mappings</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"pf=&gt;f"</span><span class="token punctuation">,</span><span class="token string">"u=&gt;you"</span><span class="token punctuation">]</span>	
			<span class="token key atrule">tokenizer</span><span class="token punctuation">:</span>
				<span class="token key atrule">customTokenizer</span><span class="token punctuation">:</span>
					<span class="token key atrule">type</span><span class="token punctuation">:</span> letter
			<span class="token key atrule">filter</span><span class="token punctuation">:</span>
				<span class="token key atrule">customFilter1</span><span class="token punctuation">:</span>
					<span class="token key atrule">type</span><span class="token punctuation">:</span> lowercase		
				<span class="token key atrule">customFilter2</span><span class="token punctuation">:</span>
					<span class="token key atrule">type</span><span class="token punctuation">:</span> kstem
</code></pre> 
<h4><a id="_854"></a>内置分析器</h4> 
<p>一个分析器包括一个可选的字符过滤器、一个单个分词器、0个或多个分词过滤器。</p> 
<h5><a id="standard_857"></a>standard-标准分析器</h5> 
<p>当没有指定分析器的时候，标准分析器（standard analyzer）是文本的默认分析器。</p> 
<p>它综合了对大多欧洲语言来说合理的默认模块，包括标准分词器、标准分词过滤器、小写转换分词过滤器和停用词分词过滤器。</p> 
<h5><a id="simple_862"></a>simple-简单分析器</h5> 
<p>简单分析器（simpleanalyzer）就是那么简单！它只使用了小写转换分词器，这意味着在非字母处进行分词，并将分词自动转变为小写。</p> 
<p>简单分析器对于亚洲语言来说效果不佳，因为亚洲语言不是根据空白来分词，所以请仅仅针对欧洲语言使用它。</p> 
<h5><a id="whitespace_867"></a>whitespace-空白分析器</h5> 
<p>空白分析器（whitespaceanalyzer）什么事情都不做，只是根据空白将文本切分为若干分词，非常简单！</p> 
<h5><a id="stop_870"></a>stop-停用词分析器</h5> 
<p>停用词分析器（analyzer）和简单分析器的行为很相像，只是在分词流中额外地过滤了停用词。</p> 
<h5><a id="keyword_875"></a>keyword-关键词分析器</h5> 
<p>关键词分析器（keywordanalyzer）将整个字段当作一个单独的分词。请记住，最好是将index设置指定为notanalyzed（7.x只有改为 type=keyword方式），而不是在映射中使用关键词分析器。</p> 
<h5><a id="pattern_878"></a>pattern-模式分析器</h5> 
<p>模板分析器（pattern analyzer）允许你指定一个分词切分的模式。但是，由于可能无论如何都要指定模式，通常更有意义的做法是使用定制分析器，组合现有的模式分词器和所需的分词过滤器。</p> 
<p>Elasticsearch 的 Pattern Analyzer 是一个基于正则表达式的分词器(tokenizer)，它允许用户使用正则表达式来定义如何将文本拆分为词(token)。下面举一个 Pattern Analyzer 的使用例子。</p> 
<p>假设我们想要对一段日志数据进行分析，这些日志数据的格式大致如下：</p> 
<pre><code>127.0.0.1 - - [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326
</code></pre> 
<p>我们可能对从这些日志中提取出如 IP 地址、日期时间、请求方法、请求资源、响应状态码和响应大小等信息感兴趣。下面是如何在 Elasticsearch 中定义一个索引，使用 Pattern Analyzer 来实现这一目标。</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>log_index
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"analysis"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"log_analyzer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"pattern"</span><span class="token punctuation">,</span>
          <span class="token string">"pattern"</span><span class="token operator">:</span> <span class="token string">"\\s+"</span><span class="token punctuation">,</span> <span class="token comment">// 使用空白字符做为分词分隔符</span>
          <span class="token string">"lowercase"</span><span class="token operator">:</span> <span class="token boolean">false</span> <span class="token comment">// 不将词条转为小写</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"log"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
        <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"log_analyzer"</span><span class="token punctuation">,</span>
        <span class="token string">"fielddata"</span><span class="token operator">:</span> <span class="token boolean">true</span> <span class="token comment">// 由于后续可能需要对 log 字段进行聚合操作，需要开启 fielddata</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>在这个例子中，我们定义了一个名为 <code>log_analyzer</code> 的分词器，其类型为 <code>pattern</code>。我们指定了一个简单的正则表达式作为 <code>pattern</code>，它用于匹配一个或多个空白字符(<code>\\s+</code>)，以此来分割日志的每一部分。我们还设置了 <code>lowercase</code> 为 <code>false</code> 来保持文本的原始大小写，因为对于日志分析，区分大小写可能是重要的。</p> 
<p>创建了索引和相应的 Pattern Analyzer 之后，就可以开始索引数据了：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>log_index<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"log"</span><span class="token operator">:</span> <span class="token string">"127.0.0.1 - - [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>接着，你可以通过执行搜索查询或聚合操作来分析日志数据：</p> 
<pre><code class="prism language-json"><span class="token constant">GET</span> <span class="token operator">/</span>log_index<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"log"</span><span class="token operator">:</span> <span class="token string">"GET"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这个搜索将会返回包含 <code>GET</code> 请求方法的所有日志条目。</p> 
<p>请注意，为简单起见，这个例子只用了空格来分割日志条目，并没有详细地去解析每个字段，如 IP 地址等。在实际应用中，你可能需要一个更复杂的正则表达式来准确提取和分析每个感兴趣的部分。此外，对于复杂的日志分析，可能需要定义更详细的字段映射来区分不同的日志组成部分，如分别用不同字段存储 IP 地址、日期时间等。</p> 
<h5><a id="snowball_945"></a>snowball-雪球分析器</h5> 
<p>雪球分析器（snowbal1analyzer）除了使用标准的分词器和分词过滤器（和标准分析器一样），也使用了小写分词过滤器和停用词过滤器。它还使用了雪球词干器对文本进行词干提取。</p> 
<h4><a id="_951"></a>内置分词器</h4> 
<h5><a id="standard_952"></a>standard-标准分词器</h5> 
<p>标准分词器（standardtokenizer）是一个基于语法的分词器，对于大多数欧洲语言来说是不错的。它还处理了Unicode文本的切分，不过分词默认的最大长度是255。它也移除了逗号和句号这样的标点符号。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"The intersecting buckets e.g A&amp;C are labelled using a combination of the two filter names with a default separator of &amp;. "</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="keyword_963"></a>keyword-关键词分词器</h5> 
<p>关键词分词器（keyword tokenizer）是一种简单的分词器，将整个文本作为单个的分词，提供给分词过滤器。只想应用分词过滤器，而不做任何分词操作时，它可能非常有用。</p> 
<h5><a id="letter_966"></a>letter-字母分词器</h5> 
<p>字母分词器（letter tokenizer）根据非字母的符号，将文本切分成分词。例如，对于句子"share your experience with NoSql &amp; big data technologies”分词是share、your、experience、with、NoSql、big、data、technologies，因为&amp;、空格和句号都不是字母：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"letter"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"share your experience with NoSql &amp; big data technologies "</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="lowercase_977"></a>lowercase-小写分词器</h5> 
<p>小写分词器（lowercase tokenizer）结合了常规的字母分词器和小写分词过滤器（如你所想，它将整个分词转化为小写）的行为。通过一个单独的分词器来实现的主要原因是，一次进行两项操作会获得更好的性能。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"lowercase"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"I am man. "</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>分词结果是i、am、man，I转换成i。</p> 
<h5><a id="whitespace_989"></a>whitespace-空白分词器</h5> 
<p>空白分词器（whitespace tokenizer）通过空白来分隔不同的分词，空白包括空格、制表符、换行等。请注意，这种分词器不会删除任何标点符号。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"whitespace"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"share your experience with NoSql &amp; big data technologies"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>分词结果也包含“&amp;”。</p> 
<h5><a id="pattern_1000"></a>pattern-模式分词器</h5> 
<p>模式分词器（pattern tokenizer）允许指定一个任意的模式，将文本切分为分词。被指定的模式应该匹配间隔符号。</p> 
<h5><a id="uax_url_emall___1003"></a>uax_url_emall - 电子邮件分词器</h5> 
<p>在处理英语单词的时候，标准分词器是非常好的选择。但是，当下存在不少以网站地址和电子邮件地址结束的文本。标准分析器可能在你未注意的地方对其进行了切分。</p> 
<p>1.电子邮件分词</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"uax_url_email"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"address is joy@gmail.com"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>分词结果：address 、 is 、 joy@gmail.com</p> 
<p>2.网址分词</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"uax_url_email"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"domain is https://www.baidu.com"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>分词结果：domain、is、https://www.baidu.com?s=iPhone</p> 
<h5><a id="path_hierarchy___1028"></a>path_hierarchy - 路径层次分词器</h5> 
<p>路径层次分词器（path hierarchy tokenizer）允许以特定的方式索引文件系统的路径，这样在搜索时，共享同样路径的文件将被作为结果返回。例如，假设有一个文件名想要索引，看上去是这样的/usr/local/var/log/elasticsearch.logo路径层次分词器将其切分为：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"path_hierarchy"</span><span class="token punctuation">,</span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"/usr/local/var/log/elasticsearch.logo"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>结果：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"/usr"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"/usr/local"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">10</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"/usr/local/var"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">14</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"/usr/local/var/log"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">18</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"/usr/local/var/log/elasticsearch.logo"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">37</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>

</code></pre> 
<h4><a id="_1083"></a>分词过滤器</h4> 
<h5><a id="standard_1084"></a>standard-标准分词过滤器</h5> 
<p>不要认为标准分词过滤器（standard token filter）进行了什么复杂的计算，实际上它什么事情也没做！在更老版本的Lucene中，它用于去除单词结尾的"'s”字符，还有不必要的句点字符，但是现在这些都被其他的分词过滤器和分词器处理掉了。</p> 
<p>注意：标准分词过滤器在 7.x之后被移除。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"I am man."</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
		<span class="token punctuation">{<!-- --></span><span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">}</span>
	<span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/a2/0f/nEm3uWhf_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="lowercase___1099"></a>lowercase - 小写分词过滤器</h5> 
<p>小写分词过滤器（lowercase token filter）只是做了这件事：将任何经过的分词转换为小写。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"I am man."</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
		<span class="token punctuation">{<!-- --></span><span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"lowercase"</span><span class="token punctuation">}</span>
	<span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"i am man."</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">9</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>创建索引时指定分词过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customLowercaseAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"lowercase"</span><span class="token punctuation">,</span><span class="token string">"stop"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customLowercaseAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>查询name的词条：</p> 
<pre><code class="prism language-json"># 写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"i am Jay"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 分析词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>由于使用了lowercase分词过滤器，Jay会被转换成jay，通过Jay或者jay都可以进行搜索。</p> 
<h5><a id="length___1186"></a>length - 长度分词过滤器</h5> 
<p>长度分词过滤器（length token filter）将长度超出最短和最长限制范围的单词过滤掉。举个例子，如果将min设置为2，并将max设置为5，任何小于2个字符和任何大于5个字符的分词将会被移除。</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"I am man."</span>,
	<span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
	<span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>
		<span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"length"</span>,
		<span class="token string">"min"</span><span class="token builtin class-name">:</span> <span class="token number">2</span>,
		<span class="token string">"max"</span><span class="token builtin class-name">:</span> <span class="token number">5</span>
	<span class="token punctuation">}</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>max：参数表示最大分词长度，默认为Integer.MAX_VALUE，就是2147483647。<br> min：则表示最小长度，默认为0</p> 
<p>输出结果：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"am"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">4</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"man"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">5</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">8</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>I和符号.长度不在2-5之间都被过滤。</p> 
<p>创建索引时指定分词过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customLengthFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"length"</span><span class="token punctuation">,</span>
			      <span class="token string">"min"</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span>
			      <span class="token string">"max"</span><span class="token operator">:</span><span class="token string">"5"</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
			  
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"customLengthAnaylyzer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"custom"</span><span class="token punctuation">,</span>
						<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
						<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"customLengthFilter"</span><span class="token punctuation">]</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customLengthAnaylyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"i am Jay"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>查询name的词条：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"_index"</span> <span class="token builtin class-name">:</span> <span class="token string">"mall_user"</span>,
  <span class="token string">"_type"</span> <span class="token builtin class-name">:</span> <span class="token string">"_doc"</span>,
  <span class="token string">"_id"</span> <span class="token builtin class-name">:</span> <span class="token string">"1"</span>,
  <span class="token string">"_version"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
  <span class="token string">"found"</span> <span class="token builtin class-name">:</span> true,
  <span class="token string">"took"</span> <span class="token builtin class-name">:</span> <span class="token number">0</span>,
  <span class="token string">"term_vectors"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"name"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"field_statistics"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"sum_doc_freq"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>,
        <span class="token string">"doc_count"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
        <span class="token string">"sum_ttf"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>
      <span class="token punctuation">}</span>,
      <span class="token string">"terms"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"Jay"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
          <span class="token string">"tokens"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>,
              <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">5</span>,
              <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">8</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span>,
        <span class="token string">"am"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
          <span class="token string">"tokens"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
              <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>,
              <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">4</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>从结果看，i 长度不在2-5之间，已被过滤。</p> 
<p>查询：</p> 
<pre><code class="prism language-json"># 查询 name 包含 i 的信息，输出为空
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"i"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

# 查询 name 包含 Jay 的信息，找到对应的记录
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"Jay"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="stop___1366"></a>stop - 停用词分词过滤器</h5> 
<p>停用词分词过滤器（stop token filter）将停用词从分词流中移除。对于英文而言，这意味着停用词列表中的所有分词都将每会被完全移除。可以添加指定一个待移除单词的列表。</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"I am man."</span>,
	<span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
	<span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>
		<span class="token string">"type"</span><span class="token builtin class-name">:</span> <span class="token string">"stop"</span>,
		<span class="token string">"stopwords"</span>:<span class="token punctuation">[</span><span class="token string">"am"</span><span class="token punctuation">]</span>
	<span class="token punctuation">}</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><code>stopwords</code>:自定义的待移除单词的列表。</p> 
<p>输出：</p> 
<pre><code class="prism language-bash"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"I"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">0</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">1</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span>,
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token builtin class-name">:</span> <span class="token string">"man"</span>,
      <span class="token string">"start_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">5</span>,
      <span class="token string">"end_offset"</span> <span class="token builtin class-name">:</span> <span class="token number">8</span>,
      <span class="token string">"type"</span> <span class="token builtin class-name">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span>,
      <span class="token string">"position"</span> <span class="token builtin class-name">:</span> <span class="token number">2</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>从结果来看，已经移除am单词。</p> 
<p>创建索引时指定分词过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customStopFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"stop"</span><span class="token punctuation">,</span>
			      <span class="token string">"stopwords"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"am"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
				    <span class="token string">"customStopAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
				      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
				      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
				      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"customStopFilter"</span><span class="token punctuation">]</span>
				    <span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customStopAnalyzer"</span><span class="token punctuation">,</span> 
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>除<code>stopwords</code>参数还，也可以使用<code>stopwords_path</code>参数指定停用词文件。</p> 
<p>写入数据：</p> 
<pre><code class="prism language-bash">POST /mall_user/_doc/1
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token builtin class-name">:</span><span class="token string">"am Jay"</span>,
  <span class="token string">"age"</span><span class="token builtin class-name">:</span><span class="token string">"20"</span>,
  <span class="token string">"birth"</span><span class="token builtin class-name">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>查询分词信息：</p> 
<pre><code class="prism language-bash">POST /mall_user/_doc/1/_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/bc/75/6NqFilN2_o.png" alt="在这里插入图片描述"><br> 只有Jay分词。</p> 
<p>使用am词搜索：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"am"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>搜索结果为空。</p> 
<h5><a id="reverse___1489"></a>reverse - 反转分词过滤器</h5> 
<p>反转分词过滤器（reverse token filter）允许处理一个分词流，并反转每个分词。</p> 
<p>使用反转分词过滤器测试文本：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"I am man."</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"reverse"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>创建索引时指定分析器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customReverseAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"reverse"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customReverseAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据并且查看name词条和搜索：</p> 
<pre><code class="prism language-json">#写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"i am Jay"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 查询name的词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>

# 搜索
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"Jay"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="unique___1570"></a>unique - 唯一分词过滤器</h5> 
<p>唯一分词过滤器（unique token filter）只保留唯一的分词，它保留第一个匹配分词的元数据，而将其后出现的重复删除：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"foo bar foo bar"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"unique"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>创建索引时指定唯一分词过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customUniqueAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"unique"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customUniqueAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>查看name词条：</p> 
<pre><code class="prism language-json"># 写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"foo bar foo bar"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 查看name词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="synonym___1640"></a>synonym - 同义词分词过滤器</h5> 
<p>同义词分词过滤器（synonym token filter）在分词流中的同样位移处，使用关键词的同义词取代原分词。</p> 
<p>例如文本“i have a automobile”，automobile 的同义词为car。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"i have a automobile"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"synonym"</span><span class="token punctuation">,</span>
	    <span class="token string">"synonyms"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"automobile=&gt;car"</span><span class="token punctuation">]</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出结果没有automobile，已替换成car。</p> 
<p>创建索引时使用同义词：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customSynonymFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"synonym"</span><span class="token punctuation">,</span>
			      <span class="token string">"synonyms"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"automobile=&gt;car"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customSynonymAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"customSynonymFilter"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customSynonymAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据和测试name的词条：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"i have a automobile"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 查看词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>

# 使用car 或者 automobile搜索 都可以搜索到结果
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"automobile"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"car"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="ngram__N_1741"></a>ngram - N元语法分词过滤器</h5> 
<p>什么是N元语法？<br> N元语法（ngram）是Elasticsearch中更为独特的分词方式。N元语法是将一个单词切分为多个子单词。</p> 
<p>N元语法具体表现为1元语法、2元语法等。</p> 
<p>1元语法（1-ngram）分词“automobile”，结果时a、u、t、o、m、o、b、i、l、e。</p> 
<p>2元语法（2-ngram）分词“automobile”，结果时au、ut、to、om、mo、ob、bi、il、le。</p> 
<p>由此可以看出N元语法是按字符的顺序往后截取N个字符。</p> 
<p>使用N元语法分析文本：</p> 
<pre><code class="prism language-json"># <span class="token number">1</span>元语法
<span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"automobile"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"ngram"</span><span class="token punctuation">,</span>
	    <span class="token string">"min_gram"</span><span class="token operator">:</span><span class="token number">1</span><span class="token punctuation">,</span>
	    <span class="token string">"max_gram"</span><span class="token operator">:</span><span class="token number">1</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>

#<span class="token number">2</span>元语法
<span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"automobile"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"ngram"</span><span class="token punctuation">,</span>
	    <span class="token string">"min_gram"</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span>
	    <span class="token string">"max_gram"</span><span class="token operator">:</span><span class="token number">2</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p><code>min_gram</code>:最小分隔的大小；<br> <code>max_gram</code>:最大分隔的大小；</p> 
<p><code>max_gram</code> 和 <code>min_gram</code> 的差值需要在<code>index.max_ngram_diff</code>之间。</p> 
<p>如果min_gram=1、max_gram=2，automobile分割成：a、u、t、o、m、o、b、i、l、e、au、ut、to、om、mo、ob、bi、il、le。</p> 
<p>创建索引时使用N元语法：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customNgramFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"ngram"</span><span class="token punctuation">,</span>
			      <span class="token string">"min_gram"</span><span class="token operator">:</span><span class="token number">1</span><span class="token punctuation">,</span>
			      <span class="token string">"max_gram"</span><span class="token operator">:</span><span class="token number">1</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customNgramAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"customNgramFilter"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customNgramAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据并且查看name词条、通过name查询：</p> 
<pre><code class="prism language-json"># 写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"automobile"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 查看词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>

# 查询name类似a的文档，能查询到
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"a"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="edge_ngram__N_1866"></a>edge_ngram - 侧边N元语法</h5> 
<p>侧边N元语法是普通N元语法切分的一种变体，仅仅从左边的边缘开始构建N元语法。</p> 
<p>例如“automobile”，从左边“a”的边缘开始截取N个词。</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"automobile"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"edge_ngram"</span><span class="token punctuation">,</span>
	    <span class="token string">"min_gram"</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span>
	    <span class="token string">"max_gram"</span><span class="token operator">:</span><span class="token number">4</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出au、aut、auto，此时已经注意到超过max_ngram部分不会分割（意味着不能搜索）。</p> 
<p>创建索引时使用侧边N元语法：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customEdgeNgramFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"edge_ngram"</span><span class="token punctuation">,</span>
			      <span class="token string">"min_gram"</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span>
			      <span class="token string">"max_gram"</span><span class="token operator">:</span><span class="token number">4</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customEdgeNgramAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"customEdgeNgramFilter"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customEdgeNgramAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据、查看name词条和根据name查询数据：</p> 
<pre><code class="prism language-json">#写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"automobile"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

#查询name词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>

# 根据name查询
# au、aut、auto 可以查询到数据
# mobile 字符是不能搜索到数据的，超过max_gram没有分割
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"au"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<h5><a id="shingle___1964"></a>shingle - 滑动窗口分词过滤器</h5> 
<p>滑动窗口分词过滤器（shingles），和N元语法以及侧边N元语法沿用了同样的方式。滑动窗口分词过滤器基本上是分词级别的N元语法，而不是字符级别的N元语法。</p> 
<p>例如“i has a automobile”分割成：i、i has、has、has a、a、a automobile：</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"i has a automobile"</span>,
	<span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
	<span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token builtin class-name">:</span><span class="token string">"shingle"</span>,
	    <span class="token string">"min_shingle_size"</span>:2,
	    <span class="token string">"max_shingle_size"</span>:2
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>创建索引时使用滑动窗口分词过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>mall_user
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"index"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"analysis"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			  <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customShingleFilter"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"shingle"</span><span class="token punctuation">,</span>
			      <span class="token string">"min_shingle_size"</span><span class="token operator">:</span><span class="token number">2</span><span class="token punctuation">,</span>
			      <span class="token string">"max_shingle_size"</span><span class="token operator">:</span><span class="token number">2</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span><span class="token punctuation">,</span>
			  <span class="token string">"analyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			    <span class="token string">"customShingleAnalyzer"</span><span class="token operator">:</span><span class="token punctuation">{<!-- --></span>
			      <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"custom"</span><span class="token punctuation">,</span>
			      <span class="token string">"tokenizer"</span><span class="token operator">:</span><span class="token string">"standard"</span><span class="token punctuation">,</span>
			      <span class="token string">"filter"</span><span class="token operator">:</span><span class="token punctuation">[</span><span class="token string">"customShingleFilter"</span><span class="token punctuation">]</span>
			    <span class="token punctuation">}</span>
			  <span class="token punctuation">}</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"properties"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
			<span class="token string">"name"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
				<span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"customShingleAnalyzer"</span><span class="token punctuation">,</span>
				<span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
					<span class="token string">"raw"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
						<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"keyword"</span>
					<span class="token punctuation">}</span>
				<span class="token punctuation">}</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"age"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"integer"</span>
			<span class="token punctuation">}</span><span class="token punctuation">,</span>
			<span class="token string">"birth"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
				<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"date"</span><span class="token punctuation">,</span>
				<span class="token string">"format"</span><span class="token operator">:</span> <span class="token string">"yyyy-MM-dd"</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>写入数据、查询name的词条、使用name查询：</p> 
<pre><code class="prism language-json"># 写入数据
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"name"</span><span class="token operator">:</span><span class="token string">"i has a automobile"</span><span class="token punctuation">,</span>
  <span class="token string">"age"</span><span class="token operator">:</span><span class="token string">"20"</span><span class="token punctuation">,</span>
  <span class="token string">"birth"</span><span class="token operator">:</span><span class="token string">"2002-01-01"</span>
<span class="token punctuation">}</span>

# 查看name的词条
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_doc<span class="token operator">/</span><span class="token number">1</span><span class="token operator">/</span>_termvectors
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>

# 根据name搜索
<span class="token constant">POST</span> <span class="token operator">/</span>mall_user<span class="token operator">/</span>_search
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"query"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"match"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"has"</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre> 
<h5><a id="_2060"></a>提取词干-词干分词过滤器</h5> 
<p>提取词干是将单词缩减到基本或词根的形式。在搜索的时候，这种处理是非常方便的，因为这意味着用户可以匹配单词的复数，以及有同样词根的单词（因此名字称为“提取词干"）。</p> 
<p>提取词干的算法有snowball过滤器、porter_stem过滤器、kstem过滤器。他们表现基本一致，不过在提取词干有多激进的方面有一些细微的差别。这里的“激进"，是指相对于不激进的词干提取器，更为激进的词干提取器会砍掉单词更多的部分。</p> 
<p>snowball过滤器：</p> 
<pre><code class="prism language-bash">POST _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token builtin class-name">:</span> <span class="token string">"administrations administrators Administrate"</span>,
	<span class="token string">"tokenizer"</span><span class="token builtin class-name">:</span> <span class="token string">"standard"</span>,
	<span class="token string">"filter"</span><span class="token builtin class-name">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token builtin class-name">:</span><span class="token string">"snowball"</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>porter_stem 过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"administrations administrators Administrate"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"porter_stem"</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>kstem过滤器：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"administrations administrators Administrate"</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token string">"standard"</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
	  <span class="token punctuation">{<!-- --></span>
	    <span class="token string">"type"</span><span class="token operator">:</span><span class="token string">"kstem"</span>
	  <span class="token punctuation">}</span> 
	 <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出结果对比：</p> 
<table><thead><tr><th align="left">算法</th><th>administrations</th><th>administrators</th><th>Administrate</th></tr></thead><tbody><tr><td align="left">snowball</td><td>administr</td><td>administr</td><td>Administr</td></tr><tr><td align="left">porter_stem</td><td>administr</td><td>administr</td><td>Administr</td></tr><tr><td align="left">kstem</td><td>administration</td><td>administrator</td><td>Administrate</td></tr></tbody></table> 
<p>除此之外，还可以使用词典提取词干</p> 
<p>有的时候，算法词干提取会以一种奇怪的方式来提取单词的词干，因为它们并不理解基层的语言。正因为此，存在更为精确的方式来提取词干，那就是使用单词字典。在Elasticsearch中，可以使用<code>hunspell</code>分词过滤器，结合一个字典，来处理词干。</p> 
<p>基于此，词干提取的质量就和所用字典的质量是直接相关的。词干提取器只能处理字典里存在的单词。</p> 
<p>当创建一个hunspell分析器的时候，字典文件应该是在名为hunspell的目录里，并且hunspell目录和elasticsearch.yml处于同一个目录中。在hunspell目录中，每种语言的字典是一个以其关联地区命名的目录。</p> 
<h4><a id="API_2127"></a>使用分析API分析文本</h4> 
<p>当跟踪信息是如何在Elasticsearch索引中存储的时候，使用分析API来测试分析的过程是十分有用的。API允许你向Elasticsearch发送任何文本，指定所使用的分析器、分词器或者分词过滤器，然后获取分析后的分词。使用标准分析分析了文本"share your experience with NoSql &amp; big data technologies"</p> 
<p>1.使用内置的分析器分析</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"share your experience with NoSql &amp; big data technologies"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token string">"analyzer"</span><span class="token operator">:</span> <span class="token string">"standard"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出结果：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"share"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"your"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">6</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">10</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">1</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"experience"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">11</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">21</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">2</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"with"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">22</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">26</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">3</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"nosql"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">27</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">4</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"big"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">35</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">38</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">5</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"data"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">39</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">43</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">6</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"technologies"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">44</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">56</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"&lt;ALPHANUM&gt;"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">7</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>分析API中最为重要的输出是token键。输出是一组这样映射的列表，代表了处理后的分词（实际上，这些分词将会被写人到索引中）。</p> 
<p>2.通过组合即时使用分析器分析</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> _analyze
<span class="token punctuation">{<!-- --></span>
	<span class="token string">"text"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"share your experience with NoSql &amp; big data technologies"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>

	<span class="token string">"char_filter"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"mapping"</span><span class="token punctuation">,</span>
		<span class="token string">"mappings"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"&amp;=&gt;and"</span><span class="token punctuation">]</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"tokenizer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
		<span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"whitespace"</span>
	<span class="token punctuation">}</span><span class="token punctuation">,</span>
	<span class="token string">"filter"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
		# 转小写
		<span class="token punctuation">{<!-- --></span><span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"lowercase"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
		# 翻转字符
		<span class="token punctuation">{<!-- --></span><span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"reverse"</span><span class="token punctuation">}</span>
	<span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"share"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"your"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">6</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">10</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">1</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"experience"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">11</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">21</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">2</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"with"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">22</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">26</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">3</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"nosql"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">27</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">4</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"and"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">33</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">34</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">5</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"big"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">35</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">38</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">6</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"data"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">39</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">43</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">7</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span>
      <span class="token string">"token"</span> <span class="token operator">:</span> <span class="token string">"technologies"</span><span class="token punctuation">,</span>
      <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">44</span><span class="token punctuation">,</span>
      <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">56</span><span class="token punctuation">,</span>
      <span class="token string">"type"</span> <span class="token operator">:</span> <span class="token string">"word"</span><span class="token punctuation">,</span>
      <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">8</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>3.使用现有索引字段分析器分析</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>mall_order_test<span class="token operator">/</span>_analyze
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"text"</span><span class="token operator">:</span> <span class="token string">"share your experience with NoSql &amp; big data technologies"</span><span class="token punctuation">,</span>
  <span class="token string">"field"</span><span class="token operator">:</span><span class="token string">"orderTitle"</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>使用 mall_order_test 索引中orderTitle字段使用的分词器分析text文本。</p> 
<h4><a id="_2308"></a>查询索引文档的分词信息</h4> 
<p>通过<code>_termvectors</code>查询已经索引的文档orderTitle字段的分词词条（操作消耗比较大）：</p> 
<pre><code class="prism language-bash">POST /mall_order_test/_doc/sZNUsIwBSLqPVZ3tlB7z/_termvectors?pretty<span class="token operator">=</span>true
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"fields"</span>:<span class="token punctuation">[</span><span class="token string">"orderTitle"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string">"_index"</span> <span class="token operator">:</span> <span class="token string">"mall_order_test"</span><span class="token punctuation">,</span>
  <span class="token string">"_type"</span> <span class="token operator">:</span> <span class="token string">"_doc"</span><span class="token punctuation">,</span>
  <span class="token string">"_id"</span> <span class="token operator">:</span> <span class="token string">"sZNUsIwBSLqPVZ3tlB7z"</span><span class="token punctuation">,</span>
  <span class="token string">"_version"</span> <span class="token operator">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
  <span class="token string">"found"</span> <span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
  <span class="token string">"took"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
  <span class="token string">"term_vectors"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    # orderTitle 词条信息
    <span class="token string">"orderTitle"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      <span class="token string">"field_statistics"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
	    # 该字段中所有词条的文档频率之和
        <span class="token string">"sum_doc_freq"</span> <span class="token operator">:</span> <span class="token number">9</span><span class="token punctuation">,</span>
        # 包含该字段的文档数量
        <span class="token string">"doc_count"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        # 字段中所有词条频率之和
        <span class="token string">"sum_ttf"</span> <span class="token operator">:</span> <span class="token number">9</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token string">"terms"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
      	# <span class="token number">16000</span> 词条的结果数据
        <span class="token string">"16000"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          # 词条在该字段中出现的次数
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">7</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">20</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">25</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"iphone15"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">8</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"max"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">13</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">16</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"pro"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">9</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">12</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"元"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">25</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">26</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"单"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">17</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">18</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"订"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">16</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">17</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"金"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">18</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">19</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">"额"</span> <span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
          <span class="token string">"term_freq"</span> <span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
          <span class="token string">"tokens"</span> <span class="token operator">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{<!-- --></span>
              <span class="token string">"position"</span> <span class="token operator">:</span> <span class="token number">6</span><span class="token punctuation">,</span>
              <span class="token string">"start_offset"</span> <span class="token operator">:</span> <span class="token number">19</span><span class="token punctuation">,</span>
              <span class="token string">"end_offset"</span> <span class="token operator">:</span> <span class="token number">20</span>
            <span class="token punctuation">}</span>
          <span class="token punctuation">]</span>
        <span class="token punctuation">}</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>

</code></pre> 
<h3><a id="_2437"></a>快照和恢复</h3> 
<p>Elasticsearch 的快照和恢复功能允许用户备份集群中的数据并在需要时恢复。快照可以是手动触发的，也可以通过自动化机制进行定期备份。以下是有关如何配置和使用 Elasticsearch 自动快照和恢复机制的步骤：</p> 
<h4><a id="_2440"></a>快照仓库注册</h4> 
<p>在配置自动快照之前，首先需要定义一个快照仓库。Elasticsearch 支持多种类型的仓库，包括文件系统（FS）、Amazon S3、Google Cloud Storage、Azure Blob Storage 等。仓库需要在集群中预先定义好，如下所示：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>_snapshot<span class="token operator">/</span>my_backup_repository
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"type"</span><span class="token operator">:</span> <span class="token string">"fs"</span><span class="token punctuation">,</span>
  <span class="token string">"settings"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"location"</span><span class="token operator">:</span> <span class="token string">"/mount/backups/my_backup"</span><span class="token punctuation">,</span>
    <span class="token string">"compress"</span><span class="token operator">:</span> <span class="token boolean">true</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>在上述例子中，注册了一个文件系统类型的仓库，名为 <code>my_backup_repository</code>，并指定了备份文件的位置和压缩选项。</p> 
<h4><a id="_2457"></a>创建自动快照策略</h4> 
<p>通过 Elasticsearch 的 SLM（Snapshot Lifecycle Management）功能，你可以定期自动创建快照。你需要创建一个快照生命周期策略，如下所示：</p> 
<pre><code class="prism language-json"><span class="token constant">PUT</span> <span class="token operator">/</span>_slm<span class="token operator">/</span>policy<span class="token operator">/</span>daily_snapshots
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"schedule"</span><span class="token operator">:</span> <span class="token string">"0 30 1 * * ?"</span><span class="token punctuation">,</span> 
  <span class="token string">"name"</span><span class="token operator">:</span> <span class="token string">"&lt;my-snap-{now/d}&gt;"</span><span class="token punctuation">,</span>
  <span class="token string">"repository"</span><span class="token operator">:</span> <span class="token string">"my_backup_repository"</span><span class="token punctuation">,</span>
  <span class="token string">"config"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"indices"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"*"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"ignore_unavailable"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
    <span class="token string">"include_global_state"</span><span class="token operator">:</span> <span class="token boolean">false</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string">"retention"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"expire_after"</span><span class="token operator">:</span> <span class="token string">"30d"</span><span class="token punctuation">,</span>
    <span class="token string">"min_count"</span><span class="token operator">:</span> <span class="token number">5</span><span class="token punctuation">,</span>
    <span class="token string">"max_count"</span><span class="token operator">:</span> <span class="token number">50</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这个例子中定义了一个名为 <code>daily_snapshots</code> 的策略，它会在每天凌晨 1:30（服务器时间）自动创建一个快照。快照会被保存在前面注册的 <code>my_backup_repository</code> 仓库中。策略还定义了保留策略，以控制保存快照的时间长短。</p> 
<h4><a id="_2482"></a>恢复快照</h4> 
<p>若要恢复先前创建的快照，可以使用以下命令：</p> 
<pre><code class="prism language-json"><span class="token constant">POST</span> <span class="token operator">/</span>_snapshot<span class="token operator">/</span>my_backup_repository<span class="token operator">/</span>my<span class="token operator">-</span>snap<span class="token operator">-</span><span class="token number">2023.04</span><span class="token number">.06_01</span><span class="token operator">:</span><span class="token number">30</span><span class="token operator">/</span>_restore
<span class="token punctuation">{<!-- --></span>
  <span class="token string">"indices"</span><span class="token operator">:</span> <span class="token string">"index_1,index_2"</span><span class="token punctuation">,</span>
  <span class="token string">"ignore_unavailable"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
  <span class="token string">"include_global_state"</span><span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>上面的命令会从 <code>my_backup_repository</code> 仓库中恢复名为 <code>my-snap-2023.04.06_01:30</code> 的快照，包括 <code>index_1</code> 和 <code>index_2</code> 这两个索引。</p> 
<h4><a id="_2497"></a>监控和管理快照</h4> 
<p>你可以通过 Elasticsearch 的各种 API 来监控和管理快照和快照策略。例如，检查快照状态：</p> 
<pre><code class="prism language-json"><span class="token constant">GET</span> <span class="token operator">/</span>_snapshot<span class="token operator">/</span>my_backup_repository<span class="token operator">/</span>my<span class="token operator">-</span>snap<span class="token operator">-</span><span class="token number">2023.04</span><span class="token number">.06_01</span><span class="token operator">:</span><span class="token number">30</span>
</code></pre> 
<p>或者获取当前生命周期策略的状态：</p> 
<pre><code class="prism language-json"><span class="token constant">GET</span> <span class="token operator">/</span>_slm<span class="token operator">/</span>policy
</code></pre> 
<p>上面介绍了 Elasticsearch 自动快照和恢复机制的基本步骤。请注意，一切配置和操作都要考虑到你的实际环境和需求。Elasticsearch 的版本和配置可能会影响具体步骤和可用功能。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ba952a0130c84156b60cbedf7894fa58/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">文本挖掘解锁信息时代的秘密武器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4e922ba0541b42bf4a06634bcb302d3e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ES-API约定</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>