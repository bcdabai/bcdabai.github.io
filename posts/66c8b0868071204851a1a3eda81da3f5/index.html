<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>VMWare ESXi接口 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="VMWare ESXi接口" />
<meta property="og:description" content="在上个实验中，我们已经初步实现了通过DPDK纳管主机网卡。
不知道大家注意到没有，我们在查看主机网卡的主线信息时，后面的描述信息是不一样的，可以理解为是主机的网卡信息，一个是VMXNET3类型，另一个是82574L类型。
其实这个是和配置主机时选择的适配器类型相对应的，适配器类型选择VMXNET3，在主机上看到的就是VMXNET3；如果选择的是E1000e，显示就成了82574L了。
当然，还有一个SR-IOV直通，指的就是Single Root I/O Virtualization了，据说是将一个物理网卡虚拟出来多个轻量化的PCI-e物理设备，再分配给虚拟机使用。启用SR-IOV直通之后，将大大减轻宿主机的CPU负荷，提高网络性能，降低网络时延等。
但是很不幸，我的服务器网卡（型号BCM5719）不支持。
那E1000e和VMXNET3这两种类型有什么区别呢？
E1000e虚拟网络适配器模拟的是千兆网卡，而VMXNET3则模拟的是万兆网卡，而且VMXNET3虚拟网络适配器可以没有物理网卡对应，也就是不需要借助底层的硬件网卡，并且通过对虚拟机中的性能进行优化，使得虚拟机之间的网络交换不受底层网卡的限制。比如我的服务器，底层都是千兆电口网卡，而主机转发却可以达到万兆，就是这个原因。
所以，VMware ESXi的最佳实践是使用VMXNET3虚拟NIC，除非存在无法使用特定驱动程序或兼容性等原因，建议使用VMXNET3来替换掉E1000e，以有效提升网络性能。
所以今天的主要任务是测试一下这两种网卡的性能差距主要有多大。
测试1：VMXNET3直连 将两台主机的适配器类型都配置为VMXNET3，并连接在同一个端口组下。
用iperf3打流测试，带宽竟然不止万兆，达到了11.6 Gbps。
测试2：E1000e直连 将两台主机的适配器类型都配置为E1000e，并连接在同一个端口组下。
用iperf3打流测试，带宽比千兆好一些，达到了4.24 Gbps，相比VMXNET3，只有三分之一的性能。
测试3：E1000e和VMXNET3对接1 将一台主机142的适配器类型配置为E1000e，另一台主机141的适配器类型配置为VMXNET3，并将两台主机连接在同一个端口组下。
先用适配器类型为E1000e的主机142作为服务器，打流测试，带宽为6.12 Gbps，介于前两次测试之间。
再用适配器类型为VMXNET3的主机141作为服务器，打流测试，带宽为7.01 Gbps，优于使用主机使用E1000e作为服务器的场景。
测试4：E1000e和VMXNET3对接2 为了避免主机的影响，我将两台主机的网卡配置对调了一下，并将两台主机连接在同一个端口组下。
同样的，先用适配器类型为E1000e的主机141作为服务器，打流测试，带宽为7.01 Gbps。
再用适配器类型为VMXNET3的主机142作为服务器，打流测试，带宽为6.71 Gbps，比6.12 Gbps这个值稍好一点。
总结 通过本次的4个测试可知，VMWare ESXi通过对虚拟机的性能进行优化，使得虚拟机之间的网络交换不受底层网卡的限制。虽然E1000e模拟的是千兆网卡，但实际性能仍然不止千兆；虽然VMXNET3模拟的是万兆网卡，实际性能也可以超出万兆。两者底层都不需要对应物理网卡，这点是SR-IOV直通所不支持的。
通过组合对比，E1000e网卡的转发性能最差，只有VMXNET3网卡的三分之一，当两者组合使用时，转发性能大概能达到VMXNET3网卡的三分之二左右。
所以，最起码我们可以确认，VMware ESXi 6.7的最佳实践就是配置虚拟机的网络适配器类型为VMXNET3，相比于E1000e，能大幅提升网络性能。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/66c8b0868071204851a1a3eda81da3f5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-28T16:08:01+08:00" />
<meta property="article:modified_time" content="2022-07-28T16:08:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">VMWare ESXi接口</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:.0001pt;text-align:justify;">在上个实验中，我们已经初步实现了通过DPDK纳管主机网卡。</p> 
<p style="margin-left:.0001pt;text-align:justify;">不知道大家注意到没有，我们在查看主机网卡的主线信息时，后面的描述信息是不一样的，可以理解为是主机的网卡信息，一个是VMXNET3类型，另一个是82574L类型。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="87" src="https://images2.imgbox.com/4b/56/zmNhuAaH_o.png" width="554"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">其实这个是和配置主机时选择的适配器类型相对应的，适配器类型选择VMXNET3，在主机上看到的就是VMXNET3；如果选择的是E1000e，显示就成了82574L了。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="208" src="https://images2.imgbox.com/cd/9e/IsisIxXS_o.png" width="530"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">当然，还有一个SR-IOV直通，指的就是Single Root I/O Virtualization了，据说是将一个物理网卡虚拟出来多个轻量化的PCI-e物理设备，再分配给虚拟机使用。启用SR-IOV直通之后，将大大减轻宿主机的CPU负荷，提高网络性能，降低网络时延等。</p> 
<p style="margin-left:.0001pt;text-align:justify;">但是很不幸，我的服务器网卡（型号BCM5719）不支持。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="124" src="https://images2.imgbox.com/e1/e0/cxo3UzgB_o.png" width="554"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">那E1000e和VMXNET3这两种类型有什么区别呢？</p> 
<p style="margin-left:.0001pt;text-align:justify;">E1000e虚拟网络适配器模拟的是千兆网卡，而VMXNET3则模拟的是万兆网卡，而且VMXNET3虚拟网络适配器可以没有物理网卡对应，也就是不需要借助底层的硬件网卡，并且通过对虚拟机中的性能进行优化，使得虚拟机之间的网络交换不受底层网卡的限制。比如我的服务器，底层都是千兆电口网卡，而主机转发却可以达到万兆，就是这个原因。</p> 
<p style="margin-left:.0001pt;text-align:justify;">所以，VMware ESXi的最佳实践是使用VMXNET3虚拟NIC，除非存在无法使用特定驱动程序或兼容性等原因，建议使用VMXNET3来替换掉E1000e，以有效提升网络性能。</p> 
<p style="margin-left:.0001pt;text-align:justify;">所以今天的主要任务是测试一下这两种网卡的性能差距主要有多大。</p> 
<h3 style="margin-left:0;text-align:justify;">测试1：VMXNET3直连</h3> 
<p style="margin-left:.0001pt;text-align:justify;">将两台主机的适配器类型都配置为VMXNET3，并连接在同一个端口组下。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="186" src="https://images2.imgbox.com/ce/7f/aM6QC2Lw_o.png" width="304"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">用iperf3打流测试，带宽竟然不止万兆，达到了11.6 Gbps。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="542" src="https://images2.imgbox.com/fe/d8/3o60KTgU_o.png" width="554"></p> 
<p> </p> 
<h3 style="margin-left:0;text-align:justify;">测试2：E1000e直连</h3> 
<p style="margin-left:.0001pt;text-align:justify;">将两台主机的适配器类型都配置为E1000e，并连接在同一个端口组下。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="191" src="https://images2.imgbox.com/62/c5/mp5gKgxz_o.png" width="318"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">用iperf3打流测试，带宽比千兆好一些，达到了4.24 Gbps，相比VMXNET3，只有三分之一的性能。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="539" src="https://images2.imgbox.com/6e/f7/OdY5E2m5_o.png" width="554"></p> 
<p> </p> 
<h3 style="margin-left:0;text-align:justify;">测试3：E1000e和VMXNET3对接1</h3> 
<p style="margin-left:.0001pt;text-align:justify;">将一台主机142的适配器类型配置为E1000e，另一台主机141的适配器类型配置为VMXNET3，并将两台主机连接在同一个端口组下。</p> 
<p style="margin-left:.0001pt;text-align:justify;">先用适配器类型为E1000e的主机142作为服务器，打流测试，带宽为6.12 Gbps，介于前两次测试之间。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="531" src="https://images2.imgbox.com/72/1a/dbfzjEPd_o.png" width="554"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">再用适配器类型为VMXNET3的主机141作为服务器，打流测试，带宽为7.01 Gbps，优于使用主机使用E1000e作为服务器的场景。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="535" src="https://images2.imgbox.com/57/e1/UyczVEnm_o.png" width="554"></p> 
<p> </p> 
<h3 style="margin-left:0;text-align:justify;">测试4：E1000e和VMXNET3对接2</h3> 
<p style="margin-left:.0001pt;text-align:justify;">为了避免主机的影响，我将两台主机的网卡配置对调了一下，并将两台主机连接在同一个端口组下。</p> 
<p style="margin-left:.0001pt;text-align:justify;">同样的，先用适配器类型为E1000e的主机141作为服务器，打流测试，带宽为7.01 Gbps。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="539" src="https://images2.imgbox.com/e5/d3/pHopZAx2_o.png" width="554"></p> 
<p> </p> 
<p style="margin-left:.0001pt;text-align:justify;">再用适配器类型为VMXNET3的主机142作为服务器，打流测试，带宽为6.71 Gbps，比6.12 Gbps这个值稍好一点。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><img alt="" height="537" src="https://images2.imgbox.com/2c/dc/3vhI5cjJ_o.png" width="554"></p> 
<p> </p> 
<h2 style="margin-left:0;text-align:justify;">总结</h2> 
<p style="margin-left:.0001pt;text-align:justify;">通过本次的4个测试可知，VMWare ESXi通过对虚拟机的性能进行优化，使得虚拟机之间的网络交换不受底层网卡的限制。虽然E1000e模拟的是千兆网卡，但实际性能仍然不止千兆；虽然VMXNET3模拟的是万兆网卡，实际性能也可以超出万兆。两者底层都不需要对应物理网卡，这点是SR-IOV直通所不支持的。</p> 
<p style="margin-left:.0001pt;text-align:justify;">通过组合对比，E1000e网卡的转发性能最差，只有VMXNET3网卡的三分之一，当两者组合使用时，转发性能大概能达到VMXNET3网卡的三分之二左右。</p> 
<p style="margin-left:.0001pt;text-align:justify;">所以，最起码我们可以确认，VMware ESXi 6.7的最佳实践就是配置虚拟机的网络适配器类型为VMXNET3，相比于E1000e，能大幅提升网络性能。</p> 
<p style="margin-left:.0001pt;text-align:justify;"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/5c01686c8cfdcbcf9208f82f0778b667/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux命令ll输出后各个字段的含义</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3d8bac3bef15b61f347d51c7d782cb8d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Maven】Could not transfer artifact xxx from/to xxx的解决方案</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>