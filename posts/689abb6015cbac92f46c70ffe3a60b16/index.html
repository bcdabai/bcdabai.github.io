<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ElasticSearch底层相关原理 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ElasticSearch底层相关原理" />
<meta property="og:description" content="一、ElasticSearch文档分值_score计算底层原理 查询的时候会对搜索到的文档进行打分（filter方式不会打分）。
1.boolean model 根据用户的query条件，先过滤出包含指定term的doc，这一步是不会打分的。
2.relevance score算法 [ˈreləvəns] 相关度
简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度
Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为TF/IDF算法，ES底层是基于lucence，而lucence使用的算法：TF/IDF算法。
Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关
#搜索请求：hello world doc1：hello you, and world is very good doc2：hello, how are you # doc1更相关 Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关。不是针对单个文档了，是针对整个索引库中的文档。（越多越不相关，可以这样理解：主要过滤一些通用词，像：的，啊之类的，所以说是反转。）
# 搜索请求：hello world # 情形：在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次； # world这个单词在所有的document中，一共出现了100次 doc1：hello, tuling is very good doc2：hi world, how are you # 这个时候doc2的相关度分数要高一些，因为doc1含有hello这个单词 Field-length norm：（field长度）field越长，相关度越弱。
# 搜索请求：hello world doc1：{ &#34;title&#34;: &#34;hello article&#34;, &#34;content&#34;: &#34;...... N个单词&#34; } doc2：{ &#34;title&#34;: &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/689abb6015cbac92f46c70ffe3a60b16/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-11T15:04:58+08:00" />
<meta property="article:modified_time" content="2022-04-11T15:04:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ElasticSearch底层相关原理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>一、ElasticSearch文档分值_score计算底层原理</h2> 
<p>查询的时候会对搜索到的文档进行打分（filter方式不会打分）。</p> 
<h3>1.boolean model</h3> 
<p>        根据用户的query条件，先过滤出包含指定term的doc，这一步是不会打分的。</p> 
<h3>2.relevance score算法</h3> 
<p>        [ˈreləvəns] 相关度</p> 
<p>        简单来说，就是计算出，一个索引中的文本，与搜索文本，他们之间的关联匹配程度</p> 
<p>Elasticsearch使用的是 term frequency/inverse document frequency算法，简称为<span style="color:#fe2c24;">TF/IDF算法</span>，ES底层是基于lucence，而lucence使用的算法：TF/IDF算法。</p> 
<p><strong>Term frequency：</strong>搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关</p> 
<pre><code class="language-bash">#搜索请求：hello world

doc1：hello you, and world is very good

doc2：hello, how are you

# doc1更相关</code></pre> 
<p><strong>Inverse document frequency：</strong>搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关。不是针对单个文档了，是针对整个索引库中的文档。（越多越不相关，可以这样理解：主要过滤一些通用词，像：的，啊之类的，所以说是反转。）</p> 
<pre><code class="language-bash"># 搜索请求：hello world
# 情形：在index中有1万条document，hello这个单词在所有的document中，一共出现了1000次；
# world这个单词在所有的document中，一共出现了100次

doc1：hello, tuling is very good

doc2：hi world, how are you

# 这个时候doc2的相关度分数要高一些，因为doc1含有hello这个单词</code></pre> 
<p><strong>Field-length norm：（</strong>field长度）field越长，相关度越弱。</p> 
<pre><code class="language-bash"># 搜索请求：hello world

doc1：{ "title": "hello article", "content": "...... N个单词" }

doc2：{ "title": "my article", "content": "...... N个单词，hi world" }

# doc1分数要高，命中内容的title字段更短。
# doc2是在文章内容中（好几百个词）才匹配上了一个world这个单词；
# 而doc1是在title中（几个单词的长度）就匹配上了一个词。
</code></pre> 
<h3>3.分析一个document上的_score是如何被计算出来的</h3> 
<pre><code class="language-bash"># _explain：关键字查询打分详情
GET /es_db/_doc/1/_explain
{
  "query": {
    "match": {
      "remark": "java developer"
    }
  }
}</code></pre> 
<p>打分机制牵扯到空间向量模型：</p> 
<ul><li>query vector：查询向量</li><li>doc vector：文档向量</li></ul> 
<p>多个term对一个doc的总分数分析：</p> 
<p><strong>Query Vector：</strong></p> 
<p>        hello world --&gt; es会根据hello world在所有doc中的评分情况，计算出一个query vector</p> 
<p>        hello这个term，假设基于所有doc算出的一个评分就是3</p> 
<p>        world这个term，假设基于所有doc算出的一个评分就是6</p> 
<p>        那么[3, 6]就是这次查询的query vector的评分。</p> 
<p><strong> Doc Vector：</strong></p> 
<p>        假设根据hello world查询出3个doc，一个包含hello，一个包含world，一个包含hello 以及 world</p> 
<p>        doc1：包含hello --&gt;那么这个doc的文档向量就是 [3, 0]</p> 
<p>        doc2：包含world --&gt; 那么这个doc的文档向量就是[0, 6]</p> 
<p>        doc3：包含hello, world --&gt;那么这个doc的文档向量就是 [3, 6]</p> 
<p>        即会给每一个doc，拿每个term计算出一个分数来，hello有一个分数，world有一个分数，再拿所有term的分数组成一个doc vector。</p> 
<p><strong>匹配度：</strong></p> 
<p>        画在一个图中，取每个doc的doc vector对和query vector之间的的弧度，基于这个弧度计算出出每个doc对多个term的总分数。</p> 
<p>        底层lucence的算法是很复杂的，利用空间向量模型表现在图上就是<span style="color:#fe2c24;">弧度越大，分数越低; 弧度越小，分数越高</span><span style="color:#0d0016;">。如下图</span> ：（红色的为doc vector，黑色的为query vector） </p> 
<p>                        <img alt="" height="267" src="https://images2.imgbox.com/a2/aa/7q1Zxoyc_o.png" width="396"></p> 
<p>                       </p> 
<p>        但是这只是两个关键词hello和world，但是用户不可能只输入两个关键词，可能会有多个关键词，这样使用二维图就不好表现了，只能使用线性代数来计算，无法用图来表示，脑中可以理解为一个空间立体模型，但是也是越近也匹配。但这里就只用二维坐标图做个演示。</p> 
<p></p> 
<p class="img-center"><img alt="" height="125" src="https://images2.imgbox.com/2c/a2/ikCBTNRt_o.png" width="425"></p> 
<h2>二、分词器工作流程</h2> 
<h3>1.分词器工作流程</h3> 
<p>    分词器是es中专门处理分词的组件，成为Analyzer，它的组成如下：</p> 
<ul><li>    Character Filters:针对原始文本进行处理，比如说去除html标记符</li><li>    Tokenizer：将原始文本按照一定规则切分成单词</li><li>    Token Filters：针对Tokenizer的单词进行再加工，比如说转小写、删除或新增等处理</li></ul> 
<p>        1）<strong>character filter：</strong>在一段文本进行分词之前，先进行预处理（标签处理、特殊字符转换）。比如说最常见的就是，过滤html标签（&lt;span&gt;hello&lt;span&gt; --&gt; hello），&amp; --&gt; and（I&amp;you --&gt; I and you）</p> 
<p>        2）<strong>tokenizer：</strong>分词，hello you and me --&gt; hello, you, and, me</p> 
<p>        3）<strong>token filter：</strong>lowercase（转小写），stop word（停用词处理），synonymom（同义词处理）。如：liked --&gt; like，Tom --&gt; tom，a/the/an --&gt; 干掉停用词，small --&gt; little</p> 
<h3>2.内置分词器</h3> 
<p>因为ES不是国内开发的，内置分词器一般可以适用于英文环境，但是中文环境一般是不适用的。</p> 
<p>        1）standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard）</p> 
<p>        2） simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans</p> 
<p>        3）whitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5)</p> 
<p>         4）stop analyzer:移除停用词，比如a the it等等</p> 
<pre><code class="language-bash"># 分词器分词分析
POST _analyze
{
    "analyzer":"standard",
    "text":"Set the shape to semi-transparent by calling set_trans(5)"
}</code></pre> 
<h3>3.定制化分词器</h3> 
<pre><code class="language-bash">PUT /my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        # es_std代表当前索引库自定义分词器的名称
        "es_std": {
          # 代表在standard分词器基础上做的变更、优化、升级
          "type": "standard",
          # 在英文模式下过滤停用词，比如a、an、the
          "stopwords": "_english_"
        }
      }
    }
  }
}

# 如果还是使用standard这个分词器进行分析的话，则a、an、the这类词也会被拆分
GET /my_index/_analyze
{
  "analyzer": "standard", 
  "text": "a dog is in the house"
}


# 如果使用了我们改良后的分词器，则a、an、the等词不会做分词
# 注意使用索引库的自定义分词器的时候，只能在当前索引库使用
GET /my_index/_analyze
{
  # 使用我们改良后的分词器
  "analyzer": "es_std",
  "text":"a dog is in the house"
}
</code></pre> 
<p>上面的例子，只是使用ES内置的一些关键字对分词器做一些小的改动，我们也可以使用下面的方式来进行优化分词器，更大程度满足我们的业务需求。</p> 
<p><span style="color:#fe2c24;">某个索引库的自定义分词器，只试用于所声明的索引库，否则在所声明的索引库外的索引库使用的话会报错：找不到分词器</span></p> 
<pre><code class="language-bash">PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": {
        # &amp;_to_and是字符过滤器的名称，可以随便起
        "&amp;_to_and": {
          # 映射类型
          "type": "mapping",
          # 具体的映射规则：如果含有&amp;则转为and
          "mappings": [
            "&amp;=&gt; and"
          ]
        }
      },
      "filter": {
        # 停用词过滤器的名称，可以随表起
        "my_stopwords": {
          # 停用词类型
          "type": "stop",
          # 指出具体的停用词
          "stopwords": [
            "the",
            "a"
          ]
        }
      },
      "analyzer": {
        # 定制化分词器的名称：my_analyzer
        "my_analyzer": {
          # 表示自定义类型
          "type": "custom",
          # 定制化体现的地方，表示使用上方定义的规则
          "char_filter": [
            # 是ES内置的内容，专门处理html标签
            "html_strip",
            "&amp;_to_and"
          ],
          # 代表当前定制化的分词器是在standard分词器的基础上做的扩展、优化
          "tokenizer": "standard",
          # 定制化体现的地方，表示使用上方定义的规则
          "filter": [
            # 是ES内置的内容，转为小写
            "lowercase",
            "my_stopwords"
          ]
        }
      }
    }
  }
}</code></pre> 
<pre><code class="language-bash">GET /my_index/_analyze
{
  "text": "tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!",
  "analyzer": "my_analyzer"
}

# 执行效果
{
  "tokens" : [
    {
      "token" : "tomandjerry",
      "start_offset" : 0,
      "end_offset" : 9,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 0
    },
    {
      "token" : "are",
      "start_offset" : 10,
      "end_offset" : 13,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 1
    },
    {
      "token" : "friend",
      "start_offset" : 16,
      "end_offset" : 22,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 3
    },
    {
      "token" : "in",
      "start_offset" : 23,
      "end_offset" : 25,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 4
    },
    {
      "token" : "house",
      "start_offset" : 30,
      "end_offset" : 35,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 6
    },
    {
      "token" : "haha",
      "start_offset" : 42,
      "end_offset" : 46,
      "type" : "&lt;ALPHANUM&gt;",
      "position" : 7
    }
  ]
}</code></pre> 
<h3>4.IK分词器详解</h3> 
<p>ik配置文件地址：es/plugins/ik/config目录，下面是一些IK的配置文件的说明：</p> 
<ul><li>IKAnalyzer.cfg.xml：用来配置自定义词库</li><li>main.dic：ik原生内置的中文词库，总共有27万多条，只要是这些单词，都会被分在一起</li><li>quantifier.dic：放了一些单位相关的词</li><li>suffix.dic：放了一些后缀</li><li>surname.dic：中国的姓氏</li><li>stopword.dic：英文停用词</li></ul> 
<p>ik原生最重要的两个配置文件：</p> 
<p>        main.dic：包含了原生的中文词语，会按照这个里面的词语去分词</p> 
<p>        stopword.dic：包含了英文的停用词，一般，像停用词，会在分词的时候，直接被干掉，不会建立在倒排索引中</p> 
<h4>（1）IKAnalyzer.cfg</h4> 
<pre><code class="language-bash">&lt;properties&gt;
	&lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt;
	&lt;!--用户可以在这里配置自己的扩展字典 --&gt;
	&lt;entry key="ext_dict"&gt;location&lt;/entry&gt;
	 &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;
	&lt;entry key="ext_stopwords"&gt;location&lt;/entry&gt;
	&lt;!--用户可以在这里配置远程扩展字典 --&gt;
	&lt;entry key="remote_ext_dict"&gt;words_location&lt;/entry&gt; 
	&lt;!--用户可以在这里配置远程扩展停止词字典--&gt;
	&lt;entry key="remote_ext_stopwords"&gt;words_location&lt;/entry&gt;
&lt;/properties&gt;</code></pre> 
<p>      </p> 
<p>（1）自己建立扩展词库：每年都会涌现一些特殊的流行词，网红，蓝瘦香菇，喊麦，鬼畜，一般不会在ik的原生词典里，那也就不会对这次词语进行分词，建立倒排索引，则自己补充自己的最新的词语，到ik的词库里面去。<span style="color:#fe2c24;">补充自己的词语，然后需要重启es才能生效。</span></p> 
<pre><code class="language-bash"># 假设custom在config目录下
&lt;entry key="ext_dict"&gt;custom/mydict.dic&lt;/entry&gt;</code></pre> 
<p>（2）自己建立停用词库：比如了，的，啥，么，我们可能并不想去建立索引让人家搜索到。已经有了常用的中文停用词，可以根据业务要求补充自己的停用词，然后<span style="color:#fe2c24;">重启es才能生效</span>。</p> 
<pre><code class="language-bash">&lt;entry key="ext_stopwords"&gt;custom/ext_stopword.dic&lt;/entry&gt;</code></pre> 
<p><span style="color:#fe2c24;"> （扩展词的格式就是和ik中的dic的格式一样，一行一个词就可以了）</span></p> 
<p><strong><span style="color:#0d0016;">思考：</span></strong>如果像上面进行扩展词典，每次都是在es的扩展词典中，手动添加新词语，很坑。</p> 
<p>        1）每次添加完，都要重启es才能生效，非常麻烦。</p> 
<p>        2）es是分布式的，可能有数百个节点，你不能每次都一个一个节点上面去修改。</p> 
<h4>（2）IK热更新</h4> 
<p>主要解决问题：实现es不停机的情况下，我们可以随意动态修改我们的扩展词库，并使ES识别到我们做的修改。</p> 
<p><strong>        1）远程扩展词典：</strong>把我们的字典放到远程一个tomcat服务器中，假设我们把文件放到tomcat的ROOT文件夹下，则可以这样配置：</p> 
<pre><code class="language-bash">&lt;entry key="remote_ext_dict"&gt;http://ip:port/host.dic&lt;/entry&gt; 
&lt;entry key="remote_ext_stopwords"&gt;http://ip:port/stop.dic&lt;/entry&gt;</code></pre> 
<p>只要配置了这个远程字典的key（remote_ext_dict、remote_ext_stopwords），ik读到并识别到了这个key的配置，就会开启一个定时器，会定时去加载这个文件的内容。这样每次修改就修改远程服务器上的文件就行了，相当于实现了热更新，但是也会有一些不方便。<span style="color:#fe2c24;">但是这种方式官方也不推荐，不稳定。</span></p> 
<p>       <strong> 2）使用另一种方式，使用mysql，需要修改ik的源码。</strong></p> 
<p>之后补充</p> 
<h2>三、高亮显示</h2> 
<h3>1.高亮显示</h3> 
<p>百度会把搜索到的内容，会把关键字做成高亮的效果，这样也比较醒目，比如搜一个文章，如果没有高亮，文章内容这么多，我们怎么知道文章包含我们搜索的内容。</p> 
<p><span style="color:#fe2c24;">em标签</span>会把关键字变红，如果不喜欢这个颜色，可以指定别的标签，都是可以指定的。</p> 
<pre><code class="language-bash"># 定义一个索引库
PUT /news_website
{
  "mappings": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "ik_max_word"
        },
        "content": {
          "type": "text",
          "analyzer": "ik_max_word"
        }
      }
    }
  
}

# 插入测试数据
PUT /news_website/_doc/1
{
  "title": "这是我写的第一篇文章",
  "content": "大家好，这是我写的第一篇文章，特别喜欢这个文章门户网站！！！"
}

# 我们查询title中含有“文章”的文档，并高亮显示
GET /news_website/_doc/_search 
{
  "query": {
    "match": {
      "title": "文章"
    }
  },
  # 高亮配置
  "highlight": {
    "fields": {
      # title中的关键字高亮
      "title": {}
    }
  }
}</code></pre> 
<pre><code class="language-bash"># 执行结果
{
  "took" : 32,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.2876821,
    # 返回的原始数据
    "hits" : [
      {
        "_index" : "news_website",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.2876821,
        "_source" : {
          "title" : "这是我写的第一篇文章",
          "content" : "大家好，这是我写的第一篇文章，特别喜欢这个文章门户网站！！！"
        },
        # 这里会返回高亮的部分
        # em标签会把关键字变红，如果不喜欢这个颜色，查询的时候可以指定别的标签，都是可以指定的。
        "highlight" : {
          "title" : [
            "这是我写的第一篇&lt;em&gt;文章&lt;/em&gt;"
          ]
        }
      }
    ]
  }
}</code></pre> 
<p>只会把<span style="color:#fe2c24;">包含在查询条件中</span>并且<span style="color:#fe2c24;">在高亮配置中指明的</span>字段中的关键字做成高亮。如果查询条件中没有某个字段，即使你在高亮相关语法中指明要做成高亮，也是不起作用的。</p> 
<pre><code class="language-bash"># 这种写法content中的关键字是不会被高亮显示的
GET /news_website/_doc/_search 
{
  "query": {
    "match": {
      "title": "文章"
    }
  },
  "highlight": {
    "fields": {
      "title": {}
      "content": {}
    }
  }
}

# 只有把content也作为查询条件，并且在highlight中指定高亮，才会将content中的关键字的高亮展示
GET /news_website/_doc/_search 
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "title": "文章"
          }
        },
        {
          "match": {
            "content": "文章"
          }
        }
      ]
    }
  },
  "highlight": {
    "fields": {
      "title": {},
      "content": {}
    }
  }
}</code></pre> 
<h3>2.常用的highlight（高亮方式）介绍</h3> 
<ul><li>plain highlight：ucene highlight默认</li><li>posting highlight：index_options=offsets</li><li>fast vector highlight：term_vector=with_positions_offsets</li></ul> 
<h4>（1）plain和posting对比：</h4> 
<p>        1）posting性能比plain highlight要高，因为不需要重新对高亮文本进行分词</p> 
<p>        2）对磁盘的消耗更少</p> 
<h4>（2）posting highlight</h4> 
<p>在创建索引映射的时候，如果在相应字段中配置了index_option之后，那么在进行高亮查询的时候，这个字段的高亮方式会采用posting highlight高亮方式。</p> 
<pre><code class="language-bash">PUT /news_website
{
  "mappings": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "ik_max_word"
        },
        "content": {
          "type": "text",
          "analyzer": "ik_max_word",
          # 做高亮搜索的时候会采用posting高亮方式
          "index_options": "offsets"
        }
      }
  }
}</code></pre> 
<h4>（3）fast vector highlight</h4> 
<p>对大field而言（大于1mb），性能更高</p> 
<pre><code class="language-bash">PUT /news_website
{
  "mappings": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "ik_max_word"
        },
        "content": {
          "type": "text",
          "analyzer": "ik_max_word",
          # 指定使用fast vector高亮方式
          "term_vector" : "with_positions_offsets"
        }
      }
  }
}</code></pre> 
<h4>（4）强制使用某种highlighter</h4> 
<p>如果一开始在映射中指定了使用哪种高亮方式，但我们在实际查询中也可以强制使用其他的高亮方式。</p> 
<pre><code class="language-bash">GET /news_website/_doc/_search 
{
  "query": {
    "match": {
      "content": "文章"
    }
  },
  "highlight": {
    "fields": {
      "content": {
        # 代表强制使用plain默认的高亮方式
        "type": "plain"
      }
    }
  }
}</code></pre> 
<h4>（6）高亮片段fragment的设置（展示最优片段大小）</h4> 
<p>举个例子：百度搜索的时候会展示标题和内容等信息，但是内容又不把所有内容都显示出来，只显示包含关键字的一小部分就行，即最优的片段；标题中可能只显示20个字就行了等等。那么这个最优的片段需要多少文字，都可以自己去声明。</p> 
<pre><code class="language-bash">GET /_search
{
    "query" : {
        "match": { "content": "文章" }
    },
    "highlight" : {
        "fields" : {
            "content" : {"fragment_size" : 150, "number_of_fragments" : 3 }
        }
    }
}</code></pre> 
<ul><li><span style="color:#fe2c24;">fragment_size:</span> 设置要显示出来的fragment文本判断的长度，即一个最优的片段需要多少文字，默认是100。假设你的一个Field的值的长度是1万，但是你不可能在页面上把内容都显示出来，那么就可以使用这个属性声明。</li><li><span style="color:#fe2c24;">number_of_fragments：</span><span style="color:#0d0016;">你想要</span>你的高亮的fragment文本片段有多个片段，你就可以指定显示几个片段。</li></ul> 
<h4>（7）设置高亮html标签</h4> 
<p>默认是&lt;em&gt;标签，关键字展示为红色，你也可以修改展示样式：</p> 
<pre><code class="language-bash">GET /news_website/_doc/_search 
{
  "query": {
    "match": {
      "content": "文章"
    }
  },
  "highlight": {
    # 包裹关键字的前置标签，这里指明css属性color为红色，即字体为红色
    "pre_tags": ["&lt;span color='red'&gt;"],
    # 包裹关键字的后置标签
    "post_tags": ["&lt;/span&gt;"], 
    "fields": {
      "content": {
        "type": "plain"
      }
    }
  }
}</code></pre> 
<h4>（5）总结</h4> 
<p>        其实可以根据你的实际情况去考虑，一般情况下，用plain highlight也就足够了，不需要做其他额外的设置；如果对高亮的性能要求很高，可以尝试启用posting highlight；如果field的值特别大，超过了1M，那么可以用fast vector highlight。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/779d3b181a21c2fa29be02bfd52d76bf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">iOS-TestFlight的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/154dfffff93ad2904a6aa422a2ae7a06/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">尚硅谷-康师傅-MySQL详细笔记(10-18章)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>