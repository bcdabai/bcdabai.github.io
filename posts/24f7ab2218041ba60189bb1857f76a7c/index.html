<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>python-scrapy框架（四）settings.py文件的用法详解实例 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="python-scrapy框架（四）settings.py文件的用法详解实例" />
<meta property="og:description" content="settings.py文件是Scrapy框架中用来配置爬取相关设置的文件。在Scrapy中，我们可以通过修改settings.py文件来自定义爬虫的行为，包括设置全局变量、配置下载延迟、配置ua池、设置代理以及其他爬虫相关的配置项。下面是对settings.py文件用法的详细解释和一个实例：
1.设置全局变量
在settings.py文件中，我们可以定义一些全局变量，这些变量在整个爬虫过程中都可以使用。例如，我们可以定义一个USER_AGENT变量，用来设置请求的User-Agent头信息： USER_AGENT = &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3&#39; 2.配置下载延迟
在settings.py文件中，可以通过设置DOWNLOAD_DELAY参数来配置下载延迟，以控制爬取速度。DOWNLOAD_DELAY的单位是秒，可以设置为1或更大的值。例如： DOWNLOAD_DELAY = 1 3.配置UA池
为了防止网站对爬虫的识别，我们可以设置一个User-Agent池，让每个请求随机选择一个User-Agent进行发送。可以在settings.py文件中设置USER_AGENT_POOL，如下所示： USER_AGENT_POOL = [ &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0&#39;, &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebK... ] 然后，在Spider中随机选择一个User-Agent进行请求发送：
from scrapy import Spider from scrapy.utils.project import get_project_settings from scrapy.utils.httpobj import urlparse_cached class MySpider(Spider): name = &#39;my_spider&#39; def __init__(self, name=None, **kwargs): self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/24f7ab2218041ba60189bb1857f76a7c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-03T17:01:47+08:00" />
<meta property="article:modified_time" content="2023-07-03T17:01:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">python-scrapy框架（四）settings.py文件的用法详解实例</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>settings.py文件是Scrapy框架中用来配置爬取相关设置的文件。在Scrapy中，我们可以通过修改settings.py文件来自定义爬虫的行为，包括设置全局变量、配置下载延迟、配置ua池、设置代理以及其他爬虫相关的配置项。下面是对settings.py文件用法的详细解释和一个实例：</p> 
<h5>1.设置全局变量<br> 在settings.py文件中，我们可以定义一些全局变量，这些变量在整个爬虫过程中都可以使用。例如，我们可以定义一个USER_AGENT变量，用来设置请求的User-Agent头信息：</h5> 
<pre><code class="language-python">USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
</code></pre> 
<p></p> 
<h5>2.配置下载延迟<br> 在settings.py文件中，可以通过设置DOWNLOAD_DELAY参数来配置下载延迟，以控制爬取速度。DOWNLOAD_DELAY的单位是秒，可以设置为1或更大的值。例如：</h5> 
<pre><code class="language-python">DOWNLOAD_DELAY = 1
</code></pre> 
<p></p> 
<h5>3.配置UA池<br> 为了防止网站对爬虫的识别，我们可以设置一个User-Agent池，让每个请求随机选择一个User-Agent进行发送。可以在settings.py文件中设置USER_AGENT_POOL，如下所示：</h5> 
<pre><code class="language-python">USER_AGENT_POOL = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101 Firefox/78.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebK...
]
</code></pre> 
<p></p> 
<p>然后，在Spider中随机选择一个User-Agent进行请求发送：</p> 
<pre><code class="language-python">from scrapy import Spider
from scrapy.utils.project import get_project_settings
from scrapy.utils.httpobj import urlparse_cached

class MySpider(Spider):
    name = 'my_spider'
    
    def __init__(self, name=None, **kwargs):
        self.settings = get_project_settings()
    
    def start_requests(self):
        # ...
        yield scrapy.Request(url, headers={'User-Agent': self.settings['USER_AGENT_POOL'][random.randint(0, len(self.settings['USER_AGENT_POOL'])-1)]})
</code></pre> 
<p></p> 
<h5>4.设置代理<br> 如果需要通过代理来进行爬取，可以在settings.py文件中设置PROXIES参数。例如：</h5> 
<pre><code class="language-python">PROXIES = [
    'http://proxy1.example.com:8888',
    'http://proxy2.example.com:8888',
    'http://proxy3.example.com:8888',
]
</code></pre> 
<p></p> 
<p>然后，在Spider中随机选择一个代理进行请求发送：</p> 
<pre><code class="language-python">from scrapy import Spider
from scrapy.utils.project import get_project_settings
from scrapy.utils.httpobj import urlparse_cached

class MySpider(Spider):
    name = 'my_spider'
    
    def __init__(self, name=None, **kwargs):
        self.settings = get_project_settings()
    
    def start_requests(self):
        # ...
        yield scrapy.Request(url, meta={'proxy': self.settings['PROXIES'][random.randint(0, len(self.settings['PROXIES'])-1)]})
</code></pre> 
<p></p> 
<h5>5.其他爬虫相关配置项<br> 在settings.py文件中，还可以设置其他的爬虫相关配置项，如日志级别、保存路径、爬取深度等。以下是一些常见的配置项：</h5> 
<pre><code class="language-python"># 日志级别
LOG_LEVEL = 'INFO'

# 爬虫名称
BOT_NAME = 'my_bot'

# 爬取深度限制
DEPTH_LIMIT = 3

# 是否遵循robots.txt
ROBOTSTXT_OBEY = True

# 是否启用缓存
HTTPCACHE_ENABLED = True

# 缓存过期时间
HTTPCACHE_EXPIRATION_SECS = 0

# 缓存存储路径
HTTPCACHE_DIR = 'httpcache'

# 缓存存储方式
HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
</code></pre> 
<p></p> 
<p>这些只是settings.py文件中一些常见的配置项，你可以根据需要添加或修改更多的配置项。以下是更多可能用到的配置项：</p> 
<h5>6.开启并配置自定义的扩展<br> Scrapy框架允许开发者编写自定义的扩展来增强爬虫的功能。在settings.py文件中，可以通过EXTENSIONS参数来启用和配置这些扩展。例如，启用并配置自定义的扩展MyExtension：</h5> 
<pre><code class="language-python">EXTENSIONS = {
    'myextension.MyExtension': 500,
}
</code></pre> 
<p></p> 
<h5>7.配置重试次数<br> 在爬虫过程中，可能会发生请求失败的情况，可以通过配置RETRY_TIMES和RETRY_HTTP_CODES参数来控制自动重试的次数和HTTP响应状态码。例如，设置最大重试次数为3次，仅在遇到500和502的情况下进行重试：</h5> 
<pre><code class="language-python">RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502]
</code></pre> 
<h5></h5> 
<h5>8.配置并发请求数量<br> 通过并发发送请求可以提高爬取效率，可以通过配置CONCURRENT_REQUESTS参数来设置同时发送的请求数量。例如，设置同时发送10个请求：</h5> 
<pre><code class="language-python">CONCURRENT_REQUESTS = 10
</code></pre> 
<h5></h5> 
<h5>9.配置下载器中间件和爬虫中间件<br> Scrapy框架提供了下载器中间件和爬虫中间件，用于在请求和响应的处理过程中进行自定义的操作。可以通过配置DOWNLOADER_MIDDLEWARES和SPIDER_MIDDLEWARES参数来启用和配置这些中间件。例如，启用并配置自定义的下载器中间件MyDownloaderMiddleware和爬虫中间件MySpiderMiddleware：</h5> 
<pre><code class="language-python">DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.MyDownloaderMiddleware': 543,
}
SPIDER_MIDDLEWARES = {
    'myproject.middlewares.MySpiderMiddleware': 543,
}
</code></pre> 
<p></p> 
<h5>10.配置请求头信息<br> 可以通过设置DEFAULT_REQUEST_HEADERS参数来配置默认的请求头信息。例如，设置Referer和Cookie：</h5> 
<pre><code class="language-python">DEFAULT_REQUEST_HEADERS = {
    'Referer': 'http://www.example.com',
    'Cookie': 'session_id=xxxxx',
}
</code></pre> 
<p></p> 
<h5>11.配置是否启用重定向<br> 可以通过配置REDIRECT_ENABLED参数来控制是否启用请求的重定向。例如，禁用重定向：</h5> 
<pre><code class="language-python">REDIRECT_ENABLED = False
</code></pre> 
<p></p> 
<h5>12.配置去重过滤器<br> Scrapy框架内置了去重过滤器，用于过滤已经爬取过的URL。可以通过配置DUPEFILTER_CLASS参数来选择使用的去重过滤器。例如，使用基于Redis的去重过滤器：</h5> 
<pre><code class="language-python">DUPEFILTER_CLASS = 'scrapy_redis.dupefilter.RFPDupeFilter'
</code></pre> 
<p></p> 
<p>这些只是settings.py文件中一些可能用到的配置项。根据实际需求，你可以根据Scrapy框架提供的各种功能来对settings.py文件进行自定义的配置，以满足你的爬虫需求。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/efb87db21f0fe973dfd6e2ce497881c2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用docker安装mysql主从集群</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c9eb085a7e3efda71a55b8668094f1ed/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vs studio生成dll并调用的实现示例</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>