<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于堆叠自编码器(SAE)的图像特征提取与图像分类——附代码 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于堆叠自编码器(SAE)的图像特征提取与图像分类——附代码" />
<meta property="og:description" content="目录
摘要：
一、自动编码器基本概念（Auto-Encoder，AE）
二、堆叠自编码器（SAE）的原理
2.1 第一层AE
2.2 第二层AE
2.3 第三层
2.4 组合
三、具体的实现步骤：
3.1 准备训练数据
3.2 分步训练SAE
3.3 分类结果
4. 本文Matlab代码
摘要： 基于Matlab平台，构建并训练堆叠自编码器对手写图片进行特征提取，然后使用提取到的图像特征信息对手写图像（0-9的手写数字图）进行识别与分类，并测试其分类的准确度。堆叠自编码器（SAE）具有多个隐含层的神经网络可用于处理复杂数据（例如图像）的分类问题。每个层都可以学习不同抽象级别的特征。然而，在实际工作中，训练具有多个隐含层的神经网络可能会很困难。本文使用一种有效训练具有多个层的神经网络的方法是一次训练一个层。为此，您可以为每个所需的隐含层训练一种称为自编码器的特殊类型的网络。此示例说明如何训练具有两个隐含层的神经网络以对图像中的数字进行分类。首先，使用自编码器以无监督方式单独训练各隐含层。然后训练最终 softmax 层，并将这些层连接在一起形成堆叠网络，该网络最后以有监督方式进行训练。
一、自动编码器基本概念（Auto-Encoder，AE） 自编码器（autoencoder)是神经网络的一种，经过训练后能尝试将输入复制到输出。自编 码器内部有一个隐藏层h,可以产生编码(code)表示输入。该网络可以看作由两部分组 成：一个由函数h=f(X)表示的编码器和一个生成重构的解码器r=g(h)。我们不应该将自 编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似 地复制，并只能复制与训练数据相似的输入。自动编码机由三层网络组成，其中输入层神经元数量与输出层神经元数量相等，中间层神 经元数量少于输入层和输出层。
搭建一个自动编码器需要完成下面三样工作：搭建编码器， 搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息（自编码器是有损的）。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化.
自动编码机(Auto-encoder)是一个自监督的算法，并不是一个无监督算法，它不需要对 训练样本进行标记，其标签产生自输入数据。因此自编码器很容易对指定类的输入训练出 一种特定的编码器，而不需要完成任何新工作。自动编码器是数据相关的，只能压缩那些 与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树 木时性能很差，因为它学习到的特征是与人脸相关的。~ 自动编码器运算过程：原始input(设为)经过加权(W、b)、映射(Sigmoid)之后得到 y,再对y反向加权映射回来成为z。通过反复迭代训练两组(W、b),目的就是使输出信 号与输入信号尽量相似。训练结束之后自动编码器可以由两部分组成：
1.输入层和中间层，可以用这个网络来对信号进行压缩
2.中间层和输出层，我们可以将压缩的信号进行还原
二、堆叠自编码器（SAE）的原理 之前之所以将自编码器模型表示为3层的神经网络，那是因为训练的需要，我们将原始数据作为假想的目标输出，以此构建监督误差来训练整个网络。等训练结束后，输出层就可以去掉了，因为我们只关心的是从(x)到(h)的变换。
接下来的思路就很自然了，我们已经得到特征表达(h)，那么我们可不可以将(h)再作为原始信息，训练一个新的自编码器，得到新的特征表达呢？当软可以，而且这就是所谓的堆叠自编码器（Stacked Autoencoder，SAE）。Stacked就是逐层堆叠的意思，这个跟“栈”有点像。当把多个自编码器Stack起来之后，这个系统看起来就像这样：
​
2.1 第一层AE 这样就把自编码器改成了深度结构了，即《learning multiple levels of representation and abstraction》(Hinton, Bengio, LeCun, 2015)。需要注意的是，整个网络的训练不是一蹴而就的，而是逐层进行的。比如说我们要训练一个(n -&gt; m -&gt; k) 结构的网络，实际上我们是先训练网络(n -&gt; m -&gt; n)，得到(n -&gt; m)的变换，然后再训练(m -&gt; k -&gt; m)网络，得到(m -&gt; k)的变换。最终堆叠成SAE，即为(n -&gt; m -&gt; k)的结果，整个过程就像一层层往上面盖房子，这就是大名鼎鼎的 layer-wise unsuperwised pre-training （逐层非监督预训练）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/212d34f45909724cc421331a2f66d470/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-21T11:24:57+08:00" />
<meta property="article:modified_time" content="2023-02-21T11:24:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于堆叠自编码器(SAE)的图像特征提取与图像分类——附代码</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E6%91%98%E8%A6%81%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E6%91%98%E8%A6%81%EF%BC%9A" rel="nofollow">摘要：</a></p> 
<p id="%E4%B8%80%E3%80%81%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%88Auto-Encoder%EF%BC%8CAE%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%88Auto-Encoder%EF%BC%8CAE%EF%BC%89" rel="nofollow">一、自动编码器基本概念（Auto-Encoder，AE）</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%A0%86%E5%8F%A0%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88SAE%EF%BC%89%E7%9A%84%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%A0%86%E5%8F%A0%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88SAE%EF%BC%89%E7%9A%84%E5%8E%9F%E7%90%86" rel="nofollow">二、堆叠自编码器（SAE）的原理</a></p> 
<p id="2.1%20%E7%AC%AC%E4%B8%80%E5%B1%82AE-toc" style="margin-left:40px;"><a href="#2.1%20%E7%AC%AC%E4%B8%80%E5%B1%82AE" rel="nofollow">2.1 第一层AE</a></p> 
<p id="2.2%20%E7%AC%AC%E4%BA%8C%E5%B1%82AE-toc" style="margin-left:40px;"><a href="#2.2%20%E7%AC%AC%E4%BA%8C%E5%B1%82AE" rel="nofollow">2.2 第二层AE</a></p> 
<p id="2.3%20%E7%AC%AC%E4%B8%89%E5%B1%82-toc" style="margin-left:40px;"><a href="#2.3%20%E7%AC%AC%E4%B8%89%E5%B1%82" rel="nofollow">2.3 第三层</a></p> 
<p id="2.4%20%E7%BB%84%E5%90%88-toc" style="margin-left:40px;"><a href="#2.4%20%E7%BB%84%E5%90%88" rel="nofollow">2.4 组合</a></p> 
<p id="%E4%B8%89%E3%80%81%E5%85%B7%E4%BD%93%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E5%85%B7%E4%BD%93%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4%EF%BC%9A" rel="nofollow">三、具体的实现步骤：</a></p> 
<p id="3.1%20%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-toc" style="margin-left:40px;"><a href="#3.1%20%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE" rel="nofollow">3.1 准备训练数据</a></p> 
<p id="3.2%20%E5%88%86%E6%AD%A5%E8%AE%AD%E7%BB%83SAE-toc" style="margin-left:40px;"><a href="#3.2%20%E5%88%86%E6%AD%A5%E8%AE%AD%E7%BB%83SAE" rel="nofollow">3.2 分步训练SAE</a></p> 
<p id="3.3%20%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;"><a href="#3.3%20%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C" rel="nofollow">3.3 分类结果</a></p> 
<p id="4.%20%E6%9C%AC%E6%96%87Matlab%E4%BB%A3%E7%A0%81-toc" style="margin-left:0px;"><a href="#4.%20%E6%9C%AC%E6%96%87Matlab%E4%BB%A3%E7%A0%81" rel="nofollow">4. 本文Matlab代码</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E6%91%98%E8%A6%81%EF%BC%9A" style="margin-left:0px;text-align:left;"><strong>摘要：</strong></h2> 
<p style="margin-left:0;text-align:left;">        基于Matlab平台，构建并训练堆叠自编码器对手写图片进行特征提取，然后使用提取到的图像特征信息对手写图像（0-9的手写数字图）进行识别与分类，并测试其分类的准确度。堆叠自编码器（SAE）具有多个隐含层的神经网络可用于处理复杂数据（例如图像）的分类问题。每个层都可以学习不同抽象级别的特征。然而，在实际工作中，训练具有多个隐含层的神经网络可能会很困难。本文使用一种有效训练具有多个层的神经网络的方法是一次训练一个层。为此，您可以为每个所需的隐含层训练一种称为自编码器的特殊类型的网络。此示例说明如何训练具有两个隐含层的神经网络以对图像中的数字进行分类。首先，使用自编码器以无监督方式单独训练各隐含层。然后训练最终 softmax 层，并将这些层连接在一起形成堆叠网络，该网络最后以有监督方式进行训练。</p> 
<h2 id="%E4%B8%80%E3%80%81%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%88Auto-Encoder%EF%BC%8CAE%EF%BC%89" style="margin-left:0px;text-align:left;"><strong>一、自动编码器基本概念（Auto-Encoder，AE）</strong></h2> 
<p style="margin-left:0;text-align:left;">        自编码器（autoencoder)是神经网络的一种，经过训练后能尝试将输入复制到输出。自编 码器内部有一个隐藏层h,可以产生编码(code)表示输入。该网络可以看作由两部分组 成：一个由函数h=f(X)表示的编码器和一个生成重构的解码器r=g(h)。我们不应该将自 编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似 地复制，并只能复制与训练数据相似的输入。自动编码机由三层网络组成，其中输入层神经元数量与输出层神经元数量相等，中间层神 经元数量少于输入层和输出层。</p> 
<p style="margin-left:0;text-align:left;">        搭建一个自动编码器需要完成下面三样工作：搭建编码器， 搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息（自编码器是有损的）。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化.</p> 
<p class="img-center"><img alt="" height="126" src="https://images2.imgbox.com/b6/e8/dh9lJUWc_o.png" width="414"></p> 
<p style="margin-left:0;text-align:left;">        自动编码机(Auto-encoder)是一个自监督的算法，并不是一个无监督算法，它不需要对 训练样本进行标记，其标签产生自输入数据。因此自编码器很容易对指定类的输入训练出 一种特定的编码器，而不需要完成任何新工作。自动编码器是数据相关的，只能压缩那些 与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树 木时性能很差，因为它学习到的特征是与人脸相关的。~ 自动编码器运算过程：原始input(设为)经过加权(W、b)、映射(Sigmoid)之后得到 y,再对y反向加权映射回来成为z。通过反复迭代训练两组(W、b),目的就是使输出信 号与输入信号尽量相似。训练结束之后自动编码器可以由两部分组成：</p> 
<p style="margin-left:0;text-align:left;"><strong>1.</strong><strong>输入层和中间层，可以用这个网络来对信号进行压缩</strong></p> 
<p style="margin-left:0;text-align:left;"><strong>2.</strong><strong>中间层和输出层，我们可以将压缩的信号进行还原</strong></p> 
<h2 id="%E4%BA%8C%E3%80%81%E5%A0%86%E5%8F%A0%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88SAE%EF%BC%89%E7%9A%84%E5%8E%9F%E7%90%86" style="margin-left:0px;text-align:left;"><strong>二、堆叠自编码器（SAE）的原理</strong></h2> 
<p style="margin-left:0;text-align:left;">        之前之所以将自编码器模型表示为3层的神经网络，那是因为训练的需要，我们将原始数据作为假想的目标输出，以此构建监督误差来训练整个网络。等训练结束后，输出层就可以去掉了，因为我们只关心的是从(x)到(h)的变换。</p> 
<p style="margin-left:0;text-align:left;">        接下来的思路就很自然了，我们已经得到特征表达(h)，那么我们可不可以将(h)再作为原始信息，训练一个新的自编码器，得到新的特征表达呢？当软可以，而且这就是所谓的堆叠自编码器（Stacked Autoencoder，SAE）。Stacked就是逐层堆叠的意思，这个跟“栈”有点像。当把多个自编码器Stack起来之后，这个系统看起来就像这样：</p> 
<p style="margin-left:0;text-align:left;"><img alt="" height="91" src="https://images2.imgbox.com/68/bd/5PKXFdv8_o.png" width="865">​</p> 
<h3 id="2.1%20%E7%AC%AC%E4%B8%80%E5%B1%82AE" style="margin-left:0px;text-align:left;"><strong>2.1 </strong><strong>第一层AE</strong></h3> 
<p style="margin-left:0;text-align:left;">        这样就把自编码器改成了深度结构了，即《learning multiple levels of representation and abstraction》(Hinton, Bengio, LeCun, 2015)。需要注意的是，整个网络的训练不是一蹴而就的，而是逐层进行的。比如说我们要训练一个(n -&gt; m -&gt; k) 结构的网络，实际上我们是先训练网络(n -&gt; m -&gt; n)，得到(n -&gt; m)的变换，然后再训练(m -&gt; k -&gt; m)网络，得到(m -&gt; k)的变换。最终堆叠成SAE，即为(n -&gt; m -&gt; k)的结果，整个过程就像一层层往上面盖房子，这就是大名鼎鼎的 layer-wise unsuperwised pre-training （逐层非监督预训练）。</p> 
<p style="margin-left:0;text-align:left;">        接下来我们来看一个具体的例子，假设你想要训练一个包含两个隐藏层的堆叠自编码器，用来训练 MNIST 手写数字分类。</p> 
<p style="margin-left:0;text-align:left;">        首先，你需要用原始输入(x(k))训练第一个稀疏自编码器中，它能够学习得到原始输入的一阶特征表示(h(1)(k))，如下图所示：</p> 
<p class="img-center"><img alt="" height="384" src="https://images2.imgbox.com/b5/0a/q4DnioC9_o.png" width="281"></p> 
<h3 id="2.2%20%E7%AC%AC%E4%BA%8C%E5%B1%82AE" style="margin-left:0px;text-align:left;"><strong>2.2 </strong><strong>第二层AE</strong></h3> 
<p style="margin-left:0;text-align:left;">        接着，你需要把原始数据输入到上述训练好的稀疏自编码器中，对于每一个输入(x(k))，都可以得到它对应的一阶特征表示(h(1)(k))。然后你再用这些一阶特征作为另一个稀疏自编码器的输入，使用它们来学习二阶特征(h(2)(k))，如下图：</p> 
<p class="img-center"><img alt="" height="377" src="https://images2.imgbox.com/ec/7e/NCQSTLsF_o.png" width="299"></p> 
<h3 id="2.3%20%E7%AC%AC%E4%B8%89%E5%B1%82" style="margin-left:0px;text-align:left;"><strong>2.3 </strong><strong>第三层</strong></h3> 
<p style="margin-left:0;text-align:left;">        同样，再把一阶特征输入到刚训练好的第二层稀疏自编码器中，得到每个(h(1)(k))对应的二阶特征激活值$h(2)(k) $。接下来，你可以把这些二阶特征作为softmax分类器的输入，训练得到一个能将二阶特征映射到数字标签的模型。如下图：</p> 
<p class="img-center"><img alt="" height="357" src="https://images2.imgbox.com/1e/f5/PhVdcH4w_o.png" width="405"></p> 
<h3 id="2.4%20%E7%BB%84%E5%90%88" style="margin-left:0px;text-align:left;"><strong>2.4 组合</strong></h3> 
<p style="margin-left:0;text-align:left;">        最终，你可以将这三层结合起来构建一个包含两个隐藏层和一个最终softmax分类器层的堆叠自编码网络，这个网络能够如你所愿地对MNIST数据集进行分类。最终模型如下图：</p> 
<p class="img-center"><img alt="" height="454" src="https://images2.imgbox.com/21/24/TOISG5ec_o.png" width="523"></p> 
<h2 id="%E4%B8%89%E3%80%81%E5%85%B7%E4%BD%93%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4%EF%BC%9A" style="margin-left:0px;text-align:left;"><strong>三、具体的实现步骤：</strong></h2> 
<h3 id="3.1%20%E5%87%86%E5%A4%87%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE" style="margin-left:0px;text-align:left;"><strong>3.1 </strong><strong>准备训练数据</strong></h3> 
<p style="margin-left:0;text-align:left;">        此示例始终使用合成数据进行训练和测试。已通过对使用不同字体创建的数字图像应用随机仿射变换来生成合成图像。每个数字图像为 28×28 像素，共有 5000 个训练样本。</p> 
<p class="img-center"><img alt="" height="396" src="https://images2.imgbox.com/53/c8/GXdeA65p_o.png" width="507"></p> 
<h3 id="3.2%20%E5%88%86%E6%AD%A5%E8%AE%AD%E7%BB%83SAE" style="margin-left:0px;text-align:left;"><strong>3.2 </strong><strong>分步训练SAE</strong></h3> 
<p style="margin-left:0;text-align:left;">   <strong>     </strong><strong>（AE.1）：</strong>首先在不使用标签的情况下基于训练数据训练稀疏自编码器。自编码器是一种神经网络，该网络会尝试在其输出端复制其输入。因此，其输入的大小将与其输出的大小相同。当隐藏层中的神经元数量小于输入的大小时，自编码器将学习输入的压缩表示。第一个AE的隐含层大小设计为100。</p> 
<p class="img-center"><img alt="" height="469" src="https://images2.imgbox.com/23/35/pgBRx81c_o.png" width="338"></p> 
<p style="margin-left:0;text-align:left;">        <strong>（AE.2）</strong>训练完第一个自编码器后，您需要以相似的方式训练第二个自编码器。主要区别在于您将使用从第一个自编码器生成的特征作为第二个自编码器中的训练数据。此外，您还需要将隐含表示的大小减小到 50，以便第二个自编码器中的编码器学习输入数据的更小表示。</p> 
<p style="margin-left:0;text-align:left;">       <strong>（分类层）</strong>训练 softmax 层以对 50 维特征向量进行分类。与自编码器不同，将使用训练数据的标签以有监督方式训练 softmax 层。</p> 
<p class="img-center"><img alt="" height="566" src="https://images2.imgbox.com/ca/a1/lSB0WkkQ_o.png" width="400"></p> 
<h3 id="3.3%20%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C" style="margin-left:0px;text-align:left;"><strong>3.3 </strong><strong>分类结果</strong></h3> 
<p style="margin-left:0;text-align:left;">   <strong>     </strong><strong>通过构建测试集的混淆矩阵查看最后的网络分类测试结果，可以看到SAE网络在对手写图像的特征提取后，使得图像识别的分类准确度达到了99.8%，实现了较好的图像分类的效果。</strong></p> 
<p></p> 
<p class="img-center"><img alt="" height="556" src="https://images2.imgbox.com/3a/08/O8TzlniP_o.png" width="515"></p> 
<h2 id="4.%20%E6%9C%AC%E6%96%87Matlab%E4%BB%A3%E7%A0%81"><strong>4. 本文Matlab代码</strong></h2>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0e8d8c894fc0c968ceb7b27d71beeed7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于Unity的PoissonDiscSampling泊松盘采样 随机分布位置不均匀的点</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f859063ceab1182e84f96317bc33ae35/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">无法ping 通华为云ECS服务器公网IP的解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>