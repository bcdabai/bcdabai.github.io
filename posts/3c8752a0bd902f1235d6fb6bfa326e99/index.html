<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop、hive、sqoop入门及完整小例子 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hadoop、hive、sqoop入门及完整小例子" />
<meta property="og:description" content="Hadoop MapReduce 和 HDFS
有自己的RPC和序列化机制
hadoop1.x HDFS：在1.x中的NameNode只可能有一个，虽然可以通过SecondaryNameNode与NameNode进行数据同步备份，但是总会存在一定的时延，如果NameNode挂掉，但是如果有部份数据还没有同步到SecondaryNameNode上，是可能会存在着数据丢失的问题（冷备份、定期checkpoint、单点问题）。
MapReduce：
（1）JobTracker 是 Map-reduce 的集中处理点，存在单点故障；
（2）JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker 失效的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限；
流程：
（1）首先用户程序 （JobClient） 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。
（2）TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。
（3）TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat发送给JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。
hadoop2.x YARN&#43;MapReduce HDFS：HA
YARN 并不是下一代MapReduce（MRv2），下一代MapReduce与第一代MapReduce（MRv1）在编程接口、数据处理引擎（MapTask和ReduceTask）是完全一样的， 可认为MRv2重用了MRv1的这些模块，不同的是资源管理和作业管理系统，MRv1中资源管理和作业管理均是由JobTracker实现的，集两个功能于一身，而在MRv2中，将这两部分分开了。 其中，作业管理由ApplicationMaster实现，而资源管理由新增系统YARN完成，由于YARN具有通用性，因此YARN也可以作为其他计算框架的资源管理系统，不仅限于MapReduce，也是其他计算框架（例如Spark）的管理平台。
ResourceManager：全局资源管理器，基于应用程序对资源的需求进行调度的 ; 每一个应用程序需要不同类型的资源因此就需要不同的容器。资源按需调度，包括：内存，CPU，磁盘，网络等等。
NodeManager：节点代理，监控资源使用情况并且向调度器汇报。
ApplicationMaster：类似JobTracker，向调度器索要适当的资源容器，结合从 ResourceManager 获得的资源和 NodeManager 协同工作来运行和监控任务。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3c8752a0bd902f1235d6fb6bfa326e99/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-07-27T16:08:31+08:00" />
<meta property="article:modified_time" content="2017-07-27T16:08:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop、hive、sqoop入门及完整小例子</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 style="margin:30px 0px 0px; font-size:24px; font-weight:normal; line-height:1.25; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Hadoop</strong></h2> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> MapReduce 和 HDFS</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 有自己的RPC和序列化机制</p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>hadoop1.x</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">HDFS：在</span><span style="color:rgb(0,0,0)">1.x</span><span style="color:rgb(0,0,0)">中的</span><span style="color:rgb(0,0,0)">NameNode</span><span style="color:rgb(0,0,0)">只可能有一个，虽然可以通过</span><span style="color:rgb(0,0,0)">SecondaryNameNode</span><span style="color:rgb(0,0,0)">与</span><span style="color:rgb(0,0,0)">NameNode</span><span style="color:rgb(0,0,0)">进行数据同步备份，但是总会存在一定的时延，如果</span><span style="color:rgb(0,0,0)">NameNode</span><span style="color:rgb(0,0,0)">挂掉，但是如果有部份数据还没有同步到</span><span style="color:rgb(0,0,0)">SecondaryNameNode</span><span style="color:rgb(0,0,0)">上，是可能会存在着数据丢失的问题（冷备份、定期checkpoint、单点问题）。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">MapReduce：<br> </span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （1）JobTracker 是 Map-reduce 的集中处理点，存在单点故障；</p> 
<p style="margin-top:10px; margin-bottom:0px; margin-left:auto; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （2）JobTracker 完成了太多的任务，造成了过多的资源消耗，当 map-reduce job 非常多的时候，会造成很大的内存开销，潜在来说，也增加了 JobTracker 失效的风险，这也是业界普遍总结出老 Hadoop 的 Map-Reduce 只能支持 4000 节点主机的上限；</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)"> </span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <img src="https://images2.imgbox.com/c4/ce/thIcl27Q_o.png" alt=""><br> </p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 流程：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （1）首先用户程序 （JobClient） 提交了一个 job，job 的信息会发送到 Job Tracker 中，Job Tracker 是 Map-reduce 框架的中心，他需要与集群中的机器定时通信 (heartbeat), 需要管理哪些程序应该跑在哪些机器上，需要管理所有 job 失败、重启等操作。</p> 
<p style="margin-top:10px; margin-bottom:0px; margin-left:auto; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （2）TaskTracker 是 Map-reduce 集群中每台机器都有的一个部分，他做的事情主要是监视自己所在机器的资源情况。</p> 
<p style="margin-top:10px; margin-bottom:0px; margin-left:auto; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （3）TaskTracker 同时监视当前机器的 tasks 运行状况。TaskTracker 需要把这些信息通过 heartbeat发送给JobTracker，JobTracker 会搜集这些信息以给新提交的 job 分配运行在哪些机器上。</p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>hadoop2.x YARN+MapReduce</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> HDFS：HA</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">YARN 并不是下一代MapReduce（MRv2），下一代MapReduce与第一代MapReduce（MRv1）在编程接口、数据处理引擎（MapTask和ReduceTask）是完全一样的， 可认为MRv2重用了MRv1的这些模块，</span>不同的是资源管理和作业管理系统<span style="color:rgb(0,0,0)">，MRv1中资源管理和作业管理均是由JobTracker实现的，集两个功能于一身，而在MRv2中，将这两部分分开了。 其中，</span>作业管理由ApplicationMaster实现<span style="color:rgb(0,0,0)">，而资</span>源管理由新增系统YARN完成<span style="color:rgb(0,0,0)">，由于YARN具有通用性，因此YARN也可以作为其他计算框架的资源管理系统，不仅限于MapReduce，也是其他计算框架（例如Spark）的管理平台。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <img src="https://images2.imgbox.com/d5/c1/hAVZQ9WG_o.jpg" alt=""><br> </p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">ResourceManager：全局资源管理器，基于应用程序对资源的需求进行调度的 ; 每一个应用程序需要不同类型的资源因此就需要不同的容器。资源按需调度，包括：内存，CPU，磁盘，网络等等。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">NodeManager：节点代理，监控资源使用情况并且向调度器汇报。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">ApplicationMaster：类似JobTracker，向调度器索要适当的资源容器，结合从 ResourceManager 获得的资源和 NodeManager 协同工作来运行和监控任务。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0)">Container：类似TaskTracker，运行Map和Reduce任务。</span></p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong><span style="color:rgb(0,0,0)">编程</span></strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Mapper：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-java">protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String[] splited = line.split("\t");  　　　　　　
            for (String word : splited) {          
                context.write(new Text(word), new LongWritable(1));
            }
        }</code></pre> 
<br> 
<br> Reducer： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-java">protected void reduce(Text k2, Iterable&lt;LongWritable&gt; v2s, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {        
            long count = 0L;
            for (LongWritable v2 : v2s) {
                count += v2.get();
            }
            LongWritable v3 = new LongWritable(count);
            context.write(k2, v3);
        }</code></pre> 
<br> 
<br> 执行过程： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 输入分片（Input split）：跟HDFS块大小有关，输入分片调整，合并小文件</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Map阶段</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Combiner阶段：提高网络传输效率，原则是combiner的输入不会影响到reduce计算的最终输入，例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Shuffle阶段：map输出一般比较大，先放到内存缓冲区，再spill到磁盘（对应溢出文件），多个溢出文件最后合并、partition（对应reducer）</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Reduce阶段</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 进阶小例子：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-java">public static class AverageMaper extends Mapper&lt;Object,Text,Text,Text&gt;
    {
        //private final static IntWritable one=new IntWritable(1);
        private static Text word=new Text();
        public void map(Object key,Text value,Context context) throws
        IOException,InterruptedException
        {
            StringTokenizer itr=new StringTokenizer(value.toString());
            while(itr.hasMoreTokens())
            {
                word.set(itr.nextToken());
                if(itr.hasMoreTokens())
                    context.write(word, new Text(itr.nextToken()+",1"));
            }
        }
    }
    public static class AveragerCombine extends Reducer&lt;Text,Text,Text,Text&gt;
    {
        public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws
        IOException,InterruptedException
        {
            int sum=0;
            int cnt=0;
            for(Text val:values)
            {
                String []str=val.toString().split(",");
                sum+=Integer.parseInt(str[0]);
                cnt+=Integer.parseInt(str[1]);
            }
            context.write(key,new Text(sum+","+cnt));
        }
    }
    public static class AveragerReduce extends Reducer&lt;Text,Text,Text,DoubleWritable&gt;
    {
        public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws 
        IOException,InterruptedException
        {
            int sum=0;
            int cnt=0;
            for(Text val:values)
            {
                String []str=val.toString().split(",");
                sum+=Integer.parseInt(str[0]);
                cnt+=Integer.parseInt(str[1]);                
            }
            double res=(sum*1.0)/cnt;
            context.write(key, new DoubleWritable(res));
        }
    }</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(0,0,0); font-size:20px"><strong>试验</strong></span></p> 
<p><span style="font-family:Arial,sans-serif; color:#333333"><span style="font-size:14px; background-color:rgb(240,240,240)"><br> </span></span></p> 
<p><span style="font-family:Arial,sans-serif; color:#333333"><span style="font-size:14px; background-color:rgb(240,240,240)"></span></span></p> 
<pre><code class="language-java">hdfs dfs -mkdir -p /user/who
hdfs dfs -mkdir input
hdfs dfs -put ./etc/hadoop/*.xml input
hdfs dfs -ls /user/who/input 或者 hdfs dfs -ls input
 
// output 目录不能存在
hdfs dfs -rm -r output
// hadoop grep 例子
hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.1.jar grep input output 'dfs[a-z.]'
hdfs dfs -cat output/*  或者先取回本地再查看  hdfs dfs -get output ./output 
// 直接文本查看
hdfs dfs -text /user/who/input/core-site.xml
// 本地验证
find ./etc/ -type f -name *.xml|xargs grep 'dfs[a-z.]'  -  --color
// hadoop wordcount 例子
hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.1.jar wordcount input output</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> HDFS信息：<a target="_blank" href="http://192.168.56.102:50070/dfshealth.html" rel="nofollow noopener noreferrer" style="color:rgb(59,115,175)!important">http://192.168.56.102:50070/dfshealth.html</a></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> MapReduce查看：<a target="_blank" href="http://192.168.56.102:8088/cluster/apps" rel="nofollow noopener noreferrer" style="color:rgb(59,115,175)!important">http://192.168.56.102:8088/cluster/apps</a></p> 
<h2 style="margin:30px 0px 0px; font-size:24px; font-weight:normal; line-height:1.25; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Hive</strong></h2> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> Hive是基于Hadoop的一个<a target="_blank" href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93" rel="nofollow noopener noreferrer" style="color:rgb(59,115,175)!important">数据仓库</a>工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合<a target="_blank" href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93" rel="nofollow noopener noreferrer" style="color:rgb(59,115,175)!important">数据仓库</a>的统计分析。</p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>关键配置</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 创建目录并赋予权限：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-plain">hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir -p /user/hive/tmp
hdfs dfs -mkdir -p /user/hive/log
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -chmod g+w /usr/hive/tmp
hdfs dfs -chmod g+w /usr/hive/log</code></pre> 
<br> 配置目录： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-html">&lt;property&gt;
    &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;
    &lt;value&gt;/user/hive/tmp&lt;/value&gt;
    &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;
    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;
    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;
  &lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hive.querylog.location&lt;/name&gt;
    &lt;value&gt;/user/hive/log&lt;/value&gt;
    &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
  &lt;/property&gt;</code></pre> 
<br> 
<p>Hive MetaStore配置，默认使用derby</p> 
<p></p> 
<pre><code class="language-html">&lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
    &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
    &lt;value&gt;hive&lt;/value&gt;
  &lt;/property&gt;</code></pre> 
<span style="font-family:Arial,sans-serif; font-size:14px; color:rgb(255,0,0)"><br> 注意</span> 
<span style="color:rgb(61,70,77); font-family:Arial,sans-serif; font-size:14px">：mysql-connector-java-*.jar 放入Hive的lib目录</span> 
<p></p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>语法</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(44,62,80)">许多关系型数据库都提供了命名空间的概念，用于划分不同的数据库或者Schema。例如MySQL支持的Database概念，PostgreSQL支持的namespace概念。Hive也有自己的schema的概念：</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(44,62,80)"></span></p> 
<pre><code class="language-sql">CREATE DATABASE dbname;
USE dbname;
DROP DATABASE dbname;
SHOW DATABASES; // 默认 default</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （1）如何执行：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">// 进入hive命令行后可以直接执行dfs命令，如 dfs -ls /user/hive
hive
hive -e "show tables"
hive -f create_table.hql</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （2）表操作：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 内部表和外部表的区别：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 内部表：<span style="color:rgb(44,62,80)">将数据移动到对应的warehouse目录下，速度非常快，不会对数据是否符合定义的Schema做校验，这个工作通常在读取的时候进行，称为Schema On Read。使用DROP语句删除后，其数据和表的元数据都被删除。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 外部表：<span style="color:rgb(44,62,80)">不会把数据移动到warehouse目录中。事实上，Hive甚至不会校验外部表的目录是否存在。这使得我们可以在创建表格之后再创建数据。当删除外部表时，Hive只删除元数据，而外部数据不动。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(44,62,80)">适用场景：大多数情况下，这两者的区别不是很明显。如果数据的所有处理都在Hive中进行，那么更倾向于选择内部表。但是如果Hive和其他工具针对相同的数据集做处理，外部表更合适。一种常见的模式是使用外部表访问存储的HDFS（通常由其他工具创建）中的初始数据，然后使用Hive转换数据并将其结果放在内部表中。相反，外部表可以用于将Hive的处理结果导出供其他应用使用。使用外部表的另一种场景是针对一个数据集，关联多个Schema。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 外部表创建：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">CREATE EXTERNAL TABLE external_table (dummy STRING)
LOCATION '/user/root/external_table';</code></pre> 
<br> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 表操作：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">SHOW TABLES;
ALTER TABLE source RENAME TO target;
ALTER TABLE source ADD COLUMNS (col3 STRING);
DROP TABLE source;
TRUNCATE TABLE my_table;</code></pre> 
<br> 直接创建： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">CREATE TABLE  
IF NOT EXISTS useracc (  
    dt string,  
    time string,  
    x_forwarded_for string,  
    client_ip string,  
    server_ip string,  
    http_status string,  
    content_length string,  
    time_taken string,  
    http_method string,  
    url string,  
    query_string string  
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;  </code></pre> 
<br> 复制表结构： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">create table if not exists table002 like table001;
</code></pre> 
<br> CTAS语句： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">CREATE TABLE table002
AS select ip, time from table001; </code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （3）插入数据：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 直接插入：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">insert into users values (0,'Nat'),(2,'Joe'),(3,'Kay'),(4,'Ann');
</code></pre> 
<br> 本地文件系统导入： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">LOAD DATA LOCAL INPATH "/home/hadoopUser/data/test1.txt"  INTO TABLE test1;
</code></pre> 
<br> HDFS导入： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">LOAD DATA INPATH "/input/test1.txt" OVERWRITE INTO TABLE test1;  </code></pre> 
<p></p> 
<p>从查询结果导入：</p> 
<p></p> 
<pre><code class="language-sql">INSERT INTO TABLE test4 SELECT * FROM test1; 
</code></pre> 
<br> CTAS语句： 
<p></p> 
<p></p> 
<pre><code class="language-sql">CREATE TABLE table002
AS select ip, time from table001; </code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 多表插入：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">FROM records
INSERT INTO TABLE stations_by_year
  SELECT year ,COUNT(DISTINCT station)
  GROUP BY year
INSERT INTO TABLE record_by_year
  SELECT year,count(1)
  GROUP BY year
INSERT INTO TABLE good_records_by_year
  SELECT year , count(1)
  WHERE temperature != 9999 AND quality in (0,1,4,5,9)
  GROUP BY year;</code></pre> 
<br> （4）分区： 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(44,62,80)">Hive将表划分为分区，Partition根据分区字段进行。分区可以让数据的部分查询变得更快。</span><span style="color:rgb(44,62,80)">我们将用于分区的字段成为分区字段，但是在数据文件中，不存在这些字段的值，这些值是从目录中推断出来的。但是在SELECT语句中，我们依然可使用分区字段。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 静态分区：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">create table if not exists partition_table002 like partition_table001;  
insert overwrite table partition_table002 partition (dt='20150617', ht='00') select name, ip from partition_table001 where dt='20150607';</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 动态分区：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">set hive.exec.dynamic.partition=true;  
set hive.exec.dynamic.partition.mode=nonstrict;  
insert overwrite table partition_table002 partition (dt, ht) select * from partition_table001 where dt='20150617';  
INSERT OVERWRITE TABLE T PARTITION (dt, ht)  SELECT key, value, dt, ht FROM srcpart WHERE dt is not null and ht&gt;10;  </code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(85,85,85)">hive先获取select的最后两个位置的dt和ht参数值，然后将这两个值填写到insert语句partition中的两个dt和ht变量中，即动态分区是通过位置来对应分区值的。原始表select出来的值和输出partition的值的关系仅仅是通过位置来确定的，和名字并没有关系，比如这里dt和st的名称完全没有关系。</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(85,85,85)">混合使用：</span></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(85,85,85)"></span></p> 
<pre><code class="language-sql">INSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr)  SELECT key, value, /*ds,*/ hr FROM srcpart WHERE ds is not null and hr&gt;10;  // right
INSERT OVERWRITE TABLE T PARTITION (ds, hr = 11)  SELECT key, value, ds/*, hr*/ FROM srcpart WHERE ds is not null and hr=11;  // wrong</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> （5）分桶：</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 对于每一个表（table）或者分区，Hive可以进一步组织成桶。Hive也是针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。采用桶能够带来一些好处，比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> bucket主要作用：数据sampling；提升某些查询操作效率，例如mapside join。</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql">set hive.enforce.bucketing = true; 
 
create table student(id INT, age INT, name STRING)
partitioned by(stat_date STRING) 
clustered by(id) sorted by(age) into 2 bucket
row format delimited fields terminated by ',';
 
from student_tmp 
insert overwrite table student partition(stat_date="20120802") 
select id,age,name where stat_date="20120802" sort by age;</code></pre> 
<br> 'set hive.enforce.bucketing = true'  可以自动控制上一轮reduce的数量从而适配bucket的个数，当然，用户也可以自主设置mapred.reduce.tasks去适配bucket 个数，推荐使用'set hive.enforce.bucketing = true' 。 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 抽样语法：TABLESAMPLE(BUCKET x OUT OF y)</p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> </p> 
<pre><code class="language-sql"> select * from student tablesample(bucket 1 out of 2 on id);</code></pre> 
<p></p> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取 (64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例 如，table总bucket数为32，tablesample(bucket 3 out of  16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>试验</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 具体步骤参考另一篇文章：<a target="_blank" href="http://blog.csdn.net/fight4gold/article/details/70231322" style="color:rgb(59,115,175)!important" rel="noopener noreferrer">大数据串讲-从日志文件分析访问量最高的10个接口及响应访问量</a></p> 
<h2 style="margin:30px 0px 0px; font-size:24px; font-weight:normal; line-height:1.25; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Sqoop</strong></h2> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <strong><br> </strong></p> 
<div class="para" style="color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px">
  Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库 
 <em>（例如 ： MySQL ,Oracle ,Postgres等）</em>中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。 
</div> 
<div class="para" style="color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px">
  Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个 
 <a target="_blank" href="https://baike.baidu.com/item/Apache/6265" rel="nofollow noopener noreferrer" style="color:rgb(59,115,175)!important">Apache</a>项目。 
</div> 
<h2 class="para" style="margin:30px 0px 0px; font-size:24px; font-weight:normal; line-height:1.25; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>数据收集</strong></h2> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>手动SCP</strong></h3> 
<h3 style="margin:10px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Rsync命令同步文件</strong></h3> 
<h3 style="margin:10px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Sqoop从关系型数据库导入</strong></h3> 
<h3 style="margin:10px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong>Flume收集</strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> 具体参考另一篇文章：<a target="_blank" href="http://blog.csdn.net/fight4gold/article/details/71078721" style="color:rgb(59,115,175)!important" rel="noopener noreferrer">Apache Flume 日志收集案例</a></p> 
<h3 style="margin:30px 0px 0px; font-size:20px; font-weight:normal; line-height:1.5; color:rgb(51,51,51); border-bottom-color:rgb(204,204,204); font-family:Arial,sans-serif"> <strong><span style="color:rgb(68,68,68)">Logstash收集</span></strong></h3> 
<p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <span style="color:rgb(68,68,68)">ELK（ElasticSearch、Logstash、Kibana）三搭档之一</span></p> 
<div style="top:0px"> 
 <p style="margin-top:10px; margin-bottom:0px; color:rgb(51,51,51); font-family:Arial,sans-serif; font-size:14px"> <br> </p> 
 <div> 
  <span style="color:rgb(68,68,68)"><br> </span> 
 </div> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/81a65463d1ccfa70c0543faa73ccc54a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">python四种方法实现去除列表中的重复元素</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/daaae4cbd1dd3cfdba5304a0c040b3c0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">matlab矩阵复制函数：【repmat】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>