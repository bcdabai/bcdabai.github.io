<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch之反向传播(三) - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch之反向传播(三)" />
<meta property="og:description" content="反向传播 反向传播算法原理关于层与层之间的顺序 pytorch中的反向传播算法Tensor类型数据代码中说明方法 反向传播算法 反向传播算法，简称BP算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。
看下面这张图，当我们面对这样复杂的数据关系时，如果想用解析式写出他们之间的关系，可能很难达到，那么我们想是否有这样一个算法就像是一张图，图中有着每一层变量和变量之间的关系，我们可以通过这样的关系做出相应的结果。
原理 算法分为两个阶段：
前向传播阶段：将训练输入送入网络以获得激励响应；
反向传播阶段：将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。
在输入一些数据的情况下，在运行图中每层数据时会反馈给我们一些值，这些值中包括一般的中间数据，也包括像均方误差一样能评估我们模型指标的数据，若这些评估指标没有达到我们的预期结果，我们可以让数据继续运行反向传播，返回一个梯度值，并根据梯度值进一步对于我们的权重（参数）进行进一步调整。
关于层与层之间的顺序 假设还是使用比较简单的一次函数举例：
W1和W2，b1和b2均是输入层中所需要输入的数据，MM和ADD是属于隐含层，MM就是乘上一个W，ADD就是加上一个b值，每一个Layer都是神经网络中的一层数据运算，y就是输出层数据，包含我们的评估指标等。另外，在运算每一层时该算法都会不断的求出每一层和上一层之间偏导数关系，这样在进行反向传播时就可以根据层与层之间的偏导数关系以及输出层的结果返回均方误差关于权值（参数）的偏导函数。有了这些偏导函数就可以对于权值进行进一步调整。
但是对于一元函数若仅仅是这样的操作得到的一个表达式，通过化简就会得到：
好像经过了两层网络和没有经过神经网络的函数结果基本相同，所以我们通常会在隐含层中添加一些特殊的运算σ以增加模型复杂度：
举个例子：
下面这一张图，w是权值（参数），x，y为输入层，绿色方块就是隐含层，黄色圆圈就是输出层，橙黄色内是反向传播层。
对于这样一个函数y=x*w，并有他的均方误差表达式
前向传播：在传入w值后，我们就可以由内而外的 将上一层数据结果作为该层的变量进行求偏导数，同时也将数据带入该层表达式中得出相应的输出层值，直到得到评估指标均方误差 loss
反向传播：运用链式法则将逐步求出 loss 对于权值w的偏导数。
这样的图叫做计算图
pytorch中的反向传播算法 Tensor类型数据 在pytorch中有一个数据类型叫做Tensor，其中存储着data权值和grad梯度，用于在框架中权值的更新，和梯度的存储供反向传播时应用。
仍然以上述的简单一维数据举例：
代码中说明方法 import matplotlib.pyplot as plt import numpy as np import torch def forward(x): # 这里的w就是初始化时的Tensor对象 # 在和基本数据相乘时，基本数据类型会转化为Tensor对象与之运算 # 所以x*w就是Tensor的数据类型 return x * w def loss(x, y): # Tensor类的数据类型 y_pred = forward(x) # 反回Tensor类的数据类型 return (y_pred - y) ** 2 # 训练的数据集 x_data = [1, 2, 3] y_data = [2, 4, 6] # 创建Tensor对象，初始化权值 w # 在后续以Tensor数据类型为单位的运算中，都会进行自动求偏导数存储在Tensor类型中,并以返回值的形式存储 w = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/9eaea09b87250f7e107f750a82a8c72a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-14T21:35:19+08:00" />
<meta property="article:modified_time" content="2021-08-14T21:35:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch之反向传播(三)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>反向传播</h4> 
 <ul><li><ul><li><a href="#_2" rel="nofollow">反向传播算法</a></li><li><ul><li><a href="#_9" rel="nofollow">原理</a></li><li><a href="#_16" rel="nofollow">关于层与层之间的顺序</a></li></ul> 
   </li><li><a href="#pytorch_33" rel="nofollow">pytorch中的反向传播算法</a></li><li><ul><li><a href="#Tensor_34" rel="nofollow">Tensor类型数据</a></li><li><a href="#_38" rel="nofollow">代码中说明方法</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_2"></a>反向传播算法</h3> 
<p>反向传播算法，简称BP算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。</p> 
<p>看下面这张图，当我们面对这样复杂的数据关系时，如果想用解析式写出他们之间的关系，可能很难达到，那么我们想是否有这样一个算法就像是一张图，图中有着每一层变量和变量之间的关系，我们可以通过这样的关系做出相应的结果。<br> <img src="https://images2.imgbox.com/c0/9f/zILJ82O1_o.png" alt="在这里插入图片描述" width="450"></p> 
<h4><a id="_9"></a>原理</h4> 
<p>算法分为两个阶段：<br> <strong>前向传播阶段</strong>：将训练输入送入网络以获得激励响应；<br> <strong>反向传播阶段</strong>：将激励响应同训练输入对应的目标输出求差，从而获得隐层和输出层的响应误差。</p> 
<p>在输入一些数据的情况下，在运行图中每层数据时会反馈给我们一些值，这些值中包括一般的中间数据，也包括像均方误差一样能评估我们模型指标的数据，若这些评估指标没有达到我们的预期结果，我们可以让数据继续运行反向传播，返回一个梯度值，并根据梯度值进一步对于我们的权重（参数）进行进一步调整。<br> <img src="https://images2.imgbox.com/a5/ad/om7NxIQO_o.png" alt="在这里插入图片描述" width="450"></p> 
<h4><a id="_16"></a>关于层与层之间的顺序</h4> 
<p>假设还是使用比较简单的一次函数举例：<br> <img src="https://images2.imgbox.com/9c/41/fVizFVfp_o.png" alt="在这里插入图片描述" width="450"><br> W1和W2，b1和b2均是输入层中所需要输入的数据，MM和ADD是属于隐含层，MM就是乘上一个W，ADD就是加上一个b值，每一个Layer都是神经网络中的一层数据运算，y就是输出层数据，包含我们的评估指标等。另外，在运算每一层时该算法都会不断的求出每一层和上一层之间偏导数关系，这样在进行反向传播时就可以根据层与层之间的偏导数关系以及输出层的结果返回均方误差关于权值（参数）的偏导函数。有了这些偏导函数就可以对于权值进行进一步调整。</p> 
<p>但是对于一元函数若仅仅是这样的操作得到的一个表达式，通过化简就会得到：<br> <img src="https://images2.imgbox.com/fb/26/4av8jmDO_o.png" alt="在这里插入图片描述" width="450"><br> 好像经过了两层网络和没有经过神经网络的函数结果基本相同，所以我们通常会在隐含层中添加一些特殊的运算σ以增加模型复杂度：<br> <img src="https://images2.imgbox.com/c0/a2/z2ViHWAY_o.png" alt="在这里插入图片描述" width="450"><br> <strong>举个例子：</strong><br> 下面这一张图，w是权值（参数），x，y为输入层，绿色方块就是隐含层，黄色圆圈就是输出层，橙黄色内是反向传播层。<br> 对于这样一个函数y=x*w，并有他的均方误差表达式<br> <strong>前向传播</strong>：在传入w值后，我们就可以由内而外的 将上一层数据结果作为该层的变量进行求偏导数，同时也将数据带入该层表达式中得出相应的输出层值，直到得到评估指标均方误差 <em>loss</em><br> <strong>反向传播</strong>：运用链式法则将逐步求出 <em>loss</em> 对于权值w的偏导数。<br> <img src="https://images2.imgbox.com/e9/ef/Kp6UQhF7_o.png" alt="在这里插入图片描述" width="450"><br> <img src="https://images2.imgbox.com/e5/69/a35v0IBl_o.jpg" alt="在这里插入图片描述" width="450"><br> 这样的图叫做计算图</p> 
<h3><a id="pytorch_33"></a>pytorch中的反向传播算法</h3> 
<h4><a id="Tensor_34"></a>Tensor类型数据</h4> 
<p>在pytorch中有一个数据类型叫做Tensor，其中存储着data权值和grad梯度，用于在框架中权值的更新，和梯度的存储供反向传播时应用。<br> <img src="https://images2.imgbox.com/29/33/dgdZ2AfC_o.png" alt="在这里插入图片描述" width="450"><br> 仍然以上述的简单一维数据举例：</p> 
<h4><a id="_38"></a>代码中说明方法</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch


<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 这里的w就是初始化时的Tensor对象</span>
    <span class="token comment"># 在和基本数据相乘时，基本数据类型会转化为Tensor对象与之运算</span>
    <span class="token comment"># 所以x*w就是Tensor的数据类型</span>
    <span class="token keyword">return</span> x <span class="token operator">*</span> w


<span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Tensor类的数据类型</span>
    y_pred <span class="token operator">=</span> forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token comment"># 反回Tensor类的数据类型</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>

<span class="token comment"># 训练的数据集</span>
x_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>
y_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span>
<span class="token comment"># 创建Tensor对象，初始化权值 w</span>
<span class="token comment"># 在后续以Tensor数据类型为单位的运算中，都会进行自动求偏导数存储在Tensor类型中,并以返回值的形式存储</span>
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 是否存储运算过程中的计算图，这里设置为True，默认False</span>
<span class="token comment"># 在后续中通过 Tensor对象.backward() 的方式存储在初始Tensor的变量中</span>
w<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token comment"># w=torch.tensor([1.0],requires_grad=True) 上两句可以用一句代替</span>

l_list<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
w_list<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 存储总体样本的误差值</span>
    l_sum<span class="token operator">=</span><span class="token number">0</span>
    <span class="token keyword">for</span> x_val<span class="token punctuation">,</span> y_val <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> y_data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 经过loss函数调用，返回Tensor类的数据类型，用l变量接收</span>
        <span class="token comment"># l中data数据就是每个样本差值的平方</span>
        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>x_val<span class="token punctuation">,</span> y_val<span class="token punctuation">)</span>

        l_sum<span class="token operator">+=</span>l<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 将l中已存在计算图返回给初始的Tensor对象w</span>
        <span class="token comment"># 这样w就拥有了Tensor中grad的参数存储梯度计算图</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token triple-quoted-string string">"""
        w下有两个数据data和grad他们的类型均为torch.Tensor
        w       :           &lt;class 'torch.Tensor'&gt;   
        w.data  :           &lt;class 'torch.Tensor'&gt;   权重本身
        w.grad  :           &lt;class 'torch.Tensor'&gt;   损失函数关于权重的导数
        w.grad.data :       &lt;class 'torch.Tensor'&gt;   w的grad数据下也有data和grad(空)，data就是偏导数值
        
        w.item()    :       &lt;class 'float'&gt;          对于所有的Tensor和Tensor下的数据类型data，grad
        w.data.item()   :   &lt;class 'float'&gt;          通过item()取出的值都是float类型的数据
        w.grad.data.item(): &lt;class 'float'&gt;
        w.grad.item()   :   &lt;class 'float'&gt;

		float在和Tensor运算时自动成为Tensor，不能将float直接赋值给Tensor
        """</span>
        <span class="token comment"># 打印出样本值和偏导数</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\tgrad:'</span><span class="token punctuation">,</span>x_val<span class="token punctuation">,</span>y_val<span class="token punctuation">,</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 权值刷新 这种一个参数刷新一次的方式就是随机梯度下降的一种简单体现</span>
        w<span class="token punctuation">.</span>data <span class="token operator">=</span> w<span class="token punctuation">.</span>data<span class="token operator">-</span><span class="token number">0.001</span><span class="token operator">*</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data
        <span class="token comment"># w.data.item() = w.data.item()-0.001*w.grad.data.item()错误</span>

        <span class="token comment"># 将所有偏导数设置为0,否则在后续运算过程中会有累加效应</span>
        w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 输出迭代次数，误差值和1当前权值</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"progress:"</span><span class="token punctuation">,</span>epoch<span class="token punctuation">,</span> l_sum<span class="token punctuation">,</span>w<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    l_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l_sum<span class="token punctuation">)</span>
    w_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>w<span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

fig <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
ax <span class="token operator">=</span> fig<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">,</span>w_list<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"the value of w"</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">,</span>l_list<span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"the value of loss"</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"epoch"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/09/f6/GDoo8KJl_o.png" alt="在这里插入图片描述" width="450"><br> 实例化Tensor对象和创建计算图：<br> <img src="https://images2.imgbox.com/1a/f7/W6qaKwOO_o.png" alt="在这里插入图片描述" width="450"><br> 返回计算图，赋值给初始变量中的grad：<img src="https://images2.imgbox.com/0d/a4/73tWc4Om_o.png" alt="在这里插入图片描述" width="450"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/887ccb6a43a4a3e00104583b7edc94fc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JavaScript(ES6)数据结构与算法——队列</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d6a443a8ff5cbc3e312818ee8064482c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">flask web应用项目配置和访问</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>