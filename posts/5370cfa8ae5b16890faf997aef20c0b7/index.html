<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop&#43;Hdfs&#43;Hbase&#43;Kafka&#43;Zookeeper集群搭建 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hadoop&#43;Hdfs&#43;Hbase&#43;Kafka&#43;Zookeeper集群搭建" />
<meta property="og:description" content="摘要： 本文详细介绍了基于centos7的Hadoop集群的搭建过程，给出了Hadoop的基础知识和项目搭建的流程，搭建了一个master结点和两个slave结点的Hadoop集群，在master和slave服务器上配置HDFS(hadoop分布式文件系统) ,HBase(集群数据库)，zookeeper(集群维护服务),以及 kafka（消息队列），同时详细给出搭建过程和测试结果，并对搭建过程遇到的bug进行详细的记录
文章目录 摘要： 1 关于Hadoop1.1 Hadoop是什么1.2 Hadoop能做什么1.3 怎么使用Hadoop 2 项目准备2.1 Linux centos环境准备2.2安装 java 环境（三台机器都要安装）2.3 修改三台主机 hostname2.4 配置集群机器之间的免密登录 3 安装 hadoop3.1 解压 hadoop 压缩包3.2 配置环境变量3.3 修改配置文件3.4 拷贝文件3.5 创建临时文件目录3.6 格式化 hdfs 文件系统3.7 启动hadoop 4 测试Hadoop4.1 web 页面测试4.2 hdfs 功能测试4.3 mapreduce 功能测试 5 安装配置Zookeeper集群5.1 修改配置文件zoo.cfg5.2 新建并编辑myid文件 6 安装配置HBase集群6.1 修改配置文件6.2分发并同步安装包6.3 启动hbase 7 安装配置kafka集群7.1 修改配置文件7.2 分发并同步安装包7.3 slave1和slave2上更改broker id7.4 启动kafka 8 启动集群9 测试kafka10 测试Hbase11 遇到的bug11.1 启动hdfs时，datanodes报错11.2 启动hdfs时，datanode无法正常启动11.3 yarn无法正常启动 （resourceManager/nodeManager）11.4 webHDFS出错 1 关于Hadoop 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5370cfa8ae5b16890faf997aef20c0b7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-12-26T17:18:38+08:00" />
<meta property="article:modified_time" content="2019-12-26T17:18:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop&#43;Hdfs&#43;Hbase&#43;Kafka&#43;Zookeeper集群搭建</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>摘要：</h3> 
<p>本文详细介绍了基于centos7的Hadoop集群的搭建过程，给出了Hadoop的基础知识和项目搭建的流程，搭建了一个master结点和两个slave结点的Hadoop集群，在master和slave服务器上配置HDFS(hadoop分布式文件系统) ,HBase(集群数据库)，zookeeper(集群维护服务),以及 kafka（消息队列），同时详细给出搭建过程和测试结果，并对搭建过程遇到的bug进行详细的记录<br> </p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#_0" rel="nofollow">摘要：</a></li></ul> 
  </li><li><a href="#1_Hadoop_4" rel="nofollow">1 关于Hadoop</a></li><li><ul><li><a href="#11_Hadoop_5" rel="nofollow">1.1 Hadoop是什么</a></li><li><a href="#12_Hadoop_11" rel="nofollow">1.2 Hadoop能做什么</a></li><li><a href="#13_Hadoop_26" rel="nofollow">1.3 怎么使用Hadoop</a></li></ul> 
  </li><li><a href="#2__40" rel="nofollow">2 项目准备</a></li><li><ul><li><a href="#21_Linux_centos_41" rel="nofollow">2.1 Linux centos环境准备</a></li><li><a href="#22_java__48" rel="nofollow">2.2安装 java 环境（三台机器都要安装）</a></li><li><a href="#23__hostname_63" rel="nofollow">2.3 修改三台主机 hostname</a></li><li><a href="#24__84" rel="nofollow">2.4 配置集群机器之间的免密登录</a></li></ul> 
  </li><li><a href="#3__hadoop_117" rel="nofollow">3 安装 hadoop</a></li><li><ul><li><a href="#31__hadoop__118" rel="nofollow">3.1 解压 hadoop 压缩包</a></li><li><a href="#32__121" rel="nofollow">3.2 配置环境变量</a></li><li><a href="#33__128" rel="nofollow">3.3 修改配置文件</a></li><li><a href="#34__144" rel="nofollow">3.4 拷贝文件</a></li><li><a href="#35__152" rel="nofollow">3.5 创建临时文件目录</a></li><li><a href="#36__hdfs__160" rel="nofollow">3.6 格式化 hdfs 文件系统</a></li><li><a href="#37_hadoop_171" rel="nofollow">3.7 启动hadoop</a></li></ul> 
  </li><li><a href="#4_Hadoop_184" rel="nofollow">4 测试Hadoop</a></li><li><ul><li><a href="#41_web__185" rel="nofollow">4.1 web 页面测试</a></li><li><a href="#42_hdfs__200" rel="nofollow">4.2 hdfs 功能测试</a></li><li><a href="#43_mapreduce__207" rel="nofollow">4.3 mapreduce 功能测试</a></li></ul> 
  </li><li><a href="#5_Zookeeper_243" rel="nofollow">5 安装配置Zookeeper集群</a></li><li><ul><li><a href="#51_zoocfg_246" rel="nofollow">5.1 修改配置文件zoo.cfg</a></li><li><a href="#52_myid_263" rel="nofollow">5.2 新建并编辑myid文件</a></li></ul> 
  </li><li><a href="#6_HBase_282" rel="nofollow">6 安装配置HBase集群</a></li><li><ul><li><a href="#61__284" rel="nofollow">6.1 修改配置文件</a></li><li><a href="#62_335" rel="nofollow">6.2分发并同步安装包</a></li><li><a href="#63_hbase_343" rel="nofollow">6.3 启动hbase</a></li></ul> 
  </li><li><a href="#7_kafka_351" rel="nofollow">7 安装配置kafka集群</a></li><li><ul><li><a href="#71__354" rel="nofollow">7.1 修改配置文件</a></li><li><a href="#72__368" rel="nofollow">7.2 分发并同步安装包</a></li><li><a href="#73_slave1slave2broker_id_374" rel="nofollow">7.3 slave1和slave2上更改broker id</a></li><li><a href="#74_kafka_377" rel="nofollow">7.4 启动kafka</a></li></ul> 
  </li><li><a href="#8__389" rel="nofollow">8 启动集群</a></li><li><a href="#9_kafka_408" rel="nofollow">9 测试kafka</a></li><li><a href="#10_Hbase_434" rel="nofollow">10 测试Hbase</a></li><li><a href="#11_bug_454" rel="nofollow">11 遇到的bug</a></li><li><ul><li><a href="#111_hdfsdatanodes_456" rel="nofollow">11.1 启动hdfs时，datanodes报错</a></li><li><a href="#112_hdfsdatanode_463" rel="nofollow">11.2 启动hdfs时，datanode无法正常启动</a></li><li><a href="#113_yarn_resourceManagernodeManager_479" rel="nofollow">11.3 yarn无法正常启动 （resourceManager/nodeManager）</a></li><li><a href="#114_webHDFS_489" rel="nofollow">11.4 webHDFS出错</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1_Hadoop_4"></a>1 关于Hadoop</h2> 
<h3><a id="11_Hadoop_5"></a>1.1 Hadoop是什么</h3> 
<p>Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架，其核心部件是HDFS与MapReduce。</p> 
<p>HDFS是一个分布式文件系统：引入存放文件元数据信息的服务器Namenode和实际存放数据的服务器Datanode，对数据进行分布式储存和读取。</p> 
<p>MapReduce是一个分布式计算框架：MapReduce的核心思想是把计算任务分配给集群内的服务器里执行。通过对计算任务的拆分（Map计算/Reduce计算）再根据任务调度器（JobTracker）对任务进行分布式计算。</p> 
<h3><a id="12_Hadoop_11"></a>1.2 Hadoop能做什么</h3> 
<p>大数据存储：分布式存储</p> 
<p>日志处理：擅长日志分析</p> 
<p>ETL:数据抽取到oracle、mysql、DB2、mongdb及主流数据库</p> 
<p>机器学习: 比如Apache Mahout项目</p> 
<p>搜索引擎:Hadoop + lucene实现</p> 
<p>数据挖掘：目前比较流行的广告推荐，个性化广告推荐</p> 
<p>Hadoop是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。</p> 
<h3><a id="13_Hadoop_26"></a>1.3 怎么使用Hadoop</h3> 
<ul><li> <p>Hadoop集群的搭建<br> 无论是在windows上装几台虚拟机玩Hadoop，还是真实的服务器来玩，说简单点就是把Hadoop的安装包放在每一台服务器上，改改配置，启动就完成了Hadoop集群的搭建。</p> </li><li> <p>上传文件到Hadoop集群，实现文件存储<br> Hadoop集群搭建好以后，可以通过web页面查看集群的情况，还可以通过Hadoop命令来上传文件到hdfs集群，通过Hadoop命令在hdfs集群上建立目录，通过Hadoop命令删除集群上的文件等等。</p> </li><li> <p>编写map/reduce程序，完成计算任务<br> 通过集成开发工具（例如eclipse）导入Hadoop相关的jar包，编写map/reduce程序，将程序打成jar包扔在集群上执行，运行后出计算结果。</p> </li></ul> 
<h2><a id="2__40"></a>2 项目准备</h2> 
<h3><a id="21_Linux_centos_41"></a>2.1 Linux centos环境准备</h3> 
<ul><li>三台centos 7机器，本例中使用 vmware 创建三台虚拟机作为替代</li><li>准备好 jdk 8(不要使用jdk8以上的版本，会出n多bug) 和 hadoop-3.1.3 安装包<br> <img src="https://images2.imgbox.com/21/e0/2oFsUXW3_o.png" alt="在这里插入图片描述"></li><li>准备三台虚拟机<a href="https://blog.csdn.net/Boxbiu/article/details/103624840">配置静态ip点这里</a><br> 创建虚拟机，并且将网络适配器选用 NAT模式，并且配置为静态 ip, 机器静态ip配置参考 静态ip设置 分别将三台机器的ip 设置为 <strong>[192.168.19.200,192.168.19.201,192.168.19.202]</strong></li></ul> 
<h3><a id="22_java__48"></a>2.2安装 java 环境（三台机器都要安装）</h3> 
<p>由于 hadoop 框架的启动是依赖 java 环境，因此需要准备 jdk 环境，本例中使用的 jdk8 在/home/min/tar目录下进行演示，使用 jdk-8u202-linux-x64.tar.gz 解压jdk ，然后使用 mv jdk1.8.0_201 jdk8 将目录改名</p> 
<ul><li>配置 jdk 环境变量<br> 使用命令 vi /etc/profile 将如下代码添加到文件末尾，JAVA_HOME 变量设置为自己的目录</li></ul> 
<pre><code>export JAVA_HOME=/home/min/tar/jdk8 
export CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
</code></pre> 
<p>配置好以后使用 source /etc/profile 重新加载配置文件，使用 java -version 测试jdk 配置是否成功</p> 
<h3><a id="23__hostname_63"></a>2.3 修改三台主机 hostname</h3> 
<p>本例中Linux操作系统为为 centos 7，使用自带的 hostnamectl 修改hostname</p> 
<pre><code>//192.168.19.200 上执行
hostnamectl set-hostname master
//192.168.19.201 上执行
hostnamectl set-hostname slave1
//192.168.19.202 上执行
hostnamectl set-hostname slave2
</code></pre> 
<ul><li>修改 host文件<br> 使用 vi /etc/hosts 编辑该文件，在文件最后追加主机与ip的对应关系 最后重启机器，将这些配置生效</li></ul> 
<pre><code>192.168.19.200 master
192.168.19.201 slave1
192.168.19.202 slave2
</code></pre> 
<h3><a id="24__84"></a>2.4 配置集群机器之间的免密登录</h3> 
<p>hadoop 集群是通过主节点的 rpc 调用来对整个集群进行统一的操作管理，如果不配置免密登录，在每次启动集群时需要输入每个从节点的机器密码，免密登录很好的解决此问题</p> 
<ol><li>分别在三台机器上生成 ssh 链接的私钥和公钥（一直回车，直到结束），在用户的家目录下生成一个隐藏文件 .ssh</li></ol> 
<pre><code>ssh-keygen -t rsa
</code></pre> 
<p><img src="https://images2.imgbox.com/df/13/8VdjSVnm_o.png" alt="在这里插入图片描述"><br> 2. 将本机的公钥拷贝到其余两台机器和本机，以 master 机器为例</p> 
<pre><code>ssh-copy-id -i .ssh/id_rsa.pub slave1
ssh-copy-id -i .ssh/id_rsa.pub slave2
ssh-copy-id -i .ssh/id_rsa.pub master
</code></pre> 
<p><img src="https://images2.imgbox.com/b8/9d/6yEqi5x7_o.png" alt="在这里插入图片描述"></p> 
<ol start="3"><li> <p>将三台机器公钥都拷贝完成后，会在 ~/.ssh 文件下生成 authorized_keys，known_hosts 两个文件，存放的是一些免密登录的信息</p> </li><li> <p>关闭防火墙<br> 关闭防火墙，并且设置开机不启动</p> </li></ol> 
<pre><code>systemctl stop firewalld.service 
systemctl disable firewalld.service
</code></pre> 
<h2><a id="3__hadoop_117"></a>3 安装 hadoop</h2> 
<h3><a id="31__hadoop__118"></a>3.1 解压 hadoop 压缩包</h3> 
<p>使用命令 tar -zxvf hadoop-3.1.3.tar.gz 解压 hadoop 压缩包</p> 
<h3><a id="32__121"></a>3.2 配置环境变量</h3> 
<p>在三台机器上都配置 HADOOP_HOME ，vi /etc/profile 添加图中内容，保存后，使用 source /etc/profile 使配置文件生效<br> export JAVA_HOME=/home/min/tar/jdk-11.0.4<br> export CLASSPATH=.<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         J 
        
       
         A 
        
       
         V 
        
        
        
          A 
         
        
          H 
         
        
       
         O 
        
       
         M 
        
       
         E 
        
       
         / 
        
       
         l 
        
       
         i 
        
       
         b 
        
       
         / 
        
       
         d 
        
       
         t 
        
       
         . 
        
       
         j 
        
       
         a 
        
       
         r 
        
       
         : 
        
       
      
        JAVA_HOME/lib/dt.jar: 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.09618em;">J</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.02778em;">O</span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">b</span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mord mathdefault">t</span><span class="mord">.</span><span class="mord mathdefault" style="margin-right: 0.05724em;">j</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span>JAVA_HOME/lib/tools.jar<br> export HADOOP_HOME=/home/min/tar/hadoop-3.1.3<br> export PATH=JAVA_HOME/bin:<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         P 
        
       
         A 
        
       
         T 
        
       
         H 
        
       
         : 
        
       
      
        PATH: 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span>HADOOP_HOME/bin</p> 
<h3><a id="33__128"></a>3.3 修改配置文件</h3> 
<p>修改Hadoop目录下的 etc/hadoop文件夹中的core-site.xml，hadoop-env.sh，hdfs-site.xml，mapred-site.xml，yarn-site.xml，workers。先修改 master 节点上的配置文件，然后通过远程复制到 slave1 与 slave2</p> 
<ol><li>修改core-site.xml<br> <img src="https://images2.imgbox.com/da/0a/eOkC6mN0_o.png" alt="在这里插入图片描述"></li><li>修改 hadoop-env.sh<br> <img src="https://images2.imgbox.com/01/0a/oehyXvmf_o.png" alt="在这里插入图片描述"></li><li>修改hdfs-site.xml<br> <img src="https://images2.imgbox.com/c8/99/wvO4iYbi_o.png" alt="在这里插入图片描述"></li><li>修改mapred-site.xml<br> <img src="https://images2.imgbox.com/63/18/M8NqNWKy_o.png" alt="在这里插入图片描述"></li><li>修改workers<br> <img src="https://images2.imgbox.com/73/b6/zNzD7p3M_o.png" alt="在这里插入图片描述"></li><li>修改yarn-site.xml<br> <img src="https://images2.imgbox.com/cc/21/jzho1CRy_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="34__144"></a>3.4 拷贝文件</h3> 
<p>将 hadoop 目录从 master 拷贝到 slave1 与 slave2，由于之前配置了免密登录，所以不需要输入密码即可完成拷贝</p> 
<pre><code>scp -qr /home/min/tar/hadoop-3.1.3 slave1:/home/min/tar
scp -qr /home/min/tar/hadoop-3.1.3 slave2:/home/min/tar
</code></pre> 
<h3><a id="35__152"></a>3.5 创建临时文件目录</h3> 
<p>创建临时文件目录 （三台机器都需创建）， 创建的目录对应于 第六步 hdfs-site.xml 中配置的目录</p> 
<pre><code>mkdir -p /data/hadoop/hdfs/data
mkdir -p /data/hadoop/hdfs/name
</code></pre> 
<h3><a id="36__hdfs__160"></a>3.6 格式化 hdfs 文件系统</h3> 
<p>在master结点上的hadoop的安装目录下的 bin 目录下执行以下命令</p> 
<pre><code>./hdfs namenode -format
</code></pre> 
<p><strong>注意：</strong><br> 如果需要重新格式化NameNode,需要先将原来NameNode和DataNode下的文件全部删除，不然会报错，NameNode和DataNode所在目录是在core-site.xml中hadoop.tmp.dir、dfs.namenode.name.dir、dfs.datanode.data.dir属性配置的。<br> 因为每次格式化，默认是创建一个集群ID，并写入NameNode和DataNode的VERSION文件中（VERSION文件所在目录为hdfs/name/current 和 hdfs/data/current），重新格式化时，默认会生成一个新的集群ID,如果不删除原来的目录，会导致namenode中的VERSION文件中是新的集群ID,而DataNode中是旧的集群ID，不一致时会报错。</p> 
<h3><a id="37_hadoop_171"></a>3.7 启动hadoop</h3> 
<p>主节点上在hadoop目录下执行：</p> 
<pre><code>./sbin/start-all.sh
</code></pre> 
<ul><li>主节点上jps进程如下：<br> <img src="https://images2.imgbox.com/f3/b0/pqDJLxs2_o.png" alt="在这里插入图片描述"></li><li>每个子节点上的jps进程如下：<br> <img src="https://images2.imgbox.com/73/39/1I890xR8_o.png" alt="在这里插入图片描述"><br> 如果这样表示hadoop集群配置成功</li></ul> 
<h2><a id="4_Hadoop_184"></a>4 测试Hadoop</h2> 
<h3><a id="41_web__185"></a>4.1 web 页面测试</h3> 
<p>以下几个web页面是否可以正常打开（注意关闭防火墙）</p> 
<pre><code>http://192.168.19.200:8088/cluster   hadoop 集群信息

http://192.168.19.200:9870/dfshealth.html#tab-overview  hdfs 地址

http://192.168.19.200:9864/datanode.html  dataNode 地址

http://192.168.19.200:8042/node    nodeManager 地址

http://192.168.19.200:9868/status.html   secondaryNameNode
</code></pre> 
<h3><a id="42_hdfs__200"></a>4.2 hdfs 功能测试</h3> 
<ol><li>提前打开web界面查看 hdfs 文件系统，目前根目录下没有任何文件<br> <img src="https://images2.imgbox.com/4d/d6/yTLFrsZv_o.png" alt="在这里插入图片描述"></li><li>使用命令 hdfs dfs -put start-dfs.sh / 将sbin文件夹中的 start-dfs.sh 文件上传到 hdfs 文件系统的根目录下</li><li>重新打开web界面查看 hdfs 文件系统文件是否上传成功<br> <img src="https://images2.imgbox.com/2f/9c/Ca3HLx9F_o.png" alt="在这里插入图片描述"></li></ol> 
<h3><a id="43_mapreduce__207"></a>4.3 mapreduce 功能测试</h3> 
<ol><li>/home/min 目录下创建一个文件 wordtest 文件内容如下，下面我们用自带的库函数 统计各个单词出现的次数</li></ol> 
<pre><code>Good morning, everyone. Thank you for taking your time. It’s really my honor to have this opportunity to take part in this interview. Now, I would like to introduce myself briefly.
</code></pre> 
<ol start="2"><li>将该文件上传到 hdfs 文件系统上（因为 mapreduce 功能是在 hdfs 基础之上）【使用的是另一种 hdfs 上传文件的命令】</li></ol> 
<pre><code>hadoop fs -put wordtest /wordtest
</code></pre> 
<p><img src="https://images2.imgbox.com/ce/48/Xi0g0Vfp_o.png" alt="在这里插入图片描述"></p> 
<ol start="3"><li>执行 mapreduce 后生成结果文件</li></ol> 
<pre><code>hadoop jar /home/min/hadoop-3.1.3/share/hadoop/mapreduce
/hadoop-mapreduce-examples-3.1.3.jar wordcount /wordtest /result
</code></pre> 
<p><img src="https://images2.imgbox.com/0a/ea/DqQDx4Sv_o.png" alt="在这里插入图片描述"></p> 
<ol start="4"><li>查看统计结果<br> <img src="https://images2.imgbox.com/bd/9d/jz8VbNvC_o.png" alt="在这里插入图片描述"></li></ol> 
<h2><a id="5_Zookeeper_243"></a>5 安装配置Zookeeper集群</h2> 
<p>解压zookeeper安装包，并重命名为zookeeper，然后进行以下操作。</p> 
<h3><a id="51_zoocfg_246"></a>5.1 修改配置文件zoo.cfg</h3> 
<p>进入~/zookeeper/conf目录，拷贝zoo_sample.cfg文件为zoo.cfg</p> 
<pre><code>cp zoo_sample.cfg zoo.cfg
</code></pre> 
<p>对zoo.cfg进行编辑，内容如下：</p> 
<pre><code>dataDir=/home/min/tar/zookeeper/data

server.1=192.168.19.200:2888:3888
server.2=192.168.19.201:2888:3888
server.3=192.168.19.202:2888:3888
</code></pre> 
<h3><a id="52_myid_263"></a>5.2 新建并编辑myid文件</h3> 
<p>在dataDir目录下新建myid文件，输入一个数字（master为1，slave1为2，slave2为3），比如master主机上的操作如下：</p> 
<pre><code>mkdir /home/min/tar/zookeeper/data
echo "1" &gt; /home/min/tar/zookeeper/data/myid
</code></pre> 
<p>同样，也可以使用scp命令进行远程复制，只不过要修改每个节点上myid文件中的数字</p> 
<pre><code>scp -qr /home/min/tar/zookeeper slave1:/home/min/tar
scp -qr /home/min/tar/zookeeper slave2:/home/min/tar
</code></pre> 
<p><strong>注意事项：</strong><br> 如果启动报类似异常:QuorumCnxManager@384] – Cannot open channel to 2 at election address slave-02/192.168.0.178:3888 是可以忽略的,因为该服务启动时会尝试连接所有节点,而其他节点尚未启动。通过后面部分可以看到，集群在选出一个Leader后，最后稳定 了。其他结点可能也出现类似问题，属于正常。</p> 
<h2><a id="6_HBase_282"></a>6 安装配置HBase集群</h2> 
<p>将hbase安装包进行解压，并重命名为hbase，然后进入habase的bin目录进行如下配置。</p> 
<h3><a id="61__284"></a>6.1 修改配置文件</h3> 
<ol><li>配置hbase-env.sh</li></ol> 
<pre><code>export JAVA_HOME=/home/min/tar/jdk-11.0.4
export HBASE_CLASSPATH=/home/min/tar/hadoop-3.1.3/etc/hadoop/
export HBASE_MANAGES_ZK=false
</code></pre> 
<ol start="2"><li>配置hbase-site.xml</li></ol> 
<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.master&lt;/name&gt;
    &lt;value&gt;master&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
    &lt;value&gt;2181&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;master,slave1,slave2&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;
    &lt;value&gt;60000000&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.support.append&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;&lt;/configuration&gt;
</code></pre> 
<ol start="3"><li>配置regionservers<br> 在 regionservers 文件中添加slave列表：</li></ol> 
<pre><code>master
slave1
slave2
</code></pre> 
<h3><a id="62_335"></a>6.2分发并同步安装包</h3> 
<p>将整个hbase安装目录都远程拷贝到所有slave服务器：</p> 
<pre><code>scp -qr /home/min/tar/hbase slave1:/home/min/tar
scp -qr /home/min/tar/hbase slave2:/home/min/tar
</code></pre> 
<h3><a id="63_hbase_343"></a>6.3 启动hbase</h3> 
<pre><code>./hbase/bin/start-hbase.sh
</code></pre> 
<p><img src="https://images2.imgbox.com/f8/4d/dto4d82S_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="7_kafka_351"></a>7 安装配置kafka集群</h2> 
<p>将kafka安装包进行解压，并重命名为kafka，对config目录下的<strong>server.properties</strong>添加配置</p> 
<h3><a id="71__354"></a>7.1 修改配置文件</h3> 
<ol><li><strong>添加broker.id</strong></li></ol> 
<pre><code>broker.id=01
host.name=master
</code></pre> 
<ol start="2"><li><strong>添加zookeeper节点地址和端口</strong></li></ol> 
<pre><code>zookeeper.connect=master:2181,slave1:2181,slave2:2181
</code></pre> 
<h3><a id="72__368"></a>7.2 分发并同步安装包</h3> 
<pre><code>scp -qr /home/min/tar/kafka slave1:/home/min/tar
</code></pre> 
<h3><a id="73_slave1slave2broker_id_374"></a>7.3 slave1和slave2上更改broker id</h3> 
<p>将broker id改为02和03</p> 
<h3><a id="74_kafka_377"></a>7.4 启动kafka</h3> 
<p>由于没有配置环境，需要再kafka目录下启动</p> 
<pre><code> ./bin/kafka-server-start.sh -daemon config/server.properties
</code></pre> 
<p><img src="https://images2.imgbox.com/cd/45/78p3domv_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="8__389"></a>8 启动集群</h2> 
<ol><li> <p><strong>启动ZooKeeper</strong><br> zookeeper/bin/zkServer.sh start</p> </li><li> <p><strong>启动hadoop</strong><br> ./hadoop-3.1.3/sbin/start-all.sh</p> </li><li> <p><strong>启动hbase</strong><br> ./hbase/bin/start-hbase.sh</p> </li><li> <p><strong>启动kafka</strong><br> ./bin/kafka-server-start.sh -daemon config/server.properties</p> </li><li> <p><strong>启动后，master上进程和slave进程列表</strong><br> Mater<br> <img src="https://images2.imgbox.com/30/3e/kzmEaYWq_o.png" alt="在这里插入图片描述"><br> Slave<br> <img src="https://images2.imgbox.com/ee/7c/Q9rjfmQg_o.png" alt="在这里插入图片描述"></p> </li></ol> 
<h2><a id="9_kafka_408"></a>9 测试kafka</h2> 
<ol><li><strong>在master上创建topic-test</strong></li></ol> 
<pre><code>./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 3 --partitions 3 --topic test
Created topic "test".
</code></pre> 
<ol start="2"><li><strong>在master,slave1,2上查看已创建的topic列表</strong></li></ol> 
<pre><code>./bin/kafka-topics.sh --list --zookeeper localhost:2181
</code></pre> 
<p><img src="https://images2.imgbox.com/5e/c1/qw6KoyMo_o.png" alt="在这里插入图片描述"><br> 3. <strong>在master上启动生产者</strong></p> 
<pre><code> ./bin/kafka-console-producer.sh --broker-list master:9092,slave1:9092,slave2:9092 --topic test
</code></pre> 
<p><img src="https://images2.imgbox.com/dc/c8/CTOU0SXq_o.png" alt="在这里插入图片描述"><br> 4. <strong>在其他节点上启动控制台消费者</strong></p> 
<pre><code>./bin/kafka-console-consumer.sh --bootstrap-server master:9092,slave1:9092,slave2:9092 --from-beginning --topic test
</code></pre> 
<p><img src="https://images2.imgbox.com/81/85/gjfc8MCm_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="10_Hbase_434"></a>10 测试Hbase</h2> 
<p>这个测试是通过HBase Shell 命令来完成一个 HBase 表的创建、插入、查找、删除操作。</p> 
<ol><li> <p><strong>进入shell模式</strong><br> <img src="https://images2.imgbox.com/28/c0/IO2Dwj5T_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>创建test1表</strong><br> <img src="https://images2.imgbox.com/4b/2f/A9gA0BYV_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>显示表</strong><br> <img src="https://images2.imgbox.com/f2/5c/vnmweZr0_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>插入数据</strong><br> <img src="https://images2.imgbox.com/8a/ba/L5vmweCx_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>查看数据</strong><br> <img src="https://images2.imgbox.com/86/31/DaBRFIov_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>获取数据</strong><br> <img src="https://images2.imgbox.com/b8/81/pVXHLqlW_o.png" alt="在这里插入图片描述"></p> </li><li> <p><strong>删除表</strong><br> <img src="https://images2.imgbox.com/1d/cd/IiHb9bQD_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/ac/e5/ul5UVJAb_o.png" alt="在这里插入图片描述"></p> </li></ol> 
<h2><a id="11_bug_454"></a>11 遇到的bug</h2> 
<h3><a id="111_hdfsdatanodes_456"></a>11.1 启动hdfs时，datanodes报错</h3> 
<p>Starting datanodes<br> ERROR: Refusing to run as root: root account is not found. Aborting.<br> <strong>解决办法：</strong><br> 在hadoop/etc/hadoop/hadoop-env.sh中的配置文件中的root多打了个空格，更正即可<br> <img src="https://images2.imgbox.com/84/4e/xsmjQtvK_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="112_hdfsdatanode_463"></a>11.2 启动hdfs时，datanode无法正常启动</h3> 
<p>当我们多次格式化文件系统（hadoop namenode -format）时，会出现DataNode无法启动。<br> 多次启动中发现<strong>有NameNode节点，并没有DataNode节点</strong><br> 如图所示：<br> <img src="https://images2.imgbox.com/5f/c5/1s8Lt00S_o.png" alt="在这里插入图片描述"><br> <strong>经查看相关日志发现问题所在：</strong></p> 
<p><strong>datanode的clusterID 和 namenode的clusterID 不匹配。</strong><br> 当我们执行文件系统格式化时，会在namenode数据文件夹（即配置文件中dfs.name.dir在本地系统的路径）中保存一个current/VERSION文件，记录namespaceID，标志了所有格式化的namenode版本。如果我们频繁的格式化namenode，那么datanode中保存（即dfs.data.dir在本地系统的路径）的current/VERSION文件只是你地第一次格式化时保存的namenode的ID，因此就会造成namenode和datanode之间的ID不一致。</p> 
<p><strong>解决办法：</strong><br> 删除DataNode的所有资料及将集群中每个datanode节点的/dfs/data/current中的VERSION删除，然后重新执行hadoop namenode -format进行格式化，重启集群，错误消失。</p> 
<p><strong>出现该问题的原因：</strong><br> 在第一次格式化dfs后，启动并使用了hadoop，后来又重新执行了格式化命令（hdfs namenode -format)，这时namenode的clusterID会重新生成，而datanode的clusterID 保持不变。</p> 
<h3><a id="113_yarn_resourceManagernodeManager_479"></a>11.3 yarn无法正常启动 （resourceManager/nodeManager）</h3> 
<pre><code>./sbin/start-yarn.sh
</code></pre> 
<p>jps上找不到 resourceManager 和 nodeManger的进程， 查询日志发现是jdk版本问题</p> 
<p><strong>解决办法：</strong> 从jdk9以后 java9默认禁用访问许多javax. * API。更换jdk8即可</p> 
<h3><a id="114_webHDFS_489"></a>11.4 webHDFS出错</h3> 
<p>如下图，提示"Failed to retrieve data from /webhdfs/v1/?op=LISTSTATUS:Server Error“，也无法透过Web界面上传文件<br> <img src="https://images2.imgbox.com/de/a3/SaDQisKu_o.png" alt="在这里插入图片描述"><br> 经过一番查找之后(可找Hadoop的Log日志)确认是Jdk的版本问题，更换jdk版本为 jdk8<br> 这件事得到一个教训，软件并不是版本越高越好</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/632167f26a7a6f93350db097b1a3b7f0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">总结pycharm运行爬虫代码只显示Process finished with exit code 0的解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a7a777dfdb9949b661f582159998c8a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java中请求URL</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>