<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>强化学习中On-policy与off-policy的概念 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="强化学习中On-policy与off-policy的概念" />
<meta property="og:description" content="Off-policy方法——将收集数据当做一个单独的任务 off-policy的方法将收集数据作为RL算法中单独的一个任务，它准备两个策略：行为策略(behavior policy)与目标策略(target policy)。行为策略是专门负责学习数据的获取，具有一定的随机性，总是有一定的概率选出潜在的最优动作。而目标策略借助行为策略收集到的样本以及策略提升方法提升自身性能，并最终成为最优策略。Off-policy是一种灵活的方式，如果能找到一个“聪明的”行为策略，总是能为算法提供最合适的样本，那么算法的效率将会得到提升。
On-policy——行为策略与目标策略相同 on-policy里面只有一种策略，它既为目标策略又为行为策略。SARSA算法即为典型的on-policy的算法。
需要注意的问题： 1.为什么有时候off-policy需要与重要性采样配合使用？
重要性采样是用一个概率分布的样本来估计某个随机变量关于另一个概率分布的期望。
假设已知随机策略π(a|s)，现在需要估计策略对应的状态值Vπ，但是只能用另一个策略π&#39;(a|s)获取样本。对于这种需要用另外一个策略的数据(off-policy)来精确估计状态值的任务，需要用到重要性采样的方法，具体做法是在对应的样本估计量上乘上一个权重(π与π&#39;的相对概率)，称为重要性采样率。
参考链接：
强化学习中的奇怪概念(一)——On-policy与off-policy" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/82a7db0a9ab230d4b6b2bfe97b68f454/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-21T19:51:22+08:00" />
<meta property="article:modified_time" content="2023-11-21T19:51:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">强化学习中On-policy与off-policy的概念</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="Offpolicy_0"></a>Off-policy方法——将收集数据当做一个单独的任务</h3> 
<p>off-policy的方法将收集数据作为RL算法中单独的一个任务，它准备两个策略：行为策略(behavior policy)与目标策略(target policy)。行为策略是专门负责学习数据的获取，具有一定的随机性，总是有一定的概率选出潜在的最优动作。而目标策略借助行为策略收集到的样本以及策略提升方法提升自身性能，并最终成为最优策略。Off-policy是一种灵活的方式，如果能找到一个“聪明的”行为策略，总是能为算法提供最合适的样本，那么算法的效率将会得到提升。</p> 
<h3><a id="Onpolicy_2"></a>On-policy——行为策略与目标策略相同</h3> 
<p>on-policy里面只有一种策略，它既为目标策略又为行为策略。SARSA算法即为典型的on-policy的算法。</p> 
<h3><a id="_5"></a>需要注意的问题：</h3> 
<p>1.为什么有时候off-policy需要与重要性采样配合使用？<br> 重要性采样是用一个概率分布的样本来估计某个随机变量关于另一个概率分布的期望。<br> 假设已知随机策略<code>π(a|s)</code>，现在需要估计策略对应的状态值<code>Vπ</code>，但是只能用另一个策略<code>π'(a|s)</code>获取样本。对于这种需要用另外一个策略的数据(off-policy)来精确估计状态值的任务，需要用到重要性采样的方法，具体做法是在对应的样本估计量上乘上一个权重(<code>π</code>与<code>π'</code>的相对概率)，称为重要性采样率。<br> <img src="https://images2.imgbox.com/b9/f8/2BWcNCAs_o.png" alt="在这里插入图片描述"><br> 参考链接：<br> <a href="https://zhuanlan.zhihu.com/p/346433931" rel="nofollow">强化学习中的奇怪概念(一)——On-policy与off-policy</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/792ac0e35c7fea862ea34e76854d7369/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">shell脚本字典创建遍历打印</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4b236acd05276b992ba6e3ef21e8ad14/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">仪表盘：pyecharts绘制</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>