<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>自动编码器如何提取特征_ACM MM18 | 用于跨模态检索的综合距离保持自编码器 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="自动编码器如何提取特征_ACM MM18 | 用于跨模态检索的综合距离保持自编码器" />
<meta property="og:description" content="作者丨黄澄楷
研究方向丨多媒体信息检索/内容理解
本文是发表在 MM18 上的一篇跨模态检索文章，作者提出了一种采用综合保持距离的自编码器（CDPAE）的新颖方法，用以解决无监督的跨模态检索任务。
之前的无监督方法大部分使用属于相同对象的跨模态空间的成对表示距离进行度量学习。但是，除了成对距离之外，作者还考虑了从跨媒体空间提取的异构表示距离，以及从属于不同对象的单个媒体空间提取的齐次表示距离，从而达到了更高的检索精度。
研究动机 虽然先前的无监督跨模态检索方法已经有了不错的表现，但是仍然有两个问题叩待解决，第一，如何减少特征中冗余的噪声的负面影响。
▲ 背景中的SIFT特征会影响Cat图像的检索 第二，如何直接使用不同对象的表示（representation）来表达它们之间的关系（relationship）。
即在大多数非监督方法中，不考虑虚线的关系。这两个问题在无监督跨模态检索的研究中涉及的较少。
研究方法 上图就是作者提出的 CDPAE 的框架结构图，总体上看，CDPAE 包含四个并行的去噪编码器，并定义了综合的保距公共空间，其中根据输入保留三种距离，然后使用联合损失函数将自编码器的重构损失和相关损失结合起来。最后，还提出了一种无监督跨模态相似度的度量方法。 具体来看，CDPAE 包含四个部分：去噪编码器、综合保距空间、联合损失函数和无监督跨模态相似度测量，由于数据集的限制，本文与大部分其他跨模态检索任务一样，只进行图文互搜的实验。接下来分别对每个部分进行介绍。
1. CDPAE 的第一部分由四个去噪编码器组成，其中两个提取图像相关的特征，另外两个与文本特征相关，去噪的自编码器负责相同的模态，它们共享相同的参数，因此相同模态的表示也具有相同的转换。
在具体的训练迭代中，从两个对象中提取的两种模式之间的四种表示形式用作输入。如：图中海鸥图、描述海鸥图的文本、自行车图、描述自行车的文本作为输入。 在去噪自动编码器中，将固定数量的输入分量随机设置为零，其余的保持不变。该方法模拟了从输入端去除冗余噪声的过程；因此，它减少了冗余噪声的负面影响。此外，归零过程可以看作是一种数据扩充的过程，它加强了从不同模态中提取的表示中局部结构之间的联系。
去噪自编码器的重构损失定义为:
V={v} 代表的是图像的数据集，T={t} 代表的是对应的文本数据集，根据之前提到的输入方法，使用自编码器从提取两组来自两个对象对应的特征(图像文本对特征)，(vi,ti)--&gt;（海鸥图的图像特征和文本特征）和 (vj,tj)--&gt;（自行车的图像特征和文本特征），av,wv,θv 表达的是图像自编码器的参数，at,wt,θt 代表的是文本自编码器的参数，Z(*) 是随机置零过程，F(*) 是编码过程，G(*) 是解码过程。 2. 第二部分是综合保距空间的构造：CDPAE 使用余弦距离来测量相同模态空间中的特征相似性。测量的公式:
在综合保距空间中有三种距离：成对距离、异质距离、齐次距离，分别给出定义：
a. 成对距离的损失：
就是其他无监督跨模态检索都考虑的距离。其中 D 为：
这个距离的作用是：成对的距离会导致公共空间，其中属于相同对象的不同模态的表示会聚在一起（海鸥的图像文本聚在一起、自行车的图像文本聚在一起）。
b. 异质距离的损失：
反映了不同对象在不同模态中的表示之间的关系，这里度量的时候限制它们与原始模态空间相对应的对象之间的距离一致。
c. 齐次距离的损失：
齐次距离反映的是同一模态下来自不同对象的表示之间的关系，因为每次迭代中，都计算相同两个对象之间的异质与齐次距离，所以设置它们的值相同。
所以综合的保距空间如下：
3. 然后又使用了一种联合损失函数，同时计算去噪自编码器的重构损失和综合保距公共空间的相关损失：
4. 最后作者又提出了一种新型的无监督跨模态相似度度量方法，在公共空间中，训练数据集中变换后的特征之间的距离通常会比测试数据中的距离更具有可信度。
所以讲两个特征之间的相似性定义为基于 KNN 分类器的边缘概率，该分类器用于对训练样本中的每个模态的表示进行分类，两种表示的相似性可以定义为：
注意：这里的 pi/qj 分别是图像模态/文本模态的 top k 近邻样本（这里的 top k 近邻样本不区分模态）。同时，假设两个表示之间的距离反映了它们属于同一语义范畴的可能性，因此，如果训练数据集中的两种表示形式成对对应的话，它的可能性就是 1，否则：
D 采用的是余弦相似距离，取值范围是 0~1，距离越小，对应的表示属于同一类别可能性就越大，进一步，去定义一个测试样本表示与其 k 个最近的训练数据属于同一个类别的条件概率为：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/4c1869d1faaab85a153d9c212ff4c46c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-03T05:39:57+08:00" />
<meta property="article:modified_time" content="2020-12-03T05:39:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">自动编码器如何提取特征_ACM MM18 | 用于跨模态检索的综合距离保持自编码器</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/ee/ca/MJUhmOpC_o.png" alt="7799e485af31b8d9c75ce040928bc4bb.png"> 
 </div> 
 <p><b>作者丨黄澄楷</b></p> 
 <p><b>研究方向丨多媒体信息检索/内容理解</b></p> 
 <p>本文是发表在 MM18 上的一篇跨模态检索文章，<b>作者提出了一种采用综合保持距离的自编码器（CDPAE）的新颖方法，用以解决无监督的跨模态检索任务。</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/53/ad/EwHXRKPj_o.png" alt="7ba22ee6b8a7209af50ad9bf8b10e7c5.png"> 
 </div> 
 <p>之前的无监督方法大部分使用属于相同对象的跨模态空间的成对表示距离进行度量学习。但是，除了成对距离之外，作者还考虑了从跨媒体空间提取的异构表示距离，以及从属于不同对象的单个媒体空间提取的齐次表示距离，从而达到了更高的检索精度。</p> 
 <h3><b>研究动机</b></h3> 
 <p>虽然先前的无监督跨模态检索方法已经有了不错的表现，但是仍然有两个问题叩待解决，<b>第一，如何减少特征中冗余的噪声的负面影响。</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/7b/19/emerVwy0_o.png" alt="543bf02c50b42cddcb85e6aa82b49985.png"> 
 </div> 
 <figcaption>
   ▲ 背景中的SIFT特征会影响Cat图像的检索 
 </figcaption> 
 <p><b>第二，如何直接使用不同对象的表示（representation）来表达它们之间的关系（relationship）。</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/d3/56/eGdC9siz_o.png" alt="d772e0a9d95c46df33e15b65ee6bf7c4.png"> 
 </div> 
 <p>即在大多数非监督方法中，不考虑虚线的关系。这两个问题在无监督跨模态检索的研究中涉及的较少。</p> 
 <h3><b>研究方法</b></h3> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/4c/2c/3YMOhhKY_o.png" alt="414667b1465ed42deb5747cdf24aa58a.png"> 
 </div> 
 <p>上图就是作者提出的 CDPAE 的框架结构图，总体上看，CDPAE 包含四个并行的去噪编码器，并定义了综合的保距公共空间，其中根据输入保留三种距离，然后使用联合损失函数将自编码器的重构损失和相关损失结合起来。最后，还提出了一种无监督跨模态相似度的度量方法。 </p> 
 <p>具体来看，<b>CDPAE 包含四个部分：去噪编码器、综合保距空间、联合损失函数和无监督跨模态相似度测量</b>，由于数据集的限制，本文与大部分其他跨模态检索任务一样，只进行图文互搜的实验。接下来分别对每个部分进行介绍。</p> 
 <p><b>1. CDPAE 的第一部分由四个去噪编码器组成</b>，其中两个提取图像相关的特征，另外两个与文本特征相关，去噪的自编码器负责相同的模态，它们共享相同的参数，因此相同模态的表示也具有相同的转换。</p> 
 <p>在具体的训练迭代中，从两个对象中提取的两种模式之间的四种表示形式用作输入。如：图中海鸥图、描述海鸥图的文本、自行车图、描述自行车的文本作为输入。 </p> 
 <p>在去噪自动编码器中，将固定数量的输入分量随机设置为零，其余的保持不变。该方法模拟了从输入端去除冗余噪声的过程；因此，它减少了冗余噪声的负面影响。此外，归零过程可以看作是一种数据扩充的过程，它加强了从不同模态中提取的表示中局部结构之间的联系。</p> 
 <p>去噪自编码器的重构损失定义为:</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/d7/0c/V2vGnQ3W_o.png" alt="ba57b870cf7ad0dbdd388cb144771397.png"> 
 </div> 
 <p>V={v} 代表的是图像的数据集，T={t} 代表的是对应的文本数据集，根据之前提到的输入方法，使用自编码器从提取两组来自两个对象对应的特征(图像文本对特征)，(vi,ti)--&gt;（海鸥图的图像特征和文本特征）和 (vj,tj)--&gt;（自行车的图像特征和文本特征），av,wv,θv 表达的是图像自编码器的参数，at,wt,θt 代表的是文本自编码器的参数，Z(*) 是随机置零过程，F(*) 是编码过程，G(*) 是解码过程。 </p> 
 <p><b>2. 第二部分是综合保距空间的构造</b>：CDPAE 使用余弦距离来测量相同模态空间中的特征相似性。测量的公式:</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/56/29/I7J6xzd5_o.png" alt="ba10ff207fe110618e650f3a91dca23e.png"> 
 </div> 
 <p><b>在综合保距空间中有三种距离：成对距离、异质距离、齐次距离</b>，分别给出定义：</p> 
 <p><b>a. 成对距离的损失：</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/2f/8d/KZDR4uK0_o.png" alt="96b185f50fc3d07851a2776f0fdd749c.png"> 
 </div> 
 <p>就是其他无监督跨模态检索都考虑的距离。其中 D 为：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/4b/ba/LBfIfG8X_o.png" alt="cc3902de083eaa1c48a40a709b553af7.png"> 
 </div> 
 <p>这个距离的作用是：成对的距离会导致公共空间，其中属于相同对象的不同模态的表示会聚在一起（海鸥的图像文本聚在一起、自行车的图像文本聚在一起）。</p> 
 <p><b>b. 异质距离的损失：</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/b9/6a/afJlrXbn_o.png" alt="134c7029a446bf3b589f67b2041976fe.png"> 
 </div> 
 <p>反映了不同对象在不同模态中的表示之间的关系，这里度量的时候限制它们与原始模态空间相对应的对象之间的距离一致。</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/aa/38/RniS6YS9_o.png" alt="7add91ba7746b547cb6bca6848795016.png"> 
 </div> 
 <p><b>c. 齐次距离的损失：</b></p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/c7/dd/1Xi5EM1S_o.png" alt="fdc9b855902d800f30cd7681f5721d00.png"> 
 </div> 
 <p>齐次距离反映的是同一模态下来自不同对象的表示之间的关系，因为每次迭代中，都计算相同两个对象之间的异质与齐次距离，所以设置它们的值相同。</p> 
 <p>所以综合的保距空间如下：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/8d/51/t9tPT4MW_o.png" alt="6d1100f51abcca766a5b2fd431613557.png"> 
 </div> 
 <p><b>3. 然后又使用了一种联合损失函数</b>，同时计算去噪自编码器的重构损失和综合保距公共空间的相关损失：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/fa/9b/V17BzHYT_o.png" alt="4c9f2370822f6260bb68fa5391f507e3.png"> 
 </div> 
 <p><b>4. 最后作者又提出了一种新型的无监督跨模态相似度度量方法</b>，在公共空间中，训练数据集中变换后的特征之间的距离通常会比测试数据中的距离更具有可信度。</p> 
 <p>所以讲两个特征之间的相似性定义为基于 KNN 分类器的边缘概率，该分类器用于对训练样本中的每个模态的表示进行分类，两种表示的相似性可以定义为：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/c3/a0/RuQXraq8_o.png" alt="7bdb2e77da3995b6657267bcfb3508e1.png"> 
 </div> 
 <p>注意：这里的 pi/qj 分别是图像模态/文本模态的 top k 近邻样本（这里的 top k 近邻样本不区分模态）。同时，假设两个表示之间的距离反映了它们属于同一语义范畴的可能性，因此，如果训练数据集中的两种表示形式成对对应的话，它的可能性就是 1，否则：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/37/ad/dHd5HokZ_o.png" alt="67d24be84511337bce14b85198e66657.png"> 
 </div> 
 <p>D 采用的是余弦相似距离，取值范围是 0~1，距离越小，对应的表示属于同一类别可能性就越大，进一步，去定义一个测试样本表示与其 k 个最近的训练数据属于同一个类别的条件概率为：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/7c/64/ejhyRj0K_o.png" alt="352d09f7f6cd94e66ac70c1a937f99f4.png"> 
 </div> 
 <h3><b>实验结果</b></h3> 
 <p>作者在 Wikipedia，NUS-WIDE-10k，Pascal Sentence 以及 XMedia 数据集上进行了实验：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/88/1b/ILNj72RH_o.png" alt="d42488154c71919870ef0d653f844fd7.png"> 
 </div> 
 <p>图表显示返回的是 MAP@50 的结果，加 * 的是有监督的方法，三角形代表的是半监督的方法，剩下的都是无监督方法。另外，作者还用 t-SNE 可视化了综合保距空间的数据分布：</p> 
 <p></p> 
 <div style="text-align:center;"> 
  <img src="https://images2.imgbox.com/c6/fa/6ua2WbOI_o.png" alt="fc8f5310c6b1617950340afac12439e5.png"> 
 </div> 
 <p>上图显示的是 Wikipedia-Multiple 数据集中不同公共空间的 t-SNE 可视化。可以看出，在成对保持距离的公共空间中，图像和文本的表征往往是混合的。然而，来自相同类别的表示并没有得到令人满意的聚类，这就是传统无监督跨模态检索只使用的距离。</p> 
 <p>在异质的保距离公共空间中，图像和文本的表征就有明显的区别，这是因为模内距离远小于模间距离。此外，在相同距离保持的公共空间中，来自相同模态的表示按其各自的类别聚在一起。</p> 
 <p>在综合的保距离公共空间中，图像和文本的变换表示达到了最佳的方式分布。大量具有相同语义标签的表示形式被聚集在一起，而与它们的模态类型无关。这表明，综合保距离公共空间具有其他三种保距离公共空间的优点，对于跨模态检索任务是非常有效的。</p> 
 <h3><b>结论与点评</b></h3> 
 <p>与之前的无监督跨模态检索方法相比，<b>本文最大的亮点在于引入了不同对象在不同模态间的距离以及不同对象在相同模态间的距离</b>，就是文中提出的学习到的一个综合保距空间，这是之前大部分跨模态检索方法没有考虑到的。</p> 
 <p>然后利用联合损失函数将距离的损失函数与自编码器重构损失一起训练，达到一个很好的效果，与目前所有的无监督跨模态检索方法相比，平均性能高出 12.5%，与半监督与有监督方法相比，在多个数据集上表现也有前三的水平。</p> 
 <h3><b>#投 稿 通 道#</b></h3> 
 <p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <b>答案就是：你不认识的人。</b></p> 
 <p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p> 
 <p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<b>最新论文解读</b>，也可以是<b>学习心得</b>或<b>技术干货</b>。我们的目的只有一个，让知识真正流动起来。</p> 
 <p> <b>来稿标准：</b></p> 
 <p>• 稿件确系个人<b>原创作品</b>，来稿需注明作者个人信息（姓名+学校/工作单位+学历/职位+研究方向）</p> 
 <p>• 如果文章并非首发，请在投稿时提醒并附上所有已发布链接</p> 
 <p>• PaperWeekly 默认每篇文章都是首发，均会添加“原创”标志</p> 
 <p><b> 投稿方式：</b></p> 
 <p>• 方法一：在PaperWeekly知乎专栏页面点击“投稿”，即可递交文章</p> 
 <p>• 方法二：发送邮件至：<u>hr@paperweekly.site</u> ，所有文章配图，请单独在附件中发送</p> 
 <p>• 请留下即时联系方式（微信或手机），以便我们在编辑发布时和作者沟通</p> 
 <h3><b>关于PaperWeekly</b></h3> 
 <p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<b>「交流群」</b>，小助手将把你带入 PaperWeekly 的交流群里。</p> 
 <p><b>加入社区：</b>http://paperweek.ly</p> 
 <p><b>微信公众号：PaperWeekly</b></p> 
 <p><b>新浪微博：@PaperWeekly</b></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/947b1fbb512d90e00c2dbf97682ff5db/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">字符串去掉两端的引号_Python3.7知其然知其所以然-第六章 字符串</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/97e4455be9a2949630802f679cc3aa1e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python 山脊图_纯Python绘制艺术感满满的山脊地图，创意满分</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>