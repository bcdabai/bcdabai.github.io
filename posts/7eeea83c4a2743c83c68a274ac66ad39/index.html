<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>免费可商用开源GPT模型问世，50G权重直接下载，性能不输GPT-3 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="免费可商用开源GPT模型问世，50G权重直接下载，性能不输GPT-3" />
<meta property="og:description" content="萧箫 发自 凹非寺
量子位 | 公众号 QbitAI 真·开源GPT模型，终于来了。
参数量级130亿，大小比肩最近Meta开放的LLaMA-13B，但从数据集、模型权重到计算优化训练，全部开源。
最关键的是，可商用。
没错，虽然就GPT-3而言，之前DeepMind、Meta等组织陆陆续续开源过几个模型，不过基本都是半遮半掩。
尤其最接近GPT-3的Meta OPT模型，不仅权重只开放给研究者，而且不可商用：
这意味着之前企业就算能抄作业，抄来的也没办法直接用。
现在，一家名叫Cerebras的公司开源了这一系列GPT模型，业界终于有机会追赶了。
模型性能如何？ Cerebras一共开源了7个GPT模型，参数量分别达到1.11亿、2.56亿、5.9亿、13亿、27亿、67亿和130亿。
据Cerebras公司表示，他们开放出来的模型不仅包含数据集，可用于研究也可商用，而且关键是预训练模型权重开放（从下图来看文件大小近50G）。
基于他们公开的预训练模型，大伙儿只需要用少量的数据对对模型进行微调，就能构建出效果不错的模型来。
除此之外，这次GPT模型的训练还额外考虑到了计算优化训练 （Compute-Optimal Training）。
这个方法最早由DeepMind在2022年提出，名叫Chinchilla，它认为大语言模型的语料数量和模型效果之间符合一个凸曲线，因此模型参数量和训练程度成一定比例。
依据这个方法，DeepMind认为，包括GPT-3在内的超大参数LLM模型，有很多都是训练不足的。
基于此，Cerebras搞出了这一系列GPT模型，并将背后的流程进行了开源。
所以，Cerebras-GPT系列模型性能如何呢？
团队将Cerebras-GPT系列和LLaMA、GPT-3等模型的性能进行了对比。
这是包括GPT-3、Gopher、Chinchilla和LLaMA在内的其他GPT模型，在完成句子、问答等特定任务上表现的效果。
这是不同大小的Cerebras-GPT模型零次学习（0-shot）的效果：
数据对比不是特别直观，团队还将结果进行了可视化。
可以看出，在最终性能相差不大的情况下，Cerebras-GPT的训练效率要更高一些。
曾开发最大AI芯片 其实，Cerebras的“本职”是一家AI芯片公司。
Cerebras公司由Sean Lie和Andrew Feldman等人于2016年创立。
其中，Andrew Feldman曾创建微型服务器公司SeaMicro，并以3.34亿美元的价格出售给AMD。
与其他AI芯片公司不同，Cerebras开发的芯片超大，像晶圆一样（但确实是芯片）：
他们当年做出来过一个名叫“晶圆级引擎”（Cerebras Wafer Scale Engine，简称WSE）的AI芯片，将逻辑运算、通讯和存储器集成到单个硅片上，一口气创下了4项世界纪录：
晶体管数量最多的运算芯片：总共包含1.2万亿个晶体管。虽然三星曾造出2万亿个晶体管的芯片，却是用于存储的eUFS。
芯片面积最大：尺寸约20厘米×23厘米，总面积46,225平方毫米。面积和一块晶圆差不多。
片上缓存最大：包含18GB的片上SRAM存储器。
运算核心最多：包含40万个处理核心。
后来这个超大WSE又升级了二代，然后团队基于WSE-2打造出了一个名叫Cerebras CS-2的AI超算。
这次的Cerebras-GPT系列模型，就是在这个Cerebras CS-2的AI超算中训练出来的。对此这家公司表示：
虽然训练这么大体量的模型通常需要几个月时间，但我们几周就能搞定。
Cerebras还表示，虽然很多硬件公司都声称训练效果能接近英伟达GPU的水平，但他们还没看到任何一家亲自推动开源LLM的硬件公司，这势必不利于开源LLM的发展。
这波啊，这波Cerebras格局大了（手动狗头）
模型地址：
https://huggingface.co/cerebras/Cerebras-GPT-13B
参考链接：
https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/7eeea83c4a2743c83c68a274ac66ad39/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-02T15:57:01+08:00" />
<meta property="article:modified_time" content="2023-04-02T15:57:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">免费可商用开源GPT模型问世，50G权重直接下载，性能不输GPT-3</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h6>萧箫 发自 凹非寺<br>量子位 | 公众号 QbitAI</h6> 
 <p style="text-align:left;">真·开源GPT模型，终于来了。</p> 
 <p style="text-align:left;">参数量级130亿，大小比肩最近Meta开放的LLaMA-13B，但从数据集、模型权重到计算优化训练，<strong>全部开源</strong>。</p> 
 <p style="text-align:left;">最关键的是，<strong>可商用</strong>。</p> 
 <p style="text-align:left;">没错，虽然就GPT-3而言，之前DeepMind、Meta等组织陆陆续续开源过几个模型，不过基本都是半遮半掩。</p> 
 <p style="text-align:left;">尤其最接近GPT-3的Meta OPT模型，不仅权重只开放给研究者，而且不可商用：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5b/cb/GivHoUYV_o.png" alt="c18bd4dcd1a70e5fff83b965bde1f046.png"></p> 
 <p style="text-align:left;">这意味着之前企业就算能抄作业，抄来的也没办法直接用。</p> 
 <p style="text-align:left;">现在，一家名叫Cerebras的公司开源了这一系列GPT模型，业界终于有机会追赶了。</p> 
 <h3>模型性能如何？</h3> 
 <p style="text-align:left;">Cerebras一共开源了7个GPT模型，参数量分别达到1.11亿、2.56亿、5.9亿、13亿、27亿、67亿和130亿。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b0/34/WHT6Bo8l_o.png" alt="b9d46e0ec2296ef252b16a0f73763ad3.png"></p> 
 <p style="text-align:left;">据Cerebras公司表示，他们开放出来的模型不仅包含数据集，可用于研究也可商用，而且关键是预训练模型权重开放（从下图来看文件大小近50G）。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ff/2d/5kYDRoTO_o.png" alt="a324b0cb513aa840c1da3c60c370fa9e.png"></p> 
 <p style="text-align:left;">基于他们公开的预训练模型，大伙儿只需要用少量的数据对对模型进行微调，就能构建出效果不错的模型来。</p> 
 <p style="text-align:left;">除此之外，这次GPT模型的训练还额外考虑到了<strong>计算优化训练</strong> （Compute-Optimal Training）。</p> 
 <p style="text-align:left;">这个方法最早由DeepMind在2022年提出，名叫Chinchilla，它认为大语言模型的语料数量和模型效果之间符合一个凸曲线，因此模型参数量和训练程度成一定比例。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d1/8e/QU5RCmrs_o.png" alt="aa0e02615cc46324e7f67eb96af8382d.png"></p> 
 <p style="text-align:left;">依据这个方法，DeepMind认为，包括GPT-3在内的超大参数LLM模型，有很多都是<strong>训练不足</strong>的。</p> 
 <p style="text-align:left;">基于此，Cerebras搞出了这一系列GPT模型，并将背后的流程进行了开源。</p> 
 <p style="text-align:left;">所以，Cerebras-GPT系列模型性能如何呢？</p> 
 <p style="text-align:left;">团队将Cerebras-GPT系列和LLaMA、GPT-3等模型的性能进行了对比。</p> 
 <p style="text-align:left;">这是包括GPT-3、Gopher、Chinchilla和LLaMA在内的其他GPT模型，在完成句子、问答等特定任务上表现的效果。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/69/d1/Y0tKmBO3_o.png" alt="5b914c336fd33d842ba91dff22099617.png"></p> 
 <p style="text-align:left;">这是不同大小的Cerebras-GPT模型零次学习（0-shot）的效果：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/63/5d/h4cEEULq_o.png" alt="349edf2f148525f2a9d6ca9555782002.png"></p> 
 <p style="text-align:left;">数据对比不是特别直观，团队还将结果进行了可视化。</p> 
 <p style="text-align:left;">可以看出，在最终性能相差不大的情况下，Cerebras-GPT的<strong>训练效率</strong>要更高一些。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5b/c6/jAsmoMaI_o.png" alt="0b13f8bfb24f0be1c995e9c3a9f4e7b7.png"></p> 
 <h3>曾开发最大AI芯片</h3> 
 <p style="text-align:left;">其实，Cerebras的“本职”是一家AI芯片公司。</p> 
 <p style="text-align:left;">Cerebras公司由Sean Lie和Andrew Feldman等人于2016年创立。</p> 
 <p style="text-align:left;">其中，Andrew Feldman曾创建微型服务器公司SeaMicro，并以3.34亿美元的价格出售给AMD。</p> 
 <p style="text-align:left;">与其他AI芯片公司不同，Cerebras开发的芯片超大，像晶圆一样（但确实是芯片）：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3e/31/YnV4cKVd_o.png" alt="dd8b19b24ad178fec84009d2da8116a3.png"></p> 
 <p style="text-align:left;">他们当年做出来过一个名叫“晶圆级引擎”（Cerebras Wafer Scale Engine，简称WSE）的AI芯片，将逻辑运算、通讯和存储器集成到单个硅片上，一口气创下了<strong>4项世界纪录</strong>：</p> 
 <ul><li><p>晶体管数量最多的运算芯片：总共包含1.2万亿个晶体管。虽然三星曾造出2万亿个晶体管的芯片，却是用于存储的eUFS。</p></li><li><p>芯片面积最大：尺寸约20厘米×23厘米，总面积46,225平方毫米。面积和一块晶圆差不多。</p></li><li><p>片上缓存最大：包含18GB的片上SRAM存储器。</p></li><li><p>运算核心最多：包含40万个处理核心。</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/64/cb/k7xsdnG1_o.png" alt="851dcf9105a3400d9257e99f361cc562.png"></p> 
 <p style="text-align:left;">后来这个超大WSE又升级了二代，然后团队基于WSE-2打造出了一个名叫Cerebras CS-2的AI超算。</p> 
 <p style="text-align:left;">这次的Cerebras-GPT系列模型，就是在这个Cerebras CS-2的AI超算中训练出来的。对此这家公司表示：</p> 
 <blockquote> 
  <p>虽然训练这么大体量的模型通常需要几个月时间，但我们几周就能搞定。</p> 
 </blockquote> 
 <p style="text-align:left;">Cerebras还表示，虽然很多硬件公司都声称训练效果能接近<strong>英伟达GPU</strong>的水平，但他们还没看到任何一家亲自推动开源LLM的硬件公司，这势必不利于开源LLM的发展。</p> 
 <p style="text-align:left;">这波啊，这波Cerebras格局大了（手动狗头）</p> 
 <p style="text-align:left;">模型地址：<br>https://huggingface.co/cerebras/Cerebras-GPT-13B</p> 
 <p style="text-align:left;">参考链接：<br>https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fe4ae90c47163f4fe243ec2d2a1ae929/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于Docker搭建Hadoop集群</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6a0bd61437f36dbfefdc36d27d9b926a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">STM32CubeIDE（MX）下载安装、汉化、使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>