<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch 最全入门介绍，Pytorch入门看这一篇就够了 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch 最全入门介绍，Pytorch入门看这一篇就够了" />
<meta property="og:description" content="本文通过详细且实践性的方式介绍了 PyTorch 的使用，包括环境安装、基础知识、张量操作、自动求导机制、神经网络创建、数据处理、模型训练、测试以及模型的保存和加载。
1. Pytorch简介 在这一部分，我们将会对Pytorch做一个简单的介绍，包括它的历史、优点以及使用场景等。
1.1 Pytorch的历史 PyTorch是一个由Facebook的人工智能研究团队开发的开源深度学习框架。在2016年发布后，PyTorch很快就因其易用性、灵活性和强大的功能而在科研社区中广受欢迎。下面我们将详细介绍PyTorch的发展历程。
在2016年，Facebook的AI研究团队（FAIR）公开了PyTorch，其旨在提供一个快速，灵活且动态的深度学习框架。PyTorch的设计哲学与Python的设计哲学非常相似：易读性和简洁性优于隐式的复杂性。PyTorch用Python语言编写，是Python的一种扩展，这使得其更易于学习和使用。
PyTorch在设计上取了一些大胆的决定，其中最重要的一项就是选择动态计算图（Dynamic Computation Graph）作为其核心。动态计算图与其他框架（例如TensorFlow和Theano）中的静态计算图有着本质的区别，它允许我们在运行时改变计算图。这使得PyTorch在处理复杂模型时更具灵活性，并且对于研究人员来说，更易于理解和调试。
在发布后的几年里，PyTorch迅速在科研社区中取得了广泛的认可。在2019年，PyTorch发布了1.0版本，引入了一些重要的新功能，包括支持ONNX、一个新的分布式包以及对C&#43;&#43;的前端支持等。这些功能使得PyTorch在工业界的应用更加广泛，同时也保持了其在科研领域的强劲势头。
到了近两年，PyTorch已经成为全球最流行的深度学习框架之一。其在GitHub上的星标数量超过了50k，被用在了各种各样的项目中，从最新的研究论文到大规模的工业应用。
综上，PyTorch的发展历程是一部充满创新和挑战的历史，它从一个科研项目发展成为了全球最流行的深度学习框架之一。在未来，我们有理由相信，PyTorch将会在深度学习领域继续发挥重要的作用。
1.2 Pytorch的优点 PyTorch不仅是最受欢迎的深度学习框架之一，而且也是最强大的深度学习框架之一。它有许多独特的优点，使其在学术界和工业界都受到广泛的关注和使用。接下来我们就来详细地探讨一下PyTorch的优点。
1. 动态计算图
PyTorch最突出的优点之一就是它使用了动态计算图（Dynamic Computation Graphs，DCGs），与TensorFlow和其他框架使用的静态计算图不同。动态计算图允许你在运行时更改图的行为。这使得PyTorch非常灵活，在处理不确定性或复杂性时具有优势，因此非常适合研究和原型设计。
2. 易用性
PyTorch被设计成易于理解和使用。其API设计的直观性使得学习和使用PyTorch成为一件非常愉快的事情。此外，由于PyTorch与Python的深度集成，它在Python程序员中非常流行。
3. 易于调试
由于PyTorch的动态性和Python性质，调试PyTorch程序变得相当直接。你可以使用Python的标准调试工具，如PDB或PyCharm，直接查看每个操作的结果和中间变量的状态。
4. 强大的社区支持
PyTorch的社区非常活跃和支持。官方论坛、GitHub、Stack Overflow等平台上有大量的PyTorch用户和开发者，你可以从中找到大量的资源和帮助。
5. 广泛的预训练模型
PyTorch提供了大量的预训练模型，包括但不限于ResNet，VGG，Inception，SqueezeNet，EfficientNet等等。这些预训练模型可以帮助你快速开始新的项目。
6. 高效的GPU利用
PyTorch可以非常高效地利用NVIDIA的CUDA库来进行GPU计算。同时，它还支持分布式计算，让你可以在多个GPU或服务器上训练模型。
综上所述，PyTorch因其易用性、灵活性、丰富的功能以及强大的社区支持，在深度学习领域中备受欢迎。
1.3 Pytorch的使用场景 PyTorch的强大功能和灵活性使其在许多深度学习应用场景中都能够发挥重要作用。以下是PyTorch在各种应用中的一些典型用例：
1. 计算机视觉
在计算机视觉方面，PyTorch提供了许多预训练模型（如ResNet，VGG，Inception等）和工具（如TorchVision），可以用于图像分类、物体检测、语义分割和图像生成等任务。这些预训练模型和工具大大简化了开发计算机视觉应用的过程。
2. 自然语言处理
在自然语言处理（NLP）领域，PyTorch的动态计算图特性使得其非常适合处理变长输入，这对于许多NLP任务来说是非常重要的。同时，PyTorch也提供了一系列的NLP工具和预训练模型（如Transformer，BERT等），可以帮助我们处理文本分类、情感分析、命名实体识别、机器翻译和问答系统等任务。
3. 生成对抗网络
生成对抗网络（GANs）是一种强大的深度学习模型，被广泛应用于图像生成、图像到图像的转换、样式迁移和数据增强等任务。PyTorch的灵活性使得其非常适合开发和训练GAN模型。
4. 强化学习
强化学习是一种学习方法，其中智能体通过与环境的交互来学习如何执行任务。PyTorch的动态计算图和易于使用的API使得其在实现强化学习算法时表现出极高的效率。
5. 时序数据分析
在处理时序数据的任务中，如语音识别、时间序列预测等，PyTorch的动态计算图为处理可变长度的序列数据提供了便利。同时，PyTorch提供了包括RNN、LSTM、GRU在内的各种循环神经网络模型。
总的来说，PyTorch凭借其强大的功能和极高的灵活性，在许多深度学习的应用场景中都能够发挥重要作用。无论你是在研究新的深度学习模型，还是在开发实际的深度学习应用，PyTorch都能够提供强大的支持。
2. Pytorch基础 在我们开始深入使用PyTorch之前，让我们先了解一些基础概念和操作。这一部分将涵盖PyTorch的基础，包括tensor操作、GPU加速以及自动求导机制。
2.1 Tensor操作 Tensor是PyTorch中最基本的数据结构，你可以将其视为多维数组或者矩阵。PyTorch tensor和NumPy array非常相似，但是tensor还可以在GPU上运算，而NumPy array则只能在CPU上运算。下面，我们将介绍一些基本的tensor操作。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/353a341997de9608e14b7f6110ccb56f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-05T14:13:49+08:00" />
<meta property="article:modified_time" content="2024-01-05T14:13:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch 最全入门介绍，Pytorch入门看这一篇就够了</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><strong>本文通过详细且实践性的方式介绍了 PyTorch 的使用，包括环境安装、基础知识、张量操作、自动求导机制、神经网络创建、数据处理、模型训练、测试以及模型的保存和加载。</strong></p> 
<h2><a id="1_Pytorch_4"></a>1. Pytorch简介</h2> 
<p><img src="https://images2.imgbox.com/36/7f/C7wPoDHx_o.png" alt=""></p> 
<p>在这一部分，我们将会对Pytorch做一个简单的介绍，包括它的历史、优点以及使用场景等。</p> 
<h3><a id="11_Pytorch_11"></a>1.1 Pytorch的历史</h3> 
<p>PyTorch是一个由Facebook的人工智能研究团队开发的开源深度学习框架。在2016年发布后，PyTorch很快就因其易用性、灵活性和强大的功能而在科研社区中广受欢迎。下面我们将详细介绍PyTorch的发展历程。</p> 
<p>在2016年，Facebook的AI研究团队（FAIR）公开了PyTorch，其旨在提供一个快速，灵活且动态的深度学习框架。PyTorch的设计哲学与Python的设计哲学非常相似：易读性和简洁性优于隐式的复杂性。PyTorch用Python语言编写，是Python的一种扩展，这使得其更易于学习和使用。</p> 
<p>PyTorch在设计上取了一些大胆的决定，其中最重要的一项就是选择动态计算图（Dynamic Computation Graph）作为其核心。动态计算图与其他框架（例如TensorFlow和Theano）中的静态计算图有着本质的区别，它允许我们在运行时改变计算图。这使得PyTorch在处理复杂模型时更具灵活性，并且对于研究人员来说，更易于理解和调试。</p> 
<p>在发布后的几年里，PyTorch迅速在科研社区中取得了广泛的认可。在2019年，PyTorch发布了1.0版本，引入了一些重要的新功能，包括支持ONNX、一个新的分布式包以及对C++的前端支持等。这些功能使得PyTorch在工业界的应用更加广泛，同时也保持了其在科研领域的强劲势头。</p> 
<p>到了近两年，PyTorch已经成为全球最流行的深度学习框架之一。其在GitHub上的星标数量超过了50k，被用在了各种各样的项目中，从最新的研究论文到大规模的工业应用。</p> 
<p>综上，PyTorch的发展历程是一部充满创新和挑战的历史，它从一个科研项目发展成为了全球最流行的深度学习框架之一。在未来，我们有理由相信，PyTorch将会在深度学习领域继续发挥重要的作用。</p> 
<h3><a id="12_Pytorch_26"></a>1.2 Pytorch的优点</h3> 
<p>PyTorch不仅是最受欢迎的深度学习框架之一，而且也是最强大的深度学习框架之一。它有许多独特的优点，使其在学术界和工业界都受到广泛的关注和使用。接下来我们就来详细地探讨一下PyTorch的优点。</p> 
<p><strong>1. 动态计算图</strong></p> 
<p>PyTorch最突出的优点之一就是它使用了动态计算图（Dynamic Computation Graphs，DCGs），与TensorFlow和其他框架使用的静态计算图不同。动态计算图允许你在运行时更改图的行为。这使得PyTorch非常灵活，在处理不确定性或复杂性时具有优势，因此非常适合研究和原型设计。</p> 
<p><strong>2. 易用性</strong></p> 
<p>PyTorch被设计成易于理解和使用。其API设计的直观性使得学习和使用PyTorch成为一件非常愉快的事情。此外，由于PyTorch与Python的深度集成，它在Python程序员中非常流行。</p> 
<p><strong>3. 易于调试</strong></p> 
<p>由于PyTorch的动态性和Python性质，调试PyTorch程序变得相当直接。你可以使用Python的标准调试工具，如PDB或PyCharm，直接查看每个操作的结果和中间变量的状态。</p> 
<p><strong>4. 强大的社区支持</strong></p> 
<p>PyTorch的社区非常活跃和支持。官方论坛、GitHub、Stack Overflow等平台上有大量的PyTorch用户和开发者，你可以从中找到大量的资源和帮助。</p> 
<p><strong>5. 广泛的预训练模型</strong></p> 
<p>PyTorch提供了大量的预训练模型，包括但不限于ResNet，VGG，Inception，SqueezeNet，EfficientNet等等。这些预训练模型可以帮助你快速开始新的项目。</p> 
<p><strong>6. 高效的GPU利用</strong></p> 
<p>PyTorch可以非常高效地利用NVIDIA的CUDA库来进行GPU计算。同时，它还支持分布式计算，让你可以在多个GPU或服务器上训练模型。</p> 
<p>综上所述，PyTorch因其易用性、灵活性、丰富的功能以及强大的社区支持，在深度学习领域中备受欢迎。</p> 
<h3><a id="13_Pytorch_57"></a>1.3 Pytorch的使用场景</h3> 
<p>PyTorch的强大功能和灵活性使其在许多深度学习应用场景中都能够发挥重要作用。以下是PyTorch在各种应用中的一些典型用例：</p> 
<p><strong>1. 计算机视觉</strong></p> 
<p>在计算机视觉方面，PyTorch提供了许多预训练模型（如ResNet，VGG，Inception等）和工具（如TorchVision），可以用于图像分类、物体检测、语义分割和图像生成等任务。这些预训练模型和工具大大简化了开发计算机视觉应用的过程。</p> 
<p><strong>2. 自然语言处理</strong></p> 
<p>在自然语言处理（NLP）领域，PyTorch的动态计算图特性使得其非常适合处理变长输入，这对于许多NLP任务来说是非常重要的。同时，PyTorch也提供了一系列的NLP工具和预训练模型（如Transformer，BERT等），可以帮助我们处理文本分类、情感分析、命名实体识别、机器翻译和问答系统等任务。</p> 
<p><strong>3. 生成对抗网络</strong></p> 
<p>生成对抗网络（GANs）是一种强大的深度学习模型，被广泛应用于图像生成、图像到图像的转换、样式迁移和数据增强等任务。PyTorch的灵活性使得其非常适合开发和训练GAN模型。</p> 
<p><strong>4. 强化学习</strong></p> 
<p>强化学习是一种学习方法，其中智能体通过与环境的交互来学习如何执行任务。PyTorch的动态计算图和易于使用的API使得其在实现强化学习算法时表现出极高的效率。</p> 
<p><strong>5. 时序数据分析</strong></p> 
<p>在处理时序数据的任务中，如语音识别、时间序列预测等，PyTorch的动态计算图为处理可变长度的序列数据提供了便利。同时，PyTorch提供了包括RNN、LSTM、GRU在内的各种循环神经网络模型。</p> 
<p>总的来说，PyTorch凭借其强大的功能和极高的灵活性，在许多深度学习的应用场景中都能够发挥重要作用。无论你是在研究新的深度学习模型，还是在开发实际的深度学习应用，PyTorch都能够提供强大的支持。</p> 
<hr> 
<h2><a id="2_Pytorch_86"></a>2. Pytorch基础</h2> 
<p><img src="https://images2.imgbox.com/cb/8f/LcgdGWBs_o.png" alt="">在我们开始深入使用PyTorch之前，让我们先了解一些基础概念和操作。这一部分将涵盖PyTorch的基础，包括tensor操作、GPU加速以及自动求导机制。</p> 
<h3><a id="21_Tensor_91"></a>2.1 Tensor操作</h3> 
<p>Tensor是PyTorch中最基本的数据结构，你可以将其视为多维数组或者矩阵。PyTorch tensor和NumPy array非常相似，但是tensor还可以在GPU上运算，而NumPy array则只能在CPU上运算。下面，我们将介绍一些基本的tensor操作。</p> 
<p>首先，我们需要导入PyTorch库：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch  
</code></pre> 
<p>然后，我们可以创建一个新的tensor。以下是一些创建tensor的方法：</p> 
<pre><code class="prism language-python"><span class="token comment"># 创建一个未初始化的5x3矩阵  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
  
<span class="token comment"># 创建一个随机初始化的5x3矩阵  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
  
<span class="token comment"># 创建一个5x3的零矩阵，类型为long  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
  
<span class="token comment"># 直接从数据创建tensor  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
</code></pre> 
<p>我们还可以对已有的tensor进行操作。以下是一些基本操作：</p> 
<pre><code class="prism language-python"><span class="token comment"># 创建一个tensor，并设置requires_grad=True以跟踪计算历史  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
  
<span class="token comment"># 对tensor进行操作  </span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  
  
<span class="token comment"># y是操作的结果，所以它有grad_fn属性  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>  
  
<span class="token comment"># 对y进行更多操作  </span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>  
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> out<span class="token punctuation">)</span>  
</code></pre> 
<p>上述操作的结果如下：</p> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
        <span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>  
<span class="token operator">&lt;</span>AddBackward0 <span class="token builtin">object</span> at <span class="token number">0x7f36c0a7f1d0</span><span class="token operator">&gt;</span>  
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">27.</span><span class="token punctuation">,</span> <span class="token number">27.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
        <span class="token punctuation">[</span><span class="token number">27.</span><span class="token punctuation">,</span> <span class="token number">27.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">27.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MeanBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在PyTorch中，我们可以使用<code>.backward()</code>方法来计算梯度。例如：</p> 
<pre><code class="prism language-python"><span class="token comment"># 因为out包含一个标量，out.backward()等价于out.backward(torch.tensor(1.))  </span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 打印梯度 d(out)/dx  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
</code></pre> 
<p>以上是PyTorch tensor的基本操作，我们可以看到PyTorch tensor操作非常简单和直观。在后续的学习中，我们将会使用到更多的tensor操作，例如索引、切片、数学运算、线性代数、随机数等等。</p> 
<h3><a id="22_GPU_167"></a>2.2 GPU加速</h3> 
<p>在深度学习训练中，GPU（图形处理器）加速是非常重要的一部分。GPU的并行计算能力使得其比CPU在大规模矩阵运算上更具优势。PyTorch提供了简单易用的API，让我们可以很容易地在CPU和GPU之间切换计算。</p> 
<p>首先，我们需要检查系统中是否存在可用的GPU。在PyTorch中，我们可以使用<code>torch.cuda.is_available()</code>来检查：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch  
  
<span class="token comment"># 检查是否有可用的GPU  </span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"There is a GPU available."</span><span class="token punctuation">)</span>  
<span class="token keyword">else</span><span class="token punctuation">:</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"There is no GPU available."</span><span class="token punctuation">)</span>  
</code></pre> 
<p>如果存在可用的GPU，我们可以使用<code>.to()</code>方法将tensor移动到GPU上：</p> 
<pre><code class="prism language-python"><span class="token comment"># 创建一个tensor  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 移动tensor到GPU上  </span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  
</code></pre> 
<p>我们也可以直接在创建tensor的时候就指定其设备：</p> 
<pre><code class="prism language-python"><span class="token comment"># 直接在GPU上创建tensor  </span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在进行模型训练时，我们通常会将模型和数据都移动到GPU上：</p> 
<pre><code class="prism language-python"><span class="token comment"># 创建一个简单的模型  </span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 创建一些数据  </span>
data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 移动模型和数据到GPU  </span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  
    data <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span>  
</code></pre> 
<p>以上就是在PyTorch中进行GPU加速的基本操作。使用GPU加速可以显著提高深度学习模型的训练速度。但需要注意的是，数据在CPU和GPU之间的传输会消耗一定的时间，因此我们应该尽量减少数据的传输次数。</p> 
<h3><a id="23__220"></a>2.3 自动求导</h3> 
<p>在深度学习中，我们经常需要进行梯度下降优化。这就需要我们计算梯度，也就是函数的导数。在PyTorch中，我们可以使用自动求导机制（autograd）来自动计算梯度。</p> 
<p>在PyTorch中，我们可以设置<code>tensor.requires_grad=True</code>来追踪其上的所有操作。完成计算后，我们可以调用<code>.backward()</code>方法，PyTorch会自动计算和存储梯度。这个梯度可以通过<code>.grad</code>属性进行访问。</p> 
<p>下面是一个简单的示例：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch  
  
<span class="token comment"># 创建一个tensor并设置requires_grad=True来追踪其计算历史  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 对这个tensor做一次运算：  </span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>  
  
<span class="token comment"># y是计算的结果，所以它有grad_fn属性  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span>  
  
<span class="token comment"># 对y进行更多的操作  </span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>  
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> out<span class="token punctuation">)</span>  
  
<span class="token comment"># 使用.backward()来进行反向传播，计算梯度  </span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 输出梯度d(out)/dx  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
</code></pre> 
<p>以上示例中，<code>out.backward()</code>等同于<code>out.backward(torch.tensor(1.))</code>。如果<code>out</code>不是一个标量，因为tensor是矩阵，那么在调用<code>.backward()</code>时需要传入一个与<code>out</code>同形的权重向量进行相乘。</p> 
<p>例如：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
  
y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span>  
<span class="token keyword">while</span> y<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">1000</span><span class="token punctuation">:</span>  
    y <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">2</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>  
  
v <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.0001</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>  
y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>v<span class="token punctuation">)</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
</code></pre> 
<p>以上就是PyTorch中自动求导的基本使用方法。自动求导是PyTorch的重要特性之一，它为深度学习模型的训练提供了极大的便利。</p> 
<hr> 
<h2><a id="3_PyTorch__277"></a>3. PyTorch 神经网络</h2> 
<p><img src="https://images2.imgbox.com/87/74/9EDL2TNe_o.png" alt="">在掌握了PyTorch的基本使用方法之后，我们将探索一些更为高级的特性和用法。这些高级特性包括神经网络构建、数据加载以及模型保存和加载等等。</p> 
<h3><a id="31__282"></a>3.1 构建神经网络</h3> 
<p>PyTorch提供了<code>torch.nn</code>库，它是用于构建神经网络的工具库。<code>torch.nn</code>库依赖于<code>autograd</code>库来定义和计算梯度。<code>nn.Module</code>包含了神经网络的层以及返回输出的<code>forward(input)</code>方法。</p> 
<p>以下是一个简单的神经网络的构建示例：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch  
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn  
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F  
  
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
        <span class="token comment"># 输入图像channel：1，输出channel：6，5x5卷积核  </span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  
  
        <span class="token comment"># 全连接层  </span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>  
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>  
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  
  
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># 使用2x2窗口进行最大池化  </span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
        <span class="token comment"># 如果窗口是方的，只需要指定一个维度  </span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  
  
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_flat_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  
  
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  
  
        <span class="token keyword">return</span> x  
  
    <span class="token keyword">def</span> <span class="token function">num_flat_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 获取除了batch维度之外的其他维度  </span>
        num_features <span class="token operator">=</span> <span class="token number">1</span>  
        <span class="token keyword">for</span> s <span class="token keyword">in</span> size<span class="token punctuation">:</span>  
            num_features <span class="token operator">*=</span> s  
        <span class="token keyword">return</span> num_features  
  
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>  
</code></pre> 
<p>以上就是一个简单的神经网络的构建方法。我们首先定义了一个<code>Net</code>类，这个类继承自<code>nn.Module</code>。然后在<code>__init__</code>方法中定义了网络的结构，在<code>forward</code>方法中定义了数据的流向。在网络的构建过程中，我们可以使用任何tensor操作。</p> 
<p>需要注意的是，<code>backward</code>函数（用于计算梯度）会被<code>autograd</code>自动创建和实现。你只需要在<code>nn.Module</code>的子类中定义<code>forward</code>函数。</p> 
<p>在创建好神经网络后，我们可以使用<code>net.parameters()</code>方法来返回网络的可学习参数。</p> 
<h3><a id="32__338"></a>3.2 数据加载和处理</h3> 
<p>在深度学习项目中，除了模型设计之外，数据的加载和处理也是非常重要的一部分。PyTorch提供了<code>torch.utils.data.DataLoader</code>类，可以帮助我们方便地进行数据的加载和处理。</p> 
<h4><a id="321_DataLoader_343"></a>3.2.1 DataLoader介绍</h4> 
<p><code>DataLoader</code>类提供了对数据集的并行加载，可以有效地加载大量数据，并提供了多种数据采样方式。常用的参数有：</p> 
<ul><li> <p>dataset：加载的数据集（Dataset对象）</p> </li><li> <p>batch_size：batch大小</p> </li><li> <p>shuffle：是否每个epoch时都打乱数据</p> </li><li> <p>num_workers：使用多进程加载的进程数，0表示不使用多进程</p> </li></ul> 
<p>以下是一个简单的使用示例：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader  
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms  
  
<span class="token comment"># 数据转换  </span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>  
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
<span class="token punctuation">]</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载训练集  </span>
trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
trainloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载测试集  </span>
testset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
testloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
</code></pre> 
<h4><a id="322__377"></a>3.2.2 自定义数据集</h4> 
<p>除了使用内置的数据集，我们也可以自定义数据集。自定义数据集需要继承<code>Dataset</code>类，并实现<code>__len__</code>和<code>__getitem__</code>两个方法。</p> 
<p>以下是一个自定义数据集的简单示例：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader  
  
<span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x_tensor<span class="token punctuation">,</span> y_tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x_tensor  
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y_tensor  
  
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>  
  
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">)</span>  
  
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>  
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>  
  
my_dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>my_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  
  
<span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> loader<span class="token punctuation">:</span>  
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x:"</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> <span class="token string">"y:"</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>  
</code></pre> 
<p>这个例子中，我们创建了一个简单的数据集，包含10个数据。然后我们使用<code>DataLoader</code>加载数据，并设置了batch大小和shuffle参数。</p> 
<p>以上就是PyTorch中数据加载和处理的主要方法，通过这些方法，我们可以方便地对数据进行加载和处理。</p> 
<h3><a id="33__411"></a>3.3 模型的保存和加载</h3> 
<p>在深度学习模型的训练过程中，我们经常需要保存模型的参数以便于将来重新加载。这对于中断的训练过程的恢复，或者用于模型的分享和部署都是非常有用的。</p> 
<p>PyTorch提供了简单的API来保存和加载模型。最常见的方法是使用<code>torch.save</code>来保存模型的参数，然后通过<code>torch.load</code>来加载模型的参数。</p> 
<h4><a id="331__418"></a>3.3.1 保存和加载模型参数</h4> 
<p>以下是一个简单的示例：</p> 
<pre><code class="prism language-python"><span class="token comment"># 保存  </span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>  
  
<span class="token comment"># 加载  </span>
model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>  
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">)</span>  
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在保存模型参数时，我们通常使用<code>.state_dict()</code>方法来获取模型的参数。<code>.state_dict()</code>是一个从参数名字映射到参数值的字典对象。</p> 
<p>在加载模型参数时，我们首先需要实例化一个和原模型结构相同的模型，然后使用<code>.load_state_dict()</code>方法加载参数。</p> 
<p>请注意，<code>load_state_dict()</code>函数接受一个字典对象，而不是保存对象的路径。这意味着在你传入<code>load_state_dict()</code>函数之前，你必须反序列化你的保存的<code>state_dict</code>。</p> 
<p>在加载模型后，我们通常调用<code>.eval()</code>方法将dropout和batch normalization层设置为评估模式。否则，它们会在评估模式下保持训练模式。</p> 
<h4><a id="332__440"></a>3.3.2 保存和加载整个模型</h4> 
<p>除了保存模型的参数，我们也可以保存整个模型。</p> 
<pre><code class="prism language-python"><span class="token comment"># 保存  </span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>  
  
<span class="token comment"># 加载  </span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>  
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<p>保存整个模型会将模型的结构和参数一起保存。这意味着在加载模型时，我们不再需要手动创建模型实例。但是，这种方式需要更多的磁盘空间，并且可能在某些情况下导致代码的混乱，所以并不总是推荐的。</p> 
<p>以上就是PyTorch中模型的保存和加载的基本方法。适当的保存和加载模型可以帮助我们更好地进行模型的训练和评估。</p> 
<hr> 
<h2><a id="4_PyTorch_GPT_459"></a>4. PyTorch GPT加速</h2> 
<p><img src="https://images2.imgbox.com/89/18/AxDLL9Dt_o.png" alt="">掌握了PyTorch的基础和高级用法之后，我们现在要探讨一些PyTorch的进阶技巧，帮助我们更好地理解和使用这个强大的深度学习框架。</p> 
<h3><a id="41_GPU_464"></a>4.1 使用GPU加速</h3> 
<p>PyTorch支持使用GPU进行计算，这可以大大提高训练和推理的速度。使用GPU进行计算的核心就是将Tensor和模型转移到GPU上。</p> 
<h5><a id="411_GPU_469"></a>4.1.1 判断是否支持GPU</h5> 
<p>首先，我们需要判断当前的环境是否支持GPU。这可以通过<code>torch.cuda.is_available()</code>来实现：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 输出：True 或 False  </span>
</code></pre> 
<h5><a id="412_TensorCPUGPU_477"></a>4.1.2 Tensor在CPU和GPU之间转移</h5> 
<p>如果支持GPU，我们可以使用<code>.to(device)</code>或<code>.cuda()</code>方法将Tensor转移到GPU上。同样，我们也可以使用<code>.cpu()</code>方法将Tensor转移到CPU上：</p> 
<pre><code class="prism language-python"><span class="token comment"># 判断是否支持CUDA  </span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 创建一个Tensor  </span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 将Tensor转移到GPU上  </span>
x_gpu <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  
  
<span class="token comment"># 或者  </span>
x_gpu <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 将Tensor转移到CPU上  </span>
x_cpu <span class="token operator">=</span> x_gpu<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<h5><a id="413_GPU_498"></a>4.1.3 将模型转移到GPU上</h5> 
<p>类似的，我们也可以将模型转移到GPU上：</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token punctuation">)</span>  
model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  
</code></pre> 
<p>当模型在GPU上时，我们需要确保输入的Tensor也在GPU上，否则会报错。</p> 
<p>注意，将模型转移到GPU上后，模型的所有参数和缓冲区都会转移到GPU上。</p> 
<p>以上就是使用GPU进行计算的基本方法。通过合理的使用GPU，我们可以大大提高模型的训练和推理速度。</p> 
<h3><a id="42_torchvision_513"></a>4.2 使用torchvision进行图像操作</h3> 
<p>torchvision是一个独立于PyTorch的包，提供了大量的图像数据集，图像处理工具和预训练模型等。</p> 
<h4><a id="421_torchvisiondatasets_518"></a>4.2.1 torchvision.datasets</h4> 
<p>torchvision.datasets模块提供了各种公共数据集，如CIFAR10、MNIST、ImageNet等，我们可以非常方便地下载和使用这些数据集。例如，下面的代码展示了如何下载和加载CIFAR10数据集：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms  
  
<span class="token comment"># 数据转换  </span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>  
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
<span class="token punctuation">]</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载训练集  </span>
trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载测试集  </span>
testset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
</code></pre> 
<h4><a id="422_torchvisiontransforms_540"></a>4.2.2 torchvision.transforms</h4> 
<p>torchvision.transforms模块提供了各种图像转换的工具，我们可以使用这些工具进行图像预处理和数据增强。例如，上面的代码中，我们使用了Compose函数来组合了两个图像处理操作：ToTensor（将图像转换为Tensor）和Normalize（标准化图像）。</p> 
<h4><a id="423_torchvisionmodels_544"></a>4.2.3 torchvision.models</h4> 
<p>torchvision.models模块提供了预训练的模型，如ResNet、VGG、AlexNet等。我们可以非常方便地加载这些模型，并使用这些模型进行迁移学习。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models  
  
<span class="token comment"># 加载预训练的resnet18模型  </span>
resnet18 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  
</code></pre> 
<p>以上就是torchvision的基本使用，它为我们提供了非常丰富的工具，可以大大提升我们处理图像数据的效率。</p> 
<h3><a id="43_TensorBoard_557"></a>4.3 使用TensorBoard进行可视化</h3> 
<p>TensorBoard 是一个可视化工具，它可以帮助我们更好地理解，优化，和调试深度学习模型。PyTorch 提供了对 TensorBoard 的支持，我们可以非常方便地使用 TensorBoard 来监控模型的训练过程，比较不同模型的性能，可视化模型结构，等等。</p> 
<h4><a id="431__TensorBoard_562"></a>4.3.1 启动 TensorBoard</h4> 
<p>要启动 TensorBoard，我们需要在命令行中运行 <code>tensorboard --logdir=runs</code> 命令，其中 <code>runs</code> 是保存 TensorBoard 数据的目录。</p> 
<h4><a id="432__566"></a>4.3.2 记录数据</h4> 
<p>我们可以使用 <code>torch.utils.tensorboard</code> 模块来记录数据。首先，我们需要创建一个 <code>SummaryWriter</code> 对象，然后通过这个对象的方法来记录数据。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter  
  
<span class="token comment"># 创建一个 SummaryWriter 对象  </span>
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">'runs/experiment1'</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 使用 writer 来记录数据  </span>
<span class="token keyword">for</span> n_iter <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/train'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>  
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/test'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>  
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/train'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>  
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/test'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> n_iter<span class="token punctuation">)</span>  
  
<span class="token comment"># 关闭 writer  </span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<h4><a id="433__587"></a>4.3.3 可视化模型结构</h4> 
<p>我们也可以使用 TensorBoard 来可视化模型结构。</p> 
<pre><code class="prism language-python"><span class="token comment"># 添加模型  </span>
writer<span class="token punctuation">.</span>add_graph<span class="token punctuation">(</span>model<span class="token punctuation">,</span> images<span class="token punctuation">)</span>  
</code></pre> 
<h4><a id="434__596"></a>4.3.4 可视化高维数据</h4> 
<p>我们还可以使用 TensorBoard 的嵌入功能来可视化高维数据，如图像特征、词嵌入等。</p> 
<pre><code class="prism language-python"><span class="token comment"># 添加嵌入  </span>
writer<span class="token punctuation">.</span>add_embedding<span class="token punctuation">(</span>features<span class="token punctuation">,</span> metadata<span class="token operator">=</span>class_labels<span class="token punctuation">,</span> label_img<span class="token operator">=</span>images<span class="token punctuation">)</span>  
</code></pre> 
<p>以上就是 TensorBoard 的基本使用方法。通过使用 TensorBoard，我们可以更好地理解和优化我们的模型。</p> 
<hr> 
<h2><a id="5_PyTorch_609"></a>5. PyTorch实战案例</h2> 
<p><img src="https://images2.imgbox.com/69/0f/fMcQd1wk_o.png" alt="">在这一部分中，我们将通过一个实战案例来详细介绍如何使用PyTorch进行深度学习模型的开发。我们将使用CIFAR10数据集来训练一个卷积神经网络（Convolutional Neural Network，CNN）。</p> 
<h3><a id="51__614"></a>5.1 数据加载和预处理</h3> 
<p>首先，我们需要加载数据并进行预处理。我们将使用torchvision包来下载CIFAR10数据集，并使用transforms模块来对数据进行预处理。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch  
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms  
  
<span class="token comment"># 定义数据预处理操作  </span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>  
    transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 数据增强：随机翻转图片  </span>
    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 数据增强：随机裁剪图片  </span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 将PIL.Image或者numpy.ndarray数据类型转化为torch.FloadTensor，并归一化到[0.0, 1.0]  </span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.4914</span><span class="token punctuation">,</span> <span class="token number">0.4822</span><span class="token punctuation">,</span> <span class="token number">0.4465</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.2023</span><span class="token punctuation">,</span> <span class="token number">0.1994</span><span class="token punctuation">,</span> <span class="token number">0.2010</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 标准化（这里的均值和标准差是CIFAR10数据集的）  </span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载训练数据集  </span>
trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 下载并加载测试数据集  </span>
testset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>  
testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这段代码中，我们首先定义了一系列的数据预处理操作，然后使用<code>datasets.CIFAR10</code>来下载CIFAR10数据集并进行预处理，最后使用<code>torch.utils.data.DataLoader</code>来创建数据加载器，它可以帮助我们在训练过程中按照批次获取数据。</p> 
<h3><a id="52__642"></a>5.2 定义网络模型</h3> 
<p>接下来，我们定义我们的卷积神经网络模型。在这个案例中，我们将使用两个卷积层和两个全连接层。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn  
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F  
  
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>  
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># 输入通道数3，输出通道数6，卷积核大小5  </span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 最大池化，核大小2，步长2  </span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># 输入通道数6，输出通道数16，卷积核大小5  </span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>  <span class="token comment"># 全连接层，输入维度16*5*5，输出维度120  </span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>  <span class="token comment"># 全连接层，输入维度120，输出维度84  </span>
        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 全连接层，输入维度84，输出维度10（CIFAR10有10类）  </span>
  
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>  
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 第一层卷积+ReLU激活函数+池化  </span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 第二层卷积+ReLU激活函数+池化  </span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># 将特征图展平  </span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 第一层全连接+ReLU激活函数  </span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 第二层全连接+ReLU激活函数  </span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 第三层全连接  </span>
        <span class="token keyword">return</span> x  
  
<span class="token comment"># 创建网络  </span>
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这个网络模型中，我们使用<code>nn.Module</code>来定义我们的网络模型，然后在<code>__init__</code>方法中定义网络的层，最后在<code>forward</code>方法中定义网络的前向传播过程。</p> 
<h3><a id="53__676"></a>5.3 定义损失函数和优化器</h3> 
<p>现在我们已经有了数据和模型，下一步我们需要定义损失函数和优化器。损失函数用于衡量模型的预测与真实标签的差距，优化器则用于优化模型的参数以减少损失。</p> 
<p>在这个案例中，我们将使用交叉熵损失函数（Cross Entropy Loss）和随机梯度下降优化器（Stochastic Gradient Descent，SGD）。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim  
  
<span class="token comment"># 定义损失函数  </span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 定义优化器  </span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这段代码中，我们首先使用<code>nn.CrossEntropyLoss</code>来定义损失函数，然后使用<code>optim.SGD</code>来定义优化器。我们需要将网络的参数传递给优化器，然后设置学习率和动量。</p> 
<h3><a id="54__695"></a>5.4 训练网络</h3> 
<p>一切准备就绪后，我们开始训练网络。在训练过程中，我们首先通过网络进行前向传播得到输出，然后计算输出与真实标签的损失，接着通过后向传播计算梯度，最后使用优化器更新模型参数。</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 在数据集上训练两遍  </span>
  
    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>  
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
        <span class="token comment"># 获取输入数据  </span>
        inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data  
  
        <span class="token comment"># 梯度清零  </span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
        <span class="token comment"># 前向传播  </span>
        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  
  
        <span class="token comment"># 计算损失  </span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>  
  
        <span class="token comment"># 反向传播  </span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
        <span class="token comment"># 更新参数  </span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
        <span class="token comment"># 打印统计信息  </span>
        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>  
        <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">2000</span> <span class="token operator">==</span> <span class="token number">1999</span><span class="token punctuation">:</span>  <span class="token comment"># 每2000个批次打印一次  </span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d, %5d] loss: %.3f'</span> <span class="token operator">%</span>  
                  <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> running_loss <span class="token operator">/</span> <span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
            running_loss <span class="token operator">=</span> <span class="token number">0.0</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这段代码中，我们首先对数据集进行两轮训练。在每轮训练中，我们遍历数据加载器，获取一批数据，然后通过网络进行前向传播得到输出，计算损失，进行反向传播，最后更新参数。我们还在每2000个批次后打印一次损失信息，以便我们了解训练过程。</p> 
<h3><a id="55__735"></a>5.5 测试网络</h3> 
<p>训练完成后，我们需要在测试集上测试网络的性能。这可以让我们了解模型在未见过的数据上的表现如何，以评估其泛化能力。</p> 
<pre><code class="prism language-python"><span class="token comment"># 加载一些测试图片  </span>
dataiter <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>testloader<span class="token punctuation">)</span>  
images<span class="token punctuation">,</span> labels <span class="token operator">=</span> dataiter<span class="token punctuation">.</span><span class="token builtin">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 打印图片  </span>
imshow<span class="token punctuation">(</span>torchvision<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>make_grid<span class="token punctuation">(</span>images<span class="token punctuation">)</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 显示真实的标签  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'GroundTruth: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>labels<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 让网络做出预测  </span>
outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>  
  
<span class="token comment"># 预测的标签是最大输出的标签  </span>
_<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 显示预测的标签  </span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Predicted: '</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">'%5s'</span> <span class="token operator">%</span> classes<span class="token punctuation">[</span>predicted<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  
  
<span class="token comment"># 在整个测试集上测试网络  </span>
correct <span class="token operator">=</span> <span class="token number">0</span>  
total <span class="token operator">=</span> <span class="token number">0</span>  
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  
    <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>  
        images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data  
        outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>images<span class="token punctuation">)</span>  
        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  
        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>  
  
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of the network on the 10000 test images: %d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>  
    <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">)</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这段代码中，我们首先加载一些测试图片，并打印出真实的标签。然后我们让网络对这些图片做出预测，并打印出预测的标签。最后，我们在整个测试集上测试网络，并打印出网络在测试集上的准确率。</p> 
<h3><a id="56__777"></a>5.6 保存和加载模型</h3> 
<p>在训练完网络并且对其进行了测试后，我们可能希望保存训练好的模型，以便于将来使用，或者继续训练。</p> 
<pre><code class="prism language-python"><span class="token comment"># 保存模型  </span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'./cifar_net.pth'</span><span class="token punctuation">)</span>  
</code></pre> 
<p>在这段代码中，我们使用<code>torch.save</code>函数，将训练好的模型参数（通过<code>net.state_dict()</code>获得）保存到文件中。</p> 
<p>当我们需要加载模型时，首先需要创建一个新的模型实例，然后使用<code>load_state_dict</code>方法将参数加载到模型中。</p> 
<pre><code class="prism language-python"><span class="token comment"># 加载模型  </span>
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 创建新的网络实例  </span>
net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'./cifar_net.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 加载模型参数  </span>
</code></pre> 
<p>需要注意的是，<code>load_state_dict</code>方法加载的是模型的参数，而不是模型本身。因此，在加载模型参数之前，你需要先创建一个模型实例，这个模型需要与保存的模型具有相同的结构。</p> 
<hr> 
<h2><a id="6__801"></a>6. 总结</h2> 
<p>这篇文章通过详细且实践性的方式介绍了 PyTorch 的使用，包括环境安装、基础知识、张量操作、自动求导机制、神经网络创建、数据处理、模型训练、测试以及模型的保存和加载。</p> 
<p>我们利用 PyTorch 从头到尾完成了一个完整的神经网络训练流程，并在 CIFAR10 数据集上测试了网络的性能。在这个过程中，我们深入了解了 PyTorch 提供的各种功能和工具。</p> 
<p>希望这篇文章能对你学习 PyTorch 提供帮助，对于想要更深入了解 PyTorch 的读者，我建议参考 PyTorch 的官方文档以及各种开源教程。实践是最好的学习方法，只有通过大量的练习和实践，才能真正掌握 PyTorch 和深度学习。</p> 
<h2><a id="Python_810"></a>关于Python学习指南</h2> 
<p>学好 Python 不论是就业还是做副业赚钱都不错，但要学会 Python 还是要有一个学习规划。最后给大家分享一份全套的 Python 学习资料，给那些想学习 Python 的小伙伴们一点帮助！</p> 
<p><mark>包括：Python激活码+安装包、Python web开发，Python爬虫，Python数据分析，人工智能、自动化办公等学习教程。带你从零基础系统性的学好Python！</mark></p> 
<h4><a id="Python_821"></a>👉Python所有方向的学习路线👈</h4> 
<p>Python所有方向路线就是把Python常用的技术点做整理，形成各个领域的知识点汇总，它的用处就在于，你可以按照上面的知识点去找对应的学习资源，保证自己学得较为全面。<mark><strong>（全套教程文末领取）</strong></mark></p> 
<p><img src="https://images2.imgbox.com/f3/b5/689vpnGP_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Python600_832"></a>👉Python学习视频600合集👈</h4> 
<p>观看零基础学习视频，看视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。</p> 
<p><img src="https://images2.imgbox.com/f8/6f/EJmCT9YF_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="mark_840"></a><mark>温馨提示：篇幅有限，已打包文件夹，获取方式在：文末</mark></h4> 
<h4><a id="Python70_847"></a>👉Python70个实战练手案例&amp;源码👈</h4> 
<p>光学理论是没用的，要学会跟着一起敲，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。</p> 
<p><img src="https://images2.imgbox.com/34/85/oWuONH6d_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Python_855"></a>👉Python大厂面试资料👈</h4> 
<p>我们学习Python必然是为了找到高薪的工作，下面这些面试题是来自<strong>阿里、腾讯、字节等一线互联网大厂</strong>最新的面试资料，并且有阿里大佬给出了权威的解答，刷完这一套面试资料相信大家都能找到满意的工作。</p> 
<p><img src="https://images2.imgbox.com/3f/58/ck4tyHYZ_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/97/08/5knXUFvp_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Python_864"></a>👉Python副业兼职路线&amp;方法👈</h4> 
<p>学好 Python 不论是就业还是做副业赚钱都不错，但要学会兼职接单还是要有一个学习规划。</p> 
<p><img src="https://images2.imgbox.com/44/be/JaSYyZk1_o.png" alt="在这里插入图片描述"></p> 
<p><strong>👉</strong> <strong>这份完整版的Python全套学习资料已经上传，朋友们如果需要可以扫描下方CSDN官方认证二维码或者点击链接免费领取</strong>【<strong><code>保证100%免费</code></strong>】</p> 
<p><font color="red"><strong>点击免费领取《CSDN大礼包》：<a href="https://mp.weixin.qq.com/s/t6nHTsrTe7Qt70EpdZTTaw" rel="nofollow">Python入门到进阶资料 &amp; 实战源码 &amp; 兼职接单方法 </a> 安全链接免费领取</strong></font></p> 
<img src="https://images2.imgbox.com/7c/cf/ok4GKPkH_o.jpg">
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a100fefc4a4054ab2b1dfca8fd77f650/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">阿拉伯数字转中文数字</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c6a54fccb41389cc9d7dc0a406e9d734/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">可视化库Cartopy安装教程（非常详细）从零基础入门到精通，看完这一篇就够了</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>