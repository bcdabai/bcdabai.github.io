<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Flink API介绍 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Flink API介绍" />
<meta property="og:description" content="Flink API介绍 Flink提供了三层API，每层在简洁性和表达性之间进行了不同的权衡。
flink-api
ProcessFunction是Flink提供的最具表现力的功能接口，它提供了对时间和状态的细粒度控制，能够任意修改状态。所以ProcessFunction能够为许多有事件驱动的应用程序实现复杂的事件处理逻辑。
DataStream API为许多通用的流处理操作提供原语，比如window。DataStream API适用于Java和Scala，它基于函数实现，比如map()、reduce()等。我们也可以自己扩展接口自定义函数。
SQL &amp; Table API 这两个都是关系API，是批处理和流处理统一的API。Table API和SQL利用Apache Calcite进行解析、验证和查询优化。它们可以与DataStream和DataSet API无缝集成，并支持用户定义标量、聚合和表值函数。关系API(relational api)目标在于简化数据分析、数据流水线(data pipelining)和ETL。
我们一般主要使用DataStream进行数据处理，下面介绍的API也是DataStream相关的API。
DataStream API DataStream是Flink编写流处理作业的API。我们前面说过一个完整的Flink处理程序应该包含三部分：数据源(Source)、转换操作(Transformation)、结果接收(Sink)。下面我们从这三部分来看DataStream API。
数据源(Source) Flink应用程序从数据源获取要处理的数据，DataStream通过StreamExecutionEnvironment.addResource(SourceFunction) 来添加数据源。为了方便使用，Flink预提几类预定义的数据源，比如读取文件的Source、通过Sockt读取的Source、从内存中获取的Source等。
基于集合的预定义Source 基于集合的数据源一般是指从内存集合中直接读取要处理的数据，StreamExecutionEnvironment提供了4类预定义方法。
fromCollection
fromCollection是从给定的集合中创建DataStream，StreamExecutionEnvironment提供了4种重载方法：
fromCollection(Collection&lt;T&gt; data)：通过给定的集合创建DataStream。返回数据类型为集合元素类型。fromCollection(Collection&lt;T&gt; data,TypeInformation&lt;T&gt; typeInfo)：通过给定的非空集合创建DataStream。返回数据类型为typeInfo。fromCollection(Iterator&lt;T&gt; data,Class&lt;T&gt; type)：通过给定的迭代器创建DataStream。返回数据类型为type。fromCollection(Iterator&lt;T&gt; data,TypeInformation&lt;T&gt; typeInfo)：通过给定的迭代器创建DataStream。返回数据类型为typeInfo。 fromParallelCollection
fromParallelCollection和fromCollection类似，但是是并行的从迭代器中创建DataStream。
fromParallelCollection(SplittableIterator&lt;T&gt; data,Class&lt;T&gt; type)fromParallelCollection(SplittableIterator&lt;T&gt;,TypeInfomation typeInfo) 和Iterable中Spliterator类似，这是JDK1.8新增的特性，并行读取集合元素。
fromElements
fromElements从给定的对象序列中创建DataStream，StreamExecutionEnvironment提供了2种重载方法：
fromElements(T... data)：从给定对象序列中创建DataStream，返回的数据类型为该对象类型自身。fromElements(Class&lt;T&gt; type,T... data)：从给定对象序列中创建DataStream，返回的数据类型type。 generateSequence
generateSequence(long from,long to)从给定间隔的数字序列中创建DataStream，比如from为1，to为10，则会生成1~10的序列。
基于Socket的预定义Source 我们还可以通过Socket来读取数据，通过Sockt创建的DataStream能够从Socket中无限接收字符串，字符编码采用系统默认字符集。当Socket关闭时，Source停止读取。Socket提供了5个重载方法，但是有两个方法已经标记废弃。
socketTextStream(String hostname,int port)：指定Socket主机和端口，默认数据分隔符为换行符(\n)。socketTextStream(String hostname,int port,String delimiter)：指定Socket主机和端口，数据分隔符为delimiter。socketTextStream(String hostname,int port,String delimiter,long maxRetry)：该重载方法能够当与Socket断开时进行重连，重连次数由maxRetry决定，时间间隔为1秒。如果为0则表示立即终止不重连，如果为负数则表示一直重试。 基于文件的预定义Source 基于文件创建DataStream主要有两种方式：readTextFile和readFile。(readFileStream已废弃)。readTextFile就是简单读取文件，而readFile的使用方式比较灵活。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3b66b52c06fa4405186ed23e10e181b4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-10-12T10:42:23+08:00" />
<meta property="article:modified_time" content="2019-10-12T10:42:23+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Flink API介绍</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>Flink API介绍</h2> 
<p>Flink提供了三层API，每层在简洁性和表达性之间进行了不同的权衡。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/2c/af/yM4rlJs0_o.png"></p> 
<p>flink-api</p> 
<p>ProcessFunction是Flink提供的最具表现力的功能接口，它提供了对时间和状态的细粒度控制，能够任意修改状态。所以ProcessFunction能够为许多有事件驱动的应用程序实现复杂的事件处理逻辑。<br> DataStream API为许多通用的流处理操作提供原语，比如window。DataStream API适用于Java和Scala，它基于函数实现，比如map()、reduce()等。我们也可以自己扩展接口自定义函数。<br> SQL &amp; Table API 这两个都是关系API，是批处理和流处理统一的API。Table API和SQL利用Apache Calcite进行解析、验证和查询优化。它们可以与DataStream和DataSet API无缝集成，并支持用户定义标量、聚合和表值函数。关系API(relational api)目标在于简化数据分析、数据流水线(data pipelining)和ETL。</p> 
<p>我们一般主要使用DataStream进行数据处理，下面介绍的API也是DataStream相关的API。</p> 
<h2>DataStream API</h2> 
<p>DataStream是Flink编写流处理作业的API。我们前面说过一个完整的Flink处理程序应该包含三部分：数据源(Source)、转换操作(Transformation)、结果接收(Sink)。下面我们从这三部分来看DataStream API。</p> 
<h3>数据源(Source)</h3> 
<p>Flink应用程序从数据源获取要处理的数据，DataStream通过<code>StreamExecutionEnvironment.addResource(SourceFunction)</code> 来添加数据源。为了方便使用，Flink预提几类预定义的数据源，比如读取文件的Source、通过Sockt读取的Source、从内存中获取的Source等。</p> 
<h4>基于集合的预定义Source</h4> 
<p>基于集合的数据源一般是指从内存集合中直接读取要处理的数据，StreamExecutionEnvironment提供了4类预定义方法。</p> 
<p>fromCollection</p> 
<p>fromCollection是从给定的集合中创建DataStream，StreamExecutionEnvironment提供了4种重载方法：</p> 
<ul><li>fromCollection(Collection&lt;T&gt; data)：通过给定的集合创建DataStream。返回数据类型为集合元素类型。</li><li>fromCollection(Collection&lt;T&gt; data,TypeInformation&lt;T&gt; typeInfo)：通过给定的非空集合创建DataStream。返回数据类型为typeInfo。</li><li>fromCollection(Iterator&lt;T&gt; data,Class&lt;T&gt; type)：通过给定的迭代器创建DataStream。返回数据类型为type。</li><li>fromCollection(Iterator&lt;T&gt; data,TypeInformation&lt;T&gt; typeInfo)：通过给定的迭代器创建DataStream。返回数据类型为typeInfo。</li></ul> 
<p>fromParallelCollection</p> 
<p>fromParallelCollection和fromCollection类似，但是是并行的从迭代器中创建DataStream。</p> 
<ul><li>fromParallelCollection(SplittableIterator&lt;T&gt; data,Class&lt;T&gt; type)</li><li>fromParallelCollection(SplittableIterator&lt;T&gt;,TypeInfomation typeInfo)</li></ul> 
<blockquote> 
 <p>和Iterable中Spliterator类似，这是JDK1.8新增的特性，并行读取集合元素。</p> 
</blockquote> 
<p>fromElements</p> 
<p>fromElements从给定的对象序列中创建DataStream，StreamExecutionEnvironment提供了2种重载方法：</p> 
<ul><li>fromElements(T... data)：从给定对象序列中创建DataStream，返回的数据类型为该对象类型自身。</li><li>fromElements(Class&lt;T&gt; type,T... data)：从给定对象序列中创建DataStream，返回的数据类型type。</li></ul> 
<p>generateSequence</p> 
<p>generateSequence(long from,long to)从给定间隔的数字序列中创建DataStream，比如from为1，to为10，则会生成1~10的序列。</p> 
<h4>基于Socket的预定义Source</h4> 
<p>我们还可以通过Socket来读取数据，通过Sockt创建的DataStream能够从Socket中无限接收字符串，字符编码采用系统默认字符集。当Socket关闭时，Source停止读取。Socket提供了5个重载方法，但是有两个方法已经标记废弃。</p> 
<ul><li>socketTextStream(String hostname,int port)：指定Socket主机和端口，默认数据分隔符为换行符(\n)。</li><li>socketTextStream(String hostname,int port,String delimiter)：指定Socket主机和端口，数据分隔符为delimiter。</li><li>socketTextStream(String hostname,int port,String delimiter,long maxRetry)：该重载方法能够当与Socket断开时进行重连，重连次数由maxRetry决定，时间间隔为1秒。如果为0则表示立即终止不重连，如果为负数则表示一直重试。</li></ul> 
<h4>基于文件的预定义Source</h4> 
<p>基于文件创建DataStream主要有两种方式：readTextFile和readFile。(readFileStream已废弃)。readTextFile就是简单读取文件，而readFile的使用方式比较灵活。</p> 
<p>readTextFile</p> 
<p>readTextFile提供了两个重载方法：</p> 
<ul><li>readTextFile(String filePath)：逐行读取指定文件来创建DataStream，使用系统默认字符编码读取。</li><li>readTextFile(String filePath,String charsetName)：逐行读取文件来创建DataStream，使用charsetName编码读取。</li></ul> 
<p>readFile</p> 
<p>readFile通过指定的FileInputFormat来读取用户指定路径的文件。对于指定路径文件，我们可以使用不同的处理模式来处理，<code>FileProcessingMode.PROCESS_ONCE</code>模式只会处理文件数据一次，而<code>FileProcessingMode.PROCESS_CONTINUOUSLY</code>会监控数据源文件是否有新数据，如果有新数据则会继续处理。</p> 
<pre class="has"><code>readFile(FileInputFormat&lt;T&gt; inputFormat,String filePath,FileProcessingMode watchType,long interval,TypeInformation typrInfo) 
</code></pre> 
<table><thead><tr><th>参数</th><th>说明</th><th>实例</th></tr></thead><tbody><tr><td>inputFormat</td><td>创建DataStream指定的输入格式</td></tr><tr><td>filePath</td><td>读取的文件路径，为URI格式。既可以读取普通文件，可以读取HDFS文件</td><td>file:///some/local/file 或hdfs://host:port/file/path</td></tr><tr><td>watchType</td><td>文件数据处理方式</td><td>FileProcessingMode.PROCESS_ONCE或FileProcessingMode.PROCESS_CONTINUOUSLY</td></tr><tr><td>interval</td><td>在周期性监控Source的模式下(PROCESS_CONTINUOUSLY)，指定每次扫描的时间间隔</td><td>10</td></tr><tr><td>typeInformation</td><td>返回数据流的类型</td><td> </td></tr></tbody></table> 
<p>readFile提供了几个便于使用的重载方法，但它们最终都是调用上面这个方法的。</p> 
<ul><li>readFile(FileInputFormat&lt;T&gt; inputFormat,String filePath)：处理方式默认使用FileProcessingMode.PROCESS_ONCE。</li><li>readFile(FileInputFormat&lt;T&gt; inputFormat,String filePath,FileProcessingMode watchType,long interval)：返回类型默认为inputFormat类型。</li></ul> 
<blockquote> 
 <p>需要注意：在使用FileProcessingMode.PROCESS_CONTINUOUSLY时，当修改读取文件时，Flink会将文件整体内容重新处理，也就是打破了"exactly-once"。</p> 
</blockquote> 
<h4>自定义Source</h4> 
<p>除了预定义的Source外，我们还可以通过实现<code>SourceFunction</code>来自定义Source，然后通过<code>StreamExecutionEnvironment.addSource(sourceFunction)</code>添加进来。比如读取Kafka数据的Source：</p> 
<pre class="has"><code>addSource(new FlinkKafkaConsumer08&lt;&gt;);
</code></pre> 
<p>我们可以实现以下三个接口来自定义Source：</p> 
<ul><li>SourceFunction：创建非并行数据源。</li><li>ParallelSourceFunction：创建并行数据源。</li><li>RichParallelSourceFunction：创建并行数据源。</li></ul> 
<h3>数据转换(Transformation)</h3> 
<p>数据处理的核心就是对数据进行各种转化操作，在Flink上就是通过转换将一个或多个DataStream转换成新的DataStream。<br> 为了更好的理解transformation函数，下面给出匿名类的方式来实现各个函数。<br> 所有转换函数都是依赖以下基础：</p> 
<pre class="has"><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
DataStream&lt;String&gt; dataStream = env.readTextFile("/opt/yjz/flink/flink-1.7.2/README.txt");

</code></pre> 
<h4>基础转换操作</h4> 
<p>Map</p> 
<p>接受一个元素，输出一个元素。MapFunction&lt;T,V&gt;中T代表输入数据类型(map方法的参数类型)，V代表操作结果输出类型(map方法返回数据类型)。</p> 
<pre class="has"><code>dataStream.map(new MapFunction&lt;String, String&gt;() {
    @Override
    public String map(String line) throws Exception {
        return line.toUpperCase();
    }
});
</code></pre> 
<p>flatMap</p> 
<p>输入一个元素，输出0个、1个或多个元素。FlatMapFunction&lt;T,V&gt;中T代表输入元素数据类型(flatMap方法的第一个参数类型)，V代表输出集合中元素类型(flatMap中的Collector类型参数)</p> 
<pre class="has"><code>dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() {
    @Override
    public void flatMap(String line, Collector&lt;String&gt; collector) throws Exception {
        for(String word : line.split(" "))
        collector.collect(word);
    }
});
</code></pre> 
<p>转换前后数据类型：DataStream-&gt;DataStream。</p> 
<p>filter</p> 
<p>过滤指定元素数据，如果返回true则该元素继续向下传递，如果为false则将该元素过滤掉。FilterFunction&lt;T&gt;中T代表输入元素的数据类型。</p> 
<pre class="has"><code>dataStream.filter(new FilterFunction&lt;String&gt;() {
    @Override
    public boolean filter(String line) throws Exception {
        if(line.contains("flink"))
            return true;
        else
            return false;
    }
});
</code></pre> 
<p>转换前后数据类型：DataStream-&gt;DataStream。</p> 
<p>keyBy</p> 
<p>逻辑上将数据流元素进行分区，具有相同key的记录将被划分到同一分区。指定Key的方式有多种，这个我们在之前说过了。返回类型KeyedStream&lt;T,KEY&gt;中T代表KeyedStream中元素数据类型，KEY代表虚拟KEY的数据类型。</p> 
<pre class="has"><code>KeyedStream&lt;String,Tuple&gt; keyedStream = dataStream.keyBy(0);
</code></pre> 
<blockquote> 
 <p>以下情况的元素不能作为key使用：</p> 
 <ol><li>POJO类型，但没有重写hashCode()，而是依赖Object.hashCode()。</li><li>该元素是数组类型。</li></ol> 
</blockquote> 
<p>keyBy内部使用散列来实现的。<br> 转换前后数据类型：DataStream-&gt;KeyedStream。</p> 
<p>reduce</p> 
<p>对指定的“虚拟”key相同的记录进行滚动合并，也就是当前元素与最后一次的reduce结果进行reduce操作。ReduceFunction&lt;T&gt;中的T代表KeyStream中元素的数据类型。</p> 
<pre class="has"><code>keyedStream.reduce(new ReduceFunction&lt;String&gt;() {
    @Override
    public String reduce(String value1, String value2) throws Exception {
        return value1 + value2;
    }
});
</code></pre> 
<p>转换前后数据类型：KeyedStream-&gt;DataStream。</p> 
<p>Fold(Deprecated)</p> 
<p>Fold功能和Reduce类似，但是Fold提供了初始值，从初始值开始滚动对相同的key记录进行滚动合并。FoldFunction&lt;T,V&gt;中的T为KeyStream中元素数据类型，V为初始值类型和fold方法返回值类型。</p> 
<pre class="has"><code>keyedStream.fold(0, new FoldFunction&lt;String, Integer&gt;() {
    @Override
    public Integer fold(Integer accumulator, String value) throws Exception {
        return accumulator + value.split(" ").length;
    }
});
</code></pre> 
<p><em>该方法已经标记为废弃！</em></p> 
<p>转换前后数据类型：KeyedStream-&gt;DataStream。</p> 
<p>Aggregations</p> 
<p>滚动聚合具有相同key的数据流元素，我们可以指定需要聚合的字段(field)。DataStream&lt;T&gt;中的T为聚合之后的结果。</p> 
<pre class="has"><code>//对KeyedStream中元素的第一个Filed求和
DataStream&lt;String&gt; dataStream1 = keyedStream.sum(0);
//对KeyedStream中元素的“count”字段求和
keyedStream.sum("count");
//获取keyedStream中第一个字段的最小值
keyedStream.min(0);
//获取keyedStream中count字段的最小值的元素
keyedStream.minBy("count");
keyedStream.max("count");
keyedStream.maxBy(0);
</code></pre> 
<p>min和minBy的区别是：min返回指定字段的最小值，而minBy返回最小值所在的元素。</p> 
<p>转换前后数据类型：KeyedStream-&gt;DataStream。</p> 
<p>window</p> 
<p>对已经分区的KeyedStream上定义窗口，Window会根据某些规则(比如在最后5s到达的数据)对虚拟“key”相同的记录进行分组。WindowedStream&lt;T, K, W extends Window&gt;中的T为KeyedStream中元素数据类型，K为指定Key的数据类型，W为我们所使用的窗口类型</p> 
<pre class="has"><code>WindowedStream&lt;String,Tuple,TimeWindow&gt; windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(5)));
</code></pre> 
<p>转换前后的数据类型：KeyedStream-&gt;WindowedStream</p> 
<blockquote> 
 <p>关于Window之后会拿出来专门一篇文章来说。</p> 
</blockquote> 
<p>windowAll</p> 
<p>我们也可以在常规DataStream上使用窗口，Window根据某些条件(比如最后5s到达的数据)对所有流事件进行分组。AllWindowedStream&lt;T,W extends Window&gt;中的T为DataStream中元素的数据类型，W为我们所使用的窗口类型。</p> 
<pre class="has"><code>AllWindowedStream&lt;String,TimeWindow&gt; allWindowedStream = dataStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(5)));
</code></pre> 
<p><em>注意：该方法在许多时候并不是并行执行的，所有记录都会收集到一个task中</em></p> 
<p>转换前后的数据类型：DataStream-&gt;AllWindowedStream</p> 
<p>Window Apply</p> 
<p>将整个窗口应用在指定函数上，可以对WindowedStream和AllWindowedStream使用。WindowFunction&lt;IN, OUT, KEY, W extends Window&gt;中的IN为输入元素类型，OUT为输出类型元素，KEY为指定的key类型，W为所使用的窗口类型。</p> 
<pre class="has"><code>windowedStream.apply(new WindowFunction&lt;String, String, Tuple, TimeWindow&gt;() {
    @Override
    public void apply(Tuple tuple, TimeWindow window, Iterable&lt;String&gt; input, Collector&lt;String&gt; out) throws Exception {
        int sumCount = 0;
        for(String line : input){
            sumCount += line.split(" ").length;
        }
        out.collect(String.valueOf(sumCount));
    }
});
</code></pre> 
<p>AllWindowedStream使用与WindowedStream类似。</p> 
<p>转换前后的数据类型：WindowedStream-&gt;DataStream或AllWindowedStream-&gt;DataStream</p> 
<p>Window Reduce/Fold/Aggregation/</p> 
<p>对于WindowedStream数据流我们同样也可以应用Reduce、Fold、Aggregation函数。</p> 
<pre class="has"><code>windowedStream.reduce(new ReduceFunction&lt;String&gt;() {
    @Override
    public String reduce(String value1, String value2) throws Exception {
        return value1 + value2;
    }
});
windowedStream.fold(0, new FoldFunction&lt;String, Integer&gt;() {
    @Override
    public Integer fold(Integer accumulator, String value) throws Exception {
        return accumulator + value.split(" ").length;
    }
});
windowedStream.sum(0);
windowedStream.max("count");
</code></pre> 
<p>转换前后的数据类型：WindowedStream-&gt;DataStream</p> 
<p>Union</p> 
<p>联合(Union)两个或多个DataStream，所有DataStream中的元素都会组合成一个新的DataStream。如果联合自身，则每个元素出现两次在新的DataStream。</p> 
<pre class="has"><code>dataStream.union(dataStream1);
</code></pre> 
<p>转换前后的数据类型：DataStream*-&gt;DataStream</p> 
<p>Window Join</p> 
<p>在指定key的公共窗口上连接两个数据流。JoinFunction&lt;IN1,IN2,OUT&gt;中的IN1为第一个DataStream中元素数据类型，IN2为第二个DataStream中元素数据类型，OUT为Join结果数据类型。</p> 
<pre class="has"><code>dataStream
          .join(dataStream1)
          .where(new MyKeySelector()).equalTo(new MyKeySelector())
          .window(TumblingEventTimeWindows.of(Time.seconds(5)))
          .apply(new JoinFunction&lt;String, String, String&gt;() {
              @Override
              public String join(String first, String second) throws Exception {
                  return first + second;
              }
          });
</code></pre> 
<p>转换前后的数据类型：DataStream,DataStream-&gt;DataStream</p> 
<p>Interval Join</p> 
<p>对指定的时间间隔内使用公共key来连接两个KeyedStream。ProcessJoinFunction&lt;IN1,IN2,OUT&gt;中IN1为第一个DataStream中元素数据类型，IN2为第二个DataStream中的元素数据类型，OUT为结果输出类型。</p> 
<pre class="has"><code> keyedStream
            .intervalJoin(keyedStream)
            .between(Time.milliseconds(-2),Time.milliseconds(2))//间隔时间
            .lowerBoundExclusive()//并不包含下限时间
            .upperBoundExclusive()
            .process(new ProcessJoinFunction&lt;String, String, String&gt;() {
                @Override
                public void processElement(String left, String right, Context ctx, Collector&lt;String&gt; out) throws Exception {
                    //...
                }
            });
</code></pre> 
<p>Window CoGroup</p> 
<p>对两个指定的key的DataStream在公共窗口上执行CoGroup，和Join功能类似，但是更加灵活。CoGroupFunction&lt;IN1,IN2,OUT&gt;，IN1代表第一个DataStream中元素类型，IN2代表第二个DataStream中元素类型，OUT为结果输出集合类型。</p> 
<pre class="has"><code>dataStream
          .coGroup(dataStream1)
          .where(new MyKeySelector()).equalTo(new MyKeySelector())
          .window(TumblingEventTimeWindows.of(Time.seconds(5)))
          .apply(new CoGroupFunction&lt;String, String, String&gt;() {
              @Override
              public void coGroup(Iterable&lt;String&gt; first, Iterable&lt;String&gt; second, Collector&lt;String&gt; out) throws Exception {

              }
          });
</code></pre> 
<p>转换前后的数据类型：DataStream,DataStream-&gt;DataStream</p> 
<p>Connect</p> 
<p>连接(connect)两个流，并且保留其类型。两个数据流之间可以共享状态。ConnectedStreams&lt;IN1,IN2&gt;中IN1代表第一个数据流中的数据类型，IN2代表第二个数据流中的数据类型。</p> 
<pre class="has"><code>ConnectedStreams&lt;String,String&gt; connectedStreams = dataStream.connect(dataStream);
</code></pre> 
<p>转换前后的数据类型：DataStream,DataStream-&gt;ConnectedDataStreams</p> 
<p>CoFlatMap/CoMap</p> 
<p>可以对连接流执行类似map和flatMap操作。</p> 
<pre class="has"><code>connectedStreams.map(new CoMapFunction&lt;String, String, String&gt;() {
            @Override
            public String map1(String value) throws Exception {
                return value.toUpperCase();
            }
            @Override
            public String map2(String value) throws Exception {
                return value.toLowerCase();
            }
        });
</code></pre> 
<p>转换前后的数据类型：ConnectedDataStreams-&gt;DataStream</p> 
<p>Split(Deprecated)</p> 
<p>我们可以根据某些规则将数据流切分成两个或多个数据流。</p> 
<pre class="has"><code>dataStream.split(new OutputSelector&lt;String&gt;() {
            @Override
            public Iterable&lt;String&gt; select(String value) {
                List&lt;String&gt; outList = new ArrayList&lt;&gt;();
                if(value.contains("flink"))
                    outList.add("flink");
                else 
                    outList.add("other");
                return outList;
            }
        });
</code></pre> 
<p><em>该方法底层引用以被标记为废弃！</em></p> 
<p>转换前后的数据类型：DataStream-&gt;SplitStream</p> 
<p>Select</p> 
<p>我们可以对SplitStream分开的流进行选择，可以将其转换成一个或多个DataStream。</p> 
<pre class="has"><code>splitStream.select("flink");
splitStream.select("other");
</code></pre> 
<p>Extract Timestamps(Deprecated)</p> 
<p>从记录中提取时间戳，以便使用事件时间语义窗口。之后会专门来看Flink的Event Time。</p> 
<pre class="has"><code>dataStream.assignTimestamps(new TimestampExtractor&lt;String&gt;() {
            @Override
            public long extractTimestamp(String element, long currentTimestamp) {
                return 0;
            }
            @Override
            public long extractWatermark(String element, long currentTimestamp) {
                return 0;
            }
            @Override
            public long getCurrentWatermark() {
                return 0;
            }
        });
</code></pre> 
<p><em>该方法以被标记为废弃！</em></p> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>Iterate</p> 
<p>可以使用iterate方法来获取IterativeStream。</p> 
<pre class="has"><code>IterativeStream&lt;String&gt; iterativeStream = dataStream.iterate();
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;IterativeStream</p> 
<p>Project</p> 
<p>对元组类型的DataStream可以使用Project选取子元组。</p> 
<pre class="has"><code>DataStream&lt;Tuple2&lt;String,Integer&gt;&gt; dataStream2 = dataStream.project(0,2);
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>自定义分区(partitionCustom)</p> 
<p>使用用户自定义的分区函数对指定key进行分区，partitionCustom只支持单分区。</p> 
<pre class="has"><code>dataStream.partitionCustom(new Partitioner&lt;String&gt;() {
            
            @Override
            public int partition(String key, int numPartitions) {
                return key.hashCode() % numPartitions;
            }
        },1);
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>随机分区(shuffle)</p> 
<p>均匀随机将元素进行分区。</p> 
<pre class="has"><code>dataStream.shuffle();
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>rebalance</p> 
<p>以轮询的方式为每个分区均衡分配元素，对于优化数据倾斜该方法非常有效。</p> 
<pre class="has"><code>dataStream.rebalance();
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>broadcast</p> 
<p>使用broadcast可以向每个分区广播元素。</p> 
<pre class="has"><code>dataStream.broadcast();
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<p>rescale</p> 
<p>根据上下游task数进行分区。</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/12/51/FmjqWZn6_o.png"></p> 
<p>rescale</p> 
<pre class="has"><code>dataStream.rescale();
</code></pre> 
<p>转换前后的数据类型：DataStream-&gt;DataStream</p> 
<h3>结果数据接收器(Data sink)</h3> 
<p>数据经过Flink处理之后，最终结果会写到file、socket、外部系统或者直接打印出来。数据接收器定义在DataStream类下，我们通过addSink()可以来添加一个接收器。同Source，Flink也提供了一些预定义的Data Sink让我们直接使用。</p> 
<h4>写入文本文件</h4> 
<p>DataStream提供了两个writeAsText重载方法，写入格式会调用写入对象的toString()方法。</p> 
<ul><li>writeAsText(String path)：将DataStream数据写入到指定文件。</li><li>writeAsText(String path,WriteMode writeMode)：将DataStream数据写入到指定文件，可以通过writeMode来指定如果文件已经存在应该采用什么方式，可以指定OVERWRITE或NO_OVERWRITE。</li></ul> 
<h4>写入CSV文件</h4> 
<p>DataStream提供了三个写入csv文件的重载方法，对于DataStream中的每个Filed，都会调用其对象的toString()方法作为写入格式。writeAsCsv只能用于元组(Tuple)的DataStream。</p> 
<pre class="has"><code>writeAsCsv(String path,WriteMode writeMode,String rowDelimiter,String fieldDelimiter)
</code></pre> 
<table><thead><tr><th>参数</th><th>说明</th><th>实例</th></tr></thead><tbody><tr><td>path</td><td>写入文件路径</td></tr><tr><td>writeMode</td><td>如果写入文件已经存在，采用什么方式处理</td><td>WriteMode.NO_OVERWRITE 或WriteMode.OVERWRITE</td></tr><tr><td>rowDelimiter</td><td>定义行分隔符</td></tr><tr><td>fieldDelimiter</td><td>定义列分隔符</td><td> </td></tr></tbody></table> 
<p>DataStream提供了两个简易重载方法：</p> 
<ul><li>writeAsCsv(String path)：使用"\n"作为行分隔符，使用","作为列分隔符。</li><li>writeAsCsv(String path,WriteMode writeMode)：使用"\n"作为行分隔符，使用","作为列分隔符。</li></ul> 
<h4>写入Socket</h4> 
<p>Flink提供了将DataStream作为字节数组写入Socket的方法，通过<code>SerializationSchema</code>来指定输出格式。</p> 
<pre class="has"><code>writeToSocket(String hostName,int port,SerializationSchema&lt;T&gt; schema)
</code></pre> 
<h4>指定输出格式</h4> 
<p>DataStream提供了自定义文件输出的类和方法，我们能够自定义对象到字节的转换。</p> 
<pre class="has"><code>writeUsingOutputFormat(OutputFormat&lt;T&gt; format)
</code></pre> 
<h4>结果打印</h4> 
<p>DataStream提供了print和printToErr打印标准输出/标准错误流。DataStream中的每个元素都会调用其toString()方法作为输出格式，我们也可以指定一个前缀字符来区分不同的输出。</p> 
<ul><li>print()：标准输出</li><li>print(String sinkIdentifier)：指定输出前缀</li><li>printToErr()：标准错误输出</li><li>printToErr(String sinkIdentifier)：指定输出前缀</li></ul> 
<p>对于并行度大于1的输出，输出结果也将输出任务的标识符作为前缀。</p> 
<h4>自定义输出器</h4> 
<p>我们一般会自定义输出器，通过实现<code>SinkFunction</code>接口，然后通过<code>DataStream.addSink(sinkFunction)</code>来指定数据接收器。</p> 
<pre class="has"><code>addSink(SinkFunction&lt;T&gt; sinkFunction)
</code></pre> 
<p><em>注意：对于DataStream中的writeXxx()方法一般都是用于测试使用，因为他们并没有参与chaeckpoint，所以它们只有"at-last-once"也就是至少处理一次语义。</em><br> 如果想要可靠输出，想要使用"exactly-once"语义准确将结果写入到文件系统中，我们需要使用<code>flink-connector-filesystem</code>。此外，我们也可以通过addSink()自定义输出器来使用Flink的checkpoint来完成"exactl-oncey"语义。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/50b432a0a5018d79da7bba0b4b77dfcd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">双端队列（数据结构作业第七周的一道题目）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e79eb619011c017e7f1e22813d21948c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SVN访问You don&#39;t have permission to access this resource  403错误</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>