<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pix2pix解析以及pytorch实现 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pix2pix解析以及pytorch实现" />
<meta property="og:description" content="原始论文：https://arxiv.org/pdf/1611.07004.pdf
原始pytorch版本的代码：GitHub - junyanz/pytorch-CycleGAN-and-pix2pix: Image-to-Image Translation in PyTorch
我的复现代码：
pix2pix就是用cGAN(Conditional GAN)实现成对的图像转换。例如下图
具体方法和网络结构 pix2pix与传统的GAN方法有一定区别（读者最好先了解一下传统GAN），它运用了cGAN的方法:
在生成器(G)上，通过随机噪声z和真实图像x生成图像y, 在判别器上与传统的GAN一样；
在loss上，与传统的GAN_Loss不同，pix2pix的loss除了加上了conditional特性，还增加了L1_Loss让目标域的图像更加接近源域的图像：
传统的GAN_Loss： cGAN_Loss： x是指原始图像(上图的左边)，y是指目标真实图像(上图的右边)，G(x,z)是指根据噪声和原始图像生成的图像(fake)；D(x,y)代表通过判别器得到的真实图片是否真实的概率，同理D(x,G(x,z))代表生成的图像是否真实的概率；判别器的最终目的是D(x,y)尽可能的接近于1，判别器的能力越强D(x,y)越大，D(x,G(x,z))越小，1-D(x,G(x,z))越大，因此 越大，这也解释了为什么要 ；生成器的最终目的是使G(x,z)越像真实图片，即D(x,G(x,z))越大，此时1-D(x,G(x,z))越小，同理这也说明了为什么要 再加上L1约束，最终的需要优化的目标函数如下：
网络结构 pix2pix在网络结构上的亮点主要是在生成器上运用了U-net的网络结构，同时加入跳链接(skip connections)，第i层与第n-i层直接连接，具体细节可以参考U-net原论文。除此之外，pix2pix还运用了dropout操作。
PatchGAN L1_Loss有利于对于低频信息建模，pix2pix使用L1_Loss作为辅助loss加入总loss中会使生成的图像更加清晰，论文中作者也进行了对比实验：
对于高频信息，作者提出了一种PatchGAN对高频信息进行建模，就是针对同一个图像中不同N*N大小的区域进行真伪鉴别，最后将他们平均值作为输出。N*N的区域可以比整个图像小的多，但是任然能得到高质量的结果。使用PatchGAN能减少参数量，加快训练速度，同时能应用于任意大尺度的图像。经过作者的探索，当N=70时，得到的结果最好。
上述就是pix2pix这篇论文的核心方法以及一些setting，论文中有更加详细的网络结构和PatchGAN的分析，有兴趣可以去阅读原论文。最后附上一些我自己复现的代码训出来的一些例子(代码我就先不放了，后面还会讲一下cycleGAN，到时一起放出来)，部分还没训好(太费时了,懒得训了)，从左到右：input，label，gen_image" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/16b387f63fb659bd0143f069d3baf2eb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-05T10:31:35+08:00" />
<meta property="article:modified_time" content="2021-11-05T10:31:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pix2pix解析以及pytorch实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>原始论文：<a href="https://arxiv.org/pdf/1611.07004.pdf" rel="nofollow" title="https://arxiv.org/pdf/1611.07004.pdf">https://arxiv.org/pdf/1611.07004.pdf</a></p> 
<p>原始pytorch版本的代码：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" title="GitHub - junyanz/pytorch-CycleGAN-and-pix2pix: Image-to-Image Translation in PyTorch">GitHub - junyanz/pytorch-CycleGAN-and-pix2pix: Image-to-Image Translation in PyTorch</a></p> 
<p>我的复现代码：</p> 
<p>pix2pix就是用cGAN(Conditional GAN)实现成对的图像转换。例如下图</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/5d/1e/1N4df6cB_o.png"></p> 
<h2>具体方法和网络结构</h2> 
<p>pix2pix与传统的GAN方法有一定区别（读者最好先了解一下传统GAN），它运用了cGAN的方法:</p> 
<p>在生成器(G)上，通过随机噪声z和真实图像x生成图像y, <img alt="G: \left \{ x, z \right \} \rightarrow y" class="mathcode" src="https://images2.imgbox.com/b9/c8/sdu0l4h4_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/bc/2a/thwUyhi2_o.png"></p> 
<p>在判别器上与传统的GAN一样；</p> 
<p>在loss上，与传统的GAN_Loss不同，pix2pix的loss除了加上了conditional特性，还增加了L1_Loss让目标域的图像更加接近源域的图像：</p> 
<p>传统的GAN_Loss：<img alt="L_{GAN}(G, D) = \mathbb{E}_{y}[logD(y)] + \mathbb{E}_{x, z}[log(1-D(G(z)))]" class="mathcode" src="https://images2.imgbox.com/2f/a5/VPNKi9Hc_o.png"> </p> 
<p>cGAN_Loss： <img alt="L_{cGAN}(G, D) = \mathbb{E}_{x,y}[logD(x,y)] + \mathbb{E}_{x, z}[log(1-D(x,G(x,z)))]" class="mathcode" src="https://images2.imgbox.com/f0/af/00eeHqu2_o.png"></p> 
<p style="text-align:center;"><img alt="G = arg\underset{G}{min}\underset{D}{max}L_{cGAN}(G, D)" class="mathcode" src="https://images2.imgbox.com/45/15/0iPItUIh_o.png"></p> 
<p style="text-align:center;"><img alt="" height="227" src="https://images2.imgbox.com/10/29/mQsGvYia_o.jpg" width="454"> </p> 
<p> </p> 
<ol><li>x是指原始图像(上图的左边)，y是指目标真实图像(上图的右边)，G(x,z)是指根据噪声和原始图像生成的图像(fake)；</li><li>D(x,y)代表通过判别器得到的真实图片是否真实的概率，同理D(x,G(x,z))代表生成的图像是否真实的概率；</li><li>判别器的最终目的是D(x,y)尽可能的接近于1，判别器的能力越强D(x,y)越大，D(x,G(x,z))越小，1-D(x,G(x,z))越大，因此 <img alt="L_{cGAN}(G, D)" class="mathcode" src="https://images2.imgbox.com/5f/53/JwcbTBhs_o.png"> 越大，这也解释了为什么要  <img alt="\underset{D}{max}" class="mathcode" src="https://images2.imgbox.com/17/83/m5VWkBmp_o.png">；生成器的最终目的是使G(x,z)越像真实图片，即D(x,G(x,z))越大，此时1-D(x,G(x,z))越小，同理这也说明了为什么要  <img alt="\underset{G}{min}" class="mathcode" src="https://images2.imgbox.com/be/10/i4atB3L9_o.png"></li></ol> 
<p> 再加上L1约束，最终的需要优化的目标函数如下：</p> 
<p style="text-align:center;"><img alt="G = arg\underset{G}{min}\underset{D}{max}L_{cGAN}(G, D) + \lambda L_{L1}(G)" class="mathcode" src="https://images2.imgbox.com/97/ff/3mkMgTOm_o.png"></p> 
<h4>网络结构 </h4> 
<p>pix2pix在网络结构上的亮点主要是在生成器上运用了U-net的网络结构，同时加入跳链接(skip connections)，第i层与第n-i层直接连接，具体细节可以参考U-net原论文。除此之外，pix2pix还运用了dropout操作。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3e/7c/dhbwiMNz_o.png"></p> 
<p> </p> 
<h4>PatchGAN</h4> 
<p>L1_Loss有利于对于低频信息建模，pix2pix使用L1_Loss作为辅助loss加入总loss中会使生成的图像更加清晰，论文中作者也进行了对比实验：</p> 
<p style="text-align:center;"><img alt="" height="368" src="https://images2.imgbox.com/53/8a/SzGlvT5x_o.png" width="588"></p> 
<p> 对于高频信息，作者提出了一种PatchGAN对高频信息进行建模，就是针对同一个图像中不同N*N大小的区域进行真伪鉴别，最后将他们平均值作为输出。N*N的区域可以比整个图像小的多，但是任然能得到高质量的结果。使用PatchGAN能减少参数量，加快训练速度，同时能应用于任意大尺度的图像。经过作者的探索，当N=70时，得到的结果最好。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/46/4f/RWa7V03j_o.png"></p> 
<p> 上述就是pix2pix这篇论文的核心方法以及一些setting，论文中有更加详细的网络结构和PatchGAN的分析，有兴趣可以去阅读原论文。最后附上一些我自己复现的代码训出来的一些例子(代码我就先不放了，后面还会讲一下cycleGAN，到时一起放出来)，部分还没训好(太费时了,懒得训了)，从左到右：input，label，gen_image</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/85/65/s9Wt9jxu_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f7/f7/60DROeOH_o.png"> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/08dd0a54b7436f20ef282e97b0d09eb9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于stm32f103的LM6029-12864液晶屏开发（代码资源地址在最后）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3032ff948b5c3737e71f93126ab8a698/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">最小生成树--MST性质</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>