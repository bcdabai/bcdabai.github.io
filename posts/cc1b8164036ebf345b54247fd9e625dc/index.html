<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习入门——基于TensorFlow的鸢尾花分类实现（TensorFlow_GPU版本安装、实现） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习入门——基于TensorFlow的鸢尾花分类实现（TensorFlow_GPU版本安装、实现）" />
<meta property="og:description" content="基于TensorFlow的鸢尾花分类实现 0 引言1 基本介绍和环境搭建1.1关于TensorFlow-GPU环境搭建第一步:安装Anaconda：第二步：安装GPU版本需要，明确显卡型号第三步：打开conda终端建立Tensorflow环境第四步：激活虚拟环境：第五步：安装ensorflow gpu版本：第六步：安装keras：pip install keras -i 软件源第七步：进入IDE（Pycharm或者VScode）切换环境为tensorflow-gpu调试 1.2关于鸢尾花数据集介绍 2 实现过程及分析2.1 数据集可视化化以及PCA降维：2.2 实验结果如下：2.3 分析结论 附录（完整Python代码） Author（作者）： Nirvana Of Phoenixl
Proverbs for you（送给你的哦）：There is no doubt that good things will always come, and when it comes late, it can be a surprise.
文章可以作为深度学习或者TensorFlow入门的了解学习。使用PyChram和Python实现，安装过程中最容易出现的问题是GPU版本的与显卡的问题。如果需要对应版本的TensorFlow，可以私信一下，呜呜我懒的放上来，可以发一份给你们！
0 引言 本文主要是基于TensorFlow和Keras框架实现的鸢尾花分类，主要包含关于深度学习TensorFlow-GPU环境的搭建，以及实现框架的实现，其实验目的是实现鸢尾花分类，本质是通过简单的实践理解深度学习基本流程，加深对于代码实现的理解，通过对框架中的参数修改和完善理解调参对于框架识别精度的影响。最终目标是熟悉包括软件安装在内的深度学习环境的搭建、框架的构建、参数的调整做一个系统的学习和理解。
1 基本介绍和环境搭建 1.1关于TensorFlow-GPU环境搭建 深度学习的核心概念就是以张量（矩阵）运算模拟神经网络的。TensorFlow主要的设计就是让矩阵运算达到最高性能，并且能够在各种不同的平台下运行。TensorFlow最初由谷歌开发，深度学习的发展是由前景的，谷歌希望建立一个开源的社区，强大TensorFlow使其更加完善，最后开源。TensorFlow架构主要由处理器（cpu/gpu/tpu）、平台（win/linux/android/ios/raspi）、tensoflow引擎、前端语言（python/c&#43;&#43;）、高级api（keras/TF-learn/TF-slim/TF-layer）组成。如下图所示，TensorFlow架构组成。
图1 TensorFlow架构组成
TensorFlow是比较低级深度学习API，所以在程序设计模型时必须自行设计：张量乘积、卷积等底层操作，好处是我们可以自行设计各种深度学习模型，但缺点是开发时需要编写更多的程序代码，并且需要花费很长的时间。所以网上的开发社区以TensorFlow为底层开发很多高级的深度学习API，例如Keras、TF-Learn等。这样可以使得开发者使用更简洁、更具可读性的程序代码就可以构建出各种复杂的深度学习模型。本文主要采用Keras，因为Keras功能最为完整。
下面介绍如何在Windows上安装TensorFlow-GPU版本，因为其计算能力更强。因为之前本人已经安装过了TensorFlow-GPU版本，并搭建了环境，参考代码实现了一些经典数据集的学习训练，比如Keras MINIST、Keras-CIFAR-10等。下面将讲解如何安装和踩坑出现的问题。首先明确，对于GPU版本的TensorFlow主要通过NVIDIA提供的CUDA和cudnn存取GPU，CUDA是NVIDIA推出的整合技术，实质功能就是一种通过应用显卡处理数量较大的数据问题的架构，而cudnn是NVIDIA深度学习SDK的一部分，用于提供GPU深度学习库和加速深度学习的。
具体的安装步骤：
第一步:安装Anaconda： 指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。（按步骤安装即可）
图示上下分别为基本环境和新建的虚拟的GPU环境。
第二步：安装GPU版本需要，明确显卡型号 （目前绝大多数时NVIDIA的显卡），去官方的网址下载CUDA和cudnn（注意这里需要明确版本对应查找适合是显卡型号的CUDA然后根据CUDA确定cudnn）实际在我测试过程当中可以低版本安装，但是不能高于当前适用的版本安装。（实际显卡提示安装CUAD11.0，cudnn 8.0但是我安装的是CUDA10.0和cudnn7.4）
图示 为安装CUDA版本的版本，cudnn对应可以去配置文件里面查看。
第三步：打开conda终端建立Tensorflow环境 conda create –name tensorflow-gpu python=3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/cc1b8164036ebf345b54247fd9e625dc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-22T16:04:47+08:00" />
<meta property="article:modified_time" content="2022-11-22T16:04:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习入门——基于TensorFlow的鸢尾花分类实现（TensorFlow_GPU版本安装、实现）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>基于TensorFlow的鸢尾花分类实现</h4> 
 <ul><li><a href="#0___6" rel="nofollow">0 引言</a></li><li><a href="#1%09_8" rel="nofollow">1 基本介绍和环境搭建</a></li><li><ul><li><a href="#11TensorFlowGPU_9" rel="nofollow">1.1关于TensorFlow-GPU环境搭建</a></li><li><ul><li><a href="#Anaconda_19" rel="nofollow">第一步:安装Anaconda：</a></li><li><a href="#GPU_25" rel="nofollow">第二步：安装GPU版本需要，明确显卡型号</a></li><li><a href="#condaTensorflow_30" rel="nofollow">第三步：打开conda终端建立Tensorflow环境</a></li><li><a href="#_34" rel="nofollow">第四步：激活虚拟环境：</a></li><li><a href="#ensorflow_gpu_42" rel="nofollow">第五步：安装ensorflow gpu版本：</a></li><li><a href="#keraspip_install_keras_i__50" rel="nofollow">第六步：安装keras：pip install keras -i 软件源</a></li><li><a href="#IDEPycharmVScodetensorflowgpu_51" rel="nofollow">第七步：进入IDE（Pycharm或者VScode）切换环境为tensorflow-gpu调试</a></li></ul> 
   </li><li><a href="#12_56" rel="nofollow">1.2关于鸢尾花数据集介绍</a></li></ul> 
  </li><li><a href="#2%09_60" rel="nofollow">2 实现过程及分析</a></li><li><ul><li><a href="#21__PCA_62" rel="nofollow">2.1 数据集可视化化以及PCA降维：</a></li><li><a href="#22__65" rel="nofollow">2.2 实验结果如下：</a></li><li><a href="#23__86" rel="nofollow">2.3 分析结论</a></li></ul> 
  </li><li><a href="#Python_90" rel="nofollow">附录（完整Python代码）</a></li></ul> 
</div> 
<p></p> 
<blockquote> 
 <p><em>Author（作者）： Nirvana Of Phoenixl</em><br> <em>Proverbs for you（送给你的哦）：There is no doubt that good things will always come, and when it comes late, it can be a surprise.</em></p> 
</blockquote> 
<blockquote> 
 <p><font color="red"><em><strong>文章可以作为深度学习或者TensorFlow入门的了解学习。<mark>使用PyChram和Python实现</mark>，安装过程中最容易出现的问题是GPU版本的与显卡的问题。如果需要对应版本的TensorFlow，可以私信一下，呜呜我懒的放上来，可以发一份给你们！</strong></em></font></p> 
</blockquote> 
<h2><a id="0___6"></a>0 引言</h2> 
<p>  本文主要是基于TensorFlow和Keras框架实现的鸢尾花分类，主要包含关于深度学习TensorFlow-GPU环境的搭建，以及实现框架的实现，其实验目的是实现鸢尾花分类，本质是通过简单的实践理解深度学习基本流程，加深对于代码实现的理解，通过对框架中的参数修改和完善理解调参对于框架识别精度的影响。最终目标是熟悉包括软件安装在内的深度学习环境的搭建、框架的构建、参数的调整做一个系统的学习和理解。</p> 
<h2><a id="1%09_8"></a>1 基本介绍和环境搭建</h2> 
<h3><a id="11TensorFlowGPU_9"></a>1.1关于TensorFlow-GPU环境搭建</h3> 
<p>  深度学习的核心概念就是以张量（矩阵）运算模拟神经网络的。TensorFlow主要的设计就是让矩阵运算达到最高性能，并且能够在各种不同的平台下运行。TensorFlow最初由谷歌开发，深度学习的发展是由前景的，谷歌希望建立一个开源的社区，强大TensorFlow使其更加完善，最后开源。TensorFlow架构主要由处理器（cpu/gpu/tpu）、平台（win/linux/android/ios/raspi）、tensoflow引擎、前端语言（python/c++）、高级api（keras/TF-learn/TF-slim/TF-layer）组成。如下图所示，TensorFlow架构组成。<br> <img src="https://images2.imgbox.com/1c/f5/RF2sYQj1_o.png" alt="TensorFlow架构组成"><br>         图1 TensorFlow架构组成</p> 
<p>  TensorFlow是比较低级深度学习API，所以在程序设计模型时必须自行设计：张量乘积、卷积等底层操作，好处是我们可以自行设计各种深度学习模型，但缺点是开发时需要编写更多的程序代码，并且需要花费很长的时间。所以网上的开发社区以TensorFlow为底层开发很多高级的深度学习API，例如Keras、TF-Learn等。这样可以使得开发者使用更简洁、更具可读性的程序代码就可以构建出各种复杂的深度学习模型。本文主要采用Keras，因为Keras功能最为完整。</p> 
<p>  下面介绍如何在Windows上安装TensorFlow-GPU版本，因为其计算能力更强。因为之前本人已经安装过了TensorFlow-GPU版本，并搭建了环境，参考代码实现了一些经典数据集的学习训练，比如Keras MINIST、Keras-CIFAR-10等。下面将讲解如何安装和踩坑出现的问题。首先明确，对于GPU版本的TensorFlow主要通过NVIDIA提供的CUDA和cudnn存取GPU，CUDA是NVIDIA推出的整合技术，实质功能就是一种通过应用显卡处理数量较大的数据问题的架构，而cudnn是NVIDIA深度学习SDK的一部分，用于提供GPU深度学习库和加速深度学习的。</p> 
<p><mark><strong>具体的安装步骤</strong>：</mark></p> 
<h4><a id="Anaconda_19"></a>第一步:安装Anaconda：</h4> 
<p>  指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。（按步骤安装即可）</p> 
<p><img src="https://images2.imgbox.com/ae/84/XO2etIkD_o.png" alt="在这里插入图片描述"><br>         图示上下分别为基本环境和新建的虚拟的GPU环境。</p> 
<h4><a id="GPU_25"></a>第二步：安装GPU版本需要，明确显卡型号</h4> 
<p>  （目前绝大多数时NVIDIA的显卡），去官方的网址下载CUDA和cudnn（注意这里需要明确版本对应查找适合是显卡型号的CUDA然后根据CUDA确定cudnn）实际在我测试过程当中可以低版本安装，但是不能高于当前适用的版本安装。（实际显卡提示安装CUAD11.0，cudnn 8.0但是我安装的是CUDA10.0和cudnn7.4）<br> <img src="https://images2.imgbox.com/d8/89/zJnQ1V2N_o.png" alt="在这里插入图片描述"><br>       图示 为安装CUDA版本的版本，cudnn对应可以去配置文件里面查看。</p> 
<h4><a id="condaTensorflow_30"></a>第三步：打开conda终端建立Tensorflow环境</h4> 
<blockquote> 
 <p><code>conda create –name tensorflow-gpu python=3.7</code></p> 
</blockquote> 
<p>  建立名为tensorflow-gpu的python3.7的环境（或者在后面加上anaconda直接下载python所有包）如果需要单独安装缺少的包pip install 包名 -i 软件源</p> 
<h4><a id="_34"></a>第四步：激活虚拟环境：</h4> 
<blockquote> 
 <p><code>activate tensorflow-gpu</code><br> <img src="https://images2.imgbox.com/21/0b/iui1BIJK_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<p>  创建的虚拟环境会在anaconda安装路径下envs中出现新建的虚拟环境。</p> 
<blockquote> 
 <p><img src="https://images2.imgbox.com/1f/8b/yrajq8HX_o.png" alt="在这里插入图片描述"><br> 启用<code>python</code>环境导入tensorflow，显示安装成功。</p> 
</blockquote> 
<h4><a id="ensorflow_gpu_42"></a>第五步：安装ensorflow gpu版本：</h4> 
<pre><code class="prism language-bash">pip <span class="token function">install</span> tensorflow-gpu <span class="token parameter variable">-i</span> 软件源
</code></pre> 
<p>中科大软件源https://pypi.mirrors.ustc.edu.cn/simple/，在安装tensorflow gpu版本的时候可以限制具体版本号即<code>pip install tensorflow-gpu=1.14.0 -i</code> 软件源</p> 
<h4><a id="keraspip_install_keras_i__50"></a>第六步：安装keras：pip install keras -i 软件源</h4> 
<h4><a id="IDEPycharmVScodetensorflowgpu_51"></a>第七步：进入IDE（Pycharm或者VScode）切换环境为tensorflow-gpu调试</h4> 
<p><img src="https://images2.imgbox.com/1a/6a/hT3u4SBV_o.png" alt="在这里插入图片描述"><br> 图示为Tensorflow-gpu虚拟环境。</p> 
<h3><a id="12_56"></a>1.2关于鸢尾花数据集介绍</h3> 
<p>  Iris 鸢尾花数据集是一个经典数据集，在统计学习和机器学习领域都经常被用作示例。数据集内包含 3 类，共 150 条记录，每类各 50 个数据，每条记录都有 4 项特征：花萼长度Sepal.Length、花萼宽度Sepal.Width、花瓣长度Petal.Length、花瓣宽度Petal.Width，可以通过这4个特征预测鸢尾花卉属于狗尾草iris-setosa、杂色鸢尾花 iris-versicolour、弗吉尼亚鸢尾花 iris-virginica是属于什么类别，在数据集中也包括花萼花瓣长度对应的鸢尾花种类Species数据。<br> <img src="https://images2.imgbox.com/49/16/Kh71ljzo_o.png" alt="在这里插入图片描述"><br> 一般由两种格式.csv和.txt格式。</p> 
<h2><a id="2%09_60"></a>2 实现过程及分析</h2> 
<p>  本文主要通过Pycharm开发环境实现，基于tensorflow-gpu完成深度学习框架搭建。</p> 
<h3><a id="21__PCA_62"></a>2.1 数据集可视化化以及PCA降维：</h3> 
<p><img src="https://images2.imgbox.com/be/5c/glZmTJBt_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="22__65"></a>2.2 实验结果如下：</h3> 
<p>（1）学习率0.1，循环次数为500，当次数为185时，测试准确率刚好为1<br> Epoch 185, loss: 0.05128058139234781,Test_acc: 1.0<br> <img src="https://images2.imgbox.com/73/85/dC4LFRob_o.png" alt="在这里插入图片描述"></p> 
<p>（2）学习率为0.01，循环次数为500，当次数为500时，测试准确为0.66…<br> Epoch 499, loss: 0.08802829124033451,Test_acc: 0.6666666666666666<br> <img src="https://images2.imgbox.com/4a/6d/9gk21E6C_o.png" alt="在这里插入图片描述"></p> 
<p>（3）学习率为0.5，循环次数为500，当次数为500时，精度为1.0<br> Epoch 499, loss: 0.024999298620969057,Test_acc: 1.0<br> <img src="https://images2.imgbox.com/2d/81/blUHO8sp_o.png" alt="在这里插入图片描述"></p> 
<p>（4）学习率为0.1，循环次数为3000，当次数为185时，精度为1<br> Epoch 185, loss: 0.05128058139234781,Test_acc: 1.0<br> <img src="https://images2.imgbox.com/ea/57/BlkYDnSn_o.png" alt="在这里插入图片描述"><br> （5）学习率为0.1，循环次数186，当次数为185时，精度为1<br> Epoch 185, loss: 0.05128058139234781,Test_acc: 1.0<br> <img src="https://images2.imgbox.com/d0/08/3wuio4Ym_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="23__86"></a>2.3 分析结论</h3> 
<blockquote> 
 <p>实际上我们可以对比发现，（1）（2）每轮循环次数一样学习率不同导致的情况不同，当学习率过小的时候会影响模型的收敛时间，直观来看明显使得过程缓慢；（1）（3）可以得到学习率过大，会导致梯度会在最小值附近可能来回震荡，严重的可能导致无法收敛。从一定程度上来看，（4）（5）学习率一定循环次数在达到一个固定的值后，再多次循环不影响精度值。<br> 具体实现代码见附录。</p> 
</blockquote> 
<h2><a id="Python_90"></a>附录（完整Python代码）</h2> 
<pre><code class="prism language-bash">
```python
<span class="token comment">#导入所需模块：</span>
<span class="token function">import</span> tensorflow as tf                 <span class="token comment"># 导入模型框架</span>
from sklearn <span class="token function">import</span> datasets            <span class="token comment"># 从sklearn文件包中导入含有的数据集</span>
from matplotlib <span class="token function">import</span> pyplot as plt    <span class="token comment"># 从python中导入绘图库matplotlib</span>
<span class="token function">import</span> numpy as np                      <span class="token comment"># 导入开源计算包numpy</span>
from sklearn.decomposition <span class="token function">import</span> PCA
from mpl_toolkits.mplot3d <span class="token function">import</span> Axes3D

tf.enable_eager_execution<span class="token punctuation">(</span>
                <span class="token assign-left variable">config</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">device_policy</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">execution_mode</span><span class="token operator">=</span>None
            <span class="token punctuation">)</span>     <span class="token comment"># 在即将到来的TensorFlow2.0中将对部分机制做出重大调整，其中之一就是将原有的静态图机制调整为动态图机制，这将使得TensorFlow更加灵活和易用，2.0版本之前，可以通过 tf.enable_eager_execution() 方法来启用动态图机制</span>

<span class="token comment">#导入数据可视化数据集</span>
<span class="token comment">#————————————————————————————————</span>
iris <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> iris.data<span class="token punctuation">[</span>:, :2<span class="token punctuation">]</span>  <span class="token comment"># 列切片索引：只取前两个特征.</span>
y <span class="token operator">=</span> iris.target

x_min, x_max <span class="token operator">=</span> X<span class="token punctuation">[</span>:, <span class="token number">0</span><span class="token punctuation">]</span>.min<span class="token punctuation">(</span><span class="token punctuation">)</span> - .5, X<span class="token punctuation">[</span>:, <span class="token number">0</span><span class="token punctuation">]</span>.max<span class="token punctuation">(</span><span class="token punctuation">)</span> + .5
y_min, y_max <span class="token operator">=</span> X<span class="token punctuation">[</span>:, <span class="token number">1</span><span class="token punctuation">]</span>.min<span class="token punctuation">(</span><span class="token punctuation">)</span> - .5, X<span class="token punctuation">[</span>:, <span class="token number">1</span><span class="token punctuation">]</span>.max<span class="token punctuation">(</span><span class="token punctuation">)</span> + .5

plt.figure<span class="token punctuation">(</span><span class="token number">2</span>, <span class="token assign-left variable">figsize</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span>, <span class="token number">6</span><span class="token punctuation">))</span>
plt.clf<span class="token punctuation">(</span><span class="token punctuation">)</span>

 绘制训练数据点
plt.scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span>:, <span class="token number">0</span><span class="token punctuation">]</span>, X<span class="token punctuation">[</span>:, <span class="token number">1</span><span class="token punctuation">]</span>, <span class="token assign-left variable">c</span><span class="token operator">=</span>y, <span class="token assign-left variable">cmap</span><span class="token operator">=</span>plt.cm.Set1,
            <span class="token assign-left variable">edgecolor</span><span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">)</span>
plt.xlabel<span class="token punctuation">(</span><span class="token string">'Sepal length'</span><span class="token punctuation">)</span>
plt.ylabel<span class="token punctuation">(</span><span class="token string">'Sepal width'</span><span class="token punctuation">)</span>

plt.xlim<span class="token punctuation">(</span>x_min, x_max<span class="token punctuation">)</span>
plt.ylim<span class="token punctuation">(</span>y_min, y_max<span class="token punctuation">)</span>
plt.xticks<span class="token variable"><span class="token punctuation">((</span><span class="token punctuation">))</span>
plt.yticks<span class="token punctuation">((</span><span class="token punctuation">))</span></span>

 为了对数据的各个维度之间的相互作用有一个更好地理解，
<span class="token comment">#绘制前三个 PCA dimensions。</span>
fig <span class="token operator">=</span> plt.figure<span class="token punctuation">(</span><span class="token number">1</span>, <span class="token assign-left variable">figsize</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span>, <span class="token number">6</span><span class="token punctuation">))</span>
ax <span class="token operator">=</span> Axes3D<span class="token punctuation">(</span>fig, <span class="token assign-left variable">elev</span><span class="token operator">=</span>-150, <span class="token assign-left variable">azim</span><span class="token operator">=</span><span class="token number">110</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>.fit_transform<span class="token punctuation">(</span>iris.data<span class="token punctuation">)</span>
ax.scatter<span class="token punctuation">(</span>X_reduced<span class="token punctuation">[</span>:, <span class="token number">0</span><span class="token punctuation">]</span>, X_reduced<span class="token punctuation">[</span>:, <span class="token number">1</span><span class="token punctuation">]</span>, X_reduced<span class="token punctuation">[</span>:, <span class="token number">2</span><span class="token punctuation">]</span>, <span class="token assign-left variable">c</span><span class="token operator">=</span>y,
           <span class="token assign-left variable">cmap</span><span class="token operator">=</span>plt.cm.Set1, <span class="token assign-left variable">edgecolor</span><span class="token operator">=</span><span class="token string">'k'</span>, <span class="token assign-left variable">s</span><span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">)</span>
ax.set_title<span class="token punctuation">(</span><span class="token string">"First three PCA directions"</span><span class="token punctuation">)</span>
ax.set_xlabel<span class="token punctuation">(</span><span class="token string">"1st eigenvector"</span><span class="token punctuation">)</span>
ax.w_xaxis.set_ticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
ax.set_ylabel<span class="token punctuation">(</span><span class="token string">"2nd eigenvector"</span><span class="token punctuation">)</span>
ax.w_yaxis.set_ticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
ax.set_zlabel<span class="token punctuation">(</span><span class="token string">"3rd eigenvector"</span><span class="token punctuation">)</span>
ax.w_zaxis.set_ticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#————————————————————————————————————</span>
<span class="token comment">#框架模型搭建以及实现</span>
<span class="token comment">#导入数据：共计数据150组，分别为输入特征（x）和标签（y）</span>
x_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.data
y_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.target

 输入数据预处理：随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>  <span class="token comment"># 使用相同的seed，保证输入特征和标签一一对应（seed参数每一个数对应一个随机规则，数字一样随机规则一样产生的随机结果也一样）</span>
np.random.shuffle<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>   <span class="token comment"># 重新排序返回一个随机序列，在原数组上进行，改变自身序列，无返回值</span>
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np.random.shuffle<span class="token punctuation">(</span>y_data<span class="token punctuation">)</span>
tf.compat.v1.random.set_random_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>     <span class="token comment"># 版本原因导致的一些写法错误，tf.random.set_seed(116)修改内容</span>

<span class="token comment">#分割数据集：将打乱后的数据集（共有数据150行）分割为训练集和测试集，训练集为前120行，测试集为后30行</span>
x_train <span class="token operator">=</span> x_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>         <span class="token comment"># x_data[:-30]=x_data[0:120]</span>
y_train <span class="token operator">=</span> y_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>
x_test <span class="token operator">=</span> x_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>          <span class="token comment"># x_data[-30:]=x_data[120:150]</span>
y_test <span class="token operator">=</span> y_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>

<span class="token comment">#数据处理：转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span>
x_train <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_train, tf.float32<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_test, tf.float32<span class="token punctuation">)</span>

<span class="token comment">#数据切片: from_tensor_slices（（数据特征，标签）），函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span>
<span class="token comment">#batch（数据集大小/batch大小），两个数字可能不是整除，会导致一个batch大小可能小于等于batch size</span>
train_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
test_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>

<span class="token comment">#生成神经网络的参数，4个输入特征，因此输入层为4个输入节点；因为3分类，所以输出层为3个神经元</span>
<span class="token comment">#用tf.Variable()标记参数可训练</span>
 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）
w1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span>, <span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>
b1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>

lr <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率为0.1</span>
train_loss_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span>
test_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span>
epoch <span class="token operator">=</span> <span class="token number">186</span> <span class="token comment"># 循环500轮</span>
loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span>

<span class="token comment">#训练部分</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>:  <span class="token comment"># 数据集级别的循环，每个epoch循环一次数据集</span>
    <span class="token keyword">for</span> step, <span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_db<span class="token punctuation">)</span>:  <span class="token comment"># batch级别的循环 ，每个step循环一个batch</span>
        with tf.GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> as tape:  <span class="token comment"># with结构记录梯度信息</span>
            y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_train, w1<span class="token punctuation">)</span> + b1  <span class="token comment"># 神经网络乘加运算</span>
            y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span>
            y_ <span class="token operator">=</span> tf.one_hot<span class="token punctuation">(</span>y_train, <span class="token assign-left variable">depth</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span>
            loss <span class="token operator">=</span> tf.reduce_mean<span class="token punctuation">(</span>tf.square<span class="token punctuation">(</span>y_ - y<span class="token punctuation">))</span>  <span class="token comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span>
            loss_all <span class="token operator">+=</span> loss.numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span>

        <span class="token comment"># 计算loss对各个参数的梯度</span>
        grads <span class="token operator">=</span> tape.gradient<span class="token punctuation">(</span>loss, <span class="token punctuation">[</span>w1, b1<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span>
        w1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数w1自更新</span>
        b1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数b自更新</span>

    <span class="token comment"># 每个epoch，打印loss信息</span>
    print<span class="token punctuation">(</span><span class="token string">"Epoch {}, loss: {}"</span>.format<span class="token punctuation">(</span>epoch, loss_all/4<span class="token punctuation">))</span>
    train_loss_results.append<span class="token punctuation">(</span>loss_all / <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 将4个step的loss求平均记录在此变量中</span>
    loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># loss_all归零，为记录下一个epoch的loss做准备</span>

    <span class="token comment"># 测试部分</span>
    <span class="token comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span>
    total_correct, total_number <span class="token operator">=</span> <span class="token number">0</span>, <span class="token number">0</span>
    <span class="token keyword">for</span> x_test, y_test <span class="token keyword">in</span> test_db:
        <span class="token comment"># 使用更新后的参数进行预测</span>
        y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_test, w1<span class="token punctuation">)</span> + b1
        y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        pred <span class="token operator">=</span> tf.argmax<span class="token punctuation">(</span>y, <span class="token assign-left variable">axis</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 返回y中最大值的索引，即预测的分类</span>
        <span class="token comment"># 将pred转换为y_test的数据类型</span>
        pred <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>pred, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>y_test.dtype<span class="token punctuation">)</span>
        <span class="token comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span>
        correct <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>tf.equal<span class="token punctuation">(</span>pred, y_test<span class="token punctuation">)</span>, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>tf.int32<span class="token punctuation">)</span>
        <span class="token comment"># 将每个batch的correct数加起来</span>
        correct <span class="token operator">=</span> tf.reduce_sum<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># 将所有batch中的correct数加起来</span>
        total_correct <span class="token operator">+=</span> int<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span>
        total_number <span class="token operator">+=</span> x_test.shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token comment"># 总的准确率等于total_correct/total_number</span>

    <span class="token assign-left variable">total_number</span><span class="token operator">=</span> int<span class="token punctuation">(</span>total_number<span class="token punctuation">)</span>               <span class="token comment">#错误的原因在，一个是numpy对象，一个是Dimension对象，无法相除,                                               #用int函数，将Dimension对象对象转换为int</span>
    acc <span class="token operator">=</span> total_correct / total_number
    test_<span class="token comment"># 导入所需模块：</span>
<span class="token function">import</span> tensorflow as tf                 <span class="token comment"># 导入模型框架</span>
from sklearn <span class="token function">import</span> datasets            <span class="token comment"># 从sklearn文件包中导入含有的数据集</span>
from matplotlib <span class="token function">import</span> pyplot as plt    <span class="token comment"># 从python中导入绘图库matplotlib</span>
<span class="token function">import</span> numpy as np                      <span class="token comment"># 导入开源计算包numpy</span>

tf.enable_eager_execution<span class="token punctuation">(</span>
                <span class="token assign-left variable">config</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">device_policy</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">execution_mode</span><span class="token operator">=</span>None
            <span class="token punctuation">)</span>     <span class="token comment"># 在即将到来的TensorFlow2.0中将对部分机制做出重大调整，其中之一就是将原有的静态图机制调整为动态图机制，#</span>
                  <span class="token comment"># 这将使得TensorFlow更加灵活和易用，2.0版本之前，可以通过 tf.enable_eager_execution() 方法来启用动态图机制</span>

 导入数据：共计数据150组，分别为输入特征（x）和标签（y）
x_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.data
y_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.target

 输入数据预处理：随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>  <span class="token comment"># 使用相同的seed，保证输入特征和标签一一对应（seed参数每一个数对应一个随机规则，数字一样随机规则一样产生的随机结果也一样）</span>
np.random.shuffle<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>   <span class="token comment"># 重新排序返回一个随机序列，在原数组上进行，改变自身序列，无返回值</span>
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np.random.shuffle<span class="token punctuation">(</span>y_data<span class="token punctuation">)</span>
tf.compat.v1.random.set_random_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>     <span class="token comment"># 版本原因导致的一些写法错误，tf.random.set_seed(116)修改内容</span>

<span class="token comment">#分割数据集：将打乱后的数据集（共有数据150行）分割为训练集和测试集，训练集为前120行，测试集为后30行</span>
x_train <span class="token operator">=</span> x_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>         <span class="token comment"># x_data[:-30]=x_data[0:120]</span>
y_train <span class="token operator">=</span> y_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>
x_test <span class="token operator">=</span> x_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>          <span class="token comment"># x_data[-30:]=x_data[120:150]</span>
y_test <span class="token operator">=</span> y_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>

<span class="token comment">#数据处理：转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错</span>
x_train <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_train, tf.float32<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_test, tf.float32<span class="token punctuation">)</span>

 数据切片: from_tensor_slices（（数据特征，标签）），函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）
 batch（数据集大小/batch大小），两个数字可能不是整除，会导致一个batch大小可能小于等于batch size
train_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
test_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>

<span class="token comment">#生成神经网络的参数，4个输入特征，因此输入层为4个输入节点；因为3分类，所以输出层为3个神经元</span>
<span class="token comment">#用tf.Variable()标记参数可训练</span>
<span class="token comment">#使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span>
w1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span>, <span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>
b1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>

lr <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率为0.1</span>
train_loss_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span>
test_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span>
epoch <span class="token operator">=</span> <span class="token number">186</span> <span class="token comment"># 循环500轮</span>
loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span>

 训练部分

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>:  <span class="token comment"># 数据集级别的循环，每个epoch循环一次数据集</span>
    <span class="token keyword">for</span> step, <span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_db<span class="token punctuation">)</span>:  <span class="token comment"># batch级别的循环 ，每个step循环一个batch</span>
        with tf.GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> as tape:  <span class="token comment"># with结构记录梯度信息</span>
            y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_train, w1<span class="token punctuation">)</span> + b1  <span class="token comment"># 神经网络乘加运算</span>
            y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span>
            y_ <span class="token operator">=</span> tf.one_hot<span class="token punctuation">(</span>y_train, <span class="token assign-left variable">depth</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span>
            loss <span class="token operator">=</span> tf.reduce_mean<span class="token punctuation">(</span>tf.square<span class="token punctuation">(</span>y_ - y<span class="token punctuation">))</span>  <span class="token comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span>
            loss_all <span class="token operator">+=</span> loss.numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span>

        <span class="token comment"># 计算loss对各个参数的梯度</span>
        grads <span class="token operator">=</span> tape.gradient<span class="token punctuation">(</span>loss, <span class="token punctuation">[</span>w1, b1<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span>
        w1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数w1自更新</span>
        b1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数b自更新</span>

    <span class="token comment"># 每个epoch，打印loss信息</span>
    print<span class="token punctuation">(</span><span class="token string">"Epoch {}, loss: {}"</span>.format<span class="token punctuation">(</span>epoch, loss_all/4<span class="token punctuation">))</span>
    train_loss_results.append<span class="token punctuation">(</span>loss_all / <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 将4个step的loss求平均记录在此变量中</span>
    loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># loss_all归零，为记录下一个epoch的loss做准备</span>

    <span class="token comment"># 测试部分</span>
    <span class="token comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span>
    total_correct, total_number <span class="token operator">=</span> <span class="token number">0</span>, <span class="token number">0</span>
    <span class="token keyword">for</span> x_test, y_test <span class="token keyword">in</span> test_db:
        <span class="token comment"># 使用更新后的参数进行预测</span>
        y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_test, w1<span class="token punctuation">)</span> + b1
        y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        pred <span class="token operator">=</span> tf.argmax<span class="token punctuation">(</span>y, <span class="token assign-left variable">axis</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 返回y中最大值的索引，即预测的分类</span>
        <span class="token comment"># 将pred转换为y_test的数据类型</span>
        pred <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>pred, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>y_test.dtype<span class="token punctuation">)</span>
        <span class="token comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span>
        correct <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>tf.equal<span class="token punctuation">(</span>pred, y_test<span class="token punctuation">)</span>, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>tf.int32<span class="token punctuation">)</span>
        <span class="token comment"># 将每个batch的correct数加起来</span>
        correct <span class="token operator">=</span> tf.reduce_sum<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># 将所有batch中的correct数加起来</span>
        total_correct <span class="token operator">+=</span> int<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span>
        total_number <span class="token operator">+=</span> x_test.shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token comment"># 总的准确率等于total_correct/total_number</span>

    <span class="token assign-left variable">total_number</span><span class="token operator">=</span> int<span class="token punctuation">(</span>total_number<span class="token punctuation">)</span>               <span class="token comment">#错误的原因在，一个是numpy对象，一个是Dimension对象，无法相除,                                               #用int函数，将Dimension对象对象转换为int</span>
    acc <span class="token operator">=</span> total_correct / total_number
    test_acc.append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>
    print<span class="token punctuation">(</span><span class="token string">"Test_acc:"</span>, acc<span class="token punctuation">)</span>
    print<span class="token punctuation">(</span><span class="token string">"--------------------------"</span><span class="token punctuation">)</span>


<span class="token comment">#绘制 loss 曲线</span>
plt.title<span class="token punctuation">(</span><span class="token string">'Loss Function Curve'</span><span class="token punctuation">)</span>  <span class="token comment"># 图片标题</span>
plt.xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># x轴变量名称</span>
plt.ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>  <span class="token comment"># y轴变量名称</span>
plt.plot<span class="token punctuation">(</span>train_loss_results, <span class="token assign-left variable">label</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$Loss</span>$"</span><span class="token punctuation">)</span>  <span class="token comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span>
plt.legend<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 画出曲线图标</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 画出图像</span>

 绘制 Accuracy 曲线
plt.title<span class="token punctuation">(</span><span class="token string">'Acc Curve'</span><span class="token punctuation">)</span>  <span class="token comment"># 图片标题</span>
plt.xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># x轴变量名称</span>
plt.ylabel<span class="token punctuation">(</span><span class="token string">'Acc'</span><span class="token punctuation">)</span>  <span class="token comment"># y轴变量名称</span>
plt.plot<span class="token punctuation">(</span>test_acc, <span class="token assign-left variable">label</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$Accuracy</span>$"</span><span class="token punctuation">)</span>  <span class="token comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span>
plt.legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#导入所需模块：</span>
<span class="token function">import</span> tensorflow as tf                 <span class="token comment"># 导入模型框架</span>
from sklearn <span class="token function">import</span> datasets            <span class="token comment"># 从sklearn文件包中导入含有的数据集</span>
from matplotlib <span class="token function">import</span> pyplot as plt    <span class="token comment"># 从python中导入绘图库matplotlib</span>
<span class="token function">import</span> numpy as np                      <span class="token comment"># 导入开源计算包numpy</span>

tf.enable_eager_execution<span class="token punctuation">(</span>
                <span class="token assign-left variable">config</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">device_policy</span><span class="token operator">=</span>None,
                <span class="token assign-left variable">execution_mode</span><span class="token operator">=</span>None
            <span class="token punctuation">)</span>     <span class="token comment"># 在即将到来的TensorFlow2.0中将对部分机制做出重大调整，其中之一就是将原有的静态图机制调整为动态图机制，#</span>
                  <span class="token comment"># 这将使得TensorFlow更加灵活和易用，2.0版本之前，可以通过 tf.enable_eager_execution() 方法来启用动态图机制</span>

<span class="token comment">#导入数据：共计数据150组，分别为输入特征（x）和标签（y）</span>
x_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.data
y_data <span class="token operator">=</span> datasets.load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>.target

<span class="token comment">#输入数据预处理：随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）</span>
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>  <span class="token comment"># 使用相同的seed，保证输入特征和标签一一对应（seed参数每一个数对应一个随机规则，数字一样随机规则一样产生的随机结果也一样）</span>
np.random.shuffle<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span>   <span class="token comment"># 重新排序返回一个随机序列，在原数组上进行，改变自身序列，无返回值</span>
np.random.seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>
np.random.shuffle<span class="token punctuation">(</span>y_data<span class="token punctuation">)</span>
tf.compat.v1.random.set_random_seed<span class="token punctuation">(</span><span class="token number">116</span><span class="token punctuation">)</span>     <span class="token comment"># 版本原因导致的一些写法错误，tf.random.set_seed(116)修改内容</span>

<span class="token comment">#分割数据集：将打乱后的数据集（共有数据150行）分割为训练集和测试集，训练集为前120行，测试集为后30行</span>
x_train <span class="token operator">=</span> x_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>         <span class="token comment"># x_data[:-30]=x_data[0:120]</span>
y_train <span class="token operator">=</span> y_data<span class="token punctuation">[</span>:-30<span class="token punctuation">]</span>
x_test <span class="token operator">=</span> x_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>          <span class="token comment"># x_data[-30:]=x_data[120:150]</span>
y_test <span class="token operator">=</span> y_data<span class="token punctuation">[</span>-30:<span class="token punctuation">]</span>

 数据处理：转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错
x_train <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_train, tf.float32<span class="token punctuation">)</span>
x_test <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>x_test, tf.float32<span class="token punctuation">)</span>

<span class="token comment">#数据切片: from_tensor_slices（（数据特征，标签）），函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）</span>
<span class="token comment">#batch（数据集大小/batch大小），两个数字可能不是整除，会导致一个batch大小可能小于等于batch size</span>
train_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
test_db <span class="token operator">=</span> tf.data.Dataset.from_tensor_slices<span class="token variable"><span class="token punctuation">((</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">))</span></span>.batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>

<span class="token comment">#生成神经网络的参数，4个输入特征，因此输入层为4个输入节点；因为3分类，所以输出层为3个神经元</span>
<span class="token comment">#用tf.Variable()标记参数可训练</span>
<span class="token comment">#使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）</span>
w1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span>, <span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>
b1 <span class="token operator">=</span> tf.Variable<span class="token punctuation">(</span>tf.random.truncated_normal<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>, <span class="token assign-left variable">stddev</span><span class="token operator">=</span><span class="token number">0.1</span>, <span class="token assign-left variable">seed</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">))</span>

lr <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># 学习率为0.1</span>
train_loss_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的loss记录在此列表中，为后续画loss曲线提供数据</span>
test_acc <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 将每轮的acc记录在此列表中，为后续画acc曲线提供数据</span>
epoch <span class="token operator">=</span> <span class="token number">186</span> <span class="token comment"># 循环500轮</span>
loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># 每轮分4个step，loss_all记录四个step生成的4个loss的和</span>

<span class="token comment">#训练部分</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>:  <span class="token comment"># 数据集级别的循环，每个epoch循环一次数据集</span>
    <span class="token keyword">for</span> step, <span class="token punctuation">(</span>x_train, y_train<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_db<span class="token punctuation">)</span>:  <span class="token comment"># batch级别的循环 ，每个step循环一个batch</span>
        with tf.GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> as tape:  <span class="token comment"># with结构记录梯度信息</span>
            y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_train, w1<span class="token punctuation">)</span> + b1  <span class="token comment"># 神经网络乘加运算</span>
            y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）</span>
            y_ <span class="token operator">=</span> tf.one_hot<span class="token punctuation">(</span>y_train, <span class="token assign-left variable">depth</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 将标签值转换为独热码格式，方便计算loss和accuracy</span>
            loss <span class="token operator">=</span> tf.reduce_mean<span class="token punctuation">(</span>tf.square<span class="token punctuation">(</span>y_ - y<span class="token punctuation">))</span>  <span class="token comment"># 采用均方误差损失函数mse = mean(sum(y-out)^2)</span>
            loss_all <span class="token operator">+=</span> loss.numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确</span>

        <span class="token comment"># 计算loss对各个参数的梯度</span>
        grads <span class="token operator">=</span> tape.gradient<span class="token punctuation">(</span>loss, <span class="token punctuation">[</span>w1, b1<span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad</span>
        w1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数w1自更新</span>
        b1.assign_sub<span class="token punctuation">(</span>lr * grads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 参数b自更新</span>

    <span class="token comment"># 每个epoch，打印loss信息</span>
    print<span class="token punctuation">(</span><span class="token string">"Epoch {}, loss: {}"</span>.format<span class="token punctuation">(</span>epoch, loss_all/4<span class="token punctuation">))</span>
    train_loss_results.append<span class="token punctuation">(</span>loss_all / <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 将4个step的loss求平均记录在此变量中</span>
    loss_all <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment"># loss_all归零，为记录下一个epoch的loss做准备</span>

    <span class="token comment"># 测试部分</span>
    <span class="token comment"># total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0</span>
    total_correct, total_number <span class="token operator">=</span> <span class="token number">0</span>, <span class="token number">0</span>
    <span class="token keyword">for</span> x_test, y_test <span class="token keyword">in</span> test_db:
        <span class="token comment"># 使用更新后的参数进行预测</span>
        y <span class="token operator">=</span> tf.matmul<span class="token punctuation">(</span>x_test, w1<span class="token punctuation">)</span> + b1
        y <span class="token operator">=</span> tf.nn.softmax<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        pred <span class="token operator">=</span> tf.argmax<span class="token punctuation">(</span>y, <span class="token assign-left variable">axis</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 返回y中最大值的索引，即预测的分类</span>
        <span class="token comment"># 将pred转换为y_test的数据类型</span>
        pred <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>pred, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>y_test.dtype<span class="token punctuation">)</span>
        <span class="token comment"># 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型</span>
        correct <span class="token operator">=</span> tf.cast<span class="token punctuation">(</span>tf.equal<span class="token punctuation">(</span>pred, y_test<span class="token punctuation">)</span>, <span class="token assign-left variable">dtype</span><span class="token operator">=</span>tf.int32<span class="token punctuation">)</span>
        <span class="token comment"># 将每个batch的correct数加起来</span>
        correct <span class="token operator">=</span> tf.reduce_sum<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># 将所有batch中的correct数加起来</span>
        total_correct <span class="token operator">+=</span> int<span class="token punctuation">(</span>correct<span class="token punctuation">)</span>
        <span class="token comment"># total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数</span>
        total_number <span class="token operator">+=</span> x_test.shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token comment"># 总的准确率等于total_correct/total_number</span>

    <span class="token assign-left variable">total_number</span><span class="token operator">=</span> int<span class="token punctuation">(</span>total_number<span class="token punctuation">)</span>               <span class="token comment">#错误的原因在，一个是numpy对象，一个是Dimension对象，无法相除,                                               #用int函数，将Dimension对象对象转换为int</span>
    acc <span class="token operator">=</span> total_correct / total_number
    test_acc.append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>
    print<span class="token punctuation">(</span><span class="token string">"Test_acc:"</span>, acc<span class="token punctuation">)</span>
    print<span class="token punctuation">(</span><span class="token string">"--------------------------"</span><span class="token punctuation">)</span>

<span class="token comment">#绘制 loss 曲线</span>
plt.title<span class="token punctuation">(</span><span class="token string">'Loss Function Curve'</span><span class="token punctuation">)</span>  <span class="token comment"># 图片标题</span>
plt.xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># x轴变量名称</span>
plt.ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>  <span class="token comment"># y轴变量名称</span>
plt.plot<span class="token punctuation">(</span>train_loss_results, <span class="token assign-left variable">label</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$Loss</span>$"</span><span class="token punctuation">)</span>  <span class="token comment"># 逐点画出trian_loss_results值并连线，连线图标是Loss</span>
plt.legend<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 画出曲线图标</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 画出图像</span>
<span class="token comment">#绘制 Accuracy 曲线</span>
plt.title<span class="token punctuation">(</span><span class="token string">'Acc Curve'</span><span class="token punctuation">)</span>  <span class="token comment"># 图片标题</span>
plt.xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>  <span class="token comment"># x轴变量名称</span>
plt.ylabel<span class="token punctuation">(</span><span class="token string">'Acc'</span><span class="token punctuation">)</span>  <span class="token comment"># y轴变量名称</span>
plt.plot<span class="token punctuation">(</span>test_acc, <span class="token assign-left variable">label</span><span class="token operator">=</span><span class="token string">"<span class="token variable">$Accuracy</span>$"</span><span class="token punctuation">)</span>  <span class="token comment"># 逐点画出test_acc值并连线，连线图标是Accuracy</span>
plt.legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt.show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/98f101505b578604a7b700dd9cd4f4b5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用pkg打包node.js为可执行文件（exe）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b39385835ef4916ba00df2699b658412/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VUE跨域、常用解决跨域的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>