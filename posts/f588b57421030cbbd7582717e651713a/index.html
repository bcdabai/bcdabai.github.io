<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tf.Variable - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tf.Variable" />
<meta property="og:description" content="参考 tf.Variable - 云&#43;社区 - 腾讯云
目录
__init__
__abs__
__add__
__getitem__
__gt__
__invert__
__iter__
__le__
__lt__
__matmul__
__mod__
__mul__
__ne__
__neg__
__or__
__pow__
__radd__
__rand__
__rdiv__
__rfloordiv__
__rmatmul__
__rmod__
__rmul__
__ror__
__rpow__
__rsub__
__rtruediv__
__rxor__
assign
assign_add
assign_sub
batch_scatter_update
count_up_to
eval
experimental_ref
from_proto
gather_nd
get_shape
initialized_value
load
read_value
scatter_add
scatter_div
scatter_max
scatter_min
scatter_mul
scatter_nd_add
scatter_nd_sub
scatter_nd_update
scatter_sub
scatter_update
set_shape
sparse_read
to_proto
value
变量跨run()调用在图中维护状态。通过构造类变量的实例，可以向图中添加一个变量。Variable()构造函数需要变量的初值，它可以是任何类型和形状的张量。初值定义变量的类型和形状。构造完成后，变量的类型和形状是固定的。可以使用指定方法之一更改值。如果稍后要更改变量的形状，必须使用带有validate_shape=False的赋值Op。与任何张量一样，使用Variable()创建的变量可以用作图中其他Ops的输入。此外，张量类的所有重载运算符都被传递到变量上，因此您也可以通过对变量进行算术将节点添加到图中。
import tensorflow as tf # Create a variable." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f588b57421030cbbd7582717e651713a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-09T12:49:30+08:00" />
<meta property="article:modified_time" content="2022-09-09T12:49:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tf.Variable</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>参考 <a href="https://cloud.tencent.com/developer/article/1499953" rel="nofollow" title="tf.Variable - 云+社区 - 腾讯云">tf.Variable - 云+社区 - 腾讯云</a></p> 
<p></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="__init__-toc" style="margin-left:80px;"><a href="#__init__" rel="nofollow">__init__</a></p> 
<p id="__abs__-toc" style="margin-left:80px;"><a href="#__abs__" rel="nofollow">__abs__</a></p> 
<p id="__add__-toc" style="margin-left:80px;"><a href="#__add__" rel="nofollow">__add__</a></p> 
<p id="__getitem__-toc" style="margin-left:80px;"><a href="#__getitem__" rel="nofollow">__getitem__</a></p> 
<p id="__gt__-toc" style="margin-left:80px;"><a href="#__gt__" rel="nofollow">__gt__</a></p> 
<p id="__invert__-toc" style="margin-left:80px;"><a href="#__invert__" rel="nofollow">__invert__</a></p> 
<p id="__iter__-toc" style="margin-left:80px;"><a href="#__iter__" rel="nofollow">__iter__</a></p> 
<p id="__le__-toc" style="margin-left:80px;"><a href="#__le__" rel="nofollow">__le__</a></p> 
<p id="__lt__-toc" style="margin-left:80px;"><a href="#__lt__" rel="nofollow">__lt__</a></p> 
<p id="__matmul__-toc" style="margin-left:80px;"><a href="#__matmul__" rel="nofollow">__matmul__</a></p> 
<p id="__mod__-toc" style="margin-left:80px;"><a href="#__mod__" rel="nofollow">__mod__</a></p> 
<p id="__mul__-toc" style="margin-left:80px;"><a href="#__mul__" rel="nofollow">__mul__</a></p> 
<p id="__ne__-toc" style="margin-left:80px;"><a href="#__ne__" rel="nofollow">__ne__</a></p> 
<p id="__neg__-toc" style="margin-left:80px;"><a href="#__neg__" rel="nofollow">__neg__</a></p> 
<p id="__or__-toc" style="margin-left:80px;"><a href="#__or__" rel="nofollow">__or__</a></p> 
<p id="__pow__-toc" style="margin-left:80px;"><a href="#__pow__" rel="nofollow">__pow__</a></p> 
<p id="__radd__-toc" style="margin-left:80px;"><a href="#__radd__" rel="nofollow">__radd__</a></p> 
<p id="__rand__-toc" style="margin-left:80px;"><a href="#__rand__" rel="nofollow">__rand__</a></p> 
<p id="__rdiv__-toc" style="margin-left:80px;"><a href="#__rdiv__" rel="nofollow">__rdiv__</a></p> 
<p id="__rfloordiv__-toc" style="margin-left:80px;"><a href="#__rfloordiv__" rel="nofollow">__rfloordiv__</a></p> 
<p id="__rmatmul__-toc" style="margin-left:80px;"><a href="#__rmatmul__" rel="nofollow">__rmatmul__</a></p> 
<p id="__rmod__-toc" style="margin-left:80px;"><a href="#__rmod__" rel="nofollow">__rmod__</a></p> 
<p id="__rmul__-toc" style="margin-left:80px;"><a href="#__rmul__" rel="nofollow">__rmul__</a></p> 
<p id="__ror__-toc" style="margin-left:80px;"><a href="#__ror__" rel="nofollow">__ror__</a></p> 
<p id="__rpow__-toc" style="margin-left:80px;"><a href="#__rpow__" rel="nofollow">__rpow__</a></p> 
<p id="__rsub__-toc" style="margin-left:80px;"><a href="#__rsub__" rel="nofollow">__rsub__</a></p> 
<p id="__rtruediv__-toc" style="margin-left:80px;"><a href="#__rtruediv__" rel="nofollow">__rtruediv__</a></p> 
<p id="__rxor__-toc" style="margin-left:80px;"><a href="#__rxor__" rel="nofollow">__rxor__</a></p> 
<p id="assign-toc" style="margin-left:80px;"><a href="#assign" rel="nofollow">assign</a></p> 
<p id="assign_add-toc" style="margin-left:80px;"><a href="#assign_add" rel="nofollow">assign_add</a></p> 
<p id="assign_sub-toc" style="margin-left:80px;"><a href="#assign_sub" rel="nofollow">assign_sub</a></p> 
<p id="batch_scatter_update-toc" style="margin-left:80px;"><a href="#batch_scatter_update" rel="nofollow">batch_scatter_update</a></p> 
<p id="count_up_to-toc" style="margin-left:80px;"><a href="#count_up_to" rel="nofollow">count_up_to</a></p> 
<p id="eval-toc" style="margin-left:80px;"><a href="#eval" rel="nofollow">eval</a></p> 
<p id="experimental_ref-toc" style="margin-left:80px;"><a href="#experimental_ref" rel="nofollow">experimental_ref</a></p> 
<p id="from_proto-toc" style="margin-left:80px;"><a href="#from_proto" rel="nofollow">from_proto</a></p> 
<p id="gather_nd-toc" style="margin-left:80px;"><a href="#gather_nd" rel="nofollow">gather_nd</a></p> 
<p id="get_shape-toc" style="margin-left:80px;"><a href="#get_shape" rel="nofollow">get_shape</a></p> 
<p id="initialized_value-toc" style="margin-left:80px;"><a href="#initialized_value" rel="nofollow">initialized_value</a></p> 
<p id="load-toc" style="margin-left:80px;"><a href="#load" rel="nofollow">load</a></p> 
<p id="read_value-toc" style="margin-left:80px;"><a href="#read_value" rel="nofollow">read_value</a></p> 
<p id="scatter_add-toc" style="margin-left:80px;"><a href="#scatter_add" rel="nofollow">scatter_add</a></p> 
<p id="scatter_div-toc" style="margin-left:80px;"><a href="#scatter_div" rel="nofollow">scatter_div</a></p> 
<p id="scatter_max-toc" style="margin-left:80px;"><a href="#scatter_max" rel="nofollow">scatter_max</a></p> 
<p id="scatter_min-toc" style="margin-left:80px;"><a href="#scatter_min" rel="nofollow">scatter_min</a></p> 
<p id="scatter_mul-toc" style="margin-left:80px;"><a href="#scatter_mul" rel="nofollow">scatter_mul</a></p> 
<p id="scatter_nd_add-toc" style="margin-left:80px;"><a href="#scatter_nd_add" rel="nofollow">scatter_nd_add</a></p> 
<p id="scatter_nd_sub-toc" style="margin-left:80px;"><a href="#scatter_nd_sub" rel="nofollow">scatter_nd_sub</a></p> 
<p id="scatter_nd_update-toc" style="margin-left:80px;"><a href="#scatter_nd_update" rel="nofollow">scatter_nd_update</a></p> 
<p id="scatter_sub-toc" style="margin-left:80px;"><a href="#scatter_sub" rel="nofollow">scatter_sub</a></p> 
<p id="scatter_update-toc" style="margin-left:80px;"><a href="#scatter_update" rel="nofollow">scatter_update</a></p> 
<p id="set_shape-toc" style="margin-left:80px;"><a href="#set_shape" rel="nofollow">set_shape</a></p> 
<p id="sparse_read-toc" style="margin-left:80px;"><a href="#sparse_read" rel="nofollow">sparse_read</a></p> 
<p id="to_proto-toc" style="margin-left:80px;"><a href="#to_proto" rel="nofollow">to_proto</a></p> 
<p id="value-toc" style="margin-left:80px;"><a href="#value" rel="nofollow">value</a></p> 
<hr id="hr-toc"> 
<p>变量跨run()调用在图中维护状态。通过构造类变量的实例，可以向图中添加一个变量。Variable()构造函数需要变量的初值，它可以是任何类型和形状的张量。初值定义变量的类型和形状。构造完成后，变量的类型和形状是固定的。可以使用指定方法之一更改值。如果稍后要更改变量的形状，必须使用带有validate_shape=False的赋值Op。与任何张量一样，使用Variable()创建的变量可以用作图中其他Ops的输入。此外，张量类的所有重载运算符都被传递到变量上，因此您也可以通过对变量进行算术将节点添加到图中。</p> 
<pre class="has"><code class="language-python">import tensorflow as tf

# Create a variable.
w = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)

# Use the variable in the graph like any Tensor.
y = tf.matmul(w, ...another variable or tensor...)

# The overloaded operators are available too.
z = tf.sigmoid(w + y)

# Assign a new value to the variable with `assign()` or a related method.
w.assign(w + 1.0)
w.assign_add(1.0)
</code></pre> 
<p>启动图形时，必须显式初始化变量，然后才能运行使用其值的操作系统。你可以通过运行变量的初始化器op、从保存文件中还原变量，或者简单地运行赋值op来初始化变量。事实上，变量初始化器op只是一个赋值op，它将变量的初值赋给变量本身。</p> 
<pre class="has"><code class="language-python"># Launch the graph in a session.
with tf.compat.v1.Session() as sess:
    # Run the variable initializer.
    sess.run(w.initializer)
    # ...you now can run ops that use the value of 'w'...
</code></pre> 
<p>最常见的初始化模式是使用方便的函数global_variables_initializer()将Op添加到初始化所有变量的图中。然后在启动图表之后运行该Op。</p> 
<pre class="has"><code class="language-python"># Add an Op to initialize global variables.
init_op = tf.compat.v1.global_variables_initializer()

# Launch the graph in a session.
with tf.compat.v1.Session() as sess:
    # Run the Op that initializes global variables.
    sess.run(init_op)
    # ...you can now run any Op that uses variable values...
</code></pre> 
<p>如果需要创建一个初始值依赖于另一个变量的变量，请使用另一个变量的initialized_value()。这确保变量以正确的顺序初始化。所有变量都自动收集到创建它们的图中。默认情况下，构造函数将新变量添加到graph collection GraphKeys.GLOBAL_VARIABLES。函数global_variables()返回集合的内容。在构建机器学习模型时，通常可以方便地区分包含可训练模型参数的变量和其他变量，例如用于计算训练步骤的全局步骤变量。为了简化这一点，变量构造函数支持一个可训练的=&lt;bool&gt;参数。如果为真，则新变量也将添加到graph collection GraphKeys.TRAINABLE_VARIABLES中。函数trainable_variables()返回这个集合的内容。各种优化器类都使用这个集合作为要优化的默认变量列表。</p> 
<h4 id="__init__"><code>__init__</code></h4> 
<pre class="has"><code class="language-python">__init__(
    initial_value=None,
    trainable=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    import_scope=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None
)
</code></pre> 
<p>创建一个值为initial_value的新变量。新变量被添加到集合中列出的图形集合中，集合默认为[GraphKeys.GLOBAL_VARIABLES]。如果trainable为真，则该变量还将添加到graph collection GraphKeys.TRAINABLE_VARIABLES中。这个构造函数创建一个变量Op和一个赋值Op来将变量设置为其初始值。</p> 
<p><strong>参数：</strong></p> 
<ul><li>initial_value:张量，或可转换为张量的Python对象，它是变量的初值。初始值必须指定一个形状，除非validate_shape被设置为False。也可以是可调用的，没有参数，调用时返回初始值。在这种情况下，必须指定dtype。(注意，在这里使用初始化器函数之前，必须先将init_ops.py中的初始化器函数绑定到一个形状。)</li><li>可训练的:如果是真的，gradient tape会自动监视这个变量的使用。默认值为True，除非同步设置为ON_READ，在这种情况下，同步设置为False。</li><li>validate_shape:如果为False，则允许用一个未知形状的值初始化变量。如果为真，默认情况下，initial_value的形状必须是已知的。</li><li>caching_device:可选的设备字符串，描述变量应该缓存到什么地方以便读取。变量的设备的默认值。如果没有，则缓存到另一个设备上。典型的用途是在使用该变量的操作系统所在的设备上缓存，通过Switch和其他条件语句来重复复制。</li><li>name:变量的可选名称。默认值为“Variable”，并自动进行uniquified。</li><li>VariableDef协议缓冲区。如果没有，则使用其内容重新创建变量对象，并引用图中必须已经存在的变量节点。图形没有改变。variable_def和其他参数是互斥的。</li><li>dtype:如果设置了，initial_value将转换为给定的类型。如果没有，要么保留数据类型(如果initial_value是一个张量)，要么由convert_to_张量决定。</li><li>import_scope:可选的字符串。要添加到变量的名称范围。仅在从协议缓冲区初始化时使用。</li><li><strong><code>constraint</code></strong>:优化器更新后应用于变量的可选投影函数(例如，用于为层权重实现规范约束或值约束)。函数必须将表示变量值的未投影张量作为输入，并返回投影值的张量(其形状必须相同)。在进行异步分布式培训时使用约束并不安全。</li><li><strong><code>synchronization</code></strong>:指示何时聚合分布式变量。可接受的值是在tf.VariableSynchronization类中定义的常量。默认情况下，同步设置为AUTO，当前分发策略选择何时同步。</li><li><strong><code>aggregation</code></strong>:指示如何聚合分布式变量。可接受的值是在tf.VariableAggregation类中定义的常量。</li><li>shape:(可选)这个变量的形状。如果没有，则使用initial_value的形状。当将这个参数设置为tf.TensorShape(None)(表示一个未指定的形状)时，可以用不同形状的值为变量赋值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>ValueError</code></strong>: If both <code>variable_def</code> and initial_value are specified.</li><li><strong><code>ValueError</code></strong>: If the initial value is not specified, or does not have a shape and <code>validate_shape</code> is <code>True</code>.</li><li><strong><code>RuntimeError</code></strong>: If eager execution is enabled.</li></ul> 
<h4 id="__abs__"><code>__abs__</code></h4> 
<pre class="has"><code class="language-python">__abs__(
    x,
    name=None
)
</code></pre> 
<p>计算张量的绝对值。给定一个整数或浮点值的张量，这个操作返回一个相同类型的张量，其中每个元素都包含输入中相应元素的绝对值。给定一个复数张量x，这个操作返回一个类型为float32或float64的张量，这是x中每个元素的绝对值。x中所有的元素必须是复数形式<img alt="a+bj" class="mathcode" src="https://images2.imgbox.com/f7/0f/7RWVa3C2_o.gif">，绝对值为<img alt="\large \sqrt{a^2+b^2}" class="mathcode" height="20" src="https://images2.imgbox.com/85/14/R9ti8h5J_o.gif" width="63">。例如:</p> 
<pre class="has"><code class="language-python">x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
tf.abs(x)  # [5.25594902, 6.60492229]
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>x:一个类型为float16、float32、float64、int32、int64、complex64或complex128的张量或稀疏张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量或稀疏张量，其大小、类型和稀疏性与x的绝对值相同。注意，对于complex64或complex128输入，返回的张量类型分别为float32或float64。</li></ul> 
<h4 id="__add__"><code>__add__</code></h4> 
<pre class="has"><code class="language-python">__and__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回x和y元素的真值。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x: bool型张量。</li><li>y: bool型张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__getitem__"><code>__getitem__</code></h4> 
<pre class="has"><code class="language-python">__getitem__(
    var,
    slice_spec
)
</code></pre> 
<p>创建给定变量的切片助手对象。这允许从变量当前内容的一部分创建子张量。看到tf.Tensor。获取切片的详细示例。此外，该函数还允许对切片范围赋值。这类似于Python中的_setitem__功能。但是，语法不同，因此用户可以捕获赋值操作，以便分组或传递给ssh .run()。例如,</p> 
<pre class="has"><code class="language-python">import tensorflow as tf
A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)
with tf.compat.v1.Session() as sess:
  sess.run(tf.compat.v1.global_variables_initializer())
  print(sess.run(A[:2, :2]))  # =&gt; [[1,2], [4,5]]

  op = A[:2,:2].assign(22. * tf.ones((2, 2)))
  print(sess.run(op))  # =&gt; [[22, 22, 3], [22, 22, 6], [7,8,9]]
</code></pre> 
<p>注意，当前的赋值不支持NumPy广播语义。</p> 
<p><strong>参数：</strong></p> 
<ul><li>var: ops.Variable对象</li><li>slice_spec: Tensor.getitem的参数。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>张量的适当切片，基于slice_spec。作为一个操作符。操作符还有一个assign()方法，可用于生成赋值操作符。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>ValueError</code></strong>: If a slice range is negative size.</li><li><strong><code>TypeError</code></strong>: TypeError: If the slice indices aren't int, slice, ellipsis, tf.newaxis or int32/int64 tensors.</li></ul> 
<h4 id="__gt__"><code>__gt__</code></h4> 
<pre class="has"><code class="language-python">__gt__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回元素(x &gt; y)的真值。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是下列类型之一:float32、float64、int32、uint8、int16、int8、int64、bfloat16、uint16、half、uint32、uint64。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__invert__"><code>__invert__</code></h4> 
<pre class="has"><code class="language-python">__invert__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回NOT x element-wise的真值。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x: bool型张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__iter__"><code>__iter__</code></h4> 
<pre class="has"><code class="language-python">__iter__()
</code></pre> 
<p>防止迭代的哑方法。不要打电话。注意(mrry):如果我们将getitem注册为一个重载操作符，Python将勇敢地尝试迭代变量的张量，从0到无穷。声明此方法可防止此意外行为。</p> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: when invoked.</li></ul> 
<h4 id="__le__"><code>__le__</code></h4> 
<pre class="has"><code class="language-python">__le__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回元素的真值(x &lt;= y)。注意:数学。less_equal支持广播。更多关于广播</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是下列类型之一:float32、float64、int32、uint8、int16、int8、int64、bfloat16、uint16、half、uint32、uint64。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__lt__"><code>__lt__</code></h4> 
<pre class="has"><code class="language-python">__lt__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回(x &lt; y)元素的真值。注意:math.less不支持广播。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是下列类型之一:float32、float64、int32、uint8、int16、int8、int64、bfloat16、uint16、half、uint32、uint64。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__matmul__"><code>__matmul__</code></h4> 
<pre class="has"><code class="language-python">__matmul__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>将矩阵a乘以矩阵b，得到a * b。在任何换位之后，输入必须是秩为&gt;= 2的张量，其中内部2维指定有效的矩阵乘法参数，并且任何进一步的外部维度匹配。两个矩阵必须是同一类型的。支持的类型有:float16、float32、float64、int32、complex64、complex128。通过将相应的标志之一设置为True，可以动态地对矩阵进行换位或附加(共轭和换位)。这些默认为False。如果其中一个或两个矩阵包含很多0，则可以通过将相应的a_is_sparse或b_is_稀疏标志设置为True来使用更有效的乘法算法。这些默认为False。这种优化只适用于数据类型为bfloat16或float32的普通矩阵(秩为2的张量)。</p> 
<p><strong>例如：</strong></p> 
<pre class="has"><code class="language-python"># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>a:类型为float16、float32、float64、int32、complex64、complex128的张量，秩为&gt; 1。</li><li><strong><code>b</code></strong>:与a类型和秩相同的张量。</li><li><strong><code>transpose_a</code></strong>:如果为真，则a在乘法之前转置。</li><li><strong><code>transpose_a</code></strong>:如果为真，则b在乘法之前转置。</li><li>adjoint_a:如果是真的，a是共轭和转置之前的乘法。</li><li>adjoint_b:如果为真，b是共轭和转置之前的乘法。</li><li>a_is_疏:如果为真，则将a视为一个稀疏矩阵。</li><li>b_is_sparse:如果为真，则将b视为稀疏矩阵。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个与A和b相同类型的张量，其中每个最内层的矩阵是A和b中相应矩阵的乘积，例如，如果所有转置或伴随属性都为假:对所有的指数，输出[..., i, j] = sum_k (<code>a</code>[..., i, k] * <code>b</code>[..., k, j])。注意:这是矩阵乘积，不是元素乘。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>ValueError</code></strong>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li></ul> 
<h4 id="__mod__"><code>__mod__</code></h4> 
<pre class="has"><code class="language-python">__mod__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回除法的元素剩余部分。当x &lt; 0 xor y &lt; 0为真时，这符合Python语义，因为这里的结果与地板划分一致。例如，floor(x / y) * y + mod(x, y) = x。math.floormod支持广播。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是下列类型之一:int32、int64、bfloat16、half、float32、float64。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li> <p>一个张量。与x类型相同。</p> </li></ul> 
<h4 id="__mul__"><code>__mul__</code></h4> 
<pre class="has"><code class="language-python">__mul__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>为“densedensity”和“DenseSparse”分派cwise mul。</p> 
<h4 id="__ne__"><code>__ne__</code></h4> 
<pre class="has"><code class="language-python">__ne__(other)
</code></pre> 
<p>逐元素比较两个变量是否相等。</p> 
<h4 id="__neg__"><code>__neg__</code></h4> 
<pre class="has"><code class="language-python">__neg__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>计算数值负值元素，也就是<img alt="\large y=-x" class="mathcode" src="https://images2.imgbox.com/f3/22/d4KvuPpX_o.gif">。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x：张量。必须是以下类型之一:bfloat16、half、float32、float64、int32、int64、complex64、complex128。</li><li>name：操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li> <p>一个张量。与x类型相同。</p> </li></ul> 
<h4 id="__or__"><code>__or__</code></h4> 
<pre class="has"><code class="language-python">__or__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回x或y元素的真值。注意:math.logical_or支持广播。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x: bool型张量。</li><li>y: bool型张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回：</strong></p> 
<ul><li>bool类型的张量。</li></ul> 
<h4 id="__pow__"><code>__pow__</code></h4> 
<pre class="has"><code class="language-python">__pow__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>计算一个值对另一个值的幂。给定一个张量x和一个张量y，这个操作计算x和y中对应的<img alt="\large x^y" class="mathcode" src="https://images2.imgbox.com/6b/a5/wylrhbnn_o.gif">。</p> 
<pre class="has"><code class="language-python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>x:类型为float16、float32、float64、int32、int64、complex64或complex128的张量。</li><li>y:类型为float16、float32、float64、int32、int64、complex64或complex128的张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p>返回值：</p> 
<ul><li>一个张量。</li></ul> 
<h4 id="__radd__"><code>__radd__</code></h4> 
<pre class="has"><code class="language-python">__radd__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>为字符串添加分派，为所有其他类型添加add_v2。</p> 
<h4 id="__rand__"><code>__rand__</code></h4> 
<pre class="has"><code class="language-python">__rand__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回x和y元素的真值。注意:math.logical_and支持广播。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x: bool型张量。</li><li>y: bool型张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>布尔类型的张量。</li></ul> 
<h4 id="__rdiv__"><code>__rdiv__</code></h4> 
<pre class="has"><code class="language-python">__rdiv__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>使用Python 2语义划分两个值。用于Tensor.div。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:实数型张量分子。</li><li>y:实数型张量分母。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>x / y返回x和y的商。</li></ul> 
<h4 id="__rfloordiv__"><code>__rfloordiv__</code></h4> 
<pre class="has"><code class="language-python">__rfloordiv__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>除以x / y元素，四舍五入到最负的整数。对于整数，与tf.compat.v1.div(x,y)相同，但是对于浮点参数，使用tf.floor(tf.compat.v1.div(x,y))，因此结果总是一个整数(尽管可能是一个用浮点表示的整数)。该op由python3中的x // y层划分和python2.7中的来自于future__导入划分生成。x和y必须具有相同的类型，并且结果也必须具有相同的类型。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:实数型张量分子。</li><li>y:实数型张量分母。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>x / y四舍五入。</li></ul> 
<h4 id="__rmatmul__"><code>__rmatmul__</code></h4> 
<pre class="has"><code class="language-python">__rmatmul__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>将矩阵a乘以矩阵b，得到a * b。在任何换位之后，输入必须是秩为&gt;= 2的张量，其中内部2维指定有效的矩阵乘法参数，并且任何进一步的外部维度匹配。两个矩阵必须是同一类型的。支持的类型有:float16、float32、float64、int32、complex64、complex128。通过将相应的标志之一设置为True，可以动态地对矩阵进行换位或附加(共轭和换位)。这些默认为False。如果其中一个或两个矩阵包含很多0，则可以通过将相应的a_is_sparse或b_is_稀疏标志设置为True来使用更有效的乘法算法。这些默认为False。这种优化只适用于数据类型为bfloat16或float32的普通矩阵(秩为2的张量)。</p> 
<pre class="has"><code class="language-python"># 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 2-D tensor `b`
# [[ 7,  8],
#  [ 9, 10],
#  [11, 12]]
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

# `a` * `b`
# [[ 58,  64],
#  [139, 154]]
c = tf.matmul(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 3-D tensor `b`
# [[[13, 14],
#   [15, 16],
#   [17, 18]],
#  [[19, 20],
#   [21, 22],
#   [23, 24]]]
b = tf.constant(np.arange(13, 25, dtype=np.int32),
                shape=[2, 3, 2])

# `a` * `b`
# [[[ 94, 100],
#   [229, 244]],
#  [[508, 532],
#   [697, 730]]]
c = tf.matmul(a, b)

# Since python &gt;= 3.5 the @ operator is supported (see PEP 465).
# In TensorFlow, it simply calls the `tf.matmul()` function, so the
# following lines are equivalent:
d = a @ b @ [[10.], [11.]]
d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>a:类型为float16、float32、float64、int32、complex64、complex128的张量，秩为&gt; 1。</li><li>b:与a类型和秩相同的张量。</li><li><strong><code>transpose_a</code></strong>:如果为真，则a在乘法之前转置。</li><li><strong><code>transpose_b</code></strong>:如果为真，则b在乘法之前转置。</li><li>adjoint_a:如果是真的，a是共轭和转置之前的乘法。</li><li>adjoint_b:如果为真，b是共轭和转置之前的乘法。</li><li>a_is_疏:如果为真，则将a视为一个稀疏矩阵。</li><li>b_is_sparse:如果为真，则将b视为稀疏矩阵。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>与A和b相同类型的张量，其中每个最内矩阵是A和b中相应矩阵的乘积，例如，如果所有转置或伴随属性都为假:输出[…， i, j] = sum_k (a[…， i, k] * b[…， k, j])，对于所有指标i, j。</li></ul> 
<p>注意:这是矩阵乘积，不是元素乘积。</p> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>ValueError</code></strong>: If transpose_a and adjoint_a, or transpose_b and adjoint_b are both set to True.</li></ul> 
<h4 id="__rmod__"><code>__rmod__</code></h4> 
<pre class="has"><code class="language-python">__rmod__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回除法的元素剩余部分。当x &lt; 0 xor y &lt; 0为真时，这符合Python语义，因为这里的结果与地板划分一致。例如，floor(x / y) * y + mod(x, y) = x。floormod支持广播。更多关于广播</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是下列类型之一:int32、int64、bfloat16、half、float32、float64。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量。与x类型相同。</li></ul> 
<h4 id="__rmul__"><code>__rmul__</code></h4> 
<pre class="has"><code class="language-python">__rmul__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>为“densedensity”和“DenseSparse”分派cwise mul。</p> 
<h4 id="__ror__"><code>__ror__</code></h4> 
<pre class="has"><code class="language-python">__ror__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回x或y元素的真值。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x: bool型张量。</li><li>y: bool型张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>bool类型的张量。</li></ul> 
<h4 id="__rpow__"><code>__rpow__</code></h4> 
<pre class="has"><code class="language-python">__rpow__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>计算一个值对另一个值的幂。给定一个张量x和一个张量y，这个操作计算x和y中对应的元素。</p> 
<pre class="has"><code class="language-python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>x:类型为float16、float32、float64、int32、int64、complex64或complex128的张量。</li><li>y:类型为float16、float32、float64、int32、int64、complex64或complex128的张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量。</li></ul> 
<h4 id="__rsub__"><code>__rsub__</code></h4> 
<pre class="has"><code class="language-python">__rsub__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>返回x - y元素。注:减支持广播。</p> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量。必须是以下类型之一:bfloat16、half、float32、float64、uint8、int8、uint16、int16、int32、int64、complex64、complex128。</li><li>y:张量。必须具有与x相同的类型。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量。与x类型相同。</li></ul> 
<h4 id="__rtruediv__"><code>__rtruediv__</code></h4> 
<pre class="has"><code class="language-python">__rtruediv__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<h4 id="__rxor__"><code>__rxor__</code></h4> 
<pre class="has"><code>__rxor__(
    a,
    *args,
    **kwargs
)
</code></pre> 
<p>逻辑异或函数。x ^ y = (x | y) &amp; ~(x &amp; y)输入为张量，如果张量包含多个元素，则计算元素逻辑XOR。</p> 
<p><strong>用法：</strong></p> 
<pre class="has"><code class="language-python">x = tf.constant([False, False, True, True], dtype = tf.bool)
y = tf.constant([False, True, False, True], dtype = tf.bool)
z = tf.logical_xor(x, y, name="LogicalXor")
#  here z = [False  True  True False]
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>x:张量类型bool。</li><li>y: bool型张量。</li></ul> 
<p><strong>返回值：</strong></p> 
<p></p> 
<ul><li>一个bool类型的张量，与x或y的张量大小相同。</li></ul> 
<h4 id="assign"><code>assign</code></h4> 
<pre class="has"><code class="language-python">assign(
    value,
    use_locking=False,
    name=None,
    read_value=True
)
</code></pre> 
<p>为变量分配一个新值。这本质上是赋值(self, value)的快捷方式。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>value</code></strong>:一个张量。这个变量的新值。</li><li>use_lock:如果为真，则在赋值期间使用锁定。</li><li>name:要创建的操作的名称</li><li>read_value:如果为真，将返回值为变量新值的值;if False将返回赋值op。</li></ul> 
<p><strong>返回：</strong></p> 
<ul><li>一个张量，它将在赋值完成后保留这个变量的新值。</li></ul> 
<h4 id="assign_add"><code>assign_add</code></h4> 
<pre class="has"><code class="language-python">assign_add(
    delta,
    use_locking=False,
    name=None,
    read_value=True
)
</code></pre> 
<p>为该变量添加一个值。这实际上是assign_add(self, delta)的快捷方式。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>delta</code></strong>:张量。要添加到此变量的值。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:要创建的操作的名称</li><li>read_value:如果为真，将返回值为变量新值的值;if False将返回赋值op。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在加法完成后保留这个变量的新值。</li></ul> 
<h4 id="assign_sub"><code>assign_sub</code></h4> 
<pre class="has"><code class="language-python">assign_sub(
    delta,
    use_locking=False,
    name=None,
    read_value=True
)
</code></pre> 
<p>从这个变量中减去一个值。这实际上是assign_sub(self, delta)的快捷方式。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>delta</code></strong>:张量。从这个变量中减去的值。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:要创建的操作的名称</li><li>read_value:如果为真，将返回值为变量新值的值;if False将返回赋值op。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在减法完成后保留这个变量的新值。</li></ul> 
<h4 id="batch_scatter_update"><code>batch_scatter_update</code></h4> 
<pre class="has"><code class="language-python">batch_scatter_update(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>类似于批量收集。indexedslice按批处理方式指向此变量。类似于batch_gather。这假设这个变量和sparse_delta indexedslice有一系列对它们都相同的主导维度，并且更新是在索引的最后一个维度上执行的。换句话说，维度应该如下:num_prefix_dims = sparse_delta. indexes。batch_dim = num_prefix_dims + 1 sparse_delta.updates。= sparse_delta.indices形状。shape + var.shape[batch_dim:]，其中更新了sparse_delta.updates。[:num_prefix_dims] = = sparse_delta.indices形状。shape[:num_prefix_dims] == var.shape[:num_prefix_dims]，所执行的操作可以表示为:var[i_1，…、i_n sparse_delta。指数(i_1,……， i_n, j]] = sparse_delta。更新(i_1,……， i_n, j]。指标是一维张量，这个操作等价于scatter_update。为了避免这种操作，可以循环遍历变量的第一个ndims，并对分割第一个维度的子张量使用scatter_update。对于ndims = 1，这是一个有效的选项，但是比这个实现的效率要低。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。要分配给该变量的indexedslice。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在分散分配完成后保留这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="count_up_to"><code>count_up_to</code></h4> 
<pre class="has"><code class="language-python">count_up_to(limit)
</code></pre> 
<p>递增此变量，直到它达到极限。(不推荐)当运行Op时，它试图将变量增加1。如果增加变量会使其超过限制，那么Op将抛出异常OutOfRangeError。如果没有引起错误，Op将在增量之前输出变量的值。这实际上是count_up_to(self, limit)的快捷方式。</p> 
<p><strong>参数：</strong></p> 
<ul><li>limit:值，在该值处递增变量会引发错误。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在增量之前保持变量值。如果没有其他Op修改这个变量，那么生成的值都是不同的。</li></ul> 
<h4 id="eval"><code>eval</code></h4> 
<pre class="has"><code class="language-python">eval(session=None)
</code></pre> 
<p>在会话中，计算并返回此变量的值。这不是一个图形构造方法，它不向图形添加ops。这个方便的方法需要一个会话，其中包含这个变量的图已经启动。如果没有传递会话，则使用默认会话。有关启动图表和会话的更多信息，请参见tf.compat.v1.Session。</p> 
<pre class="has"><code class="language-python">v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()

with tf.compat.v1.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    print(v.eval(sess))
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    print(v.eval())
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>session:用来计算这个变量的会话。如果没有，则使用默认会话。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>带有此变量值副本的numpy ndarray。</li></ul> 
<h4 id="experimental_ref"><code>experimental_ref</code></h4> 
<pre class="has"><code class="language-python">experimental_ref()
</code></pre> 
<p>返回此变量的可刷新引用对象。这个API的主要用途是将变量放在set/dictionary中。我们不能把变量放在set/dictionary中，因为变量变量在启动Tensorflow 2.0时不再可用。</p> 
<pre class="has"><code class="language-python">import tensorflow as tf

x = tf.Variable(5)
y = tf.Variable(10)
z = tf.Variable(10)

# The followings will raise an exception starting 2.0
# TypeError: Variable is unhashable if Variable equality is enabled.
variable_set = {x, y, z}
variable_dict = {x: 'five', y: 'ten'}
</code></pre> 
<p>相反，我们可以使用variable.experimental al_ref()。</p> 
<pre class="has"><code class="language-python">variable_set = {x.experimental_ref(),
                y.experimental_ref(),
                z.experimental_ref()}

print(x.experimental_ref() in variable_set)
==&gt; True

variable_dict = {x.experimental_ref(): 'five',
                 y.experimental_ref(): 'ten',
                 z.experimental_ref(): 'ten'}

print(variable_dict[y.experimental_ref()])
==&gt; ten</code></pre> 
<p>此外，reference对象提供.deref()函数，该函数返回原始变量。</p> 
<pre class="has"><code class="language-python">x = tf.Variable(5)
print(x.experimental_ref().deref())
==&gt; &lt;tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5&gt;
</code></pre> 
<h4 id="from_proto"><code>from_proto</code></h4> 
<pre class="has"><code class="language-python">@staticmethod
from_proto(
    variable_def,
    import_scope=None
)
</code></pre> 
<p>返回一个由variable_def创建的变量对象。</p> 
<h4 id="gather_nd"><code>gather_nd</code></h4> 
<pre class="has"><code class="language-python">gather_nd(
    indices,
    name=None
)
</code></pre> 
<p>将params中的切片收集到一个由指标指定形状的张量中。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>indices</code></strong>:一个张量。必须是下列类型之一:int32、int64。指数张量。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量。具有与params相同的类型。</li></ul> 
<h4 id="get_shape"><code>get_shape</code></h4> 
<pre class="has"><code class="language-python">get_shape()
</code></pre> 
<p>Variable.shape的别名。</p> 
<h4 id="initialized_value"><code>initialized_value</code></h4> 
<pre class="has"><code class="language-python">initialized_value()
</code></pre> 
<p>返回初始化变量的值。(弃用)。你应该使用它而不是变量本身来初始化另一个变量，该变量的值取决于该变量的值。</p> 
<pre class="has"><code class="language-python"># Initialize 'v' with a random tensor.
v = tf.Variable(tf.random.truncated_normal([10, 40]))
# Use `initialized_value` to guarantee that `v` has been
# initialized before its value is used to initialize `w`.
# The random values are picked only once.
w = tf.Variable(v.initialized_value() * 2.0)
</code></pre> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，在它的初始化器运行后保持这个变量的值。</li></ul> 
<h4 id="load"><code>load</code></h4> 
<pre class="has"><code class="language-python">load(
    value,
    session=None
)
</code></pre> 
<p>将新值加载到该变量中。(弃用)警告:不推荐使用此函数。它将在未来的版本中被删除。更新说明:首选变量。在2.X中具有相同行为的赋值。将新值写入变量的内存。没有向图中添加ops。这个方便的方法需要一个会话，其中包含这个变量的图已经启动。如果没有传递会话，则使用默认会话。有关启动图表和会话的更多信息，请参见tf.compat.v1.Session。</p> 
<pre class="has"><code class="language-python">v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()

with tf.compat.v1.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    v.load([2, 3], sess)
    print(v.eval(sess)) # prints [2 3]
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    v.load([3, 4], sess)
    print(v.eval()) # prints [3 4]
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>value</code></strong>:新变量值</li><li>session:用来计算这个变量的会话。如果没有，则使用默认会话。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>ValueError</code></strong>: Session is not passed and no default session</li></ul> 
<h4 id="read_value"><code>read_value</code></h4> 
<pre class="has"><code class="language-python">read_value()
</code></pre> 
<p>返回在当前上下文中读取的此变量的值。与value()不同，如果它在另一个设备上，具有控件依赖关系，等等。</p> 
<p>返回值：</p> 
<ul><li>包含变量值的张量。</li></ul> 
<h4 id="scatter_add"><code>scatter_add</code></h4> 
<pre class="has"><code class="language-python">scatter_add(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。要添加到此变量的indexedslice。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在散射加法完成后保持这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_div"><code>scatter_div</code></h4> 
<pre class="has"><code class="language-python">scatter_div(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>将这个变量除以tf. indexedslice。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。indexedslice将这个变量除以。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在离散除法完成后保留这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_max"><code>scatter_max</code></h4> 
<pre class="has"><code class="language-python">scatter_max(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>用tf的最大值更新此变量。IndexedSlices和本身。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。indexedslice使用这个变量作为max的参数。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在分散最大化完成后持有这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_min"><code>scatter_min</code></h4> 
<pre class="has"><code class="language-python">scatter_min(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>用tf的最小值更新这个变量。IndexedSlices和本身。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。indexedslice使用这个变量作为min的参数。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在离散极小化完成后保持这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_mul"><code>scatter_mul</code></h4> 
<pre class="has"><code class="language-python">scatter_mul(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>将这个变量乘以tf. indexedslice。</p> 
<p>将这个变量乘以tf. indexedslice。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf.indexedslice将这个变量乘以。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<p></p> 
<ul><li>一个张量，它将在散乘完成后保留这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_nd_add"><code>scatter_nd_add</code></h4> 
<pre class="has"><code class="language-python">scatter_nd_add(
    indices,
    updates,
    name=None
)
</code></pre> 
<p>对变量中的单个值或片应用稀疏加法。变量的秩为P，指标是秩为q的张量。指标必须是整数张量，包含自指标。它必须是shape [d_0，…， d_{Q-2}， K]，其中0 &lt; K &lt;= P。索引的最内层维度(长度为K)对应于沿着self的第K个维度的元素索引(如果K = P)或切片索引(如果K &lt; P)。更新量为Q-1+P-K阶张量，形状为:</p> 
<pre class="has"><code class="language-ruby">[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].
</code></pre> 
<p>例如，我们想把4个散射元素加到一个秩为1的张量到8个元素上。在Python中，更新应该是这样的:</p> 
<pre class="has"><code class="language-python"> v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
 indices = tf.constant([[4], [3], [1] ,[7]])
 updates = tf.constant([9, 10, 11, 12])
 add = v.scatter_nd_add(indices, updates)
 with tf.compat.v1.Session() as sess:
      print sess.run(add)
</code></pre> 
<p>v的最终更新如下:</p> 
<pre class="has"><code class="language-python">[1, 13, 3, 14, 14, 6, 7, 20]
</code></pre> 
<p>有关如何更新片的详细信息，请参阅scatter_nd。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>indices</code></strong>:用于操作的索引。</li><li><strong><code>updates</code></strong>:操作中使用的值。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在散射加法完成后保持这个变量的新值。</li></ul> 
<h4 id="scatter_nd_sub"><code>scatter_nd_sub</code></h4> 
<pre class="has"><code class="language-python">scatter_nd_sub(
    indices,
    updates,
    name=None
)
</code></pre> 
<p>假设变量的秩为P，指标是秩为q的张量，指标必须是整数张量，包含自指标。它必须是shape [d_0，…， d_{Q-2}， K]，其中0 &lt; K &lt;= P。索引的最内层维度(长度为K)对应于沿着self的第K个维度的元素索引(如果K = P)或切片索引(如果K &lt; P)。更新量为Q-1+P-K阶张量，形状为:</p> 
<pre class="has"><code class="language-python">[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].
</code></pre> 
<p>例如，我们想把4个散射元素加到一个秩为1的张量到8个元素上。在Python中，更新应该是这样的：</p> 
<pre class="has"><code class="language-python">v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant([9, 10, 11, 12])
op = v.scatter_nd_sub(indices, updates)
with tf.compat.v1.Session() as sess:
   print sess.run(op)
</code></pre> 
<p>v的最终更新如下：</p> 
<pre class="has"><code class="language-python">[1, -9, 3, -6, -6, 6, 7, -4]
</code></pre> 
<p>有关如何更新片的详细信息，请参阅scatter_nd。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>indices</code></strong>:用于操作的索引。</li><li><strong><code>updates</code></strong>:操作中使用的值。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在散差减法完成后保留这个变量的新值。</li></ul> 
<h4 id="scatter_nd_update"><code>scatter_nd_update</code></h4> 
<pre class="has"><code class="language-python">scatter_nd_update(
    indices,
    updates,
    name=None
)
</code></pre> 
<p>对变量中的单个值或片应用稀疏赋值。变量的秩为P，指标是秩为q的张量。指标必须是整数张量，包含自指标。它必须是shape [d_0，…， d_{Q-2}， K]，其中0 &lt; K &lt;= P。索引的最内层维度(长度为K)对应于沿着self的第K个维度的元素索引(如果K = P)或切片索引(如果K &lt; P)。更新量为Q-1+P-K阶张量，形状为：</p> 
<pre class="has"><code class="language-python">[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].
</code></pre> 
<p>例如，我们想把4个散射元素加到一个秩为1的张量到8个元素上。在Python中，更新应该是这样的:</p> 
<pre class="has"><code class="language-python">v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1] ,[7]])
updates = tf.constant([9, 10, 11, 12])
op = v.scatter_nd_assign(indices, updates)
with tf.compat.v1.Session() as sess:
   print sess.run(op)
</code></pre> 
<p>v的最终更新如下:</p> 
<pre class="has"><code class="language-python">[1, 11, 3, 10, 9, 6, 7, 12]
</code></pre> 
<p>有关如何更新片的详细信息，请参阅scatter_nd。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>indices</code></strong>:用于操作的索引。</li><li><strong><code>updates</code></strong>:操作中使用的值。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在分散分配完成后保留这个变量的新值。</li></ul> 
<h4 id="scatter_sub"><code>scatter_sub</code></h4> 
<pre class="has"><code class="language-python">scatter_sub(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>从这个变量索引切片。</p> 
<p><strong>参数：</strong></p> 
<ul><li>sparse_delta: tf。要从该变量中减去的indexedslice。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在散差减法完成后保留这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="scatter_update"><code>scatter_update</code></h4> 
<pre class="has"><code class="language-python">scatter_update(
    sparse_delta,
    use_locking=False,
    name=None
)
</code></pre> 
<p>indexedslice指向这个变量。</p> 
<p><strong>参数值：</strong></p> 
<ul><li>sparse_delta: 要分配给该变量的tf.indexedslice。</li><li>use_lock:如果为真，则在操作期间使用锁定。</li><li>name:操作的名称。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量，它将在分散分配完成后保留这个变量的新值。</li></ul> 
<p><strong>可能产生的异常：</strong></p> 
<ul><li><strong><code>TypeError</code></strong>: if <code>sparse_delta</code> is not an <code>IndexedSlices</code>.</li></ul> 
<h4 id="set_shape"><code>set_shape</code></h4> 
<pre class="has"><code class="language-python">set_shape(shape)
</code></pre> 
<p>覆盖此变量的形状。</p> 
<p><strong>参数：</strong></p> 
<ul><li>shape:表示被覆盖的形状的TensorShape。</li></ul> 
<h4 id="sparse_read"><code>sparse_read</code></h4> 
<pre class="has"><code class="language-python">sparse_read(
    indices,
    name=None
)</code></pre> 
<p>根据索引从params坐标轴中收集切片。这个函数支持tf的一个子集。收集,请参阅特遣部队。</p> 
<p><strong>参数：</strong></p> 
<ul><li><strong><code>indices</code></strong>:指标张量。必须是下列类型之一:int32、int64。必须在range [0, params.shape[axis]]中。</li><li>name:操作的名称(可选)。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个张量。具有与params相同的类型。</li></ul> 
<h4 id="to_proto"><code>to_proto</code></h4> 
<pre class="has"><code class="language-python">to_proto(export_scope=None)
</code></pre> 
<p>将变量转换为VariableDef协议缓冲区。</p> 
<p><strong>参数：</strong></p> 
<ul><li>export_scope:可选的字符串。名称要删除的范围。</li></ul> 
<p><strong>返回值：</strong></p> 
<ul><li>一个VariableDef协议缓冲区，如果变量不在指定的名称范围内，则为None。</li></ul> 
<h4 id="value"><code>value</code></h4> 
<pre class="has"><code>value()
</code></pre> 
<p>返回此变量的最后一个快照。通常不需要调用这个方法，因为所有需要变量值的ops都会通过调用convert_to_张量()自动调用它。返回一个包含变量值的张量。你不能给这个张量赋一个新的值，因为它不是对变量的引用。为了避免复制，如果返回值的使用者与变量位于相同的设备上，那么实际上将返回变量的活动值，而不是复制。消费者可以看到变量的更新。如果使用者在不同的设备上，它将获得变量的副本。</p> 
<p><strong>返回值：</strong></p> 
<ul><li>包含变量值的张量。</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/632139fbe0db1ab8748527efc25e7ed7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux基本操作命令、Vim编辑器、Jenkins、Maven</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c109a0dbf7180d33cf46ce8a9c6ffb8a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">关于使用selenium&#43;python简单获取前程无忧数据</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>