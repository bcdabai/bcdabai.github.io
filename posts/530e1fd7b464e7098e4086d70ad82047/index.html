<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>sklearn 神经网络MLPclassifier参数详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="sklearn 神经网络MLPclassifier参数详解" />
<meta property="og:description" content=" class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)[source] 参数备注hidden_​​layer_sizestuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。激活{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’ 隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 &#43; exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）slover{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：&#39;lbfgs’是准牛顿方法族的优化器；&#39;sgd’指的是随机梯度下降。&#39;adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。alphafloat，可选，默认为0.0001。L2惩罚（正则化项）参数。batch_sizeint，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）learning_rate{‘常数’，‘invscaling’，‘自适应’}，默认’常数&#34;。 用于权重更新。仅在solver =&#39;sgd’时使用。&#39;constant’是’learning_rate_init’给出的恒定学习率；&#39;invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。learning_rate_initdouble，可选，默认为0.001。使用初始学习率。它控制更新权重的步长。仅在solver =&#39;sgd’或’adam’时使用。power_tdouble，可选，默认为0.5。反缩放学习率的指数。当learning_rate设置为“invscaling”时，它用于更新有效学习率。仅在solver =&#39;sgd’时使用。max_iterint，optional，默认值200。最大迭代次数。solver迭代直到收敛（由’tol’确定）或这个迭代次数。对于随机解算器（‘sgd’，‘adam’），请注意，这决定了时期的数量（每个数据点的使用次数），而不是梯度步数。shufflebool，可选，默认为True。仅在solver =&#39;sgd’或’adam’时使用。是否在每次迭代中对样本进行洗牌。random_stateint，RandomState实例或None，可选，默认无随机数生成器的状态或种子。如果是int，则random_state是随机数生成器使用的种子;如果是RandomState实例，则random_state是随机数生成器;如果为None，则随机数生成器是np.random使用的RandomState实例。tolfloat，optional，默认1e-4 优化的容忍度，容差优化。当n_iter_no_change连续迭代的损失或分数没有提高至少tol时，除非将learning_rate设置为’adaptive’，否则认为会达到收敛并且训练停止。verbosebool，可选，默认为False 是否将进度消息打印到stdout。warm_startbool，可选，默认为False，设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。请参阅词汇表。momentumfloat，默认0.9，梯度下降更新的动量。应该在0和1之间。仅在solver =&#39;sgd’时使用。nesterovs_momentum布尔值，默认为True。是否使用Nesterov的势头。仅在solver =&#39;sgd’和momentum&gt; 0时使用。early_stoppingbool，默认为False。当验证评分没有改善时，是否使用提前停止来终止培训。如果设置为true，它将自动留出10％的训练数据作为验证，并在验证得分没有改善至少为n_iter_no_change连续时期的tol时终止训练。仅在solver =&#39;sgd’或’adam’时有效validation_fractionfloat，optional，默认值为0.1。将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用beta_1float，optional，默认值为0.9，估计一阶矩向量的指数衰减率应为[0,1)。仅在solver =&#39;adam’时使用beta_2float，可选，默认为0.999,估计一阶矩向量的指数衰减率应为[0,1)。仅在solver =&#39;adam’时使用epsilonfloat，optional，默认值1e-8, adam稳定性的价值。 仅在solver =&#39;adam’时使用n_iter_no_changeint，optional，默认值10,不符合改进的最大历元数。 仅在solver =&#39;sgd’或’adam’时有效 属性备注classes_array or list of array of shape （n_classes，）每个输出的类标签。loss_float,使用损失函数计算的当前损失。coefs_list，length n_layers - 1,列表中的第i个元素表示对应于层i的权重矩阵。intercepts_list，length n_layers - 1,列表中的第i个元素表示对应于层i &#43; 1的偏置矢量。n_iter_int，迭代次数。n_layers_int,层数。n_outputs_int,输出的个数。out_activation_string，输出激活函数的名称。 方法备注fit（X，y）使模型适合数据矩阵X和目标y。get_params（[deep]）获取此估算器的参数。predict（X）使用多层感知器分类器进行预测predict_log_proba（X）返回概率估计的对数。predict_proba（X）概率估计。score（X，y [，sample_weight]）返回给定测试数据和标签的平均准确度。set_params（** params）设置此估算器的参数。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/530e1fd7b464e7098e4086d70ad82047/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-12T11:07:25+08:00" />
<meta property="article:modified_time" content="2018-10-12T11:07:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">sklearn 神经网络MLPclassifier参数详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-dracula">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <pre><code>class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, 
batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,
 random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, 
 early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)[source]
</code></pre> 
<table><thead><tr><th>参数</th><th>备注</th></tr></thead><tbody><tr><td>hidden_​​layer_sizes</td><td>tuple，length = n_layers - 2，默认值（100，）第i个元素表示第i个隐藏层中的神经元数量。</td></tr><tr><td>激活</td><td>{‘identity’，‘logistic’，‘tanh’，‘relu’}，默认’relu’ 隐藏层的激活函数：‘identity’，无操作激活，对实现线性瓶颈很有用，返回f（x）= x；‘logistic’，logistic sigmoid函数，返回f（x）= 1 /（1 + exp（-x））；‘tanh’，双曲tan函数，返回f（x）= tanh（x）；‘relu’，整流后的线性单位函数，返回f（x）= max（0，x）</td></tr><tr><td>slover</td><td>{‘lbfgs’，‘sgd’，‘adam’}，默认’adam’。权重优化的求解器：'lbfgs’是准牛顿方法族的优化器；'sgd’指的是随机梯度下降。'adam’是指由Kingma，Diederik和Jimmy Ba提出的基于随机梯度的优化器。注意：默认解算器“adam”在相对较大的数据集（包含数千个训练样本或更多）方面在训练时间和验证分数方面都能很好地工作。但是，对于小型数据集，“lbfgs”可以更快地收敛并且表现更好。</td></tr><tr><td>alpha</td><td>float，可选，默认为0.0001。L2惩罚（正则化项）参数。</td></tr><tr><td>batch_size</td><td>int，optional，默认’auto’。用于随机优化器的minibatch的大小。如果slover是’lbfgs’，则分类器将不使用minibatch。设置为“auto”时，batch_size = min（200，n_samples）</td></tr><tr><td>learning_rate</td><td>{‘常数’，‘invscaling’，‘自适应’}，默认’常数"。 用于权重更新。仅在solver ='sgd’时使用。'constant’是’learning_rate_init’给出的恒定学习率；'invscaling’使用’power_t’的逆缩放指数在每个时间步’t’逐渐降低学习速率learning_rate_， effective_learning_rate = learning_rate_init / pow（t，power_t）；只要训练损失不断减少，“adaptive”将学习速率保持为“learning_rate_init”。每当两个连续的时期未能将训练损失减少至少tol，或者如果’early_stopping’开启则未能将验证分数增加至少tol，则将当前学习速率除以5。</td></tr><tr><td>learning_rate_init</td><td>double，可选，默认为0.001。使用初始学习率。它控制更新权重的步长。仅在solver ='sgd’或’adam’时使用。</td></tr><tr><td>power_t</td><td>double，可选，默认为0.5。反缩放学习率的指数。当learning_rate设置为“invscaling”时，它用于更新有效学习率。仅在solver ='sgd’时使用。</td></tr><tr><td>max_iter</td><td>int，optional，默认值200。最大迭代次数。solver迭代直到收敛（由’tol’确定）或这个迭代次数。对于随机解算器（‘sgd’，‘adam’），请注意，这决定了时期的数量（每个数据点的使用次数），而不是梯度步数。</td></tr><tr><td>shuffle</td><td>bool，可选，默认为True。仅在solver ='sgd’或’adam’时使用。是否在每次迭代中对样本进行洗牌。</td></tr><tr><td>random_state</td><td>int，RandomState实例或None，可选，默认无随机数生成器的状态或种子。如果是int，则random_state是随机数生成器使用的种子;如果是RandomState实例，则random_state是随机数生成器;如果为None，则随机数生成器是np.random使用的RandomState实例。</td></tr><tr><td>tol</td><td>float，optional，默认1e-4 优化的容忍度，容差优化。当n_iter_no_change连续迭代的损失或分数没有提高至少tol时，除非将learning_rate设置为’adaptive’，否则认为会达到收敛并且训练停止。</td></tr><tr><td>verbose</td><td>bool，可选，默认为False 是否将进度消息打印到stdout。</td></tr><tr><td>warm_start</td><td>bool，可选，默认为False，设置为True时，重用上一次调用的解决方案以适合初始化，否则，只需擦除以前的解决方案。请参阅词汇表。</td></tr><tr><td>momentum</td><td>float，默认0.9，梯度下降更新的动量。应该在0和1之间。仅在solver ='sgd’时使用。</td></tr><tr><td>nesterovs_momentum</td><td>布尔值，默认为True。是否使用Nesterov的势头。仅在solver ='sgd’和momentum&gt; 0时使用。</td></tr><tr><td>early_stopping</td><td>bool，默认为False。当验证评分没有改善时，是否使用提前停止来终止培训。如果设置为true，它将自动留出10％的训练数据作为验证，并在验证得分没有改善至少为n_iter_no_change连续时期的tol时终止训练。仅在solver ='sgd’或’adam’时有效</td></tr><tr><td>validation_fraction</td><td>float，optional，默认值为0.1。将训练数据的比例留作早期停止的验证集。必须介于0和1之间。仅在early_stopping为True时使用</td></tr><tr><td>beta_1</td><td>float，optional，默认值为0.9，估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用</td></tr><tr><td>beta_2</td><td>float，可选，默认为0.999,估计一阶矩向量的指数衰减率应为[0,1)。仅在solver ='adam’时使用</td></tr><tr><td>epsilon</td><td>float，optional，默认值1e-8, adam稳定性的价值。 仅在solver ='adam’时使用</td></tr><tr><td>n_iter_no_change</td><td>int，optional，默认值10,不符合改进的最大历元数。 仅在solver ='sgd’或’adam’时有效</td></tr></tbody></table> 
<table><thead><tr><th>属性</th><th>备注</th></tr></thead><tbody><tr><td>classes_</td><td>array or list of array of shape （n_classes，）每个输出的类标签。</td></tr><tr><td>loss_</td><td>float,使用损失函数计算的当前损失。</td></tr><tr><td>coefs_</td><td>list，length n_layers - 1,列表中的第i个元素表示对应于层i的权重矩阵。</td></tr><tr><td>intercepts_</td><td>list，length n_layers - 1,列表中的第i个元素表示对应于层i + 1的偏置矢量。</td></tr><tr><td>n_iter_</td><td>int，迭代次数。</td></tr><tr><td>n_layers_</td><td>int,层数。</td></tr><tr><td>n_outputs_</td><td>int,输出的个数。</td></tr><tr><td>out_activation_</td><td>string，输出激活函数的名称。</td></tr></tbody></table> 
<table><thead><tr><th>方法</th><th>备注</th></tr></thead><tbody><tr><td>fit（X，y）</td><td>使模型适合数据矩阵X和目标y。</td></tr><tr><td>get_params（[deep]）</td><td>获取此估算器的参数。</td></tr><tr><td>predict（X）</td><td>使用多层感知器分类器进行预测</td></tr><tr><td>predict_log_proba（X）</td><td>返回概率估计的对数。</td></tr><tr><td>predict_proba（X）</td><td>概率估计。</td></tr><tr><td>score（X，y [，sample_weight]）</td><td>返回给定测试数据和标签的平均准确度。</td></tr><tr><td>set_params（** params）</td><td>设置此估算器的参数。</td></tr></tbody></table>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f127256e29eb7b304ecd534ddaf827cd/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">自己是笨呢?还是笨呢?(第一次使用markdown,发布都不会,总显示不能使用默认标题,终于解决了.)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/31db1be78687d20c1f377733c917e851/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">JSP页面出现Invalid location of tag (div)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>