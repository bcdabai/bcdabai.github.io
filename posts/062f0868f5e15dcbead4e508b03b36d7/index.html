<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习知识点汇总 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习知识点汇总" />
<meta property="og:description" content="一、支持向量机，support vector machine，SVM 找到一个超平面将不同的数据分隔开，并且该使得该超平面到支持向量间的间隔最大化。
函数间隔：在超平面wx&#43;b=0确定的情况下，|y||wx&#43;b|能够表示点x到距离超平面的远近，y*(w*x&#43;b)的正负性表示分类的正确性。几何间隔：即点到超平面的距离， ∣ y ∣ ∣ w ∗ x &#43; b ∣ ∣ ∣ w ∣ ∣ \frac{|y||w*x&#43;b|}{||w||} ∣∣w∣∣∣y∣∣w∗x&#43;b∣​，如果成比例的改变超平面的w和b时几何间隔不变。最终目标函数为：
（1）其中取y的值为1和-1，是为了计算方便和几何意义明确。实际取任何值都可以，只要不同的类的点函数值有不同的符号即可。（2）支持向量刚好在虚线间隔边界上，即y*(wx&#43;b)=1。对于所有不是支持向量的点，则显然有y(w*x&#43;b)&gt;1。目标函数最大化问题转为最小化问题：
（1）目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。（2）可以用现有的Quadratic programming(二次规划)包求解。（3）或者手动求解。通过拉格朗日乘子法，目标函数变成了：
再得到与原问题等价的对偶问题：
（1）为了容易求解，将minmax原始问题转化为maxmin对偶问题，即先求L 对w、b的极小，再求L 对的极大。（2）原问题是满足 KKT 条件的，所以原问题能有最优化解法，所以可以转化成了对偶问题。 α \alpha α固定，分别对w，b求偏导数，再令 ∂L/∂w 和 ∂L/∂b 等于零。求得的结果代入 L ( w , b , α ) L(w,b,\alpha) L(w,b,α)，此时拉格朗日函数只包含了 α \alpha α变量，目标函数为：可以利用SMO算法求解对 α \alpha α的极大，得出最终的w和b。
所以分类超平面为：
（1）对于新点 x的预测，只需要计算它与训练数据点的内积即可（2）同时发现：非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据（3）直观上来理解的话，“支持向量”后方的点对超平面是没有影响的，超平面只与“支持向量”有关。通过引入核函数，将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。从而解决了非线性分类问题，此时的分类超平面为：
（1）核函数为ϕ，该非线性映射函数能将数据数据变换到另一个特征空间。（2）核函数方法：在特征空间中直接计算内积〈φ(xi · φ(x)〉，避开了直接在高维空间中进行计算。（3）手工构造出对应的核函数比较困难，通常直接用一些常用的核函数，如多项式核、高斯核、线性核。 二、核函数 三、从线性回归到逻辑回归，Logistic Regression，LR 线性回归可以建模为（回归问题）：
y = β0 &#43; β1x1 &#43; β2x2 &#43; … &#43; βn*xn &#43; ε" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/062f0868f5e15dcbead4e508b03b36d7/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-12T14:24:13+08:00" />
<meta property="article:modified_time" content="2023-05-12T14:24:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习知识点汇总</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="support_vector_machineSVM_0"></a>一、支持向量机，support vector machine，SVM</h2> 
<p>找到一个超平面将不同的数据分隔开，并且该使得该超平面到支持向量间的间隔最大化。</p> 
<ol><li>函数间隔：在超平面w<em>x+b=0确定的情况下，|y||w</em>x+b|能够表示点x到距离超平面的远近，y*(w*x+b)的正负性表示分类的正确性。</li><li>几何间隔：即点到超平面的距离，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            ∣ 
           
          
            y 
           
          
            ∣ 
           
          
            ∣ 
           
          
            w 
           
          
            ∗ 
           
          
            x 
           
          
            + 
           
          
            b 
           
          
            ∣ 
           
          
          
          
            ∣ 
           
          
            ∣ 
           
          
            w 
           
          
            ∣ 
           
          
            ∣ 
           
          
         
        
       
         \frac{|y||w*x+b|}{||w||} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.53em; vertical-align: -0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight" style="margin-right: 0.0269em;">w</span><span class="mord mtight">∣∣</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.485em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">y</span><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight" style="margin-right: 0.0269em;">w</span><span class="mbin mtight">∗</span><span class="mord mathnormal mtight">x</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，如果成比例的改变超平面的w和b时几何间隔不变。</li><li>最终目标函数为：<br> <img src="https://images2.imgbox.com/45/d1/FttqVBV8_o.png" alt="在这里插入图片描述"><br> （1）其中取y的值为1和-1，是为了计算方便和几何意义明确。实际取任何值都可以，只要不同的类的点函数值有不同的符号即可。（2）支持向量刚好在虚线间隔边界上，即y*(w<em>x+b)=1。对于所有不是支持向量的点，则显然有y</em>(w*x+b)&gt;1。</li><li>目标函数最大化问题转为最小化问题：<br> <img src="https://images2.imgbox.com/51/de/eBkBapST_o.png" alt="在这里插入图片描述"><br> （1）目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。（2）可以用现有的Quadratic programming(二次规划)包求解。（3）或者手动求解。</li><li>通过拉格朗日乘子法，目标函数变成了：<br> <img src="https://images2.imgbox.com/da/e2/rjpXTF8s_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1f/27/qEk6gEYP_o.png" alt="在这里插入图片描述"></li><li>再得到与原问题等价的对偶问题：<br> <img src="https://images2.imgbox.com/ad/72/o2A5zNlM_o.png" alt="在这里插入图片描述"><br> （1）为了容易求解，将minmax原始问题转化为maxmin对偶问题，即先求L 对w、b的极小，再求L 对的极大。（2）原问题是满足 KKT 条件的，所以原问题能有最优化解法，所以可以转化成了对偶问题。</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
       
         \alpha 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>固定，分别对w，b求偏导数，再令 ∂L/∂w 和 ∂L/∂b 等于零。求得的结果代入<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          L 
         
        
          ( 
         
        
          w 
         
        
          , 
         
        
          b 
         
        
          , 
         
        
          α 
         
        
          ) 
         
        
       
         L(w,b,\alpha) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span><span class="mclose">)</span></span></span></span></span>，此时拉格朗日函数只包含了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
       
         \alpha 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>变量，目标函数为：<img src="https://images2.imgbox.com/85/1b/pdawz3oL_o.png" alt="在这里插入图片描述"></li><li>可以利用SMO算法求解对<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
       
         \alpha 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>的极大，得出最终的w和b。<br> <img src="https://images2.imgbox.com/07/b0/CGi2j7mD_o.png" alt="在这里插入图片描述">所以分类超平面为：<br> <img src="https://images2.imgbox.com/32/25/Hjt1Jbut_o.png" alt="在这里插入图片描述"><br> （1）对于新点 x的预测，只需要计算它与训练数据点的内积即可（2）同时发现：非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据（3）直观上来理解的话，“支持向量”后方的点对超平面是没有影响的，超平面只与“支持向量”有关。</li><li>通过引入核函数，将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。从而解决了非线性分类问题，此时的分类超平面为：<img src="https://images2.imgbox.com/45/46/YgoevmTI_o.png" alt="在这里插入图片描述"><br> （1）核函数为ϕ，该非线性映射函数能将数据数据变换到另一个特征空间。（2）核函数方法：在特征空间中直接计算内积〈φ(xi · φ(x)〉，避开了直接在高维空间中进行计算。（3）手工构造出对应的核函数比较困难，通常直接用一些常用的核函数，如多项式核、高斯核、线性核。</li></ol> 
<h2><a id="httpsimgblogcsdnimgcnedb29e8faf4a4c58996877521b064df6png_23"></a>二、核函数<img src="https://images2.imgbox.com/1c/5d/C8KLxllU_o.png" alt="在这里插入图片描述"></h2> 
<p><img src="https://images2.imgbox.com/28/44/2zV5IuLx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/91/08/q9Xbufjf_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Logistic_RegressionLR_26"></a>三、从线性回归到逻辑回归，Logistic Regression，LR</h2> 
<ol><li>线性回归可以建模为（回归问题）：<br> y = β0 + β1x1 + β2x2 + … + βn*xn + ε<br> （1）其中，β0, β1, β2, …, βn 是模型的参数，ε是误差项。<br> （2）求解模型参数： 
  <ul><li>使用最小二乘法来求解参数，使得模型的预测值 ŷ 与真实值 y 的残差平方和最小化</li><li>损失函数的角度：最小化均方差损失函数，用梯度下降法。</li></ul> </li><li>逻辑回归可以建模为（分类问题）：<br> p(y=1|x) = sigmoid(β0 + β1x1 + β2x2 + … + βn*xn)<br> （1）其中，β0, β1, β2, …, βn 是模型的参数，sigmoid函数为：sigmoid(z) = 1 / (1 + exp(-z))，可以将线性回归模型的输出值映射到[0,1]的概率值。<br> （2）求解模型参数： 
  <ul><li>使用最大似然法来求解参数，使得模型对训练数据的似然值最大化<br> <img src="https://images2.imgbox.com/c4/ba/4BzG9qnp_o.png" alt="在这里插入图片描述"></li><li>损失函数的角度：最小化对数损失函数，即<br> <img src="https://images2.imgbox.com/96/8a/RO08Toxv_o.png" alt="在这里插入图片描述"><br> 这时发现，最小化对数损失函数=最小化二分类交叉熵损失函数=最大化对数似然函数，再用梯度下降法求解。</li></ul> </li></ol> 
<h2><a id="_42"></a>四、防止过拟合的方法</h2> 
<p>正则化：经验风险加正则化项来使得结构风险最小化，L1范数会趋向于产生少量的特征，而其他的特征的参数都是0，而 L2会选择更多的特征，这些特征的参数都会接近于0。</p> 
<ol><li>L1范数（Lasso正则化、曼哈顿距离、稀疏规则算子）：是指向量中各个元素绝对值之和。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ∣ 
         
        
          ∣ 
         
        
          w 
         
        
          ∣ 
         
         
         
           ∣ 
          
          
          
            p 
           
          
            = 
           
          
            1 
           
          
         
        
          = 
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
        
          ∣ 
         
         
         
           w 
          
         
           i 
          
         
        
          ∣ 
         
        
       
         ||w||_{p=1}=\sum_{i=1}^{n}|w_i| 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0361em; vertical-align: -0.2861em;"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.104em; vertical-align: -0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span>越小，参数越稀疏，去掉了没有信息的特征，也就是把这些特征对应的权重置为0，实现特征的自动选择。</li><li>L2范数（Ridge正则化、欧氏距离、权值衰减）：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ∣ 
         
        
          ∣ 
         
        
          w 
         
        
          ∣ 
         
         
         
           ∣ 
          
          
          
            p 
           
          
            = 
           
          
            1 
           
          
         
        
          = 
         
         
          
           
           
             ∑ 
            
            
            
              i 
             
            
              = 
             
            
              1 
             
            
           
             n 
            
           
           
           
             w 
            
           
             i 
            
           
             2 
            
           
          
         
        
       
         ||w||_{p=1}=\sqrt{\sum_{i=1}^{n}w_i^2} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0361em; vertical-align: -0.2861em;"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.24em; vertical-align: -0.3027em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9373em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mord" style="padding-left: 1em;"><span class="mop"><span class="mop op-symbol small-op" style="position: relative; top: 0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8043em;"><span class="" style="top: -2.4003em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.2029em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7959em;"><span class="" style="top: -2.4231em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span class="" style="top: -3.0448em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2769em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8973em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
            <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
             <path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path> 
            </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3027em;"><span class=""></span></span></span></span></span></span></span></span></span>越小，参数越小越接近于0，能够缓解过拟合问题。</li></ol> 
<p>Dropout：在训练过程中，随机丢弃一部分神经元，从而使模型对某些特定的输入不敏感，减少了模型的依赖性，避免过拟合。当用训练好的模型进行预测时，设置dropout概率为0或者移除dropout层，此时Dropout是不起作用的。</p> 
<p>Batch Normalization（批归一化）：在每一层的激活函数之前，对每个Batch的数据进行标准化处理，使得每个特征的均值为0、方差为1，所以是在 channel维度上对每个Batch的数据进行标准化。最后，通过缩放和平移操作，将标准化后的特征映射到一个新的范围来增强模型的表达能力。</p> 
<p>Layer Normalization（层归一化）：在每个样本上，即在 C、H 和 W 这 3 个维度上进行标准化处理，即对每个样本的多个特征进行标准化处理，并且针对每个样本的所有特征分别进行缩放和平移（区别：BN针对的是单个特征）。</p> 
<p>Early Stopping：根据交叉叉验证提前终止: 若每次训练前, 将训练数据划分为若干份, 取一份为测试集, 其他为训练集, 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集, 所以此方法称之为交叉验证.。交叉验证的错误率最小时可以认为泛化性能最好, 这时候训练错误率虽然还在继续下降, 但也得终止继续训练了。</p> 
<h2><a id="httpsimgblogcsdnimgcncb1ad09ec2464ef8a838e0aea93d585fpng_54"></a>五、实对称矩阵<img src="https://images2.imgbox.com/6b/af/cplJIxrW_o.png" alt="在这里插入图片描述"></h2> 
<h2><a id="_55"></a>六、决策树</h2> 
<p>ID3 算法中，我们使用信息增益来选择最优分裂特征。信息增益是在当前节点选择某个特征进行分裂后，信息熵减少的程度。<br> <img src="https://images2.imgbox.com/90/bf/on6q4B59_o.png" alt="在这里插入图片描述"><br> C4.5 算法在 ID3 算法的基础上进行了改进，它使用信息增益比来选择最优分裂，以避免属性取值数目较多时出现偏向的情况。<br> <img src="https://images2.imgbox.com/db/44/XiJDjMSS_o.png" alt="在这里插入图片描述"><br> CART 决策树使用基尼指数来计算最优分裂。基尼指数表示随机抽取两个样本，其类别标记不一致的概率，它越小表示样本集的纯度越高。最终，选择基尼指数最小的特征作为当前节点的划分特征。<br> <img src="https://images2.imgbox.com/f9/fc/9g1H1SKY_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="Boost_62"></a>七、Boost</h2> 
<p>Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票);<br> Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权)；</p> 
<ul><li> <p>Adaboost, Adaptive Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数<br> <img src="https://images2.imgbox.com/37/4c/nZHrvVYO_o.png" alt="在这里插入图片描述"></p> </li><li> <p>GBDT, Gradient Boosting Decision Tree，对函数残差近似值进行梯度下降</p> </li><li> <p>XGboost类似于GBDT的优化版, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息，是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数，对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性</p> </li></ul> 
<h2><a id="_70"></a>八、判别式模型和生成式模型</h2> 
<p>判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。</p> 
<ul><li>K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络</li></ul> 
<p>生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。</p> 
<ul><li>朴素贝叶斯、隐马尔可夫模型</li></ul> 
<h2><a id="_76"></a>九、主成分分析</h2> 
<pre><code> -&gt; 输入原始数据矩阵 X
 -&gt; 对 X 进行标准化处理
 -&gt; 计算样本协方差矩阵 S
 -&gt; 对 S 进行特征值分解，得到特征值和特征向量
 -&gt; 将特征向量按照特征值从大到小排序
 -&gt; 选择前 k 个特征向量作为主成分，其中 k 表示保留的主成分数目
 -&gt; 输出主成分方差贡献率和主成分权重向量
</code></pre> 
<p>PCA降维的目的，就是为了在尽量保证“信息量不丢失”的情况下，对原始特征进行降维，也就是尽可能将原始特征往具有最大信息量的维度上进行投影。将原特征投影到这些维度上，使降维后信息量损失最小。</p> 
<p>由于协方差矩阵对称，因此k个特征向量之间两两正交，也就是各主成分之间正交，正交就肯定线性不相关，可消除原始数据成分间的相互影响</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/636b3324bd182554e04df70cefd67946/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于vue的跨域问题配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2bfb6d6e31b37274bc3d95ef0c409cab/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue组件通信</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>