<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>视觉Transformer综述 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="视觉Transformer综述" />
<meta property="og:description" content="前几日，华为诺亚，北京大学，悉尼大学共同发表了论文《A Survey on Visual Transformer》，本文针对其重点内容进行翻译概括如下。
导言：
Transformer是一种主要基于自注意力机制的深度神经网络，最初是在自然语言处理领域中应用的。受到Transformer强大的表示能力的启发，研究人员提议将Transformer扩展到计算机视觉任务。与其他网络类型（例如卷积网络和循环网络）相比，基于Transformer的模型在各种视觉基准上显示出竞争性的甚至更好的性能。
在本文中，我们通过将这些Visual Transformer模型按不同的任务分类，并分析了这些方法的优缺点，提供了文献综述。主要类别包括基本图像分类，high-level vision，low-level vision和视频处理。由于self-attention是Transformer的基本组成部分，因此也简要回顾了计算机视觉中的自注意力。包括将Transformer推向实际应用的有效Transformer方法。最后，我们讨论了Visual Transformer的进一步研究方向。
1. 总体概述 卷积神经网络（CNN）引入了卷积层和池化层以处理图像等位移不变性的数据。递归神经网络（RNN）利用循环单元来处理顺序数据或时间序列数据。Transformer是一种新提出的神经网络，主要利用自我注意机制提取内在特征。在这些网络中，Transformer是最近发明的神经网络(2017年)，对于广泛的人工智能应用具有巨大的潜力。
Transformer最初应用于自然语言处理（NLP）任务，并带来了显着的改进。例如，Vaswani等人首先提出了一种仅基于注意力机制来实现机器翻译和英语选区解析任务的Transformer。Devlin等人引入了一种称为BERT的新语言表示模型，该模型通过共同限制左右上下文来预训练未标记文本的翻译器。BERT在当时的11个NLP任务上获得了SOTA结果。Brown等人在45TB压缩明文数据上预训练了基于巨型Transformer的GPT-3模型，该模型具有1,750亿个参数，并且无需微调即可在不同类型的下游自然语言任务上实现出色的性能。这些基于Transformer的模型显示了强大的表示能力，并在NLP领域取得了突破。
受NLP中Transformer功能的启发，最近的研究人员将Transformer扩展到计算机视觉（CV）任务。CNN曾经是视觉应用中的基本组件，但是Transformer显示出了其作为CNN替代品的能力。Chen等人训练一个sequence Transformer去自动回归预测像素并在图像分类任务上与CNN取得竞争性结果。ViT是Dosovitskiy等人最近提出的Visual Transformer模型。ViT将一个纯粹的transformer直接用于图像块序列，并在多个图像识别基准上获得SOTA性能。除了基本的图像分类，transformer还用于解决更多计算机视觉问题，例如目标检测，语义分割，图像处理和视频理解。由于其出色的性能，提出了越来越多基于transformer的模型来改善各种视觉任务。
为了获得更好的方便研究人员研究不同的主题，我们按其应用场景对transformer模型进行分类，如表1所示。特别是，主要主题包括基本图像分类，high-level vision，low-level vision和视频处理。high-level vision对图像中可以看到的内容进行提取和解析，例如对象检测，分割和车道检测。已经有许多用于解决这些high-level vision任务的transformer模型，例如DETR，deformanble DETR用于物体检测，以及Max-DeepLab用于分割。low-level vision主要涉及从图像（通常通常表示为图像本身）中提取描述子，其典型应用包括超分辨率，图像去噪和风格迁移。在 low-level vision的transformer研究很少，需要进行更多的研究。
由于视频的顺序特性，transformer可以自然地应用于视频。与传统的CNN或RNN相比，Transformer开始在这些任务上显示出具有竞争力的性能。在这里，我们对基于transformer的视觉模型给出了一个综述，以跟上该领域的进展。Visual transformer的开发时间表如图1所示。
2. Transformer结构 详情请看另一篇文章《transformer解读》
3. Visual Transformer 在本节中，我们将对基于transformer的计算机视觉模型进行全面回顾，包括图像分类，high-level vision，low-level vision和视频处理中的应用。我们还简要总结了自注意力机制和模型压缩方法在高效transformer中的应用。
3.1 Image Classification
受到transformer在自然语言处理方面取得巨大成功的启发，一些研究人员试图检验相似的模型是否可以学习图像的有用表示。 作为一种比文本更高维度，更嘈杂，更冗余的形式，人们认为图像很难生成模型。 iGPT 和ViT是仅使用transformer进行图像分类的两个模型。
3.1.1 iGPT
自从最初的生成式图像预训练方法浪潮问世以来，已经有很长时间了。Chen等人重新检查这类方法，并结合自我监督方法的最新进展。该方法包括一个预训练阶段，然后是一个fine-tune阶段。在预训练中，应尝试使用自回归和BERT目标。此外，在NLP中应用sequence transformer体系结构来预测像素而不是语言标记。预训练使得训练可以尽早地停止，因此可被视为有利的初始化或正则化。在fine-tune过程中，他们向模型添加了一个小的分类头，该分类头用于优化分类目标并适应所有权重。
给定一个由高维数据x =（x1，...，xn）组成的未标记数据集X。他们通过最小化数据的负对数似然来训练模型：
其中p（x）是图像数据的密度，可以将其建模为：
其中1 &lt;= i &lt;= n采用单位置换 πi = i，也称为栅格顺序。他们还考虑了BERT目标，该目标对子序列M⊂[1，n]进行采样，以使每个索引i独立地具有在M中出现的0.15的概率。M称为BERT掩码，模型为 通过最小化“unMasked”元素x [1，n] \ M上的“Masked”元素x[1,n]\M的负对数似然来训练：
在预训练中，他们选择LAR或Lbert中的一种，并在预训练数据集上最大程度地减少损失。他们使用transformer decoder模块的GPT-2公式表示。特别是，layer norms先于注意力和多层感知器（MLP）操作，并且所有操作都严格按照残差路径进行。 序列元素之间唯一的混合发生在注意力操作中，并且为了确保在训练AR目标时进行适当的调节，他们将标准的上三角mask应用于attention logits的n×n矩阵。当使用BERT目标时，不需要attention logits mask：将内容嵌入应用于输入序列后，它们会将位置清零。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/36679b525332d328f7e68b6fa042d81f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-26T23:48:57+08:00" />
<meta property="article:modified_time" content="2020-12-26T23:48:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">视觉Transformer综述</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>前几日，华为诺亚，北京大学，悉尼大学共同发表了论文《A Survey on Visual Transformer》，本文针对其重点内容进行翻译概括如下。</p> 
<p></p> 
<p>导言：</p> 
<p>    Transformer是一种主要基于自注意力机制的深度神经网络，最初是在自然语言处理领域中应用的。受到Transformer强大的表示能力的启发，研究人员提议将Transformer扩展到计算机视觉任务。与其他网络类型（例如卷积网络和循环网络）相比，基于Transformer的模型在各种视觉基准上显示出竞争性的甚至更好的性能。</p> 
<p>    在本文中，我们通过将这些Visual Transformer模型按不同的任务分类，并分析了这些方法的优缺点，提供了文献综述。主要类别包括基本图像分类，high-level vision，low-level vision和视频处理。由于self-attention是Transformer的基本组成部分，因此也简要回顾了计算机视觉中的自注意力。包括将Transformer推向实际应用的有效Transformer方法。最后，我们讨论了Visual Transformer的进一步研究方向。</p> 
<p></p> 
<h4>1. 总体概述</h4> 
<p>    卷积神经网络（CNN）引入了卷积层和池化层以处理图像等位移不变性的数据。递归神经网络（RNN）利用循环单元来处理顺序数据或时间序列数据。Transformer是一种新提出的神经网络，主要利用自我注意机制提取内在特征。在这些网络中，Transformer是最近发明的神经网络(2017年)，对于广泛的人工智能应用具有巨大的潜力。</p> 
<p>    Transformer最初应用于自然语言处理（NLP）任务，并带来了显着的改进。例如，Vaswani等人首先提出了一种仅基于注意力机制来实现机器翻译和英语选区解析任务的Transformer。Devlin等人引入了一种称为BERT的新语言表示模型，该模型通过共同限制左右上下文来预训练未标记文本的翻译器。BERT在当时的11个NLP任务上获得了SOTA结果。Brown等人在45TB压缩明文数据上预训练了基于巨型Transformer的GPT-3模型，该模型具有1,750亿个参数，并且无需微调即可在不同类型的下游自然语言任务上实现出色的性能。这些基于Transformer的模型显示了强大的表示能力，并在NLP领域取得了突破。</p> 
<p>    受NLP中Transformer功能的启发，最近的研究人员将Transformer扩展到计算机视觉（CV）任务。CNN曾经是视觉应用中的基本组件，但是Transformer显示出了其作为CNN替代品的能力。Chen等人训练一个sequence Transformer去自动回归预测像素并在图像分类任务上与CNN取得竞争性结果。ViT是Dosovitskiy等人最近提出的Visual Transformer模型。ViT将一个纯粹的transformer直接用于图像块序列，并在多个图像识别基准上获得SOTA性能。除了基本的图像分类，transformer还用于解决更多计算机视觉问题，例如目标检测，语义分割，图像处理和视频理解。由于其出色的性能，提出了越来越多基于transformer的模型来改善各种视觉任务。</p> 
<p>为了获得更好的方便研究人员研究不同的主题，我们按其应用场景对transformer模型进行分类，如表1所示。特别是，主要主题包括基本图像分类，high-level vision，low-level vision和视频处理。high-level vision对图像中可以看到的内容进行提取和解析，例如对象检测，分割和车道检测。已经有许多用于解决这些high-level vision任务的transformer模型，例如DETR，deformanble DETR用于物体检测，以及Max-DeepLab用于分割。low-level vision主要涉及从图像（通常通常表示为图像本身）中提取描述子，其典型应用包括超分辨率，图像去噪和风格迁移。在 low-level vision的transformer研究很少，需要进行更多的研究。</p> 
<p>由于视频的顺序特性，transformer可以自然地应用于视频。与传统的CNN或RNN相比，Transformer开始在这些任务上显示出具有竞争力的性能。在这里，我们对基于transformer的视觉模型给出了一个综述，以跟上该领域的进展。Visual transformer的开发时间表如图1所示。</p> 
<p><img alt="图片" height="224" src="https://images2.imgbox.com/4f/78/93hRYi1b_o.png" width="655"></p> 
<p></p> 
<p></p> 
<h4><strong>2. Transformer结构</strong></h4> 
<p>详情请看另一篇文章《<a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484764&amp;idx=1&amp;sn=f0edac037a2eda25675947b1a6e4cd95&amp;chksm=c197ba42f6e03354d3f931b706d440215669763b15a11495441897cc564ab4772fe1b015a8fb&amp;scene=21#wechat_redirect" rel="nofollow">transformer解读</a>》</p> 
<p></p> 
<h4><strong>3. Visual Transformer </strong></h4> 
<p>在本节中，我们将对基于transformer的计算机视觉模型进行全面回顾，包括图像分类，high-level vision，low-level vision和视频处理中的应用。我们还简要总结了自注意力机制和模型压缩方法在高效transformer中的应用。</p> 
<p><img alt="" height="377" src="https://images2.imgbox.com/96/7b/0wldKMAx_o.png" width="880"></p> 
<p><strong>3.1 Image Classification</strong></p> 
<p>受到transformer在自然语言处理方面取得巨大成功的启发，一些研究人员试图检验相似的模型是否可以学习图像的有用表示。 作为一种比文本更高维度，更嘈杂，更冗余的形式，人们认为图像很难生成模型。 iGPT 和ViT是仅使用transformer进行图像分类的两个模型。</p> 
<p><strong>3.1.1 iGPT</strong></p> 
<p>自从最初的生成式图像预训练方法浪潮问世以来，已经有很长时间了。Chen等人重新检查这类方法，并结合自我监督方法的最新进展。该方法包括一个预训练阶段，然后是一个fine-tune阶段。在预训练中，应尝试使用自回归和BERT目标。此外，在NLP中应用sequence transformer体系结构来预测像素而不是语言标记。预训练使得训练可以尽早地停止，因此可被视为有利的初始化或正则化。在fine-tune过程中，他们向模型添加了一个小的分类头，该分类头用于优化分类目标并适应所有权重。</p> 
<p>给定一个由高维数据x =（x1，...，xn）组成的未标记数据集X。他们通过最小化数据的负对数似然来训练模型：</p> 
<p><img alt="图片" src="https://images2.imgbox.com/c7/13/mEvCq2c7_o.png"></p> 
<p>其中p（x）是图像数据的密度，可以将其建模为：</p> 
<p><img alt="图片" height="81" src="https://images2.imgbox.com/e9/15/rsiNs9zp_o.png" width="438"></p> 
<p>其中1 &lt;= i &lt;= n采用单位置换 πi = i，也称为栅格顺序。他们还考虑了BERT目标，该目标对子序列M⊂[1，n]进行采样，以使每个索引i独立地具有在M中出现的0.15的概率。M称为BERT掩码，模型为 通过最小化“unMasked”元素x [1，n] \ M上的“Masked”元素x[1,n]\M的负对数似然来训练：</p> 
<p><img alt="图片" height="60" src="https://images2.imgbox.com/9d/c4/Cpvy2Czm_o.png" width="404"></p> 
<p>在预训练中，他们选择LAR或Lbert中的一种，并在预训练数据集上最大程度地减少损失。他们使用transformer decoder模块的GPT-2公式表示。特别是，layer norms先于注意力和多层感知器（MLP）操作，并且所有操作都严格按照残差路径进行。 序列元素之间唯一的混合发生在注意力操作中，并且为了确保在训练AR目标时进行适当的调节，他们将标准的上三角mask应用于attention logits的n×n矩阵。当使用BERT目标时，不需要attention logits mask：将内容嵌入应用于输入序列后，它们会将位置清零。</p> 
<p>在微调期间，它们平均将最终lay norm n(L)的输出在整个序列维上合并，以每个示例提取特征的d维向量：</p> 
<p><img alt="" height="50" src="https://images2.imgbox.com/4e/57/5s3ejk6E_o.png" width="435"></p> 
<p>他们学习了从f(L)到class logit的投影，该投影用于最小化交叉熵损失L(CLF)。 在实践中，他们凭经验发现联合目标L(GEN) + L(CLF)的效果更好，其中L(GEN)∈{L(AR)，L(BERT)}。(此处括号内的内容为下标)</p> 
<p></p> 
<p><strong>3.1.2 ViT</strong></p> 
<p>  最近，Dosovitskiy等人提出了一种pure transformer，即Vision Transformer（ViT），当直接应用于图像块的序列时，它在图像分类任务上表现良好。 它们尽可能遵循原始transformer的设计。 下图显示了Vision Transformer的框架。</p> 
<p><img alt="图片" height="413" src="https://images2.imgbox.com/39/1e/BdSFWbRQ_o.png" width="751"></p> 
<p>为了处理2D图像，将图像x∈R H×W×C整形为一系列平坦的2D块x（p）∈R N×（P^2·C）。  （H，W）是原始图像的分辨率，（P，P）是每个图像块的分辨率。 那么N = HW / P^2是该Transformer的有效序列长度。 由于Transformer的所有层都使用恒定的宽度，因此可训练的线性投影将每个矢量化路径映射到模型尺寸D，它们的输出称为patch embeddings。</p> 
<p>与BERT的[class]标记类似，可学习的embedding被用于embedding块的序列，其在Transformer encoder输出处的状态用作图像表示。在预训练和微调过程中，分类头的大小均相同。另外，将一维position embeddings添加到embeddings块中以保留位置信息。 他们对position embeddings的2D感知方法有不同的探索，它们没有比标准的1D position embeddings获得明显的收益。 joint embeddings作为encoder的输入。 值得注意的是，Vision Transformer仅使用标准Transformer的encoder，而MLP头后面是Transformer encoder的输出。</p> 
<p>通常，首先在大型数据集上对ViT进行预训练，然后对较小的下游任务进行微调。 为此，去除预训练的预测头，并附加一个零初始化的D×K前馈层，其中K是下游类别的数量。 以比预训练更高的分辨率进行微调通常是有益的。 喂入更高分辨率的图像时，色块大小保持不变，这会导致更大的有效序列长度。 Vision Transformer可以处理任意序列长度，但是预训练的position embeddings可能不再有意义。 因此，作者根据预训练position embeddings在原始图像中的位置执行2D插值。 请注意，只有将分辨率调整和色块提取手动插入有关图像2D结构的感应偏置时，才能将其手动注入到Vision Transformer中。</p> 
<p>当在中等大小的数据集（如ImageNet）上训练时，此类模型产生较差的结果，与可比较大小的ResNets相比，精度要低几个百分点。Transformer缺乏CNN固有的一些归纳偏置（inductive biases），例如翻译等方差和局部性，因此在训练不足的数据量时不能很好地概括。 但是，如果在大型数据集上训练模型（14M-300M图像），则图片会发生变化。作者发现，大规模训练胜过归纳偏置。进行足够规模的预训练并转移到数据点较少的任务时，Transformer可获得出色的结果。 在JFT-300M数据集上进行过预训练的Vision Transformer在多个图像识别基准上达到或优于SOTA，在ImageNet上达到88.36％，在CIFAR-10上达到99.50％，在CIFAR-100上达到94.55％，以及77.16％ 在VTAB套件中包含19个任务。表3显示了iGPT和ViT的详细结果</p> 
<p><img alt="图片" height="349" src="https://images2.imgbox.com/37/20/iaIia0oq_o.png" width="793"></p> 
<p>总之，iGPT回顾了生成式预训练方法，并将其与自我监督方法相结合，结果并不十分令人满意。ViT取得了更好的结果，特别是当它使用更大的数据集（JFT-300）时。但是，ViT的结构与NLP中的Transformer基本相同 ,如何在块内部(intra-patch)和块之间(inter-patch)明确相关性仍然是一个具有挑战性的问题。此外，在ViT中将相同大小的块（patch）视为相等。 众所周知，每个块的复杂性是不同的，并且该特性目前尚未得到充分利用。</p> 
<p></p> 
<p><strong>3.2 High-level Vision</strong></p> 
<p>最近，人们对采用Transformer执行High-level计算机视觉任务（例如对象检测，车道检测和分段）的兴趣日益增加。 在本节中，我们将对这些方法进行回顾。</p> 
<p></p> 
<p><strong>3.2.1 Object Detection</strong></p> 
<p>根据采用Transformer体系结构的模块，可以将基于Transformer的目标检测方法粗略地分为neck-based, head-based 和 framework-based的方法。</p> 
<p>多尺度特征融合模块（在现代检测框架中称为neck），例如特征金字塔网络（FPN），已广泛用于目标检测中，以实现更好的检测性能。 张etc 建议传统方法无法交互跨尺度特征，因此提出了特征金字塔transformer（FPT）来充分利用跨空间和尺度的特征相互作用。FPT由三种类型的Transformer组成，即self-transformer, grounding transformer 和 rendering transformer，它们分别对特征金字塔的self-level, top-down 和 bottom-up路径的信息进行编码。FPT基本上利用Transformer中的自注意模块来增强特征金字塔网络的特征融合。</p> 
<p>预测头对于物体检测器起着重要的作用。 先前的检测方法通常利用单个视觉表示（例如边界框和角点）来预测最终结果。Chi等提出了桥接视觉表示（Bridging Visual Representations --BVR），通过多头关注模块将不同的异构表示(heterogeneous representations)组合成一个单一的表示。 具体来说，将主表示作为query输入，将辅助表示作为key输入。通过类似于Transformer中的注意模块，可以获得用于主表示的增强功能，该功能将来自辅助表示的信息桥接起来并有利于最终检测性能。</p> 
<p>与上述利用Transformer的方法增强了现代检测器的特定模块不同，Carion 重新设计了目标检测框架，并提出了detection Transformer（DETR），它是一个简单且完全端到端的目标探测器。DETR将目标检测任务视为一个直观的集合预测问题，摆脱了传统的手工制作组件，例如锚点生成和非最大抑制（NMS）后处理。</p> 
<p><img alt="图片" height="271" src="https://images2.imgbox.com/11/ab/zOETZwsI_o.png" width="741"></p> 
<p>如图6所示，DETR从CNN主干开始以从输入图像中提取特征。为了用位置信息补充图像特征，将固定的位置编码添加到平坦的十个特征中，然后再馈入编码解码器转换器。Transformer解码器将编码器的embedding与N个学习的positional endcodings（object queries）一起使用，并生成N个output embeddings，其中N是预定义参数，通常大于图像中对象的数量。最终预测是通过简单前馈网络（FFN）计算的，其中包括边界框坐标和类别标签以指示对象的特定类别或没有对象。与原始Transformer顺序生成预测的原始Transformer不同，DETR同时解码N个对象。DETR采用两部分匹配算法来分配预测的和真实的对象。如方程式（11）所示。匈牙利损失(Hungarian loss)被用来计算所有匹配对象对的损失函数。</p> 
<p><img alt="图片" height="82" src="https://images2.imgbox.com/84/27/wBBCUFVt_o.png" width="517"></p> 
<p>其中y和yˆ分别是ground truth和目标的预测，σˆ是最优分配，ci和P^(Ci)是目标类别标签和预测标签，bi和b^(i)分别是ground truth和预测边界框，DETR在物体检测方面表现出令人印象深刻的性能，其精度和速度与COCO基准上广受欢迎且与公认的Faster R-CNN基线相当。</p> 
<p>DETR是基于Transformer的目标检测框架的新设计，为社区开发完整的端到端检测器提供了参考。 但是，vanilla DETR也面临一些挑战，例如，训练时间较长，小目标的性能较差。Zhu等人提出的Deformable DETR是解决上述问题的一种流行方法，并且大大提高了检测性能。代替通过原始的多头注意力在Transformer中查看图像特征图上的所有空间位置，建议使用可变形注意模块（deformable attention module）来关注参考点周围的一小组关键位置。 这样，大大降低了计算复杂度，也有利于快速收敛。 更重要的是，可变形注意模块可轻松应用于融合多尺度功能。 与DETR相比，可变形DETR的性能更好，训练成本降低了10倍，推理速度提高了1.6倍。deformable DETR还应用了一些其他改进，包括有效的迭代边界框优化方法和two-stage方案，从而进一步提高了性能。</p> 
<p>针对DETR的高计算复杂性问题，Zheng等人提出了一种自适应聚类变压器（Adaptive Clustering Transformer--ACT）来减少预训练的DETR的计算成本，而无需任何训练过程。 ACT使用局部敏感度哈希方法(locality sensitivity hashing method)自适应地对queries特征进行聚类，并将注意力输出广播到由所选原型表示的queries。 通过将预训练的DETR模型的自我注意模块替换为ACT，而无需进行任何重新训练，几乎不降低精度，可以显着降低计算成本。此外，可以通过使用多任务知识蒸馏（multi-task knowledge distillation--MTKD）方法进一步降低性能下降，该方法利用原始Transformer以少量的fine-tuning时间来蒸馏ACT模块。</p> 
<p>Sun等人研究了DETR模型的慢收敛问题，并揭示了Transformer解码器中的交叉注意模块是主要原因。为此，提出了DETR的纯编码器（encoder-only）版本，并且在检测精度和训练收敛性上实现了相当大的改进。 此外，设计了一种新的二分匹配方案，以实现更稳定的训练和更快的收敛。 提出了两种基于Transformer的集合预测模型，以改进具有特征金字塔的纯编码器DETR，即TSP-FCOS和TSP RCNN，它们比原始DETR模型具有更好的性能。</p> 
<p>Dai等人在自然语言处理中受到预训练Transformer方案的启发，提出了一种用于目标检测的无监督预训练DETR（UP-DETR）的方法。具体而言，提出了一种新的无监督借口任务，即随机query patch检测，以预训练DETR模型。通过这种方案，UP-DETR大大提高了在相对较小的数据集（即PASCAL VOC）上的检测精度。 在具有足够训练数据的COCO基准上，UP-DETR仍然无法完成DETR，这证明了无监督的预训练计划的有效性。</p> 
<p></p> 
<p><strong>3.2.2 Segmentation</strong></p> 
<p>DETR 可以自然扩展到全景分割任务，方法是在解码器上附加一个掩码头（mask head），从而获得有竞争力的结果。  Wang等提出了Max-DeepLab来直接预测通过mask transformer实现的全景分割结果，而无需替代诸如box detection之类的子任务。 与DETR相似，Max-DeepLab以端到端的方式简化了全景分割任务，并直接预测了一组不重叠的masks和相应的标签。 使用全景率（PQ）样式损失来训练模型。此外，与现有的将transformer堆叠在CNN主干网上方的方法不同，Max-DeepLab采用双路径框架（dual-path framework）将CNN与transformer更好地结合在一起。</p> 
<p>Wang等提出了一种基于transformer的视频实例分割（transformer-based video instance segmentation --VisTR）模型，该模型以图像序列作为输入并产生相应的姿态预测结果。提出了实例序列匹配策略，为预测分配ground truth。 要获取每个实例的掩码序列，VisTR利用实例序列分割模块从多个帧中累积掩码特征，并使用3D CNN分割掩码序列。</p> 
<p>也有尝试将transformer用于姿态分割，这是基于DETR全景分割模型的。所提出的Cell-DETR还添加了skip connections，以在分段头中从主干CNN和CNN解码器桥接功能，以获得更好的融合功能。 Cell-DETR显示了来自显微图像的cell实例分割的SOTA性能。</p> 
<p>Zhao等设计了一种新颖的Transformer架构（Point Transformer）来处理点云。所提出的自我注意层对于点集的排列是不变的，因此适用于点集处理任务。 Point Transformer对于3D点云的语义分割任务显示出强大的性能。</p> 
<p></p> 
<p><strong>3.2.3 Lane Detection</strong></p> 
<p>Liu等基于PolyLaneNet 提出通过学习transformer网络的全局上下文来提高曲线车道检测的性能。 与Poly LaneNet相似，提出的方法（LSTR）将车道检测视为将车道与多项式拟合的任务，并使用神经网络预测多项式的参数。为了捕获用于车道和全局环境的细长结构，LSTR将transformer网络引入到架构中，以处理通过卷积神经网络提取的低级特征。 此外，LSTR使用匈牙利损失来优化网络参数。与仅使用0.2倍参数的PolyLaneNet相比，LSTR可以实现2.82％的更高精度和3.65倍FPS。transformer网络、卷积神经网络和匈牙利损失的结合实现了一个微小，快速而精确的车道检测框架。</p> 
<p></p> 
<h4><strong>3.3 Low-level Vision</strong></h4> 
<p>除了High-level Vision任务外，很少有研究将transformer应用于low-level vision领域，例如图像超分辨率，图像生成等。与以标签或框为输出的分类，分割和检测相比，low-level vision任务 通常将图像作为输出（例如，高分辨率图像或去噪图像），这更具有挑战性。</p> 
<p>Parmar等迈出第一步，推广transformer模型来制定图像转换和生成任务，并提出Image transformer。 Image transformer由两部分组成：用于提取图像表示的编码器和用于生成像素的解码器。对于值为0 - 255的每个像素，将学习256×d维embeddings，以将每个值编码为d维向量，将其作为编码器的输入。编码器和解码器的架构与《Advances in neural information processing systems》中的相同。解码器中每一层的详细结构如图7所示。</p> 
<p></p> 
<p><img alt="图片" height="750" src="https://images2.imgbox.com/3a/74/zBLCVrI5_o.png" width="677"></p> 
<p>每个输出像素q0是通过计算输入像素q与先前生成的像素m1，m2，...之间的自注意力而生成的，并带有position embedding p1，p2，...。 ..对于图像条件生成，例如超分辨率和修复，使用了编码器-解码器体系结构，其中编码器的输入是低分辨率图像或损坏的图像。对于无条件和分类条件图像生成（即图像噪声），纯解码器用于输入噪声向量。由于用于解码器的输入是先前生成的像素，当生成高分辨率图像时，它将产生大量的计算成本，因此提出了一种局部自注意方案，该方案仅使用最近生成的像素作为解码器的输入。结果，Image transformer可以在图像生成和翻译任务上使用基于CNN的模型来获得竞争性能，这表明基于transformer的模型在low-level vision任务上的有效性。</p> 
<p>与将每个像素用作transformer模型的输入相比，最近的工作使用patch（像素集）作为输入。杨等提出了用于图像超分辨率的Texture transformer网络（TTSR）。他们在基于参考的图像超分辨率问题中使用了转换器体系结构，该问题旨在将相关纹理从参考图像转移到低分辨率图像。以低分辨率图像和参考图像作为查询Q和关键字K，计算Q中每个patch qi和K中ki之间的相关性ri，j，</p> 
<p><img alt="图片" src="https://images2.imgbox.com/60/b8/mzXMLq9W_o.png"></p> 
<p>然后提出了一种硬性注意力（hard-attention）模块，以利用参考图像根据参考图像选择高分辨率特征V以匹配低分辨率图像。hard-attention map是通过下式计算</p> 
<p><img alt="图片" height="56" src="https://images2.imgbox.com/ad/d5/B4no8rRy_o.png" width="368"></p> 
<p>那么来自的最相关的参考patch是ti = ，其中T中的ti是转移的特征。之后，使用软注意力模块将V转移到低分辨率特征F。可以通过以下方式计算软注意力：</p> 
<p><img alt="图片" height="56" src="https://images2.imgbox.com/5d/03/kiTVogxV_o.png" width="374"></p> 
<p>因此，将高分辨率纹理图像转换为低分辨率图像的公式可以表示为：</p> 
<p><img alt="图片" height="61" src="https://images2.imgbox.com/90/67/3yJIawZG_o.png" width="541"></p> 
<p>其中Fout和F表示低分辨率图像的输出和输入特征，S是柔和注意力，T是从高分辨率纹理图像传递的特征。通过引入基于transformer的体系结构，TTSR可以成功地将纹理信息从高分辨率参考图像传输到低分辨率图像，以进行超分辨率任务。</p> 
<p>上述方法在单个任务上使用变压器模型，而Chen等人提出了图像处理transformer（Image Processing Transformer--IPT），通过使用大规模的预训练来充分利用transformer的优势，并在包括超分辨率，降噪和去除降水在内的多个图像处理任务中实现SOTA性能 。</p> 
<p><img alt="图片" height="478" src="https://images2.imgbox.com/2d/50/doJ00RrH_o.png" width="786"></p> 
<p>如图8所示，IPT由multi-head、encoder、decoder和multi-tails组成。 介绍了用于不同图像处理任务的多头多尾结构和任务嵌入。 将特征分为patch以放入编码器-解码器体系结构，然后将输出reshape为具有相同大小的功能。 由于transformer模型在大规模预训练中显示出优势，因此IPT使用ImageNet数据集进行预训练。具体来说，通过手动添加噪声，雨滴或下采样将ImageNet数据集中的图像降级为生成的损坏图像。然后将降级的图像用作IPT的输入，并将干净的图像作为输出的优化目标。还引入了一种自我监督的方法来增强IPT模型的泛化能力。然后，使用相应的头，尾和任务嵌入对每个任务微调训练后的模型。 IPT极大地提高了图像处理任务的性能（例如，图像降噪任务中的2dB），这证明了基于transformer的模型在low-level vision领域中的巨大潜力。</p> 
<p></p> 
<h4><strong>3.4 Video Processing</strong></h4> 
<p>transformer在基于序列的任务（特别是在NLP任务）上表现出奇的出色。 在计算机视觉中，视频任务中青睐时空维度信息。 因此，transformer适用于许多视频任务，例如帧合成，动作识别和视频检索。</p> 
<p></p> 
<p><strong>3.4.1 High-level Video Processing</strong></p> 
<p><strong>01</strong> Human Action Recognition</p> 
<p>视频人类行为任务是指识别和定位视频中的人类行为。 上下文的内容在识别人类行为中起着至关重要的作用。  Rohit等提出了一种行为transformer来对感兴趣的人和周围事物之间的潜在关系进行建模。 具体来说，I3D用作提取高级特征图的主干。通过ROI Pooling从中间feature maps中提取的features被视为查询（Q）。 关键字（K），值（V）由中间特征计算得出。 自我注意机制在三个组成部分上进行，并输出分类和回归预测。Lohit等提出了一个可解释的微分模块，称为时间transformer网络，以减少类内方差并增加类间方差。Fayyaz和Gall提出了一个时间transformer，以在弱监督的环境下执行动作识别任务。</p> 
<p></p> 
<p><strong>02</strong> Face Alignment</p> 
<p>  基于视频的面部对齐任务旨在定位面部标志。 时间依赖性和空间信息对于最终性能很重要。 但是，前一种方法无法同时捕获连续帧上的时间信息和静止帧上的互补空间信息。 刘等人使用双流transformer网络分别学习时间和空间特征，以端到端的方式共同优化两个流，并对特征进行加权以获得最终预测。</p> 
<p></p> 
<p><strong>03</strong> Video Retrieval</p> 
<p>基于内容的视频检索的关键是找到视频之间的相似性。 为了克服缺点，仅利用视频级功能的图像级，Shao等人建议使用transformer来建模长范围语义依赖性。此外，引入了有监督的对比学习策略用于hard negative mining。 基准数据集上的结果证明了性能和速度优势。  Gabeur等提出了一种多模式transformer，以学习不同的跨模式提示，以表示视频。</p> 
<p></p> 
<p><strong>04</strong> Acitivity Recognition</p> 
<p>活动识别是指识别一组人的活动。 解决此问题的前一种方法是基于各个参与者的位置。  Gavrilyuk等提出了一个actor-transformer架构来学习表示。actor-transformer将2D和3D网络生成的静态和动态表示作为输入。transformer的输出是预测活动。</p> 
<p></p> 
<p><strong>05</strong> Video Object Detection</p> 
<p>要从视频中检测对象，需要全局和局部信息。 Chen等引入了内存增强的全局局部聚集（memory enhanced global-local aggregation--MEGA）以捕获更多内容。 表示性特征可增强整体性能并解决无效和不足的问题。  Yin等提出了一种时空transformer来聚合时空信息。 与另一个空间特征编码组件一起，这两个组件在3D视频对象检测任务中表现良好。</p> 
<p></p> 
<p><strong>06</strong> Multi-task Learning</p> 
<p>未修剪的视频通常包含许多与目标任务无关的帧。 因此，挖掘相关信息并删除冗余信息至关重要。 为了应对在未修剪的视频上进行多任务学习，Seong等人采用视频多任务transformer网络提取信息。 对于CoVieW数据集，任务是场景识别，动作识别和重要性得分预测。 ImageNet和Places365上的两个经过预先训练的网络提取了场景特征和对象特征。 堆叠多任务transformer以借助类转换矩阵（class conversion matrix --CCM）融合功能。</p> 
<p></p> 
<h4><strong>3.4.2 Low-level Video Processing</strong></h4> 
<p><strong>01</strong> Frame/Video Synthesis</p> 
<p>  帧合成任务是指在两个连续帧之间或帧序列之后合成帧。 视频合成任务旨在合成视频。 刘等提出了ConvTrans模型，该模型包括五个部分：特征嵌入，位置编码，编码器，查询解码器和综合前馈网络。 与基于LSTM的研究相比，ConvTransformer通过更可并行化的架构实现了更高的结果。  Schatz等使用循环Transformer网络从新颖的观点合成人类的行为。</p> 
<p></p> 
<p><strong>02</strong> Video Inpainting</p> 
<p>  视频修补任务旨在完成帧中的缺失区域。 这项艰巨的任务需要沿空间和时间维度合并信息。  Zeng等为此任务提出了一个时空transformer网络。 所有输入帧均作为输入，并被并行填充。 时空对抗损失用于优化transformer网络。</p> 
<p></p> 
<p><strong>3.4.3 Multimodality</strong></p> 
<p><strong>01</strong> Video Captioning/Summarization</p> 
<p>     视频字幕任务的目标是为未修剪的视频生成文本。事件检测和描述模块是两个主要部分。 zhou等提出了一种端到端的优化transformer来解决密集的视频字幕任务。 编码器将视频转换为表示形式。 提议解码器根据编码生成事件提议。 字幕解码器使用提案掩盖编码并输出描述。 Bilkhu等使用C3D和I3D网络提取特征并使用transformer生成预测。 该算法在单个摘要任务和密集摘要任务上均表现良好。 Li等利用基于注意力纠缠（ETA）模块的transformer来解决图像字幕任务。Sun等提出了一种视觉语言框架来学习表示而无需监督。 该模型可以应用于许多任务，包括视频字幕，动作分类等。</p> 
<p></p> 
<h4><strong>3.5 Self-attention for Computer Vision</strong></h4> 
<p>在以上各节中，我们回顾了将transformer体系结构用于视觉任务的方法。 自我注意是transformer的关键部分。 在本节中，我们深入研究了基于自我注意的方法来应对计算机视觉中的挑战性任务，例如语义分割，姿势分割，对象检测，关键点检测和深度估计。</p> 
<p>该部分内容请详见《<a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484764&amp;idx=1&amp;sn=f0edac037a2eda25675947b1a6e4cd95&amp;chksm=c197ba42f6e03354d3f931b706d440215669763b15a11495441897cc564ab4772fe1b015a8fb&amp;scene=21#wechat_redirect" rel="nofollow">transformer解读</a>》的前几页ppt，里面有关于attention与self-attention的详细介绍。</p> 
<p></p> 
<h4><strong>3.6 Efficient Transformer</strong></h4> 
<p>尽管Transformer模型在各种任务中都取得了成功，但仍需要高内存和计算资源，这阻碍了在资源受限的设备（例如，手机端）上的实现。 在本节中，我们回顾有关压缩和加速变压器模型以有效实施的研究，包括网络修剪，低秩分解，知识分配，网络量化，压缩体系结构设计。</p> 
<p>  表4列出了一些用于压缩基于Transformer的模型的代表性作品。</p> 
<p><img alt="图片" src="https://images2.imgbox.com/9d/4e/3ez4vJSY_o.png"></p> 
<p></p> 
<h4><strong>3.6.1 Pruning &amp; Decomposition</strong></h4> 
<p>在基于Transformer的预训练模型（例如BERT）中，并行进行多个注意操作以独立地建模不同标记(tokens)之间的关系，而对于特定任务则不需要所有的heads。Michel等从经验上观察到，可以在测试时去除很大比例的注意力头，而不会显着影响性能。 所需的attention heads在不同的层上有所不同，甚至对于某些层来说，一个 head就足够了。 考虑到attention heads上的冗余，在[85]中定义了重要性分数以估计每个head对最终输出的影响，并且可以删除不重要的head以进行有效部署。 Dalvi等进一步从两个角度分析了预训练Transformer模型中的冗余，即一般冗余和特定于任务的冗余。 遵循彩票假设( lottery ticket hypothesis)等，Prasanna等分析了BERT中的彩票，并表明在基于Transformer的模型中也存在良好的子网。 在[96]中减少了FFN层和attention heads，以实现高压缩率。</p> 
<p>除了Transformer模型的宽度外，还可以减小深度（即层数）以加快推理过程。 与可以按同等方式计算Transformer模型中不同attention heads的情况不同，由于下一层的输入取决于前一层的输出，因此必须顺序计算不同的层。 Fan等提出了一种逐层丢弃的策略来规范模型的训练，然后在测试阶段将整个层次一起删除。考虑到不同设备中的可用资源可能会有所不同，Hou等人提出自适应地减小预定义Transformer模型的宽度和深度，并同时获得具有不同尺寸的多个模型。重要的attention heads和神经元通过重新布线机制在不同的子网络之间共享。</p> 
<p>  除了直接在Transformer模型中丢弃零件模块的修剪方法之外，矩阵分解还旨在根据低秩假设，对具有多个小矩阵的大矩阵进行近似。 例如，Wang等分解Transformer模型中的标准矩阵乘法并获得更有效的推论。</p> 
<p></p> 
<h4><strong>3.6.2 Knowledge Distillation</strong></h4> 
<p>知识蒸馏的目的是通过从巨大的教师网络中转移知识来训练学生网络。 与教师网络相比，学生网络通常具有较薄和较浅的体系结构，更易于在资源受限的资源上部署。 神经网络的输出和中间特征也可以用于将有效信息从教师传递给学生。Mukherjee等人基于Transformer模型，使用经过预训练的BERT 作为老师，借助大量未标记的数据来指导小型模型的训练。Wang等训练学生网络，以模仿预训练教师模型中的自我注意层的输出。 values的点积被引入作为指导学生的一种新的知识形式。 在[127]中也引入了一个助教[86]，它可以减小大型预训练的Transformer模型与紧凑的学生网络之间的差距，从而使模仿变得更加容易。 考虑到Transformer模型中的各种类型的层（即，自我注意层，嵌入层，预测层），Jiao等人设计不同的目标功能，以将知识从教师传授给学生。 例如，学生模型的嵌入层的输出将通过MSE损失模仿教师的输出。 还施加了可学习的线性变换，以将不同的特征映射到同一空间。 对于预测层的输出，采用KL散度来度量不同模型之间的差异。</p> 
<p></p> 
<p><strong>3.6.3 Quantization</strong></p> 
<p>量化的目的是减少表示网络权重或中间特征的位数。 通用神经网络的量化方法已得到充分讨论，其性能可与原始网络媲美。 最近，如何对Transformer模型进行特殊量化已经引起了人们的广泛关注。  Shridhar等建议将输入嵌入到二进制高维向量中，然后使用二进制输入表示来训练二进制神经网络。  Cheong等通过低位（例如4位）表示来表示Transformer模型中的权重。 zhao等对各种量化方法进行了实证研究，结果表明k均值量化具有巨大的发展潜力。 针对机器翻译任务，Prato等人提出了一种完全量化的Transformer，这是第一个8位质量模型，而论文中声称没有翻译质量损失。</p> 
<p></p> 
<p><strong>3.6.4 Compact Architecture Design</strong></p> 
<p>除了将预定义的Transformer模型压缩为小型Transformer模型外，一些工作还尝试直接设计紧凑模型。 Jiang等通过提出一个新的模块，称为基于跨度的动态卷积(span-based dynamic convolution)，简化了自注意力的计算，该模块结合了完整的连接层和卷积层，如图9所示。</p> 
<p></p> 
<p><img alt="图片" src="https://images2.imgbox.com/26/67/7pWroS4k_o.png"></p> 
<p> 卷积运算可计算出不同的标记（tokens），这比标准Transformer中密集的全连接层有效得多。 深度卷积还用于进一步降低计算成本。  Interesting hamburger layers在[1]中提出，它使用矩阵分解来代替原始的自我注意层。 矩阵分解可以比标准的自我注意操作更有效地计算，同时很好地反映了不同标记之间的依赖性。</p> 
<p>Transformer模型中的自注意操作计算给定序列中不同输入标记的表示之间的点积（图像识别任务中的patch），其复杂度为O（N），其中N为序列的长度。 近来，大量方法致力于将复杂度降低到O（N），以使Transformer模型可扩展到长序列。 例如，Katharopoulos等将自我注意近似为内核特征图的线性点积，并通过递归神经网络揭示标记之间的关系。Zaheer等将每个标记（tokens）视为图形中的一个顶点，两个标记之间的内积计算称为边。 启发式图论和各种稀疏图被组合在一起以近似Transformer模型中的稠密图，这也实现了O（N）的复杂度。 从理论上讲，Yun等证明具有O（N）复杂度的稀疏Transformer足以反映标记之间的任何类型的关系并且可以进行通用逼近，这为进一步研究具有O（N）复杂度的Transformer提供了理论保证。</p> 
<p></p> 
<h4><strong>4. Conclusions and Future Prospects</strong></h4> 
<p>与卷积神经网络相比，由于其竞争性能和巨大的潜力，Transformer正成为计算机视觉领域的热门话题。 正如综述所概述的那样，为了揭示和利用Transformer的能力，近年来提出了许多解决方案。 这些方法在各种视觉任务（包括基本图像分类，high-level vision，low-level vision和视频处理）上均表现出出色的性能。 然而，用于计算机视觉的Transformer的潜力尚未得到充分的探索，还有一些挑战有待解决。尽管研究人员已经提出了许多基于Transformer的模型来解决计算机视觉任务，但是这些工作是开创性的解决方案，还有很大的改进空间。例如，ViT 中的Transformer架构遵循NLP 的标准Transformer。专门针对CV的改进版本仍有待探索。 此外，还需要将Transformer用于除上述任务之外的更多任务。</p> 
<p>此外，大多数现有的Visual transformer模型都设计用于处理单个任务。 许多NLP模型（例如GPT-3）已显示出Transformer可以在一个模型中处理多个任务的能力。  CV领域的IPT 也能够处理多种low-level vision任务，例如超分辨率，图像降噪和排水。 我们相信，一个模型可以涉及更多任务。</p> 
<p>Last but not the least，为CV开发有效的Transformer模型也是一个未解决的问题。 Transformer模块通常非常庞大且计算量很大，例如，基本的ViT模型需要18B FLOP来处理图像。 相比之下，轻巧的CNN模型Ghost Net仅用大约600M FLOP就能达到类似的性能。 尽管已经提出了几种压缩Transformer的方法，但是它们的复杂性仍然很大。 这些最初为NLP设计的方法可能不适用于CV。 因此，高效的Transformer模型是在资源受限的设备上部署Visual Transformer的基础。</p> 
<p></p> 
<p></p> 
<p><strong> 本文来源于公众号 CV技术指南 的论文分享系列。</strong></p> 
<p>​<strong>欢迎关注公众号 CV技术指南 ，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读。</strong></p> 
<p><strong> 在公众号中回复关键字 “技术总结” 可获取以下文章的汇总pdf。</strong></p> 
<h3><img alt="" height="257" src="https://images2.imgbox.com/4a/02/I1SWPtDQ_o.png" width="259"></h3> 
<h3><strong>其它文章</strong></h3> 
<p id="activity-name"><a href="https://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486760&amp;idx=2&amp;sn=b446c38f790024176de44a11af6645ad&amp;chksm=c197b236f6e03b2002658c50cc8fff9d8c775f4c4a2a44704fdcffa34867e096e7c9428ab94d&amp;token=398786529&amp;lang=zh_CN#rd" rel="nofollow">北京大学施柏鑫：从审稿人视角，谈谈怎么写一篇CVPR论文</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486760&amp;idx=1&amp;sn=b1d6de5eee7b2a2f854f247b8a15236e&amp;chksm=c197b236f6e03b20fc1046909f0ce76581afddca2f95bbac3511ea5216e8d5059e95514650a5#rd" rel="nofollow">Siamese network总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486584&amp;idx=1&amp;sn=fc6311b49b94311db3432bd01b386671&amp;chksm=c197b366f6e03a7000a2ffef9d6948725dd8a9a0d9ec3197f1d41ce9f0590a09603565326b76&amp;scene=21#wechat_redirect" rel="nofollow">计算机视觉专业术语总结(一)构建计算机视觉的知识体系</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486560&amp;idx=1&amp;sn=517620a49f279304aa13c0cffa95b977&amp;chksm=c197b37ef6e03a68fc2277248f475ca21a2e019d1b752b81892361cc5dc9cf10cf3b2cf25353&amp;scene=21#wechat_redirect" rel="nofollow">欠拟合与过拟合技术总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486031&amp;idx=1&amp;sn=dce380f7e1829eba69ce63db897e5106&amp;chksm=c197b551f6e03c470105f7265eb05e949f23ba66f32c322b97bee808cbc070bc09b66c7178a9&amp;scene=21#wechat_redirect" rel="nofollow">归一化方法总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485238&amp;idx=1&amp;sn=61b69ae9d511651908bb842ba1790dd5&amp;chksm=c197b828f6e0313eafe201d5090d3bd2aac78cc28ad144b04c7e90fd2c776c176d88b4de40b8&amp;scene=21#wechat_redirect" rel="nofollow">论文创新的常见思路总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485154&amp;idx=1&amp;sn=6a04e40b35cc0ebec98c87c39293eacd&amp;chksm=c197b9fcf6e030ea1f923cbb5ded3b86ff4511a5eddbdbe056a0aef05982185a9b23ba9bf9bc&amp;scene=21#wechat_redirect" rel="nofollow">CV方向的高效阅读英文文献方法总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486489&amp;idx=2&amp;sn=9f3ce8bbe51863f8c412d04ee11f0065&amp;chksm=c197b307f6e03a1110121aeda7f8b20d41c26b16ce4475428d23dbde3fabe19957f5ff94c965&amp;scene=21#wechat_redirect" rel="nofollow">计算机视觉中的小样本学习综述   </a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485993&amp;idx=1&amp;sn=2ead8ce5fa5b78082d03cead4d297f72&amp;chksm=c197b537f6e03c21387ad73f86dadd839605b2e11b69c18a115cf836432786467e02e039b078&amp;scene=21#wechat_redirect" rel="nofollow">知识蒸馏的简要概述</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485527&amp;idx=3&amp;sn=8aa8845f431cd118faaba9ba55d03dcb&amp;chksm=c197b749f6e03e5f80b5f9e7f7a7471b9297134f7a2093f789b31c915f486a7388c864f4990e&amp;scene=21#wechat_redirect" rel="nofollow">优化OpenCV视频的读取速度</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484615&amp;idx=1&amp;sn=195658aefb0c73220a506cc96a81745f&amp;chksm=c197bbd9f6e032cf7fc64f420ccff8bc80311f4647b7f5a40fad17ebd8f802422007f15a21b7&amp;scene=21#wechat_redirect" rel="nofollow">NMS总结</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247486538&amp;idx=1&amp;sn=c5f02b00c04fe4479ef0222736212824&amp;chksm=c197b354f6e03a42ce9cbf23d6066b87f078e657f592d1f5402fb80584fc18b4b52008bf1b81&amp;scene=21#wechat_redirect" rel="nofollow">损失函数技术总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485330&amp;idx=1&amp;sn=c4f4b6f94d216e19deb7531bde005222&amp;chksm=c197b88cf6e0319a621a78e87aa6dc0a40188473990407e9578b5d5f743a8b1a28db7c08dbc8&amp;scene=21#wechat_redirect" rel="nofollow">注意力机制技术总结</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485189&amp;idx=1&amp;sn=b97a3eef8e62ea8c9e5463069c07fa1a&amp;chksm=c197b81bf6e0310d9ad5eec929c533a2e8c5827e80c9f7295f00b06ac753d4eef199b2a736c4&amp;scene=21#wechat_redirect" rel="nofollow">特征金字塔技术总结</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484944&amp;idx=1&amp;sn=d6f30af010fd5189ecb8b592ec493478&amp;chksm=c197b90ef6e03018597160ebc24e69f6ba7de9753a762e07fac8b76763c63ba24c2257ad74b2&amp;scene=21#wechat_redirect" rel="nofollow">池化技术总结</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485145&amp;idx=1&amp;sn=bc838906390d3c7c6b802b96f25245b7&amp;chksm=c197b9c7f6e030d1b07f6d531366e86c51d25247c2bec8286533c9d1e42661510d7ae567b49c&amp;scene=21#wechat_redirect" rel="nofollow">数据增强方法总结</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485066&amp;idx=1&amp;sn=a7ff1299338c75d77a736a93459ac765&amp;chksm=c197b994f6e03082331f787357b441e5858f6ec8d6da7f065d15c27f3f8360c510178421ec92&amp;scene=21#wechat_redirect" rel="nofollow">CNN结构演变总结（一）经典模型</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485093&amp;idx=1&amp;sn=cca7d51786155ec4416df2c021144586&amp;chksm=c197b9bbf6e030ad11fb1dd0dfa3ea4df0594db9ac6f25eb043cf962f7d261dd691d895b0f4d&amp;scene=21#wechat_redirect" rel="nofollow">CNN结构演变总结（二）轻量化模型</a> </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485112&amp;idx=1&amp;sn=0169db45e1b98f384e8ad964ff6cac0c&amp;chksm=c197b9a6f6e030b0a0828a7dff5ad325be988585275645c0cf071647b7bb781833cb484d1fd6&amp;scene=21#wechat_redirect" rel="nofollow">CNN结构演变总结（三）设计原则</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485037&amp;idx=1&amp;sn=d49d568b0924d48ab7c25f745d4b47ab&amp;chksm=c197b973f6e03065b4b2a1b528a6f725bc77571794f1a23eb4ca7accb9d5627e6e5d28cce839&amp;scene=21#wechat_redirect" rel="nofollow">如何看待计算机视觉未来的走向</a>   </p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484970&amp;idx=1&amp;sn=5031cd5c13bde6d9d67c02039c2afdc8&amp;chksm=c197b934f6e030220bafa4823e2accca6e5a719e323e0c9050aac2a2466c0cb3872244b32954&amp;scene=21#wechat_redirect" rel="nofollow">CNN可视化技术总结（一）-特征图可视化</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247484987&amp;idx=1&amp;sn=d48f6267fe336d46ff26473abd052f89&amp;chksm=c197b925f6e03033282196c4e883db5716f86fca934edecc487c0287e058eb21aa09d228ab29&amp;scene=21#wechat_redirect" rel="nofollow">CNN可视化技术总结（二）-卷积核可视化</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485006&amp;idx=1&amp;sn=5991ec380dd97a0f0fb3337b9df4abc5&amp;chksm=c197b950f6e0304641c2cacdcc45925e526f6a1187943fb1eca26dfaaf516ca71c4988097eb9&amp;scene=21#wechat_redirect" rel="nofollow">CNN可视化技术总结（三）-类可视化</a></p> 
<p><a href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247485026&amp;idx=1&amp;sn=9c8ca1cf2f6a06f9c25b9a4d2b565161&amp;chksm=c197b97cf6e0306a45d8fe7e5d297b0ef9dd3f2f8c2445df5ba6950325c7b05db27201454463&amp;scene=21#wechat_redirect" rel="nofollow">CNN可视化技术总结（四）-可视化工具与项目</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1d1df573c1c42e37fb1202757f1282b7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">bigdecimal保留4位小数_BigDecimal类</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/65156331321ada1d816dbcab129c1a6c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Gradle】Starting a Gradle Daemon, 1 incompatible and 5 stopped Daemons could not be reused</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>