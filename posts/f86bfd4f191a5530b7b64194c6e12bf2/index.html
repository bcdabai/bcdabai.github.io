<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Logistic Regression 之基础知识准备 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Logistic Regression 之基础知识准备" />
<meta property="og:description" content="转载：http://www.cnblogs.com/daniel-D/ 0. 前言 这学期 Pattern Recognition 课程的 project 之一是手写数字识别，之二是做一个网站验证码的识别(鸭梨不小哇）。面包要一口一口吃，先尝试把模式识别的经典问题——手写数字识别做出来吧。这系列博客参考deep learning tutorial ，记录下用以下三种方法的实现过程:
Logistic Regression - using Theano for something simpleMultilayer perceptron - introduction to layerDeep Convolutional Network - a simplified version of LeNet5 目的在于一方面从理论上帮自己理顺思路，另一方面作为课程最后一课 presentation 的材料。这三种方法从易到难，准确率也由低到高，我们先从简单的入手。
1. Binomial logistic regression model 尽管线性分类器方法足够简单并且使用广泛，但是线性模型对于输出的 y 没有界限，y 可以取任意大或者任意小（负数）的值，对于某些问题来说不够 adequate, 比如我们想得到 0 到 1 之间的 probability 输出，这时候就要用到比 linear regression 更加强大的 logistic regression 了。
y = w • x 直觉上，一个线性模型的输出值 y 越大，这个事件 P(Y=1|x) 发生的概率就越大。 另一方面，我们可以用事件的几率（odds）来表示事件发生与不发生的比值，假设发生的概率是 p ，那么发生的几率（odds）是 p/(1-p) ， odds 的值域是 0 到正无穷，几率越大，发生的可能性越大。将我们的直觉与几率联系起来的就是下面这个（log odds）或者是 logit 函数 (有点牵强 - -!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f86bfd4f191a5530b7b64194c6e12bf2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-03-10T09:50:43+08:00" />
<meta property="article:modified_time" content="2016-03-10T09:50:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Logistic Regression 之基础知识准备</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3 style="margin-top:10px; font-size:21px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; background-color:rgb(214,211,214)"> 转载：<span style="color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:21.111112594604492px; background-color:rgb(214,211,214)">http://www.cnblogs.com/daniel-D/ </span></h3> 
<h3 style="margin-top:10px; font-size:21px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; background-color:rgb(214,211,214)"> 0. 前言</h3> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   这学期 Pattern Recognition 课程的 project 之一是手写数字识别，之二是做一个网站验证码的识别(鸭梨不小哇）。面包要一口一口吃，先尝试把模式识别的经典问题——手写数字识别做出来吧。这系列博客参考<a target="_blank" href="http://deeplearning.net/tutorial/" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">deep learning tutorial</a> ，记录下用以下三种方法的实现过程:</p> 
<ol style="padding-left:40px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)"><li style="font-size:10pt; list-style-type:decimal"><a target="_blank" href="http://deeplearning.net/tutorial/logreg.html#logreg" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">Logistic Regression</a> - using Theano for something simple</li><li style="font-size:10pt; list-style-type:decimal"><a target="_blank" href="http://deeplearning.net/tutorial/mlp.html#mlp" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">Multilayer perceptron</a> - introduction to layer</li><li style="font-size:10pt; list-style-type:decimal"><a target="_blank" href="http://deeplearning.net/tutorial/lenet.html#lenet" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">Deep Convolutional Network</a> - a simplified version of LeNet5</li></ol> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   目的在于一方面从理论上帮自己理顺思路，另一方面作为课程最后一课 presentation 的材料。这三种方法从易到难，准确率也由低到高，我们先从简单的入手。</p> 
<h3 id="binomial-logistic-regression-model" style="margin-top:10px; font-size:21px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; background-color:rgb(214,211,214)"> 1. Binomial logistic regression model</h3> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   尽管<a target="_blank" href="http://en.wikipedia.org/wiki/Linear_classification" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">线性分类器</a>方法足够简单并且使用广泛，但是线性模型对于输出的 y 没有界限，y 可以取任意大或者任意小（负数）的值，对于某些问题来说不够 adequate, 比如我们想得到 0 到 1 之间的 probability 输出，这时候就要用到比 linear regression 更加强大的 logistic regression 了。</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/75/1a/fI4tlrwI_o.png" alt="" style="border:0px; max-width:900px"> y = w • x 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   直觉上，一个线性模型的输出值 y 越大，这个事件 P(Y=1|x) 发生的概率就越大。 另一方面，我们可以用事件的几率（odds）来表示事件发生与不发生的比值，假设发生的概率是 p ，那么发生的几率（odds）是 p/(1-p) ， odds 的值域是 0 到正无穷，几率越大，发生的可能性越大。将我们的直觉与几率联系起来的就是下面这个（log odds）或者是 logit 函数 (<em>有点牵强 - -!</em>)：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/1e/e0/bovFFJip_o.gif" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   进而可以求出概率 p 关于 w 点乘 x 的表示：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/73/fa/b6gvyfIO_o.gif" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   <em>(注：为什么要写出中间一步呢？看到第三部分的你就明白啦!)</em></p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   这就是传说中的 <a target="_blank" href="http://en.wikipedia.org/wiki/Sigmoid_function" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">sigmoid function</a> 了，以 w 点乘 x 为自变量，函数图像如下:</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/d8/f4/XUKy6xUK_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <p style="margin:10px auto"><em>(注：从图中可以看到 wx 越大，p 值越高，在线性分类器中，一般 wx = 0 是分界面，对应了 logistic regression 中 p = 0.5)</em></p> 
</blockquote> 
<h3 id="parameter-estimation" style="margin-top:10px; font-size:21px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; background-color:rgb(214,211,214)"> 2. Parameter Estimation</h3> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   Logsitic regression 输出的是分到每一类的概率，参数估计的方法自然就是最大似然估计 (MLE) 咯。对于训练样本来说，假设每个样本是独立的，输出（标签）为 y = {0, 1}，样本的似然函数就是将所有训练样本 label 对应的输出节点上的概率相乘, 令 p = P(Y=1|x) ,如果 y = 1, 概率就是 p， 如果 y = 0, 概率就是 1 - p ，（<em>好吧，我是个罗嗦的家伙</em>),  将这两种情况合二为一，得到似然函数：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/9e/12/zHkL7O93_o.gif" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   嗯？有连乘，用对数化为累加， balabala 一通算下来，就得到了对数似然函数为</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/7f/8e/8FGVhv7L_o.gif" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   应用梯度下降法或者是拟牛顿法对 L(w) 求极大值，就可以得到 w 的估计值了。</p> 
<h3 id="softmax-regression" style="margin-top:10px; font-size:21px; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; background-color:rgb(214,211,214)"> 3. Softmax regression</h3> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   这一小节旨在弄清 softmax regression 和 logistic regression 的联系，更多细节请参考 Andrew Ng 的英文<a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/Softmax_Regression" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">资料</a>，需要快速浏览也可以看看中文<a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">翻译版本</a>，或者 wiki 一下。</p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   logistic regression 在多类上的推广又叫 softmax regression， 在数字手写识别中，我们要识别的是十个类别，每次从输入层输入 28×28 个像素，输出层就可以得到本次输入可能为 0, 1, 2… 的概率。得花点时间画个简易版本的，看起来更直观:</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/fc/94/DNBmviym_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   OK, 左边是输入层，输入的 x 通过中间的黑线 w (包含了 bias 项)作用下，得到 w.x, 到达右边节点， 右边节点通过红色的函数将这个值映射成一个概率，预测值就是输入概率最大的节点，这里可能的值是 {0, 1, 2}。在 softmax regression 中，输入的样本属于第 j 类的概率可以写成：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/54/6c/nCEJIiLu_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   <em>(注：对比第一部分提到过的中间一步，有什么不同？)</em>   </p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">      注意到，这个回归的参数向量减去一个常数向量，会有什么结果：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/bf/3d/HiZcwgTZ_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   没有变化！这说明如果某一个向量是代价函数的极小值点，那么这个向量在减去一个任意的常数向量也是极小值点，这是因为 softmax 模型被过度参数化了。<em>(题外话：回想一下在线性模型中，同时将 w 和 b 扩大两倍，模型的分界线没有变化，但是模型的输出可信度却增大了两倍，而在训练迭代中， w 和 b 绝对值越来越大，所以 SVM 中就有了函数距离和几何距离的概念)</em></p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   既然模型被过度参数化了，我们就事先确定一个参数，比如将 w1 替换成全零向量，将 w1.x = 0 带入 binomial softmax regression ，得到了我们最开始的二项 logistic regression (可以动手算一算), 用图就可以表示为</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/eb/00/SxFW2fRC_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   <em>(注：虚线表示为 0 的权重，在第一张图中没有画出来，可以看到 logistic regression 就是 softmax regression 的一种特殊情况)</em></p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   在实际应用中，为了使算法实现更简单清楚，往往保留所有参数，而不任意地将某一参数设置为 0。我们可以对代价函数做一个改动：加入权重衰减 (weight decay)。 权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。并且此时代价函数变成了严格的凸函数， Hessian矩阵变为可逆矩阵，保证有唯一的解。<strong>(感觉与线性分类器里限制 ||w|| 或者设置某一个 w 为全零向量一样起到了减参的作用，但是这个计算起来简单清晰，可以用高斯分布的 MAP 来推导，其结果是将 w 软性地限制在超球空间，有一点 “soft” 的味道，个人理解^ ^)</strong></p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   加入权重衰减后的代价函数是：</p> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <img src="https://images2.imgbox.com/ec/0d/adDav2yV_o.png" alt="" style="border:0px; max-width:900px"> 
</blockquote> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   等号右边第一项是训练样本 label 对应的输出节点上的概率的负对数，第二项是 weight decay ，可以使用梯度下降法和 L-BFGS 等算法可以保证收敛到全局最优解。总结起来，logistic regression 是 softmax regression 的一种特殊形式，前者是二类问题，后者是多类问题，前者的非线性函数的唯一确定的 sigmoid function (默认 label 为 0 的权重为全零向量的推导结果), 后者是 softmax, 有时候我们并不特意把它们区分开来。</p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">       好，基础知识准备完毕，下面我们就要在数字手写识别项目里面实战一下了。<em>(^_^)</em></p> 
<p style="margin:10px auto; color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)">   </p> 
<hr style="color:rgb(51,51,51); font-family:Verdana,Arial,sans-serif,'Lucida Grande'; font-size:13.333333969116211px; line-height:25.55555534362793px; background-color:rgb(214,211,214)"> 
<blockquote style="background-color:rgb(214,211,214); border:1px solid rgb(244,244,244); padding:0px 11px; margin-top:10px; margin-bottom:10px; color:rgb(91,91,91); font-size:13.333333969116211px; line-height:20px; font-family:Verdana,Arial,sans-serif,'Lucida Grande'"> 
 <p style="margin:10px auto">参考资料：</p> 
 <p style="margin:10px auto">[1] 统计学习方法， 李航 着</p> 
 <p style="margin:10px auto">[2] <a target="_blank" href="http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial" rel="nofollow noopener noreferrer" style="color:rgb(0,102,255); text-decoration:none">http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial</a> <span style="line-height:1.5"> </span></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0e0bbaeccec54a43708a76ccc0f6d233/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">mySQL中索引index详解</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1837448530ac5112233ec6145666d6f8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C#中keybd_event 用法 模拟键盘鼠标按键 打印</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>