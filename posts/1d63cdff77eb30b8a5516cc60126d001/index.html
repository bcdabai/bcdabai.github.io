<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目）" />
<meta property="og:description" content="转自：史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目）_3D视觉工坊-商业新知 (shangyexinzhi.com)来源丨自动驾驶之心
摘要 3D目标分割是计算机视觉中的一个基本且具有挑战性的问题，在自动驾驶、机器人、增强现实和医学图像分析等领域有着广泛的应用。它受到了计算机视觉、图形和机器学习社区的极大关注。传统上，3D分割是用人工设计的特征和工程方法进行的，这些方法精度较差，也无法推广到大规模数据上。在2D计算机视觉巨大成功的推动下，深度学习技术最近也成为3D分割任务的首选。近年来已涌现出大量相关工作，并且已经在不同的基准数据集上进行了评估。本文全面调研了基于深度学习的3D分割的最新进展，涵盖了150多篇论文。论文总结了最常用的范式，讨论了它们的优缺点，并分析了这些分割方法的对比结果。并在此基础上，提出了未来的研究方向。
如图1第二行所示，3D分割可分为三种类型：语义分割、实例分割和部件分割。
论文的主要贡献如下：
本文是第一篇全面涵盖使用不同3D数据表示（包括RGB-D、投影图像、体素、点云、网格和3D视频）进行3D分割的深度学习综述论文；
论文对不同类型的3D数据分割方法的相对优缺点进行了深入分析；
与现有综述不同，论文专注于专为3D分割设计的深度学习方法，并讨论典型的应用领域；
论文对几种公共基准3D数据集上的现有方法进行了全面比较，得出了有趣的结论，并确定了有前景的未来研究方向。
图2显示了论文其余部分的组织方式：
基准数据集和评估指标 3D分割数据集 数据集对于使用深度学习训练和测试3D分割算法至关重要。然而，私人收集和标注数据集既麻烦又昂贵，因为它需要领域专业知识、高质量的传感器和处理设备。因此，构建公共数据集是降低成本的理想方法。遵循这种方式对社区有另一个好处，它提供了算法之间的公平比较。表1总结了关于传感器类型、数据大小和格式、场景类别和标注方法的一些最流行和典型的数据集。
这些数据集是通过不同类型的传感器（包括RGB-D相机[123]、[124]、[127]、[49]、[20]、移动激光扫描仪[120]、[3]、静态地面扫描仪[39]和非真实引擎[7]、[155]和其他3D扫描仪[1]、[10]）用于3D语义分割而获取的。其中，从非真实引擎获得的数据集是合成数据集[7][155]，不需要昂贵的设备或标注时间。这些物体的种类和数量非常丰富。与真实世界数据集相比，合成数据集具有完整的360度3D目标，没有遮挡效果或噪声，真实世界数据集中有噪声且包含遮挡[123]、[124]、[127]、[49]、[20]、[120]、[12]、[3]、[1]、[39]、[10]。对于3D实例分割，只有有限的3D数据集，如ScanNet[20]和S3DIS[1]。这两个数据集分别包含RGB-D相机或Matterport获得的真实室内场景的扫描数据。对于3D部件分割，普林斯顿分割基准（PSB）[12]、COSEG[147]和ShapeNet[169]是三个最流行的数据集。图3中显示了这些数据集的标注示例：
评价指标 不同的评估指标可以评价分割方法的有效性和优越性，包括执行时间、内存占用和准确性。然而，很少有作者提供有关其方法的执行时间和内存占用的详细信息。本文主要介绍精度度量。对于3D语义分割，常用的有Overall Accuracy（OAcc）、mean class Accuracy（mAcc）、mean class Intersection over Union（mIoU）。
OAcc：
mAcc：
mIoU：
对于3D实例分割，常用的有Average Precision（AP）、mean class Average Precision（mAP）。
AP：
mAP：
对于3D部件分割，常用的指标是overall average category Intersection over Union（Cat.mIoU）和overall average instance Intersection over Union（Ins.mIoU）。
Cat.mIoU：
Ins.mIoU：
3D语义分割 文献中提出了许多关于3D语义分割的深度学习方法。根据使用的数据表示，这些方法可分为五类，即基于RGB-D图像、基于投影图像、基于体素、基于点云和其他表示。基于点云的方法可以根据网络架构进一步分类为基于多层感知器（MLP）的方法、基于点云卷积的方法和基于图卷积的。图4显示了近年来3D语义分割深度学习的里程碑。
基于RGB-D RGB-D图像中的深度图包含关于真实世界的几何信息，这有助于区分前景目标和背景，从而提供提高分割精度的可能。在这一类别中，通常使用经典的双通道网络分别从RGB和深度图像中提取特征。然而框架过于简单，无法提取丰富而精细的特征。为此，研究人员将几个附加模块集成到上述简单的双通道框架中，通过学习对语义分割至关重要的丰富上下文和几何信息来提高性能。这些模块大致可分为六类：多任务学习、深度编码、多尺度网络、新型神经网络结构、数据/特征/得分级融合和后处理（见图5）。表2中总结了基于RGB-D图像的语义分割方法。
多任务学习 ：深度估计和语义分割是计算机视觉中两个具有挑战性的基本任务。这些任务也有一定的相关性，因为与不同目标之间的深度变化相比，目标内的深度变化较小。因此，许多研究者选择将深度估计任务和语义分割任务结合起来。从两个任务的关系来看，多任务学习框架主要有两种类型：级联式和并行式。级联式的工作有[8]、[36]，级联框架分阶段进行深度估计和语义分割，无法端到端训练。因此，深度估计任务并没有从语义分割任务中获得任何好处。并行式的工作有[141]、[101]、[87]，读者具体可以参考相关论文。
深度编码 ：传统的2D CNN无法利用原始深度图像的丰富几何特征。另一种方法是将原始深度图像编码为适合2D-CNN的其他表示。Hoft等人[46]使用定向梯度直方图（HOG）的简化版本来表示RGB-D场景的深度通道。Gupta等人[38]和Aman等人[82]根据原始深度图像计算了三个新通道，分别为水平视差、地面高度和重力角（HHA）。Liu等人[86]指出了HHA的局限性，即某些场景可能没有足够的水平和垂直平面。因此，他们提出了一种新的重力方向检测方法，通过拟合垂直线来学习更好的表示。Hazirbas等人[42]还认为，HHA表示具有较高的计算成本，并且包含比原始深度图像更少的信息。并提出了一种称为FuseNet的架构，该架构由两个编码器-解码器分支组成，包括一个深度分支和一个RGB分支，且以较低的计算负载直接编码深度信息。
多尺度网络 ：由多尺度网络学习的上下文信息对于小目标和详细的区域分割是有用的。Couprie等人[19]使用多尺度卷积网络直接从RGB图像和深度图像中学习特征。Aman等人[111]提出了一种用于分割的多尺度deep ConvNet，其中VGG16-FC网络的粗预测在scale-2模块中被上采样。然而，这种方法对场景中的杂波很敏感，导致输出误差。Lin等人[82]利用了这样一个事实：较低场景分辨率区域具有较高的深度，而较高场景分辨率区域则具有较低的深度。他们使用深度图将相应的彩色图像分割成多个场景分辨率区域，并引入context-aware receptive field（CaRF），该感知场专注于特定场景分辨率区域的语义分割。这使得他们的管道成为多尺度网络。
新型神经网络结构 ：由于CNN的固定网格计算，它们处理和利用几何信息的能力有限。因此，研究人员提出了其他新颖的神经网络架构，以更好地利用几何特征以及RGB和深度图像之间的关系。这些架构可分为四大类：改进2D CNN，相关工作有[61]、[144]；逆卷积神经网络（DeconvNets），相关工作有[87]、[139]、[14]；循环神经网络（RNN），相关工作有[29]、[79]；图神经网络（GNN），相关工作有[110]。
数据/特征/得分融合 ：纹理（RGB通道）和几何（深度通道）信息的最优融合对于准确的语义分割非常重要。融合策略有三种：数据级、特征级和得分级，分别指早期、中期和晚期融合。数据融合最简单的方式是将RGB图像和深度图像concat为4通道输入CNN[19]中，这种方式比较粗暴，没有充分利用深度和光度通道之间的强相关性。特征融合捕获了这些相关性，相关工作有[79]、[139]、[42]、[61]。得分级融合通常使用简单的平均策略进行。然而，RGB模型和深度模型对语义分割的贡献是不同的，相关工作有[86]、[14]。
后处理 ：用于RGB-D语义分割的CNN或DCNN的结果通常非常粗糙，导致边缘粗糙和小目标消失。解决这个问题的一个常见方法是将CNN与条件随机场（CRF）耦合。Wang等人[141]通过分层CRF（HCRF）的联合推断进一步促进了两个通道之间的相互作用。它加强了全局和局部预测之间的协同作用，其中全局用于指导局部预测并减少局部模糊性，局部结果提供了详细的区域结构和边界。Mousavian等人[101]、Liu等人[87]和Long等人[86]采用了全连接CRF（FC-CRF）进行后处理，其中逐像素标记预测联合考虑几何约束，如逐像素法线信息、像素位置、强度和深度，以促进逐像素标记的一致性。类似地，Jiang等人[61]提出了将深度信息与FC-CRF相结合的密集敏感CRF（DCRF）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/1d63cdff77eb30b8a5516cc60126d001/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-23T10:14:10+08:00" />
<meta property="article:modified_time" content="2023-02-23T10:14:10+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>转自：<a href="https://www.shangyexinzhi.com/article/5397310.html" rel="nofollow" title="史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目）_3D视觉工坊-商业新知 (shangyexinzhi.com)">史上最全 | 基于深度学习的3D分割综述（RGB-D/点云/体素/多目）_3D视觉工坊-商业新知 (shangyexinzhi.com)</a>来源丨自动驾驶之心</p> 
<h3 style="text-align:center;">摘要</h3> 
<p>3D目标分割是计算机视觉中的一个基本且具有挑战性的问题，在自动驾驶、机器人、增强现实和医学图像分析等领域有着广泛的应用。它受到了计算机视觉、图形和机器学习社区的极大关注。传统上，3D分割是用人工设计的特征和工程方法进行的，这些方法精度较差，也无法推广到大规模数据上。在2D计算机视觉巨大成功的推动下，深度学习技术最近也成为3D分割任务的首选。近年来已涌现出大量相关工作，并且已经在不同的基准数据集上进行了评估。本文全面调研了基于深度学习的3D分割的最新进展，涵盖了150多篇论文。论文总结了最常用的范式，讨论了它们的优缺点，并分析了这些分割方法的对比结果。并在此基础上，提出了未来的研究方向。</p> 
<p>如图1第二行所示，3D分割可分为三种类型：语义分割、实例分割和部件分割。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2b/c0/nuFdtHyR_o.png"></p> 
<p>论文的主要贡献如下：</p> 
<ul><li> <p>本文是第一篇全面涵盖使用不同3D数据表示（包括RGB-D、投影图像、体素、点云、网格和3D视频）进行3D分割的深度学习综述论文；</p> </li><li> <p>论文对不同类型的3D数据分割方法的相对优缺点进行了深入分析；</p> </li><li> <p>与现有综述不同，论文专注于专为3D分割设计的深度学习方法，并讨论典型的应用领域；</p> </li><li> <p>论文对几种公共基准3D数据集上的现有方法进行了全面比较，得出了有趣的结论，并确定了有前景的未来研究方向。</p> </li></ul> 
<p>图2显示了论文其余部分的组织方式：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5f/82/XlLMfEwF_o.png"></p> 
<h3>基准数据集和评估指标</h3> 
<h4>3D分割数据集</h4> 
<p>数据集对于使用深度学习训练和测试3D分割算法至关重要。然而，私人收集和标注数据集既麻烦又昂贵，因为它需要领域专业知识、高质量的传感器和处理设备。因此，构建公共数据集是降低成本的理想方法。遵循这种方式对社区有另一个好处，它提供了算法之间的公平比较。表1总结了关于传感器类型、数据大小和格式、场景类别和标注方法的一些最流行和典型的数据集。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/09/98/zhlJBtL7_o.png"></p> 
<p>这些数据集是通过不同类型的传感器（包括RGB-D相机[123]、[124]、[127]、[49]、[20]、移动激光扫描仪[120]、[3]、静态地面扫描仪[39]和非真实引擎[7]、[155]和其他3D扫描仪[1]、[10]）用于3D语义分割而获取的。其中，从非真实引擎获得的数据集是合成数据集[7][155]，不需要昂贵的设备或标注时间。这些物体的种类和数量非常丰富。与真实世界数据集相比，合成数据集具有完整的360度3D目标，没有遮挡效果或噪声，真实世界数据集中有噪声且包含遮挡[123]、[124]、[127]、[49]、[20]、[120]、[12]、[3]、[1]、[39]、[10]。对于3D实例分割，只有有限的3D数据集，如ScanNet[20]和S3DIS[1]。这两个数据集分别包含RGB-D相机或Matterport获得的真实室内场景的扫描数据。对于3D部件分割，普林斯顿分割基准（PSB）[12]、COSEG[147]和ShapeNet[169]是三个最流行的数据集。图3中显示了这些数据集的标注示例：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/bd/63/BKLw3knt_o.png"></p> 
<h3>评价指标</h3> 
<p>不同的评估指标可以评价分割方法的有效性和优越性，包括执行时间、内存占用和准确性。然而，很少有作者提供有关其方法的执行时间和内存占用的详细信息。本文主要介绍精度度量。对于3D语义分割，常用的有Overall Accuracy（OAcc）、mean class Accuracy（mAcc）、mean class Intersection over Union（mIoU）。</p> 
<p>OAcc：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/83/fd/suKDVB7q_o.png"></p> 
<p>mAcc：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ff/bc/UrBWm960_o.png"></p> 
<p>mIoU：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e5/1a/8uM7Oh9g_o.png"></p> 
<p>对于3D实例分割，常用的有Average Precision（AP）、mean class Average Precision（mAP）。</p> 
<p>AP：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/51/1a/aO2DSFZj_o.png"></p> 
<p>mAP：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2e/37/NFUWWib0_o.png"></p> 
<p>对于3D部件分割，常用的指标是overall average category Intersection over Union（Cat.mIoU）和overall average instance Intersection over Union（Ins.mIoU）。</p> 
<p>Cat.mIoU：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/61/36/t8jHbdB5_o.png"></p> 
<p>Ins.mIoU：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/b4/fe/JMZAGZNQ_o.png"></p> 
<h3>3D语义分割</h3> 
<p>文献中提出了许多关于3D语义分割的深度学习方法。根据使用的数据表示，这些方法可分为五类，即基于RGB-D图像、基于投影图像、基于体素、基于点云和其他表示。基于点云的方法可以根据网络架构进一步分类为基于多层感知器（MLP）的方法、基于点云卷积的方法和基于图卷积的。图4显示了近年来3D语义分割深度学习的里程碑。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/88/2c/yINThYup_o.png"></p> 
<h4>基于RGB-D</h4> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/b5/d1/nSLnIllU_o.png"></p> 
<p>RGB-D图像中的深度图包含关于真实世界的几何信息，这有助于区分前景目标和背景，从而提供提高分割精度的可能。在这一类别中，通常使用经典的双通道网络分别从RGB和深度图像中提取特征。然而框架过于简单，无法提取丰富而精细的特征。为此，研究人员将几个附加模块集成到上述简单的双通道框架中，通过学习对语义分割至关重要的丰富上下文和几何信息来提高性能。这些模块大致可分为六类：多任务学习、深度编码、多尺度网络、新型神经网络结构、数据/特征/得分级融合和后处理（见图5）。表2中总结了基于RGB-D图像的语义分割方法。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/29/c0/mIM4oJiC_o.png"></p> 
<p>多任务学习 ：深度估计和语义分割是计算机视觉中两个具有挑战性的基本任务。这些任务也有一定的相关性，因为与不同目标之间的深度变化相比，目标内的深度变化较小。因此，许多研究者选择将深度估计任务和语义分割任务结合起来。从两个任务的关系来看，多任务学习框架主要有两种类型：级联式和并行式。级联式的工作有[8]、[36]，级联框架分阶段进行深度估计和语义分割，无法端到端训练。因此，深度估计任务并没有从语义分割任务中获得任何好处。并行式的工作有[141]、[101]、[87]，读者具体可以参考相关论文。</p> 
<p>深度编码 ：传统的2D CNN无法利用原始深度图像的丰富几何特征。另一种方法是将原始深度图像编码为适合2D-CNN的其他表示。Hoft等人[46]使用定向梯度直方图（HOG）的简化版本来表示RGB-D场景的深度通道。Gupta等人[38]和Aman等人[82]根据原始深度图像计算了三个新通道，分别为水平视差、地面高度和重力角（HHA）。Liu等人[86]指出了HHA的局限性，即某些场景可能没有足够的水平和垂直平面。因此，他们提出了一种新的重力方向检测方法，通过拟合垂直线来学习更好的表示。Hazirbas等人[42]还认为，HHA表示具有较高的计算成本，并且包含比原始深度图像更少的信息。并提出了一种称为FuseNet的架构，该架构由两个编码器-解码器分支组成，包括一个深度分支和一个RGB分支，且以较低的计算负载直接编码深度信息。</p> 
<p>多尺度网络 ：由多尺度网络学习的上下文信息对于小目标和详细的区域分割是有用的。Couprie等人[19]使用多尺度卷积网络直接从RGB图像和深度图像中学习特征。Aman等人[111]提出了一种用于分割的多尺度deep ConvNet，其中VGG16-FC网络的粗预测在scale-2模块中被上采样。然而，这种方法对场景中的杂波很敏感，导致输出误差。Lin等人[82]利用了这样一个事实：较低场景分辨率区域具有较高的深度，而较高场景分辨率区域则具有较低的深度。他们使用深度图将相应的彩色图像分割成多个场景分辨率区域，并引入context-aware receptive field（CaRF），该感知场专注于特定场景分辨率区域的语义分割。这使得他们的管道成为多尺度网络。</p> 
<p>新型神经网络结构 ：由于CNN的固定网格计算，它们处理和利用几何信息的能力有限。因此，研究人员提出了其他新颖的神经网络架构，以更好地利用几何特征以及RGB和深度图像之间的关系。这些架构可分为四大类：改进2D CNN，相关工作有[61]、[144]；逆卷积神经网络（DeconvNets），相关工作有[87]、[139]、[14]；循环神经网络（RNN），相关工作有[29]、[79]；图神经网络（GNN），相关工作有[110]。</p> 
<p>数据/特征/得分融合 ：纹理（RGB通道）和几何（深度通道）信息的最优融合对于准确的语义分割非常重要。融合策略有三种：数据级、特征级和得分级，分别指早期、中期和晚期融合。数据融合最简单的方式是将RGB图像和深度图像concat为4通道输入CNN[19]中，这种方式比较粗暴，没有充分利用深度和光度通道之间的强相关性。特征融合捕获了这些相关性，相关工作有[79]、[139]、[42]、[61]。得分级融合通常使用简单的平均策略进行。然而，RGB模型和深度模型对语义分割的贡献是不同的，相关工作有[86]、[14]。</p> 
<p>后处理 ：用于RGB-D语义分割的CNN或DCNN的结果通常非常粗糙，导致边缘粗糙和小目标消失。解决这个问题的一个常见方法是将CNN与条件随机场（CRF）耦合。Wang等人[141]通过分层CRF（HCRF）的联合推断进一步促进了两个通道之间的相互作用。它加强了全局和局部预测之间的协同作用，其中全局用于指导局部预测并减少局部模糊性，局部结果提供了详细的区域结构和边界。Mousavian等人[101]、Liu等人[87]和Long等人[86]采用了全连接CRF（FC-CRF）进行后处理，其中逐像素标记预测联合考虑几何约束，如逐像素法线信息、像素位置、强度和深度，以促进逐像素标记的一致性。类似地，Jiang等人[61]提出了将深度信息与FC-CRF相结合的密集敏感CRF（DCRF）。</p> 
<h4>基于投影图像</h4> 
<p>基于投影图像的语义分割的核心思想是使用2D CNN从3D场景/形状的投影图像中提取特征，然后融合这些特征用于标签预测。与单目图像相比，该范式不仅利用了来自大规模场景的更多语义信息，而且与点云相比，减少了3D场景的数据大小。投影图像主要包括多目图像或球形图像。表3总结了基于投影图像的语义分割方法。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/89/1d/xnyDkRBT_o.png"></p> 
<p>基于多目图像</p> 
<p>MV-CNN[130]使用统一网络将由虚拟相机形成的3D形状的多个视图中的特征组合到单个紧凑的形状描述子中，以获得更好的分类性能。这促使研究人员将同样的想法应用于3D语义分割（见图6）。例如，Lawin等人[70]将点云投影到多目合成图像中，包括RGB、深度和表面法线图像。将所有多目图像的预测分数融合到单个表示中，并将其反向投影到每个点云中。然而，如果点云的密度较低，图像可能会错误地捕捉到观测结构背后的点云，这使得深度网络误解了多目图像。为此，SnapNet[6]、[5]对点云进行预处理，以计算点云特征（如正常或局部噪声）并生成网格，这与点云密度化类似。从网格和点云中，它们通过适当的快照生成RGB和深度图像。然后使用FCN对2D快照进行逐像素标记，并通过高效缓冲将这些标记快速重投影回3D点云。其他相关算法[35]、[106]可参考具体论文。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/95/cb/zbNyziUB_o.png"></p> 
<p>基于球形图像</p> 
<p>从3D场景中选择快照并不直接。必须在适当考虑视点数量、视距和虚拟相机角度后拍摄快照，以获得完整场景的最优表示。为了避免这些复杂性，研究人员将整个点云投影到一个球体上（见图6底部）。例如，Wu等人[152]提出了一个名为SqueezeSeg的端到端管道，其灵感来自SqueezeNet[53]，用于从球形图像中学习特征，然后由CRF将其细化为循环层。类似地，PointSeg[148]通过整合特征和通道注意力来扩展SqueezeNet，以学习鲁棒表示。其他相关算法还有[153]、[98]、[160]。</p> 
<h4>基于体素</h4> 
<p>与像素类似，体素将3D空间划分为具有特定大小和离散坐标的许多体积网格。与投影图像相比，它包含更多的场景几何信息。3D ShapeNets[156]和VoxNet[94]将体积占用网格表示作为用于目标识别的3D CNN的输入，该网络基于体素指导3D语义分割。根据体素大小的统一性，基于体素的方法可分为均匀体素方法和非均匀体素法。表3总结了基于体素的语义分割方法。</p> 
<p>均匀体素</p> 
<p>3D CNN是用于处理标签预测的统一体素的通用架构。Huang等人[51]提出了用于粗体素水平预测的3D FCN。他们的方法受到预测之间空间不一致性的限制，并提供了粗略的标记。Tchapmi等人[132]引入了一种新的网络SEGCloud来产生细粒度预测。其通过三线性插值将从3D FCN获得的粗体素预测上采样到原始3D点云空间分辨率。对于固定分辨率的体素，计算复杂度随场景比例的增加而线性增长。大体素可以降低大规模场景解析的计算成本。Liu等人[84]介绍了一种称为3DCNN-DQN-RNN的新型网络。与2D语义分割中的滑动窗口一样，该网络在3D-CNN和deep Q-Network（DQN）的控制下，提出了遍历整个数据的眼睛窗口，用于快速定位和分割目标。3D-CNN和残差RNN进一步细化眼睛窗口中的特征。该流水线有效地学习感兴趣区域的关键特征，以较低的计算成本提高大规模场景解析的准确性。其他相关工作[112]、[22]、[96]可以参考论文。</p> 
<p>非均匀体素</p> 
<p>在固定比例场景中，随着体素分辨率的增加，计算复杂度呈立方增长。然而，体素表示自然是稀疏的，在对稀疏数据应用3D密集卷积时会导致不必要的计算。为了缓解这个问题，OcNet[113]使用一系列不平衡的八叉树将空间分层划分为非均匀体素。树结构允许内存分配和计算集中于相关的密集体素，而不牺牲分辨率。然而，empty space仍然给OctNet带来计算和内存负担。相比之下，Graham等人[33]提出了一种新的子流形稀疏卷积（SSC），它不在empty space进行计算，弥补了OcNet的缺陷。</p> 
<h4>基于点云</h4> 
<p>点云在3D空间中不规则地散布，缺乏任何标准顺序和平移不变性，这限制了传统2D/3D卷积神经网络的使用。最近，一系列基于点云的语义分割网络被提出。这些方法大致可分为三类：基于多层感知器（MLP）的、基于点云卷积的和基于图卷积。表4总结了这些方法。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a0/ed/o658CtFU_o.png"></p> 
<p>基于MLP</p> 
<p>这些方法直接使用MLP学习点云特征。根据其框架，可进一步分为两类：基于PN和基于PN++框架的方法，如图7（a）和（b）所示。</p> 
<p>基于PN框架</p> 
<p>PointNet[108]（PN）是一项直接处理点云的开创性工作。它使用共享MLP来挖掘逐点云特征，并采用max-pooling等对称函数来将这些特征聚合到全局特征表示中。由于max-pooling仅捕获全局点云的最大激活，因此PN无法学习利用局部特征。基于PN框架，一些网络开始定义局部区域以增强局部特征学习，并利用递归神经网络（RNN）来增加上下文特征的利用。例如，Engelmann等人[28]通过KNN聚类和K-means聚类定义局部区域，并使用简化PN提取局部特征。ESC[26]将全局区域点云划分为多尺度/网格块。连接的（局部）块特征附加到逐点云特征，并通过递归合并单元（RCU）进一步学习全局上下文特征。其他相关算法[168]可以参考论文。</p> 
<p>基于PN++框架</p> 
<p>基于PointNet，PointNet++[109]（PN++）定义了分层学习架构。它使用最远点采样（FPS）对点云进行分层采样，并使用k个最近邻搜索和球搜索对局部区域进行聚类。逐步地，简化的PointNet在多个尺度或多个分辨率下利用局部区域的功能。PN++框架扩展了感受野以共同利用更多的局部特征。受SIFT[91]的启发，PointSIFT[63]在采样层之前插入一个PointSIFT模块层，以学习局部形状信息。该模块通过对不同方向的信息进行编码，将每个点云转换为新的形状表示。类似地，PointWeb[177]在聚类层之后插入自适应特征调整（AFA）模块层，以将点云之间的交互信息嵌入到每个点云中。这些策略增强了学习到的逐点云特征的表示能力。然而，MLP仍然单独处理每个局部点云，并且不注意局部点云之间的几何连接。此外，MLP是有效的，但缺乏捕捉更广泛和更精细的局部特征的复杂性。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4c/6d/kcxsMzCg_o.png"></p> 
<p>基于点云卷积</p> 
<p>基于点云卷积的方法直接对点云进行卷积运算。与基于MLP的分割类似，这些网络也可以细分为基于PN框架的方法和基于PN++框架的方法，如图7（c）、（d）所示。</p> 
<p>基于PN</p> 
<p>基于PN框架的方法对每个点云的相邻点云进行卷积。例如，RSNet[52]使用1x1卷积利用逐点云特征，然后将它们传递给local dependency module（LDM），以利用局部上下文特征。但是，它并没有为每个点云定义邻域以了解局部特征。另一方面，PointwiseCNN[50]按照特定的顺序对点云进行排序，例如XYZ坐标或Morton曲线[100]，并动态查询最近邻，并将它们放入3x3x3 kernel中，然后使用相同的内核权重进行卷积。DPC[27]在通过dilated KNN搜索确定邻域点云的每个点云的邻域点云上调整点卷积[154]。该方法将扩张机制整合到KNN搜索中，以扩大感受野。PCNN[143]在KD-tree邻域上进行参数化CNN，以学习局部特征。然而，特征图的固定分辨率使得网络难以适应更深层次的架构。其他相关算法[133]、[34]、[77]可以参考具体论文。</p> 
<p>基于PN++</p> 
<p>基于PN++框架的方法将卷积层作为其关键层。例如，蒙特卡罗卷积近似的一个扩展叫做PointConv[154]，它考虑了点云密度。使用MLP来近似卷积核的权重函数，并使用inverse density scale来重新加权学习的权重函数。类似地，MCC[45]通过依赖点云概率密度函数（PDF）将卷积表述为蒙特卡罗积分问题，其中卷积核也由MLP表示。此外，它引入了Possion Disk Sampling（PDS）[151]来构建点云层次结构，而不是FPS，这提供了一个在感受野中获得最大样本数的机会。A-CNN[67]通过扩展的KNN定义了一个新的局部环形区域，并将点云投影到切线平面上，以进一步排序局部区域中的相邻点云。然后，对这些表示为闭环阵列的有序邻域进行标准点云卷积。其他相关算法[48]、[175]可以参考具体论文。</p> 
<p>基于图卷积</p> 
<p>基于图卷积的方法对与图结构连接的点云进行卷积。在这里，图的构造（定义）和卷积设计正成为两个主要挑战。PN框架和PN++框架的相同分类也适用于图7（e）和（f）所示的图卷积方法。</p> 
<p>基于PN</p> 
<p>基于PN框架的方法从全局点云构造图，并对每个点云的邻域点云进行卷积。例如，ECC[125]是应用空间图形网络从点云提取特征的先驱方法之一。它动态生成edge-conditioned filters，以学习描述点云与其相邻点云之间关系的边缘特征。基于PN架构，DGCN[149]在每个点云的邻域上实现称为EdgeConv的动态边缘卷积。卷积由简化PN近似。SPG[69]将点云划分为若干简单的几何形状（称为super-points），并在全局super-points上构建super graph。此外，该网络采用PointNet来嵌入这些点云，并通过门控递归单元（GRU）细化嵌入。其他相关算法[74]、[73]、[93]、[159]可以参考具体论文。</p> 
<p>基于PN++</p> 
<p>基于PN++框架的方法对具有图结构的局部点云进行卷积。图是光谱图或空间图。在前一种情况下，LS-GCN[137]采用了PointNet++的基本架构，使用标准的非参数化傅立叶kernel将MLP替换为谱图卷积，以及一种新的spectral cluster pooling替代max-pooling。然而，从空间域到频谱域的转换需要很高的计算成本。此外，谱图网络通常定义在固定的图结构上，因此无法直接处理具有不同图结构的数据。相关算法可以参考[30]、[78]、[80]、[174]、[72]。</p> 
<h4>基于其他表示</h4> 
<p>一些方法将原始点云转换为投影图像、体素和点云以外的表示。这种表示的例子包括正切图像[131]和晶格[129]、[116]。在前一种情况下，Tatargenko等人[131]将每个点云周围的局部曲面投影到一系列2D切线图像，并开发基于切线卷积的U-Net来提取特征。在后一种情况下，SPLATNet[129]采用Jampani等人[56]提出的双边卷积层（BCL）将无序点云平滑映射到稀疏网格上。类似地，LatticeNet[116]使用了一种混合架构，它将获得低级特征的PointNet与探索全局上下文特征的稀疏3D卷积相结合。这些特征嵌入到允许应用标准2D卷积的稀疏网格中。尽管上述方法在3D语义分割方面取得了重大进展，但每种方法都有其自身的缺点。例如，多目图像具有更多的语义信息，但场景的几何信息较少。另一方面，体素具有更多的几何信息，但语义信息较少。为了获得最优性能，一些方法采用混合表示作为输入来学习场景的综合特征。相关算法[21]、[15]、[90]、[58]、[97]可以参考具体论文。</p> 
<h3>3D实例分割</h3> 
<p>3D实例分割方法另外区分同一类的不同实例。作为场景理解的一项信息量更大的任务，3D实例分割越来越受到研究界的关注。3D实例分割方法大致分为两个方向：基于Proposal和无Proposal。</p> 
<h4>基于Proposal</h4> 
<p>基于Proposal的方法首先预测目标Proposal，然后细化它们以生成最终实例mask（见图8），将任务分解为两个主要挑战。因此，从Proposal生成的角度来看，这些方法可以分为基于检测的方法和无检测的方法。</p> 
<p>基于检测 的方法有时将目标Proposal定义为3D边界框回归问题。3D-SIS[47]基于3D重建的姿态对齐，将高分辨率RGB图像与体素结合，并通过3D检测主干联合学习颜色和几何特征，以预测3D目标框Proposal。在这些Proposal中，3D mask主干预测最终实例mask。其他相关算法[171]、[164]可以参考论文。</p> 
<p>无检测方法 包括SGPN[145]，它假定属于同一目标实例的点云应该具有非常相似的特征。因此，它学习相似度矩阵来预测Proposal。这些Proposal通过置信度分数过滤，以生成高度可信的实例Proposal。然而，这种简单的距离相似性度量学习并不能提供信息，并且不能分割同一类的相邻目标。为此，3D-MPA[25]从投票给同一目标中心的采样和聚类点云特征中学习目标Proposal，然后使用图卷积网络合并Proposal特征，从而实现Proposal之间的更高层次交互，从而优化Proposal特征。AS Net[60]使用分配模块来分配Proposal候选，然后通过抑制网络消除冗余候选。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/65/18/cyl6Ipmu_o.png"></p> 
<h4>Proposal Free</h4> 
<p>无Proposal 方法学习每个点云的特征嵌入，然后使用聚类以获得明确的3D实例标签（见图8），将任务分解为两个主要挑战。从嵌入学习的角度来看，这些方法可以大致分为三类：多嵌入学习、2D嵌入传播和多任务学习。</p> 
<p>多嵌入学习 ：MASC[83]等方法依靠SSCN[33]的高性能来预测多尺度和语义拓扑上相邻点云之间的相似性嵌入。简单而有效的聚类[89]适用于基于两种类型的学习嵌入将点云分割为实例。MTML[68]学习两组特征嵌入，包括每个实例唯一的特征嵌入和定向实例中心的方向嵌入，这提供了更强的聚类能力。类似地，PointGroup[62]基于原始坐标嵌入空间和偏移的坐标嵌入空间将点云聚类为不同的簇。</p> 
<p>2D嵌入传播 ：这些方法的一个例子是3D-BEVIS[23]，它通过鸟瞰整个场景来学习2D全局实例嵌入。然后通过DGCN[149]将学习到的嵌入传播到点云上。另一个例子是PanopticFusion[102]，它通过2D实例分割网络Mask R-CNN[43]预测RGB帧的逐像素实例标签。</p> 
<p>多任务联合学习 ：3D语义分割和3D实例分割可以相互影响。例如，具有不同类的目标必须是不同的实例，具有相同实例标签的目标必须为同一类。基于此，ASIS[146]设计了一个称为ASIS的编码器-解码器网络，以学习语义感知的实例嵌入，从而提高这两个任务的性能。类似地，JSIS3D[107]使用统一网络即MT-PNet来预测点云的语义标签，并将点云嵌入到高维特征向量中，并进一步提出MV-CRF来联合优化目标类和实例标签。类似地，Liu等人[83]和3D-GEL[81]采用SSCN来同时生成语义预测和实例嵌入，然后使用两个GCN来细化实例标签。OccusSeg[40]使用多任务学习网络来产生occupancy signal和空间嵌入。occupancy signal表示每个体素占用的体素数量。表5总结了3D实例分割方法。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/cd/a7/OLDf956L_o.png"></p> 
<h3>3D部件分割</h3> 
<p>3D部件分割是继实例分割之后的下一个更精细的级别，其目的是标记实例的不同部分。部件分割的管道与语义分割的管道非常相似，只是标签现在是针对单个部件的。因此，一些现有的3D语义分割网络[96]、[33]、[108]、[109]、[174]、[52]、[133]、[50]、[45]、[154]、[77]、[149]、[73]、[159]、[143]、[34]、[72]、[129]、[116]也可用于部件分割。然而，这些网络并不能完全解决部件分割的困难。例如，具有相同语义标签的各个部件可能具有不同的形状，并且具有相同语义标记的实例的部件数量可能不同。我们将3D部件分割方法细分为两类：基于规则数据的和基于不规则数据的，如下所示。</p> 
<h4>基于规则数据</h4> 
<p>规则数据通常包括投影图像[64]、体素[150]、[71]、[128]。对于投影图像，Kalogerakis等人[64]从多个视图中获得一组最佳覆盖物体表面的图像，然后使用多视图全卷积网络（FCN）和基于表面的条件随机场（CRF）分别预测和细化部件标签。体素是几何数据的有效表示。然而，像部件分割这样的细粒度任务需要具有更详细结构信息的高分辨率体素，这导致了较高的计算成本。Wang等人[150]建议VoxSegNet利用有限分辨率的体素中更详细的信息。它们在子采样过程中使用空间密集提取来保持空间分辨率，并使用attention feature aggregation（AFA）模块来自适应地选择尺度特征。其他相关算法[71]、[128]可以参考论文。</p> 
<h4>基于不规则数据</h4> 
<p>不规则数据表示通常包括网格[161]、[41]和点云[75]、[121]、[170]、[136]、[140]、[172]、[178]。网格提供了3D形状的有效近似，因为它捕捉到了平面、尖锐和复杂的表面形状、表面和拓扑。Xu等人[161]将人脸法线和人脸距离直方图作为双流框架的输入，并使用CRF优化最终标签。受传统CNN的启发，Hanocka等人[41]设计了新颖的网格卷积和池化，以对网格边缘进行操作。对于点云，图卷积是最常用的管道。在频谱图领域，SyncSpecCNN[170]引入了同步频谱CNN来处理不规则数据。特别地，提出了多通道卷积核和参数化膨胀卷积核，分别解决了多尺度分析和形状信息共享问题。在空间图域中，类似于图像的卷积核，KCNet[121]提出了point-set kernel和nearest-neighbor-graph，以改进PointNet，使其具有高效的局部特征提取结构。其他相关算法[140]、[163]、[136]、[65]、[142]、[75]、[172]、[178]可以参考论文。3D部件的相关算法总结如下表所示。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/88/50/9TGMjXON_o.png"></p> 
<h3>3D分割的应用</h3> 
<h4>无人驾驶系统</h4> 
<p>随着激光雷达和深度相机的普及，价格也越来越实惠，它们越来越多地应用于无人驾驶系统，如自动驾驶和移动机器人。这些传感器提供实时3D视频，通常为每秒30帧（fps），作为系统的直接输入，使3D视频语义分割成为理解场景的主要任务。此外，为了更有效地与环境交互，无人系统通常会构建场景的3D语义图。下面回顾基于3D视频的语义分割和3D语义地图构建。</p> 
<p>3D视频语义分割</p> 
<p>与前文介绍的3D单帧/扫描语义分割方法相比，3D视频（连续帧/扫描）语义分割方法考虑了帧之间连接的时空信息，这在稳健和连续地解析场景方面更为强大。传统的卷积神经网络（CNN）没有被设计成利用帧之间的时间信息。一种常见的策略是自适应RNN（[134]、[24]）或时空卷积网络（[44]、[17]、[122]）。</p> 
<p>3D语义地图重建</p> 
<p>无人系统不仅需要避开障碍物，还需要建立对场景的更深理解，例如目标解析、自我定位等。3D场景重建通常依赖于同时定位和建图系统（SLAM）来获得没有语义信息的3D地图。随后用2D-CNN进行2D语义分割，然后在优化（例如条件随机场）之后将2D标签转移到3D地图以获得3D语义地图[165]。这种通用管道无法保证复杂、大规模和动态场景中的3D语义地图的高性能。研究人员已经努力使用来自多帧的关联信息（[92]、[95]、[157]、[13]、[66]）、多模型融合（[59]、[176]）和新的后处理操作来增强鲁棒性。</p> 
<h4>医疗诊断</h4> 
<p>2D U-Net[115]和3D U-Net[18]通常用于医学图像分割。基于这些基本思想，设计了许多改进的体系结构，主要可分为四类：扩展的3D U-Net([9]、[173]、[117])、联合的2D-3D CNN（[105]、[2]、[138]、[76]）、带优化模块的CNN（[99]、[179]、[126]、[104]）和分层网络（[11]、[57]、[118]、[135]、[166]、[167]、[119]）。</p> 
<h3>实验结果</h3> 
<h4>3D语义分割结果</h4> 
<p>论文报告了基于RGB-D的语义分割方法在SUN-RGB-D[127]和NYUDv2[124]数据集上的结果，使用mAcc和mIoU作为评估指标。各种方法的这些结果取自原始论文，如表7所示。下表所示。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/97/1d/xaVNyqQm_o.png"></p> 
<p>论文在S3DIS[1]（5折和6折交叉验证）、ScanNet[20]（测试集）、Semantic3D[39]（缩减的8个子集）和SemanticKITTI[3]（仅xyz，无RGB）上报告了投影图像/体素/点云/其他表示语义分割方法的结果。使用mAcc、oAcc和mIoU作为评估指标。这些不同方法的结果取自原始论文。表8列出了结果。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a2/01/zXqIaKil_o.png"></p> 
<p>由于本文的主要兴趣是基于点云的语义分割方法，因此重点对这些方法的性能进行详细分析。为了捕获对语义分割性能至关重要的更广泛的上下文特征和更丰富的局部特征，在基本框架上提出了几种专用策略。</p> 
<ul><li> <p>基础网络是3D分割发展的主要推动力之一。一般来说，有两个主要的基本框架，包括PointNet和PointNet++框架，它们的缺点也指出了改进的方向；</p> </li><li> <p>自然环境中的物体通常具有各种形状。局部特征可以增强目标的细节分割；</p> </li><li> <p>3D场景中的目标可以根据与环境中的其他目标的某种关系来定位。已经证明，上下文特征（指目标依赖性）可以提高语义分割的准确性，特别是对于小的和相似的目标。</p> </li></ul> 
<h4>3D实例分割结果</h4> 
<p>论文报告了ScanNet[20]数据集上3D实例分割方法的结果，并选择mAP作为评估指标。这些方法的结果取自ScanNet Benchmark Challenge网站，如表9所示，并在图9中总结。该表和图如下所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/44/a3/06UURN6o_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/61/1d/VEerTfIY_o.png"></p> 
<ul><li> <p>OccusSeg[40]具有最先进的性能，在本文调查时，ScanNet数据集的平均精度为67.2%；</p> </li><li> <p>大多数方法在诸如“浴缸”和“厕所”之类的大规模类上具有更好的分割性能，而在诸如“柜台”、“桌子”和“图片”之类的小规模类上具有较差的分割性能。因此，小目标的实例分割是一个突出的挑战；</p> </li><li> <p>在所有类的实例分割方面，无Proposal方法比基于提案的方法具有更好的性能，尤其是对于“窗帘”、“其他”、“图片”、“淋浴帘”和“水槽”等小目标；</p> </li><li> <p>在基于Proposal的方法中，基于2D嵌入传播的方法，包括3D-BEVIS[23]、PanoticFusion[102]，与其他基于无提案的方法相比，性能较差。简单的嵌入传播容易产生错误标签。</p> </li></ul> 
<h4>3D部件分割结果</h4> 
<p>论文报告了ShapeNet[169]数据集上3D零件分割方法的结果，并使用了Ins.mIoU作为评估度量。各种方法的这些结果取自原始论文，如表10所示。我们可以看到：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/67/d5/QVyeenVa_o.png"></p> 
<ul><li> <p>LatticeNet[40]具有最先进的性能，在本文调查时，ShapeNet数据集的平均精度为93.9%；</p> </li><li> <p>所有方法的部件分割性能非常相似。</p> </li></ul> 
<h3>讨论和结论</h3> 
<p>论文使用深度学习技术，包括3D语义分割、3D实例分割和3D部件分割，对3D分割的最新发展进行了全面综述。论文对每个类别中的各种方法进行了全面的性能比较和优点。近年来，使用深度学习技术的3D分割取得了重大进展。然而，这仅仅是一个开始，重要的发展摆在我们面前。下面，论文提出一些悬而未决的问题，并确定潜在的研究方向。</p> 
<p>合成数据集为多个任务提供了更丰富的信息 ：与真实数据集相比，合成数据集成本低、场景多样，因此在语义分割方面逐渐发挥重要作用[7]、[155]。众所周知，训练数据中包含的信息决定了场景解析精度的上限。现有的数据集缺少重要的语义信息，如材料和纹理信息，这对于具有相似颜色或几何信息的分割更为关键。此外，大多数现有数据集通常是为单个任务设计的。目前，只有少数语义分割数据集还包含实例[20]和场景布局[127]的标签，以满足多任务目标。</p> 
<p>多任务的统一网络 ：对于一个系统来说，通过各种深度学习网络来完成不同的计算机视觉任务是昂贵且不切实际的。对于场景的基本特征开发，语义分割与一些任务具有很强的一致性，例如深度估计[97]、[85]、[36]、[141]、[1141]、[87]、场景补全[22]、实例分割[146]、[107]、[81]和目标检测[97]。这些任务可以相互协作，以提高统一网络中的性能。语义/实例分割可以进一步与部件分割和其他计算机视觉任务相结合，用于联合学习。</p> 
<p>场景解析的多种模式 ：使用多个不同表示的语义分割，例如投影图像、体素和点云，可能实现更高的精度。然而，由于场景信息的限制，如图像的几何信息较少，体素的语义信息较少，单一表示限制了分割精度。多重表示（多模态）将是提高性能的另一种方法[21]，[15]，[90]，[58]，[97]。</p> 
<p>高效的基于点云卷积的网络 ：基于点云的语义分割网络正成为当今研究最多的方法。这些方法致力于充分探索逐点云特征和点云/特征之间的连接。然而，他们求助于邻域搜索机制，例如KNN、ball query[109]和分层框架[154]，这很容易忽略局部区域之间的低级特征，并进一步增加了全局上下文特征开发的难度。</p> 
<p>弱监督和无监督的3D分割 ：深度学习在3D分割方面取得了显著的成功，但严重依赖于大规模标记的训练样本。弱监督和无监督学习范式被认为是缓解大规模标记数据集要求的替代方法。目前，工作[162]提出了一个弱监督网络，它只需要对一小部分训练样本进行标记。[75]、[178]提出了一种无监督网络，该网络从数据本身生成监督标签。</p> 
<p>大规模场景的语义分割 一直是研究的热点。现有方法仅限于极小的3D点云[108]、[69]（例如，4096个点云或1x1米块），在没有数据预处理的情况下，无法直接扩展到更大规模的点云（例如，数百万个点云或数百米）。尽管RandLA Net[48]可以直接处理100万个点，但速度仍然不够，需要进一步研究大规模点云上的有效语义分割问题。</p> 
<p>3D视频语义分割 ：与2D视频语义分割一样，少数作品试图在3D视频上利用4D时空特征（也称为4D点云）[17]，[122]。从这些工作中可以看出，时空特征可以帮助提高3D视频或动态3D场景语义分割的鲁棒性。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/998aeab6f5c73105fee17e61f36f59bb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">二进制按位赋值（对某位进行0或1赋值）与二进制打印输出</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dca04d986b71a8ffdac5ac334e5f8222/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【ffmpeg基础】ffmpeg视频编码</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>