<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【pytorch框架学习】nn.Embedding中的padding_idx用法示意 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【pytorch框架学习】nn.Embedding中的padding_idx用法示意" />
<meta property="og:description" content="import torch import torch.nn as nn embedding1 = nn.Embedding(10,3) embedding1.weight Parameter containing: tensor([[-0.9116, 0.5195, -1.3509], [ 0.5670, 0.8024, -0.0373], [-0.8223, -1.2181, -0.6713], [-1.2734, -1.0591, -1.1202], [-0.4734, 1.8297, 0.3880], [ 0.5687, 0.3136, 0.7541], [ 1.0070, -0.0197, -0.1715], [ 2.1003, 0.6229, 0.6720], [-0.1729, -0.6555, 0.2904], [-1.6015, -1.3011, -0.5837]], requires_grad=True) embedding2 = nn.Embedding(10,3,padding_idx=0) embedding2.weight Parameter containing: tensor([[ 0.0000, 0.0000, 0.0000], [-0.5784, -1.5044, -1.7400], [-1.1197, 0.8234, -0.6458], [ 0.8204, 2.0259, -0.9619], [ 0.1317, -0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3575bbd4fc2af8675d734e1f0f2eeeaf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-22T12:38:34+08:00" />
<meta property="article:modified_time" content="2021-08-22T12:38:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【pytorch框架学习】nn.Embedding中的padding_idx用法示意</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
</code></pre> 
<pre><code class="prism language-python">embedding1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>
embedding1<span class="token punctuation">.</span>weight
</code></pre> 
<pre><code>Parameter containing:
tensor([[-0.9116,  0.5195, -1.3509],
        [ 0.5670,  0.8024, -0.0373],
        [-0.8223, -1.2181, -0.6713],
        [-1.2734, -1.0591, -1.1202],
        [-0.4734,  1.8297,  0.3880],
        [ 0.5687,  0.3136,  0.7541],
        [ 1.0070, -0.0197, -0.1715],
        [ 2.1003,  0.6229,  0.6720],
        [-0.1729, -0.6555,  0.2904],
        [-1.6015, -1.3011, -0.5837]], requires_grad=True)
</code></pre> 
<pre><code class="prism language-python">embedding2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>padding_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
embedding2<span class="token punctuation">.</span>weight
</code></pre> 
<pre><code>Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000],
        [-0.5784, -1.5044, -1.7400],
        [-1.1197,  0.8234, -0.6458],
        [ 0.8204,  2.0259, -0.9619],
        [ 0.1317, -0.3696, -1.6996],
        [-0.2763, -0.3568,  0.2973],
        [-1.2864, -0.2396,  1.3876],
        [-1.6487, -0.0096,  0.1984],
        [-0.2213, -1.0257, -0.6359],
        [ 0.2354, -0.7799, -0.3288]], requires_grad=True)
</code></pre> 
<pre><code class="prism language-python">embedding3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>padding_idx<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
embedding3<span class="token punctuation">.</span>weight
</code></pre> 
<pre><code>Parameter containing:
tensor([[-1.0108, -1.5298, -0.3603],
        [-1.1312,  1.4528, -0.7718],
        [ 0.0000,  0.0000,  0.0000],
        [-0.8255, -0.4083,  0.7372],
        [-0.8608,  0.2809,  0.1835],
        [-0.6224, -0.1390, -0.7797],
        [-0.6382,  0.6341,  0.2778],
        [-0.6328,  0.2855, -0.3784],
        [-0.8825, -0.2000, -1.2097],
        [ 0.9235,  0.5388,  0.8158]], requires_grad=True)
</code></pre> 
<pre><code class="prism language-python">embedding4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>padding_idx<span class="token operator">=</span><span class="token number">10</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
embedding4<span class="token punctuation">.</span>weight
</code></pre> 
<pre><code>Parameter containing:
tensor([[-1.4354,  0.8168,  0.4477],
        [-0.4925, -0.3006,  0.7584],
        [-0.2400,  1.0259, -0.5391],
        [-0.5411,  0.9602,  0.1372],
        [-0.6848,  0.0278, -0.1112],
        [-0.2092, -1.8230,  1.0283],
        [ 0.5441,  0.6374, -0.9901],
        [ 0.1115,  0.2792, -0.1808],
        [-3.7124, -0.6969, -0.6027],
        [ 0.0000,  0.0000,  0.0000]], requires_grad=True)
</code></pre> 
<pre><code class="prism language-python"><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
embedding4<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>tensor([[[-0.4925, -0.3006,  0.7584],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000]],

        [[ 0.0000,  0.0000,  0.0000],
         [-0.5411,  0.9602,  0.1372],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.5441,  0.6374, -0.9901]]], grad_fn=&lt;EmbeddingBackward&gt;)
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9ca1e7b917d9b04ac4f9a89bab956a9c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vs2015 vs2017 编译zlib库</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/30512ae6cdde5083789b448dd2f92604/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">基于Jenkins&#43;maven&#43;gitlab&#43;harbor&#43;Rancher&#43;k8s的CI/CD实现（尚未完成，还在更新中）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>