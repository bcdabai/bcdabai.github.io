<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hierarchical Attention Network for Document Classification--tensorflow实现篇 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hierarchical Attention Network for Document Classification--tensorflow实现篇" />
<meta property="og:description" content="上周我们介绍了Hierarchical Attention Network for Document Classification这篇论文的模型架构，这周抽空用tensorflow实现了一下，接下来主要从代码的角度介绍如何实现用于文本分类的HAN模型。
数据集 首先介绍一下数据集，这篇论文中使用了几个比较大的数据集，包括IMDB电影评分，yelp餐馆评价等等。选定使用yelp2013之后，一开始找数据集的时候完全处于懵逼状态，所有相关的论文和资料里面出现的数据集下载链接都指向YELP官网,但是官网上怎么都找不到相关数据的下载，然后就各种搜感觉都搜不到==然后就好不容易在github上面找到了，MDZZ，我这都是在写什么，绝对不是在凑字数，单纯的吐槽数据不好找而已。链接如下： https://github.com/rekiksab/Yelp/tree/master/yelp_challenge/yelp_phoenix_academic_dataset 这里面好像不止一个数据集，还有user，business等其他几个数据集，不过在这里用不到罢了。先来看一下数据集的格式，如下，每一行是一个评论的文本，是json格式保存的，主要有vote, user_id, review_id, stars, data, text, type, business_id几项，针对本任务，只需要使用stars评分和text评论内容即可。这里我选择先将相关的数据保存下来作为数据集。代码如下所示：
{&#34;votes&#34;: {&#34;funny&#34;: 0, &#34;useful&#34;: 5, &#34;cool&#34;: 2}, &#34;user_id&#34;: &#34;rLtl8ZkDX5vH5nAx9C3q5Q&#34;, &#34;review_id&#34;: &#34;fWKvX83p0-ka4JS3dc6E5A&#34;, &#34;stars&#34;: 5, &#34;date&#34;: &#34;2011-01-26&#34;, &#34;text&#34;: &#34;My wife took me here on my birthday for breakfast and it was excellent. The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure. Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a93d1bb4b99c123ffa36e8d242bd92f3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-07-02T16:08:08+08:00" />
<meta property="article:modified_time" content="2017-07-02T16:08:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hierarchical Attention Network for Document Classification--tensorflow实现篇</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>上周我们介绍了Hierarchical Attention Network for Document Classification这篇论文的模型架构，这周抽空用tensorflow实现了一下，接下来主要从代码的角度介绍如何实现用于文本分类的HAN模型。</p> 
<h3 id="数据集">数据集</h3> 
<p>首先介绍一下数据集，这篇论文中使用了几个比较大的数据集，包括IMDB电影评分，yelp餐馆评价等等。选定使用yelp2013之后，一开始找数据集的时候完全处于懵逼状态，所有相关的论文和资料里面出现的数据集下载链接都指向YELP<a href="http://www.yelp.com/dataset_challenge" rel="nofollow noopener noreferrer" target="_blank">官网</a>,但是官网上怎么都找不到相关数据的下载，然后就各种搜感觉都搜不到==然后就好不容易在github上面找到了，MDZZ，我这都是在写什么，绝对不是在凑字数，单纯的吐槽数据不好找而已。链接如下： <br> <a href="https://github.com/rekiksab/Yelp/tree/master/yelp_challenge/yelp_phoenix_academic_dataset" target="_blank" rel="noopener noreferrer">https://github.com/rekiksab/Yelp/tree/master/yelp_challenge/yelp_phoenix_academic_dataset</a> <br> 这里面好像不止一个数据集，还有user，business等其他几个数据集，不过在这里用不到罢了。先来看一下数据集的格式，如下，每一行是一个评论的文本，是json格式保存的，主要有<code>vote, user_id, review_id, stars, data, text, type, business_id</code>几项，针对本任务，只需要使用stars评分和text评论内容即可。这里我选择先将相关的数据保存下来作为数据集。代码如下所示：</p> 
<pre class="prettyprint"><code class=" hljs json">{"<span class="hljs-attribute">votes</span>": <span class="hljs-value">{"<span class="hljs-attribute">funny</span>": <span class="hljs-value"><span class="hljs-number">0</span></span>, "<span class="hljs-attribute">useful</span>": <span class="hljs-value"><span class="hljs-number">5</span></span>, "<span class="hljs-attribute">cool</span>": <span class="hljs-value"><span class="hljs-number">2</span></span>}</span>, "<span class="hljs-attribute">user_id</span>": <span class="hljs-value"><span class="hljs-string">"rLtl8ZkDX5vH5nAx9C3q5Q"</span></span>, "<span class="hljs-attribute">review_id</span>": <span class="hljs-value"><span class="hljs-string">"fWKvX83p0-ka4JS3dc6E5A"</span></span>, "<span class="hljs-attribute">stars</span>": <span class="hljs-value"><span class="hljs-number">5</span></span>, "<span class="hljs-attribute">date</span>": <span class="hljs-value"><span class="hljs-string">"2011-01-26"</span></span>, "<span class="hljs-attribute">text</span>": <span class="hljs-value"><span class="hljs-string">"My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n\nAnyway, I can't wait to go back!"</span></span>, "<span class="hljs-attribute">type</span>": <span class="hljs-value"><span class="hljs-string">"review"</span></span>, "<span class="hljs-attribute">business_id</span>": <span class="hljs-value"><span class="hljs-string">"9yKzy9PApeiPPOUJEtnvkg"</span></span>}</code></pre> 
<p>数据集的预处理操作，这里我做了一定的简化，将每条评论数据都转化为30*30的矩阵，其实可以不用这么规划，只需要将大于30的截断即可，小鱼30的不需要补全操作，只是后续需要给每个batch选定最大长度，然后获取每个样本大小，这部分我还没有太搞清楚，等之后有时间再看一看，把这个功能加上就行了。先这样凑合用==</p> 
<pre class="prettyprint"><code class=" hljs livecodeserver"><span class="hljs-comment">#coding=utf-8</span>
import json
import pickle
import nltk
<span class="hljs-built_in">from</span> nltk.tokenize import WordPunctTokenizer
<span class="hljs-built_in">from</span> collections import defaultdict

<span class="hljs-comment">#使用nltk分词分句器</span>
sent_tokenizer = nltk.data.<span class="hljs-built_in">load</span>(<span class="hljs-string">'tokenizers/punkt/english.pickle'</span>)
word_tokenizer = WordPunctTokenizer()

<span class="hljs-comment">#记录每个单词及其出现的频率</span>
word_freq = defaultdict(int)

<span class="hljs-comment"># 读取数据集，并进行分词，统计每个单词出现次数，保存在word freq中</span>
<span class="hljs-operator">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'yelp_academic_dataset_review.json'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> <span class="hljs-operator">in</span> f:
        review = json.loads(<span class="hljs-built_in">line</span>)
        <span class="hljs-keyword">words</span> = word_tokenizer.tokenize(review[<span class="hljs-string">'text'</span>])
        <span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span> <span class="hljs-operator">in</span> <span class="hljs-keyword">words</span>:
            word_freq[<span class="hljs-built_in">word</span>] += <span class="hljs-number">1</span>

    print <span class="hljs-string">"load finished"</span>

<span class="hljs-comment"># 将词频表保存下来</span>
<span class="hljs-operator">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'word_freq.pickle'</span>, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> g:
    pickle.dump(word_freq, g)
    print <span class="hljs-built_in">len</span>(word_freq)<span class="hljs-comment">#159654</span>
    print <span class="hljs-string">"word_freq save finished"</span>

num_classes = <span class="hljs-number">5</span>
<span class="hljs-comment"># 将词频排序，并去掉出现次数最小的3个</span>
sort_words = list(sorted(word_freq.<span class="hljs-keyword">items</span>(), key=lambda x:-x[<span class="hljs-number">1</span>]))
print sort_words[:<span class="hljs-number">10</span>], sort_words[-<span class="hljs-number">10</span>:]

<span class="hljs-comment">#构建vocablary，并将出现次数小于5的单词全部去除，视为UNKNOW</span>
vocab = {}
i = <span class="hljs-number">1</span>
vocab[<span class="hljs-string">'UNKNOW_TOKEN'</span>] = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> <span class="hljs-built_in">word</span>, freq <span class="hljs-operator">in</span> word_freq.<span class="hljs-keyword">items</span>():
    <span class="hljs-keyword">if</span> freq &gt; <span class="hljs-number">5</span>:
        vocab[<span class="hljs-built_in">word</span>] = i
        i += <span class="hljs-number">1</span>
print i
UNKNOWN = <span class="hljs-number">0</span>

data_x = []
data_y = []
max_sent_in_doc = <span class="hljs-number">30</span>
max_word_in_sent = <span class="hljs-number">30</span>

<span class="hljs-comment">#将所有的评论文件都转化为30*30的索引矩阵，也就是每篇都有30个句子，每个句子有30个单词</span>
<span class="hljs-comment"># 不够的补零，多余的删除，并保存到最终的数据集文件之中</span>
<span class="hljs-operator">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'yelp_academic_dataset_review.json'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> <span class="hljs-built_in">line</span> <span class="hljs-operator">in</span> f:
        doc = []
        review = json.loads(<span class="hljs-built_in">line</span>)
        sents = sent_tokenizer.tokenize(review[<span class="hljs-string">'text'</span>])
        <span class="hljs-keyword">for</span> i, sent <span class="hljs-operator">in</span> enumerate(sents):
            <span class="hljs-keyword">if</span> i &lt; max_sent_in_doc:
                word_to_index = []
                <span class="hljs-keyword">for</span> j, <span class="hljs-built_in">word</span> <span class="hljs-operator">in</span> enumerate(word_tokenizer.tokenize(sent)):
                    <span class="hljs-keyword">if</span> j &lt; max_word_in_sent:
                            word_to_index.append(vocab.<span class="hljs-built_in">get</span>(<span class="hljs-built_in">word</span>, UNKNOWN))
                doc.append(word_to_index)

        label = int(review[<span class="hljs-string">'stars'</span>])
        labels = [<span class="hljs-number">0</span>] * num_classes
        labels[label-<span class="hljs-number">1</span>] = <span class="hljs-number">1</span>
        data_y.append(labels)
        data_x.append(doc)
    pickle.dump((data_x, data_y), <span class="hljs-built_in">open</span>(<span class="hljs-string">'yelp_data'</span>, <span class="hljs-string">'wb'</span>))
    print <span class="hljs-built_in">len</span>(data_x) <span class="hljs-comment">#229907</span>
    <span class="hljs-comment"># length = len(data_x)</span>
    <span class="hljs-comment"># train_x, dev_x = data_x[:int(length*0.9)], data_x[int(length*0.9)+1 :]</span>
    <span class="hljs-comment"># train_y, dev_y = data_y[:int(length*0.9)], data_y[int(length*0.9)+1 :]</span></code></pre> 
<p>在将数据预处理之后，我们就得到了一共229907篇文档，每篇都是30*30 的单词索引矩阵，这样在后续进行读取的时候直接根据嵌入矩阵E就可以将单词转化为词向量了。也就省去了很多麻烦。这样，我们还需要一个数据的读取的函数，将保存好的数据载入内存，其实很简单，就是一个pickle读取函数而已，然后将数据集按照9:1的比例分成训练集和测试集。其实这里我觉得9:1会使验证集样本过多（20000个），但是论文中就是这么操作的==暂且不管这个小细节，就按论文里面的设置做吧。代码如下所示：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_dataset</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">with</span> open(<span class="hljs-string">'yelp_data'</span>, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
        data_x, data_y = pickle.load(f)
        length = len(data_x)
        train_x, dev_x = data_x[:int(length*<span class="hljs-number">0.9</span>)], data_x[int(length*<span class="hljs-number">0.9</span>)+<span class="hljs-number">1</span> :]
        train_y, dev_y = data_y[:int(length*<span class="hljs-number">0.9</span>)], data_y[int(length*<span class="hljs-number">0.9</span>)+<span class="hljs-number">1</span> :]
        <span class="hljs-keyword">return</span> train_x, train_y, dev_x, dev_y</code></pre> 
<p>有了这个函数，我们就可以在训练时一键读入数据集了。接下来我们看一下模型架构的实现部分。</p> 
<h3 id="模型实现">模型实现</h3> 
<p>按照上篇博客中关于模型架构的介绍，结合下面两张图进行理解，我们应该很容易的得出模型的框架主要分为句子层面，文档层面两部分，然后每个内部有包含encoder和attention两部分。 <br> <img src="https://images2.imgbox.com/29/57/PpVFukai_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/90/09/KClnFdz8_o.png" alt="这里写图片描述" title=""> <br> 代码部分如下所示，主要是用<code>tf.nn.bidirectional_dynamic_rnn()</code>函数实现双向GRU的构造，然后Attention层就是一个MLP+softmax机制，yehe你容易理解。</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#coding=utf8</span>

<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.contrib <span class="hljs-keyword">import</span> rnn
<span class="hljs-keyword">from</span> tensorflow.contrib <span class="hljs-keyword">import</span> layers

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">length</span><span class="hljs-params">(sequences)</span>:</span>
<span class="hljs-comment">#返回一个序列中每个元素的长度</span>
    used = tf.sign(tf.reduce_max(tf.abs(sequences), reduction_indices=<span class="hljs-number">2</span>))
    seq_len = tf.reduce_sum(used, reduction_indices=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> tf.cast(seq_len, tf.int32)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HAN</span><span class="hljs-params">()</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, num_classes, embedding_size=<span class="hljs-number">200</span>, hidden_size=<span class="hljs-number">50</span>)</span>:</span>

        self.vocab_size = vocab_size
        self.num_classes = num_classes
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size

        <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'placeholder'</span>):
            self.max_sentence_num = tf.placeholder(tf.int32, name=<span class="hljs-string">'max_sentence_num'</span>)
            self.max_sentence_length = tf.placeholder(tf.int32, name=<span class="hljs-string">'max_sentence_length'</span>)
            self.batch_size = tf.placeholder(tf.int32, name=<span class="hljs-string">'batch_size'</span>)
            <span class="hljs-comment">#x的shape为[batch_size, 句子数， 句子长度(单词个数)]，但是每个样本的数据都不一样，，所以这里指定为空</span>
            <span class="hljs-comment">#y的shape为[batch_size, num_classes]</span>
            self.input_x = tf.placeholder(tf.int32, [<span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>], name=<span class="hljs-string">'input_x'</span>)
            self.input_y = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, num_classes], name=<span class="hljs-string">'input_y'</span>)

        <span class="hljs-comment">#构建模型</span>
        word_embedded = self.word2vec()
        sent_vec = self.sent2vec(word_embedded)
        doc_vec = self.doc2vec(sent_vec)
        out = self.classifer(doc_vec)

        self.out = out


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">word2vec</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-comment">#嵌入层</span>
        <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"embedding"</span>):
            embedding_mat = tf.Variable(tf.truncated_normal((self.vocab_size, self.embedding_size)))
            <span class="hljs-comment">#shape为[batch_size, sent_in_doc, word_in_sent, embedding_size]</span>
            word_embedded = tf.nn.embedding_lookup(embedding_mat, self.input_x)
        <span class="hljs-keyword">return</span> word_embedded

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sent2vec</span><span class="hljs-params">(self, word_embedded)</span>:</span>
        <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"sent2vec"</span>):
            <span class="hljs-comment">#GRU的输入tensor是[batch_size, max_time, ...].在构造句子向量时max_time应该是每个句子的长度，所以这里将</span>
            <span class="hljs-comment">#batch_size * sent_in_doc当做是batch_size.这样一来，每个GRU的cell处理的都是一个单词的词向量</span>
            <span class="hljs-comment">#并最终将一句话中的所有单词的词向量融合（Attention）在一起形成句子向量</span>

            <span class="hljs-comment">#shape为[batch_size*sent_in_doc, word_in_sent, embedding_size]</span>
            word_embedded = tf.reshape(word_embedded, [-<span class="hljs-number">1</span>, self.max_sentence_length, self.embedding_size])
            <span class="hljs-comment">#shape为[batch_size*sent_in_doce, word_in_sent, hidden_size*2]</span>
            word_encoded = self.BidirectionalGRUEncoder(word_embedded, name=<span class="hljs-string">'word_encoder'</span>)
            <span class="hljs-comment">#shape为[batch_size*sent_in_doc, hidden_size*2]</span>
            sent_vec = self.AttentionLayer(word_encoded, name=<span class="hljs-string">'word_attention'</span>)
            <span class="hljs-keyword">return</span> sent_vec

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">doc2vec</span><span class="hljs-params">(self, sent_vec)</span>:</span>
        <span class="hljs-comment">#原理与sent2vec一样，根据文档中所有句子的向量构成一个文档向量</span>
        <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"doc2vec"</span>):
            sent_vec = tf.reshape(sent_vec, [-<span class="hljs-number">1</span>, self.max_sentence_num, self.hidden_size*<span class="hljs-number">2</span>])
            <span class="hljs-comment">#shape为[batch_size, sent_in_doc, hidden_size*2]</span>
            doc_encoded = self.BidirectionalGRUEncoder(sent_vec, name=<span class="hljs-string">'sent_encoder'</span>)
            <span class="hljs-comment">#shape为[batch_szie, hidden_szie*2]</span>
            doc_vec = self.AttentionLayer(doc_encoded, name=<span class="hljs-string">'sent_attention'</span>)
            <span class="hljs-keyword">return</span> doc_vec

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classifer</span><span class="hljs-params">(self, doc_vec)</span>:</span>
        <span class="hljs-comment">#最终的输出层，是一个全连接层</span>
        <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'doc_classification'</span>):
            out = layers.fully_connected(inputs=doc_vec, num_outputs=self.num_classes, activation_fn=<span class="hljs-keyword">None</span>)
            <span class="hljs-keyword">return</span> out

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">BidirectionalGRUEncoder</span><span class="hljs-params">(self, inputs, name)</span>:</span>
        <span class="hljs-comment">#双向GRU的编码层，将一句话中的所有单词或者一个文档中的所有句子向量进行编码得到一个 2×hidden_size的输出向量，然后在经过Attention层，将所有的单词或句子的输出向量加权得到一个最终的句子/文档向量。</span>
        <span class="hljs-comment">#输入inputs的shape是[batch_size, max_time, voc_size]</span>
        <span class="hljs-keyword">with</span> tf.variable_scope(name):
            GRU_cell_fw = rnn.GRUCell(self.hidden_size)
            GRU_cell_bw = rnn.GRUCell(self.hidden_size)
            <span class="hljs-comment">#fw_outputs和bw_outputs的size都是[batch_size, max_time, hidden_size]</span>
            ((fw_outputs, bw_outputs), (_, _)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=GRU_cell_fw,
                                                                                 cell_bw=GRU_cell_bw,
                                                                                 inputs=inputs,
                                                                                 sequence_length=length(inputs),
                                                                                 dtype=tf.float32)
            <span class="hljs-comment">#outputs的size是[batch_size, max_time, hidden_size*2]</span>
            outputs = tf.concat((fw_outputs, bw_outputs), <span class="hljs-number">2</span>)
            <span class="hljs-keyword">return</span> outputs

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">AttentionLayer</span><span class="hljs-params">(self, inputs, name)</span>:</span>
        <span class="hljs-comment">#inputs是GRU的输出，size是[batch_size, max_time, encoder_size(hidden_size * 2)]</span>
        <span class="hljs-keyword">with</span> tf.variable_scope(name):
            <span class="hljs-comment"># u_context是上下文的重要性向量，用于区分不同单词/句子对于句子/文档的重要程度,</span>
            <span class="hljs-comment"># 因为使用双向GRU，所以其长度为2×hidden_szie</span>
            u_context = tf.Variable(tf.truncated_normal([self.hidden_size * <span class="hljs-number">2</span>]), name=<span class="hljs-string">'u_context'</span>)
            <span class="hljs-comment">#使用一个全连接层编码GRU的输出的到期隐层表示,输出u的size是[batch_size, max_time, hidden_size * 2]</span>
            h = layers.fully_connected(inputs, self.hidden_size * <span class="hljs-number">2</span>, activation_fn=tf.nn.tanh)
            <span class="hljs-comment">#shape为[batch_size, max_time, 1]</span>
            alpha = tf.nn.softmax(tf.reduce_sum(tf.multiply(h, u_context), axis=<span class="hljs-number">2</span>, keep_dims=<span class="hljs-keyword">True</span>), dim=<span class="hljs-number">1</span>)
            <span class="hljs-comment">#reduce_sum之前shape为[batch_szie, max_time, hidden_szie*2]，之后shape为[batch_size, hidden_size*2]</span>
            atten_output = tf.reduce_sum(tf.multiply(inputs, alpha), axis=<span class="hljs-number">1</span>)
            <span class="hljs-keyword">return</span> atten_output</code></pre> 
<p>以上就是主要的模型架构部分，其实思路也是很简单的，主要目的是熟悉一下其中一些操作的使用方法。接下来就是模型的训练部分了。</p> 
<h3 id="模型训练">模型训练</h3> 
<p>其实这部分里的数据读入部分我一开始打算使用上次博客中提到的TFRecords来做，但是实际用的时候发现貌似还有点不熟悉，尝试了好几次都有点小错误，虽然之前已经把别人的代码都看明白了，但是真正到自己写的时候还是存在一定的难度，还要抽空在学习学习==所以在最后还是回到了以前的老方法，分批次读入，恩，最起码简单易懂23333.。。。</p> 
<p>由于这部分大都是重复性的代码，所以不再进行详细赘述，不懂的可以去看看我前面几篇博客里面关于模型训练部分代码的介绍。</p> 
<p>这里重点说一下，关于梯度训练部分的梯度截断，由于RNN模型在训练过程中往往会出现梯度爆炸和梯度弥散等现象，所以在训练RNN模型时，往往会使用梯度截断的技术来防止梯度过大而引起无法正确求到的现象。然后就基本上都是使用的dennizy大神的CNN代码中的程序了。</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#coding=utf-8</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> model
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> load_data <span class="hljs-keyword">import</span> read_dataset, batch_iter


<span class="hljs-comment"># Data loading params</span>
tf.flags.DEFINE_string(<span class="hljs-string">"data_dir"</span>, <span class="hljs-string">"data/data.dat"</span>, <span class="hljs-string">"data directory"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"vocab_size"</span>, <span class="hljs-number">46960</span>, <span class="hljs-string">"vocabulary size"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"num_classes"</span>, <span class="hljs-number">5</span>, <span class="hljs-string">"number of classes"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"embedding_size"</span>, <span class="hljs-number">200</span>, <span class="hljs-string">"Dimensionality of character embedding (default: 200)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"hidden_size"</span>, <span class="hljs-number">50</span>, <span class="hljs-string">"Dimensionality of GRU hidden layer (default: 50)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"batch_size"</span>, <span class="hljs-number">32</span>, <span class="hljs-string">"Batch Size (default: 64)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"num_epochs"</span>, <span class="hljs-number">10</span>, <span class="hljs-string">"Number of training epochs (default: 50)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"checkpoint_every"</span>, <span class="hljs-number">100</span>, <span class="hljs-string">"Save model after this many steps (default: 100)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"num_checkpoints"</span>, <span class="hljs-number">5</span>, <span class="hljs-string">"Number of checkpoints to store (default: 5)"</span>)
tf.flags.DEFINE_integer(<span class="hljs-string">"evaluate_every"</span>, <span class="hljs-number">100</span>, <span class="hljs-string">"evaluate every this many batches"</span>)
tf.flags.DEFINE_float(<span class="hljs-string">"learning_rate"</span>, <span class="hljs-number">0.01</span>, <span class="hljs-string">"learning rate"</span>)
tf.flags.DEFINE_float(<span class="hljs-string">"grad_clip"</span>, <span class="hljs-number">5</span>, <span class="hljs-string">"grad clip to prevent gradient explode"</span>)

FLAGS = tf.flags.FLAGS

train_x, train_y, dev_x, dev_y = read_dataset()
<span class="hljs-keyword">print</span> <span class="hljs-string">"data load finished"</span>

<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    han = model.HAN(vocab_size=FLAGS.vocab_size,
                    num_classes=FLAGS.num_classes,
                    embedding_size=FLAGS.embedding_size,
                    hidden_size=FLAGS.hidden_size)

    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'loss'</span>):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=han.input_y,
                                                                      logits=han.out,
                                                                      name=<span class="hljs-string">'loss'</span>))
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'accuracy'</span>):
        predict = tf.argmax(han.out, axis=<span class="hljs-number">1</span>, name=<span class="hljs-string">'predict'</span>)
        label = tf.argmax(han.input_y, axis=<span class="hljs-number">1</span>, name=<span class="hljs-string">'label'</span>)
        acc = tf.reduce_mean(tf.cast(tf.equal(predict, label), tf.float32))

    timestamp = str(int(time.time()))
    out_dir = os.path.abspath(os.path.join(os.path.curdir, <span class="hljs-string">"runs"</span>, timestamp))
    print(<span class="hljs-string">"Writing to {}\n"</span>.format(out_dir))

    global_step = tf.Variable(<span class="hljs-number">0</span>, trainable=<span class="hljs-keyword">False</span>)
    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)
    <span class="hljs-comment"># RNN中常用的梯度截断，防止出现梯度过大难以求导的现象</span>
    tvars = tf.trainable_variables()
    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), FLAGS.grad_clip)
    grads_and_vars = tuple(zip(grads, tvars))
    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)

    <span class="hljs-comment"># Keep track of gradient values and sparsity (optional)</span>
    grad_summaries = []
    <span class="hljs-keyword">for</span> g, v <span class="hljs-keyword">in</span> grads_and_vars:
        <span class="hljs-keyword">if</span> g <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            grad_hist_summary = tf.summary.histogram(<span class="hljs-string">"{}/grad/hist"</span>.format(v.name), g)
            grad_summaries.append(grad_hist_summary)

    grad_summaries_merged = tf.summary.merge(grad_summaries)

    loss_summary = tf.summary.scalar(<span class="hljs-string">'loss'</span>, loss)
    acc_summary = tf.summary.scalar(<span class="hljs-string">'accuracy'</span>, acc)


    train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])
    train_summary_dir = os.path.join(out_dir, <span class="hljs-string">"summaries"</span>, <span class="hljs-string">"train"</span>)
    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])
    dev_summary_dir = os.path.join(out_dir, <span class="hljs-string">"summaries"</span>, <span class="hljs-string">"dev"</span>)
    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)

    checkpoint_dir = os.path.abspath(os.path.join(out_dir, <span class="hljs-string">"checkpoints"</span>))
    checkpoint_prefix = os.path.join(checkpoint_dir, <span class="hljs-string">"model"</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)

    sess.run(tf.global_variables_initializer())

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_step</span><span class="hljs-params">(x_batch, y_batch)</span>:</span>
        feed_dict = {
            han.input_x: x_batch,
            han.input_y: y_batch,
            han.max_sentence_num: <span class="hljs-number">30</span>,
            han.max_sentence_length: <span class="hljs-number">30</span>,
            han.batch_size: <span class="hljs-number">64</span>
        }
        _, step, summaries, cost, accuracy = sess.run([train_op, global_step, train_summary_op, loss, acc], feed_dict)

        time_str = str(int(time.time()))
        print(<span class="hljs-string">"{}: step {}, loss {:g}, acc {:g}"</span>.format(time_str, step, cost, accuracy))
        train_summary_writer.add_summary(summaries, step)

        <span class="hljs-keyword">return</span> step

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dev_step</span><span class="hljs-params">(x_batch, y_batch, writer=None)</span>:</span>
        feed_dict = {
            han.input_x: x_batch,
            han.input_y: y_batch,
            han.max_sentence_num: <span class="hljs-number">30</span>,
            han.max_sentence_length: <span class="hljs-number">30</span>,
            han.batch_size: <span class="hljs-number">64</span>
        }
        step, summaries, cost, accuracy = sess.run([global_step, dev_summary_op, loss, acc], feed_dict)
        time_str = str(int(time.time()))
        print(<span class="hljs-string">"++++++++++++++++++dev++++++++++++++{}: step {}, loss {:g}, acc {:g}"</span>.format(time_str, step, cost, accuracy))
        <span class="hljs-keyword">if</span> writer:
            writer.add_summary(summaries, step)

    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(FLAGS.num_epochs):
        print(<span class="hljs-string">'current epoch %s'</span> % (epoch + <span class="hljs-number">1</span>))
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">200000</span>, FLAGS.batch_size):
            x = train_x[i:i + FLAGS.batch_size]
            y = train_y[i:i + FLAGS.batch_size]
            step = train_step(x, y)
            <span class="hljs-keyword">if</span> step % FLAGS.evaluate_every == <span class="hljs-number">0</span>:
                dev_step(dev_x, dev_y, dev_summary_writer)
</code></pre> 
<p>当模型训练好之后，我们就可以去tensorboard上面查看训练结果如何了。</p> 
<h3 id="训练结果">训练结果</h3> 
<p>训练起来不算慢，但是也称不上快，在实验室服务器上做测试，64G内存，基本上2秒可以跑3个batch。然后我昨天晚上跑了之后就回宿舍了，回来之后发现忘了把dev的数据写到summary里面，而且现在每个epoch里面没加shuffle，也没跑很久，更没有调参，所以结果凑合能看出一种趋势，等过几天有时间在跑跑该该参数之类的看能不能有所提升，就简单上几个截图吧。 <br> <img src="https://images2.imgbox.com/e8/a0/wA4lc1Hd_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/5e/01/BopOYMRM_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/af/63/G4O21ehG_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/40/0c/hswueNdK_o.png" alt="这里写图片描述" title=""></p> 
<p>最后的最后再贴上几个链接，都是在学习和仿真这安论文的时候看到的一些感觉不错的博客之类的： <br> 1，richliao，他关于这篇文章写了三篇博客，分别从CNN/RNN/HAN逐层递进进行介绍，写得很不错，可以加深理解。不过是使用keras实现的，博客和代码链接如下： <br> <a href="https://richliao.github.io/" rel="nofollow">https://richliao.github.io/</a> <br> <a href="https://github.com/richliao/textClassifier">https://github.com/richliao/textClassifier</a> <br> 2，yelp数据集下载链接： <br> <a href="https://github.com/rekiksab/Yelp/tree/master/yelp_challenge/yelp_phoenix_academic_dataset">https://github.com/rekiksab/Yelp/tree/master/yelp_challenge/yelp_phoenix_academic_dataset</a> <br> 3，EdGENetworks，这是一个使用pytorch实现的链接，其实代码我没怎么看，但是发现背后有一个屌屌的公司explosion.ai，博客里面还是写了很干货的，要好好学习下。 <br> <a href="https://github.com/EdGENetworks/attention-networks-for-classification">https://github.com/EdGENetworks/attention-networks-for-classification</a> <br> <a href="https://explosion.ai/blog/deep-learning-formula-nlp" rel="nofollow">https://explosion.ai/blog/deep-learning-formula-nlp</a> <br> 4，ematvey，这个博主使用tensorflow实现了一个版本，我也参考了他数据处理部分的代码，但是感觉程序有点不太容易读，给出链接，仁者见仁。 <br> <a href="https://github.com/ematvey/deep-text-classifier">https://github.com/ematvey/deep-text-classifier</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e862cc335bb4ee9444c0c7e2159bedcf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">国内可用的ntp服务器地址</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7fcf75f464ba53cdfe6ceca5f592ef3e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">aspect学习（1）before&amp;after&amp;around</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>