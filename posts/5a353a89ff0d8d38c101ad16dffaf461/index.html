<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>研发中遇到的问题总结 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="研发中遇到的问题总结" />
<meta property="og:description" content="windows、linux、Python、Pytorch、AI算法开发过程中遇到的一些问题及其解决方法 自备的，以便后面遇到同样问题时查阅，如果对你没有帮助可以划走了，内容都是临时记录的，没有格式编排，但有问题目录，点击即可跳转，如果目录中没有你想要的问题，可以划走了 文章目录 windows、linux、Python、Pytorch、AI算法开发过程中遇到的一些问题及其解决方法▶windows powershell无法使用conda虚拟环境，windows cmd中无法使用conda虚拟环境▶dtype=&#34;&lt;U5&#34;表示字符串不超过5位▶Pycharm提示：cannot find reference ‘***’ in (input:(any,```),kwargs:dict) -&gt; any&#39;，但是可以正常跑？▶卷积参数量、计算量▶ 求完损失以后在进行反向传播 loss.backward()时报一个类型错误（RuntimeError: expected dtype Float but got dtype Long）：▶ 网络一个batch的输出不是我们想用来计算的数据，我们希望将一轮（多个batch/多次输出）的结果拿来计算，但是，Tensor不能append，如果用cat，那初始化也是个难题，这时借助列表，将多次输出append到列表，再用torch.cat（）将列表连接并转换成tensor数据。▶ CNN中Conv2d的padding方式▶ Pycharm中run控制台的可显示长度、显示精度、取消科学计数法……设置方式： ▶损失为Nan、nan；loss为nan▶过拟合、发现与解决▶二范数、欧氏距离、向量求模▶二维向量按某列排序用lambda匿名函数▶RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM▶PIL和OpenCV互转PIL &gt;&gt; CV2:CV2 &gt;&gt; PIL:判断图像数据是否是OpenCV格式： ▶python&#43;openCV通道拆分（R、G、B）和合并▶PIL的save命令保存下来的图片的质量问题▶ reduce failed to synchronize: cudaErrorAssert: device-side assert triggeredall elements of input should be between 0 and 1 ▶Pytorch跑RNN、LSTM等结构时报：&#39;UserWarning: RNN module weights are not part of single contiguous chunk of memory▶ValueError: Expected more than 1 value per channel when training, got input size torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5a353a89ff0d8d38c101ad16dffaf461/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-31T10:41:45+08:00" />
<meta property="article:modified_time" content="2023-05-31T10:41:45+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">研发中遇到的问题总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="windowslinuxPythonPytorchAI_0"></a>windows、linux、Python、Pytorch、AI算法开发过程中遇到的一些问题及其解决方法</h3> 
<font size="5" color="red">自备的，以便后面遇到同样问题时查阅，如果对你没有帮助可以划走了，内容都是临时记录的，没有格式编排，但有问题目录，点击即可跳转，如果目录中没有你想要的问题，可以划走了</font> 
<p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#windowslinuxPythonPytorchAI_0" rel="nofollow">windows、linux、Python、Pytorch、AI算法开发过程中遇到的一些问题及其解决方法</a></li><li><ul><li><ul><li><a href="#windows_powershellcondawindows_cmdconda_7" rel="nofollow">▶windows powershell无法使用conda虚拟环境，windows cmd中无法使用conda虚拟环境</a></li><li><a href="#dtypeU55_22" rel="nofollow">▶dtype="&lt;U5"表示字符串不超过5位</a></li><li><a href="#Pycharmcannot_find_reference__in_inputanykwargsdict__any_24" rel="nofollow">▶Pycharm提示：cannot find reference ‘***’ in (input:(any,```),kwargs:dict) -&gt; any'，但是可以正常跑？</a></li><li><a href="#_29" rel="nofollow">▶卷积参数量、计算量</a></li><li><a href="#%09_lossbackwardRuntimeError_expected_dtype_Float_but_got_dtype_Long_45" rel="nofollow">▶ 求完损失以后在进行反向传播 loss.backward()时报一个类型错误（RuntimeError: expected dtype Float but got dtype Long）：</a></li><li><a href="#%09batchbatchTensorappendcatappendtorchcattensor_67" rel="nofollow">▶ 网络一个batch的输出不是我们想用来计算的数据，我们希望将一轮（多个batch/多次输出）的结果拿来计算，但是，Tensor不能append，如果用cat，那初始化也是个难题，这时借助列表，将多次输出append到列表，再用torch.cat（）将列表连接并转换成tensor数据。</a></li><li><a href="#%09CNNConv2dpadding_103" rel="nofollow">▶ CNN中Conv2d的padding方式</a></li><li><a href="#%09Pycharmrun_110" rel="nofollow">▶ Pycharm中run控制台的可显示长度、显示精度、取消科学计数法……</a></li><li><ul><li><a href="#_127" rel="nofollow">设置方式：</a></li></ul> 
     </li><li><a href="#Nannanlossnan_181" rel="nofollow">▶损失为Nan、nan；loss为nan</a></li><li><a href="#_187" rel="nofollow">▶过拟合、发现与解决</a></li><li><a href="#_192" rel="nofollow">▶二范数、欧氏距离、向量求模</a></li><li><a href="#lambda_200" rel="nofollow">▶二维向量按某列排序用lambda匿名函数</a></li><li><a href="#RuntimeError_cuDNN_error_CUDNN_STATUS_BAD_PARAM_216" rel="nofollow">▶RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</a></li><li><a href="#PILOpenCV_228" rel="nofollow">▶PIL和OpenCV互转</a></li><li><ul><li><a href="#PIL__CV2_229" rel="nofollow">PIL &gt;&gt; CV2:</a></li><li><a href="#CV2__PIL_242" rel="nofollow">CV2 &gt;&gt; PIL:</a></li><li><a href="#OpenCV_255" rel="nofollow">判断图像数据是否是OpenCV格式：</a></li></ul> 
     </li><li><a href="#pythonopenCVRGB_260" rel="nofollow">▶python+openCV通道拆分（R、G、B）和合并</a></li><li><a href="#PILsave_273" rel="nofollow">▶PIL的save命令保存下来的图片的质量问题</a></li><li><a href="#_reduce_failed_to_synchronize_cudaErrorAssert_deviceside_assert_triggered_276" rel="nofollow">▶ reduce failed to synchronize: cudaErrorAssert: device-side assert triggered</a></li><li><ul><li><a href="#all_elements_of_input_should_be_between_0_and_1_278" rel="nofollow">all elements of input should be between 0 and 1</a></li></ul> 
     </li><li><a href="#PytorchRNNLSTMUserWarning_RNN_module_weights_are_not_part_of_single_contiguous_chunk_of_memory_282" rel="nofollow">▶Pytorch跑RNN、LSTM等结构时报：'UserWarning: RNN module weights are not part of single contiguous chunk of memory</a></li><li><a href="#ValueError_Expected_more_than_1_value_per_channel_when_training_got_input_size_torchSize1_512_304" rel="nofollow">▶ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 512]</a></li><li><a href="#MXNETlinux_308" rel="nofollow">▶MXNET警告提示(linux)：</a></li><li><a href="#_pythonopencv_318" rel="nofollow">▶解决 python-opencv打开\读取或保存\写入中文路径的问题</a></li><li><a href="#matplotlibOMP_Error_15_Initializing_libiomp5mddll_but_found_libiomp5mddll_already_initialized_328" rel="nofollow">▶matplotlib画折线图：OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h5><a id="windows_powershellcondawindows_cmdconda_7"></a>▶windows powershell无法使用conda虚拟环境，windows cmd中无法使用conda虚拟环境</h5> 
<p>问题描述：文件夹中直接右击打开终端或者cmd打开终端窗口，发现只能使用系统环境，无法切换到其他虚拟环境中去；</p> 
<p>解决办法：<br> <strong>步骤1</strong>、以管理员身份打开cmd或者powershell，打开方式：win+x——终端管理员[win11](或以管理员身份打开终端[win10])</p> 
<p><strong>步骤2</strong>、输入<code>conda init powershell</code>（该命令只初始化powershell，如果需要初始化其他终端软件，可选输入有<code>'bash', 'cmd.exe', 'fish', 'tcsh', 'xonsh', 'zsh', 'powershell'</code>，如需初始化全部<code>conda init --all</code>）</p> 
<p><strong>步骤3</strong>、然后输入<code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned</code>回车更改powershell的执行策略</p> 
<p><strong>步骤4</strong>、重启终端软件即可以使用conda虚拟环境了<br> 参考链接：<a href="https://blog.csdn.net/jrf17725020071/article/details/112320613">https://blog.csdn.net/jrf17725020071/article/details/112320613</a></p> 
<h5><a id="dtypeU55_22"></a>▶dtype="&lt;U5"表示字符串不超过5位</h5> 
<h5><a id="Pycharmcannot_find_reference__in_inputanykwargsdict__any_24"></a>▶Pycharm提示：cannot find reference ‘***’ in (input:(any,```),kwargs:dict) -&gt; any’，但是可以正常跑？</h5> 
<p><img src="https://images2.imgbox.com/6f/8f/pfTgfHS5_o.png" alt="在这里插入图片描述"><br>   这个问题困扰了我很久很久，在网上找了很多方法都没有用，包括什么设置快捷键忽略此提示，在设置的某个框框里去掉‘__ <em>init</em> __.py’等等方法，无一解决。IDE也重装过还是不行，最后终于解决：方法就是卸载Pycharm，清理pycharm在系统中的所有文件和记录，包括注册表之类的跟pycharm有关的全部清理干净，再重装Pycharm，问题解决。<br> <br></p> 
<h5><a id="_29"></a>▶卷积参数量、计算量</h5> 
<p><b>参数量：（+1为偏置项）<br> <img src="https://images2.imgbox.com/1d/fe/HXXYukWu_o.png" alt="在这里插入图片描述"></b></p> 
<p><b>计算量：<br> <font color="red">不考虑输入输出通道：<br> <img src="https://images2.imgbox.com/0a/33/hy16IhN2_o.jpg" alt="在这里插入图片描述"><br> <font color="red">考虑输入输出通道：<br> <img src="https://images2.imgbox.com/2d/2e/7n5vcwEn_o.jpg" alt="在这里插入图片描述"></font></font></b></p> 
<br> 
<h5><a id="%09_lossbackwardRuntimeError_expected_dtype_Float_but_got_dtype_Long_45"></a>▶ 求完损失以后在进行反向传播 loss.backward()时报一个类型错误（RuntimeError: expected dtype Float but got dtype Long）：</h5> 
<pre><code class="prism language-python">loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>target<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
self<span class="token punctuation">.</span>opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p><font color="red"><b>RuntimeError: expected dtype Float but got dtype Long</b></font></p> 
</blockquote> 
<p><b>原因：</b>one_hot函数返回的target数据类型为Long，可以与网络输出进行前向计算，但不能反向传播梯度更新<br> <b>解决：</b>将one_hot返回的标签转成Float类型，加<b>“.float()”</b></p> 
<pre><code class="prism language-python">loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>target<span class="token punctuation">,</span> <span class="token number">10.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><b>说明：</b>这个问题不只是在用onehot函数返回target时出现，用其他方式也会有类似的错误提示，这时就需要手动perint一下target.dtype看一下标签的数据类型了，一般网络输出的是tensor.float32，所以标签也应该是float32，否则就会造成可以计算loss，但是不能backward</p> 
<br> 
<h5><a id="%09batchbatchTensorappendcatappendtorchcattensor_67"></a>▶ 网络一个batch的输出不是我们想用来计算的数据，我们希望将一轮（多个batch/多次输出）的结果拿来计算，但是，Tensor不能append，如果用cat，那初始化也是个难题，这时借助列表，将多次输出append到列表，再用torch.cat（）将列表连接并转换成tensor数据。</h5> 
<pre><code class="prism language-python">a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

a<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
a<span class="token punctuation">.</span>append<span class="token punctuation">(</span>c<span class="token punctuation">)</span>


<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span> <span class="token punctuation">[</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
     tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>


a <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>a<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
</code></pre> 
<br> 
<h5><a id="%09CNNConv2dpadding_103"></a>▶ CNN中Conv2d的padding方式</h5> 
<p>  padding=(1, 1)时，上下左右填充；<br>   padding=(1, 0)时，上下填充；<br>   padding=(0, 1)时，左右填充；</p> 
<br> 
<h5><a id="%09Pycharmrun_110"></a>▶ Pycharm中run控制台的可显示长度、显示精度、取消科学计数法……</h5> 
<p>在运行python时，有时候明明可以一行显示完的他非要给你显示为两行，看起来让人有点难受<br> <b>设置前的显示效果：</b></p> 
<pre><code class="prism language-python">Arc_out： tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token number">0.1001</span><span class="token punctuation">,</span> <span class="token number">0.1054</span><span class="token punctuation">,</span> <span class="token number">0.1033</span><span class="token punctuation">,</span> <span class="token number">0.0972</span><span class="token punctuation">,</span> <span class="token number">0.0981</span><span class="token punctuation">,</span> <span class="token number">0.1001</span><span class="token punctuation">,</span> <span class="token number">0.0981</span><span class="token punctuation">,</span> <span class="token number">0.0971</span><span class="token punctuation">,</span>
        <span class="token number">0.0973</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Net_out： tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">9.7265e-10</span><span class="token punctuation">,</span> <span class="token number">8.8703e-33</span><span class="token punctuation">,</span> <span class="token number">1.0000e+00</span><span class="token punctuation">,</span> <span class="token number">1.1067e-09</span><span class="token punctuation">,</span> <span class="token number">0.0000e+00</span><span class="token punctuation">,</span> <span class="token number">0.0000e+00</span><span class="token punctuation">,</span>
        <span class="token number">3.1043e-33</span><span class="token punctuation">,</span> <span class="token number">0.0000e+00</span><span class="token punctuation">,</span> <span class="token number">0.0000e+00</span><span class="token punctuation">,</span> <span class="token number">0.0000e+00</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p><b>设置后的效果：</b></p> 
<pre><code class="prism language-python">Arc_out： tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token number">0.1001</span><span class="token punctuation">,</span> <span class="token number">0.1054</span><span class="token punctuation">,</span> <span class="token number">0.1033</span><span class="token punctuation">,</span> <span class="token number">0.0972</span><span class="token punctuation">,</span> <span class="token number">0.0981</span><span class="token punctuation">,</span> <span class="token number">0.1001</span><span class="token punctuation">,</span> <span class="token number">0.0981</span><span class="token punctuation">,</span> <span class="token number">0.0971</span><span class="token punctuation">,</span> <span class="token number">0.0973</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Net_out： tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>是不是舒服点了？</p> 
<h6><a id="_127"></a>设置方式：</h6> 
<p>用torch.set_printoptions()方法，</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
torch<span class="token punctuation">.</span>set_printoptions<span class="token punctuation">(</span>sci_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>
</code></pre> 
<p>顺便看下该方法的原型</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">set_printoptions</span><span class="token punctuation">(</span>
        precision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>			<span class="token comment"># 精度，小数位数</span>
        threshold<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>			<span class="token comment"># </span>
        edgeitems<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>			<span class="token comment"># 省略到每个二维和一维显示多少条数据</span>
        linewidth<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>			<span class="token comment"># 每一行可显示多少个字符</span>
        profile<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>			<span class="token comment"># </span>
        sci_mode<span class="token operator">=</span><span class="token boolean">None</span>			<span class="token comment"># 科学计数法</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r"""Set options for printing. Items shamelessly taken from NumPy

    Args:
        precision: Number of digits of precision for floating point output
            (default = 4).
        threshold: Total number of array elements which trigger summarization
            rather than full `repr` (default = 1000).
        edgeitems: Number of array items in summary at beginning and end of
            each dimension (default = 3).
        linewidth: The number of characters per line for the purpose of
            inserting line breaks (default = 80). Thresholded matrices will
            ignore this parameter.
        profile: Sane defaults for pretty printing. Can override with any of
            the above options. (any one of `default`, `short`, `full`)
        sci_mode: Enable (True) or disable (False) scientific notation. If
            None (default) is specified, the value is defined by `_Formatter`
    """</span>
</code></pre> 
<p>同样，numpy也有该方法可用，但参数有些大同小异</p> 
<pre><code class="prism language-python">np<span class="token punctuation">.</span>set_printoptions<span class="token punctuation">(</span>precision<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> edgeitems<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    linewidth<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> suppress<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> nanstr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> infstr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                    formatter<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sign<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> floatmode<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwarg<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token triple-quoted-string string">"""
precision : 输出的精度，即小数点后位数
threshold : 当数组数目过大时，设置显示几个数字，其余用省略号
edgeitems: 每个维度开始和结束时摘要中的数组项数,
linewidth：一行要print多少个字符
suppress：是否采用科学计数法
infstr：无穷大的字符串表示
nanstr：浮点非数字的字符串表示
"""</span>
</code></pre> 
<br> 
<h5><a id="Nannanlossnan_181"></a>▶损失为Nan、nan；loss为nan</h5> 
<p>梯度爆炸<br> 解决：输入加BN、换激活函数、梯度截断、dropout、减小BacthSize、L2正则化等等<br> 注意：一旦出现Nan，那么网络的权重将全部不可用，整个网络都将报废，如果在出现之后没做保存，那还可以加载之前保存的来继续训练，一旦执行了保存，那么保存的权重也会是nan，只得考虑重新训练了。<br> <br></p> 
<h5><a id="_187"></a>▶过拟合、发现与解决</h5> 
<p><b>解释：</b>“在训练数据上能够获得比其他假设更好的拟合， 但是在训练数据外的数据集上却不能很好地拟合数据”，这是百度百科的解释。通俗的讲就是模型学的太较真了，抬杠，举个栗子：训练数据里面的螃蟹都是八跪而二螯，在测试或使用的时候，输入的螃蟹是八条腿两只钳模型能认识，但输入的图片中螃蟹少了一条或多条腿，模型就判定不是螃蟹。这就是一种过拟合现象。<br> <b>解决：</b>（1）增大增强数据集；（2）dropout：按比例随机屏蔽掉一些神经元；（3）L1、L2正则化：L1会使某些神经元死掉，权重为0，L2根据神经元权重对损失的影响因子进行权重抑制，不会为0。<br> <br></p> 
<h5><a id="_192"></a>▶二范数、欧氏距离、向量求模</h5> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>dist<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>		<span class="token comment"># 求ab向量的模、距离、范数</span>
</code></pre> 
<br> 
<h5><a id="lambda_200"></a>▶二维向量按某列排序用lambda匿名函数</h5> 
<pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
a <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>		<span class="token comment"># x为枚举的a中的每个元素，x[2]为a中每个元素的下标[2]元素</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
</code></pre> 
<blockquote> 
 <p>执行结果：<br> [[1, 2, 1],<br> [1, 2, 2],<br> [1, 2, 3]]</p> 
</blockquote> 
<h5><a id="RuntimeError_cuDNN_error_CUDNN_STATUS_BAD_PARAM_216"></a>▶RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</h5> 
<p>在将输入数据送入到网络模型的时候，报错：</p> 
<blockquote> 
 <p>RuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM</p> 
</blockquote> 
<p><b>原因：</b><br>   这个错误提示并没有说是什么原因，也没说什么问题，因为数据在GPU（cuda）上，所以提示与普通错误提示不太一样，将数据放回到 CPU 上重新运行代码，可以看到一个更加清楚地错误描述，其根本问题是数据类型不匹配，模型期望的是一个 Float 类型的数据，而我们传入的是另一个数据类型。</p> 
<p><b> 解决：</b><br>   OK，问题找到了，把数据转成float再计算就搞定了。</p> 
<br> 
<h5><a id="PILOpenCV_228"></a>▶PIL和OpenCV互转</h5> 
<h6><a id="PIL__CV2_229"></a>PIL &gt;&gt; CV2:</h6> 
<pre><code class="prism language-python"><span class="token keyword">import</span> cv2  
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image  
<span class="token keyword">import</span> numpy  
  
image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"plane.jpg"</span><span class="token punctuation">)</span>  
image<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>  
img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>numpy<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>COLOR_RGB2BGR<span class="token punctuation">)</span>  
cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"OpenCV"</span><span class="token punctuation">,</span>img<span class="token punctuation">)</span>  
cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<h6><a id="CV2__PIL_242"></a>CV2 &gt;&gt; PIL:</h6> 
<pre><code class="prism language-python"><span class="token keyword">import</span> cv2  
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image  
<span class="token keyword">import</span> numpy  
  
img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>imread<span class="token punctuation">(</span><span class="token string">"plane.jpg"</span><span class="token punctuation">)</span>  
cv2<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span><span class="token string">"OpenCV"</span><span class="token punctuation">,</span>img<span class="token punctuation">)</span>  
image <span class="token operator">=</span> Image<span class="token punctuation">.</span>fromarray<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>img<span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>COLOR_BGR2RGB<span class="token punctuation">)</span><span class="token punctuation">)</span>  
image<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>  
cv2<span class="token punctuation">.</span>waitKey<span class="token punctuation">(</span><span class="token punctuation">)</span>  
</code></pre> 
<h6><a id="OpenCV_255"></a>判断图像数据是否是OpenCV格式：</h6> 
<pre><code class="prism language-python"><span class="token builtin">isinstance</span><span class="token punctuation">(</span>img<span class="token punctuation">,</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span>
</code></pre> 
<h5><a id="pythonopenCVRGB_260"></a>▶python+openCV通道拆分（R、G、B）和合并</h5> 
<p>拆分</p> 
<pre><code class="prism language-python">B<span class="token punctuation">,</span>G<span class="token punctuation">,</span>R <span class="token operator">=</span> cv2<span class="token punctuation">.</span>split<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
</code></pre> 
<p>合并</p> 
<pre><code class="prism language-python">image <span class="token operator">=</span> cv2<span class="token punctuation">.</span>merge<span class="token punctuation">(</span><span class="token punctuation">[</span>B<span class="token punctuation">,</span> G<span class="token punctuation">,</span> R<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="PILsave_273"></a>▶PIL的save命令保存下来的图片的质量问题</h5> 
<p>  该save方法有一个参数叫quality, 默认的值为95，表示保存下来的图片质量会压缩到原来的95%，会存在一些噪点噪声之类的，所以在保存的时候尽量手动设置一下该参数为quality=100，这样保存下来的照片才不会白压缩。</p> 
<h5><a id="_reduce_failed_to_synchronize_cudaErrorAssert_deviceside_assert_triggered_276"></a>▶ reduce failed to synchronize: cudaErrorAssert: device-side assert triggered</h5> 
<p>这个问题是由于GPU数据计算BCELoss出错导致的，如果转入CPU里计算的话错误提示为：</p> 
<h6><a id="all_elements_of_input_should_be_between_0_and_1_278"></a>all elements of input should be between 0 and 1</h6> 
<p>可以看出是input超出0 ~ 1的范围了，因为BCELoss是0~1之间进行二分类，所以注意检查sigmoid的作用范围，检查BCELoss的输入是否符合0 ~1分布</p> 
<h5><a id="PytorchRNNLSTMUserWarning_RNN_module_weights_are_not_part_of_single_contiguous_chunk_of_memory_282"></a>▶Pytorch跑RNN、LSTM等结构时报：'UserWarning: RNN module weights are not part of single contiguous chunk of memory</h5> 
<p><b>使用DataParallel的情况下，RNN才会报这个提示。</b></p> 
<p>这个并不是一个错误，只是一个警告，就是说RNN的权值并不是单一连续的，这些权值在每一次RNN被调用的时候都会被压缩，会很大程度上增加显存消耗。使用flatten_parameters()把权重存成连续的形式，可以提高内存利用率。<br> <font color="red">解决方案：在forward()里面加一句self.xxx.flatten_parameters()就可以解决了。<br> 比如你有这样两层rnn:</font></p> 
<pre><code class="prism language-bash">def __init__<span class="token punctuation">(</span>self<span class="token punctuation">)</span>:
	super<span class="token punctuation">(</span><span class="token punctuation">)</span>.__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
	self.lstm1 <span class="token operator">=</span> nn.LSTM<span class="token punctuation">(</span><span class="token number">180</span>, <span class="token number">512</span>, <span class="token number">2</span>, <span class="token assign-left variable">batch_first</span><span class="token operator">=</span>True<span class="token punctuation">)</span>
	self.lstm2 <span class="token operator">=</span> nn.LSTM<span class="token punctuation">(</span><span class="token number">512</span>, <span class="token number">128</span>, <span class="token number">2</span>, <span class="token assign-left variable">batch_first</span><span class="token operator">=</span>True<span class="token punctuation">)</span>
</code></pre> 
<p>那你就在forward()里面加上：</p> 
<pre><code class="prism language-bash">def forward<span class="token punctuation">(</span>self, x<span class="token punctuation">)</span>:
	self.lstm1.flatten_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
	self.lstm2.flatten_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h5><a id="ValueError_Expected_more_than_1_value_per_channel_when_training_got_input_size_torchSize1_512_304"></a>▶ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 512]</h5> 
<p>这个问题，我看了博客上很多这个类似问题，引起的原因也不尽一样，<br> 我的引起原因是在使用的时候没有将model置为eval状态，在模型初始化后加一句“model.eval()”就解决了。</p> 
<h5><a id="MXNETlinux_308"></a>▶MXNET警告提示(linux)：</h5> 
<blockquote> 
 <p>src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the bestconvolution algorithm, this can take a while… (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0to disable)</p> 
</blockquote> 
<p><b>解决方法：跑代码之前，输入:</b></p> 
<pre><code class="prism language-python">export MXNET_CUDNN_AUTOTUNE_DEFAULT <span class="token operator">=</span> <span class="token number">0</span>
</code></pre> 
<h5><a id="_pythonopencv_318"></a>▶解决 python-opencv打开\读取或保存\写入中文路径的问题</h5> 
<pre><code class="prism language-python"><span class="token comment"># 读取图片</span>
im<span class="token operator">=</span>cv2<span class="token punctuation">.</span>imdecode<span class="token punctuation">(</span>np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token string">'c:\\测试\\1.jpg'</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>IMREAD_UNCHANGED<span class="token punctuation">)</span>

<span class="token comment"># 保存图片</span>
cv2<span class="token punctuation">.</span>imencode<span class="token punctuation">(</span><span class="token string">'.jpg'</span><span class="token punctuation">,</span>im<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tofile<span class="token punctuation">(</span><span class="token string">'C:\\测试\\你好.jpg'</span><span class="token punctuation">)</span><span class="token comment">#保存图片</span>
</code></pre> 
<h5><a id="matplotlibOMP_Error_15_Initializing_libiomp5mddll_but_found_libiomp5mddll_already_initialized_328"></a>▶matplotlib画折线图：OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.</h5> 
<p>完整错误信息：</p> 
<blockquote>
  OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized. OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/. 
</blockquote> 
<p>各大网站有很多关于:</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'KMP_DUPLICATE_LIB_OK'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'True'</span>
</code></pre> 
<p>的解决方案，不过我没试过，暂不排除它的可行性。<br> <strong>我的解决方案</strong>：<br> 1、将numpy从 <code>1.23.5</code> 升级到 <code>1.24.0</code> 即解决了。2023/03/28，当前可用的最新numpy版本为<code>1.24.2</code>，考虑最新版本可能存在一些未知的bug,所以只升级到大版本<code>24.0</code></p> 
<p>2、又将numpy降到原来的 <code>1.23.5</code> ，问题依然被解决</p> 
<p>综上所述：应该是<code>numpy</code>的某个依赖库出现了变更导致此问题，重装<code>(或升级)</code>numpy即可解决。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/71e94b06910b017eaae32ce2e8be31c7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">JavaScript 删除数组中指定元素(5种方法)</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/20a181f5241638204446feff7476a737/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mac 安装Docker</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>