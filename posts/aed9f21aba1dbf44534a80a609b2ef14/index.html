<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch并行训练方法-单机多卡 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch并行训练方法-单机多卡" />
<meta property="og:description" content="简单方便的 nn.DataParallel DataParallel 可以帮助我们（使用单进程控）将模型和数据加载到多个 GPU 中，控制数据在 GPU 之间的流动，协同不同 GPU 上的模型进行并行训练（细粒度的方法有 scatter，gather 等等）。
DataParallel 使用起来非常方便，我们只需要用 DataParallel 包装模型，再设置一些参数即可。需要定义的参数包括：参与训练的 GPU 有哪些，device_ids=gpus；用于汇总梯度的 GPU 是哪个，output_device=gpus[0] 。DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总：
model = nn.DataParallel(model.cuda(), device_ids=gpus, output_device=gpus[0]) 值得注意的是，模型和数据都需要先 load 进 GPU 中，DataParallel 的 module 才能对其进行处理，否则会报错：
# 这里要 model.cuda() model = nn.DataParallel(model.cuda(), device_ids=gpus, output_device=gpus[0]) for epoch in range(100): for batch_idx, (data, target) in enumerate(train_loader): # 这里要 images/target.cuda() images = images.cuda(non_blocking=True) target = target.cuda(non_blocking=True) ... output = model(images) loss = criterion(output, target) ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/aed9f21aba1dbf44534a80a609b2ef14/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-18T11:37:19+08:00" />
<meta property="article:modified_time" content="2023-01-18T11:37:19+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch并行训练方法-单机多卡</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_nnDataParallel_0"></a>简单方便的 nn.DataParallel</h3> 
<blockquote> 
 <p>DataParallel 可以帮助我们（使用单进程控）将模型和数据加载到多个 GPU 中，控制数据在 GPU 之间的流动，协同不同 GPU 上的模型进行并行训练（细粒度的方法有 scatter，gather 等等）。</p> 
</blockquote> 
<p>DataParallel 使用起来非常方便，我们只需要用 DataParallel 包装模型，再设置一些参数即可。需要定义的参数包括：参与训练的 GPU 有哪些，<code>device_ids=gpus</code>；用于汇总梯度的 GPU 是哪个，<code>output_device=gpus[0]</code> 。DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总：</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> nn.DataParallel<span class="token punctuation">(</span>model.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span>gpus, <span class="token assign-left variable">output_device</span><span class="token operator">=</span>gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>值得注意的是，模型和数据都需要先 load 进 GPU 中，DataParallel 的 module 才能对其进行处理，否则会报错：</p> 
<pre><code class="prism language-bash"><span class="token comment"># 这里要 model.cuda()</span>
model <span class="token operator">=</span> nn.DataParallel<span class="token punctuation">(</span>model.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span>gpus, <span class="token assign-left variable">output_device</span><span class="token operator">=</span>gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      <span class="token comment"># 这里要 images/target.cuda()</span>
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>汇总一下，DataParallel 并行训练部分主要与如下代码段有关：</p> 
<pre><code class="prism language-bash"><span class="token comment"># main.py</span>
<span class="token function">import</span> torch
<span class="token function">import</span> torch.distributed as dist

gpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span>, <span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span><span class="token punctuation">]</span>
torch.cuda.set_device<span class="token punctuation">(</span><span class="token string">'cuda:{}'</span>.format<span class="token punctuation">(</span>gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">))</span>

train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>.<span class="token punctuation">)</span>

model <span class="token operator">=</span> <span class="token punctuation">..</span>.
model <span class="token operator">=</span> nn.DataParallel<span class="token punctuation">(</span>model.to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span>gpus, <span class="token assign-left variable">output_device</span><span class="token operator">=</span>gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在使用时，使用 python 执行即可：</p> 
<pre><code class="prism language-bash">python main.py
</code></pre> 
<h3><a id="_torchdistributed__64"></a>使用 torch.distributed 加速并行训练</h3> 
<blockquote> 
 <p>在 pytorch 1.0 之后，官方终于对分布式的常用方法进行了封装，支持 all-reduce，broadcast，send 和 receive 等等。通过 MPI 实现 CPU 通信，通过 NCCL 实现 GPU 通信。官方也曾经提到用 DistributedDataParallel 解决 DataParallel 速度慢，GPU 负载不均衡的问题，目前已经很成熟了～</p> 
</blockquote> 
<p>与 <strong>DataParallel 的单进程控制多 GPU 不同</strong>，在 distributed 的帮助下，我们只需要编写一份代码，torch 就会自动将其分配给 n个进程，分别在n个 GPU 上运行。</p> 
<p>在 API 层面，pytorch 为我们提供了 torch.distributed.launch 启动器，用于在命令行分布式地执行 python 文件。在执行过程中，启动器会将当前进程的（其实就是 GPU的）index 通过参数传递给 python，我们可以这样获得当前进程的 index：</p> 
<pre><code class="prism language-bash">parser <span class="token operator">=</span> argparse.ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser.add_argument<span class="token punctuation">(</span><span class="token string">'--local_rank'</span>, <span class="token assign-left variable">default</span><span class="token operator">=</span>-1, <span class="token assign-left variable">type</span><span class="token operator">=</span>int,
                    <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'node rank for distributed training'</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser.parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>
</code></pre> 
<p>接着，使用 init_process_group 设置GPU 之间通信使用的后端和端口：</p> 
<pre><code class="prism language-bash">dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">)</span>
</code></pre> 
<p>之后，使用 DistributedSampler 对数据集进行划分。如此前我们介绍的那样，它能帮助我们将每个 batch 划分成几个 partition，在当前进程中只需要获取和 rank 对应的那个 partition 进行训练：</p> 
<pre><code class="prism language-bash">train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>
</code></pre> 
<p>然后，使用 DistributedDataParallel 包装模型，它能帮助我们为不同 GPU 上求得的梯度进行 all reduce（即汇总不同 GPU 计算所得的梯度，并同步计算结果）。all reduce 后不同 GPU 中模型的梯度均为 all reduce 之前各 GPU 梯度的均值：</p> 
<pre><code class="prism language-bash">model <span class="token operator">=</span> torch.nn.parallel.DistributedDataParallel<span class="token punctuation">(</span>model, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span><span class="token punctuation">[</span>args.local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>最后，把数据和模型加载到当前进程使用的 GPU 中，正常进行正反向传播：</p> 
<pre><code class="prism language-bash">torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

model.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>汇总一下，torch.distributed 并行训练部分主要与如下代码段有关：</p> 
<pre><code class="prism language-bash"><span class="token comment"># main.py</span>
<span class="token function">import</span> torch
<span class="token function">import</span> argparse
<span class="token function">import</span> torch.distributed as dist

parser <span class="token operator">=</span> argparse.ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser.add_argument<span class="token punctuation">(</span><span class="token string">'--local_rank'</span>, <span class="token assign-left variable">default</span><span class="token operator">=</span>-1, <span class="token assign-left variable">type</span><span class="token operator">=</span>int,
                    <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'node rank for distributed training'</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser.parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">)</span>
torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.
train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>

model <span class="token operator">=</span> <span class="token punctuation">..</span>.
model <span class="token operator">=</span> torch.nn.parallel.DistributedDataParallel<span class="token punctuation">(</span>model, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span><span class="token punctuation">[</span>args.local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>在使用时，调用 torch.distributed.launch 启动器启动：</strong></p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> main.py
</code></pre> 
<h6><a id="1torchdistributedlaunch_158"></a>（1）torch.distributed.launch相关的环境变量</h6> 
<ul><li>torch.ditributed.launch参数解析：</li></ul> 
<blockquote> 
 <p><strong>nnodes</strong>: 表示有多少个节点，可以通俗的理解为有多少台机器，比如，–nnodes=2，是指有两个节点（即两台机器）参与训练<br> <strong>nproc_per_node</strong>: 表示每个节点上有多少个进程，每个进程一般独占一块GPU，但是这并不绝对，要看具体的实现；<br> <strong>node_rank</strong>: 指节点的编号，比如上例中在机器A上启动时，–node_rank=0,<br> 指A机器的节点编号是0；在机器B上启动时，–node_rank=1，指B机器上的节点编号是1<br> <strong>master_addr</strong>：master节点的ip地址<br> <strong>master_port</strong>：master节点的port号，在不同的节点上master_addr和master_port的设置是一样的，用来进行通信</p> 
</blockquote> 
<ul><li>torch.ditributed.launch相关环境变量解析：</li></ul> 
<blockquote> 
 <p><strong>WORLD_SIZE</strong>: 通俗的解释下，就是一共有多少个进程参与训练，WORLD_SIZE =nproc_per_node*nnodes，不同的进程中，WORLD_SIZE是唯一的；<br> <strong>RANK</strong>：进程的唯一表示符，不同的进程中，这个值是不同的，上述在AB两台机器上共启动了8个进程，则不同进程的RANK号是不同的<br> <strong>LOCAL_RANK</strong>：同一节点下，LOCAL_RANK是不同的，常根据LOCAL_RANK来指定GPU，但GPU跟LOCAL_RANK不一定一一对应，因为进程不一定被限制在同一块GPU上。</p> 
</blockquote> 
<p>试验train.py</p> 
<pre><code>import torch
import torch.distributed as dist
import os
import time
print(os.environ)
dist.init_process_group('nccl')
time.sleep(30)
dist.destroy_process_group()
</code></pre> 
<p>在A机器上调用如下命令<code>python -m torch.distributed.launch --nproc_per_node 4 --nnodes 2 --node_rank 0 --master_addr='10.100.37.21' --master_port='29500' train.py</code>， 在B机器上调用如下命令<code>python -m torch.distributed.launch --nproc_per_node 4 --nnodes 2 --node_rank 1 --master_addr='10.100.37.21', --master_port='29500' train.py</code></p> 
<p>机器A的显示信息如下：</p> 
<pre><code>python -m torch.distributed.launch --nproc_per_node 4 --nnodes 2 --node_rank 0 --master_addr='10.100.37.21' --master_port='29500' train1.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
environ({'XDG_SESSION_ID': '40164', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51550 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'OLDPWD': '/home/zhangwd', 'SSH_TTY': '/dev/pts/80', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/usr/local/cuda/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd/code/demo/self-supervised/pytorch_dist', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51550 10.100.37.21 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1013', 'DISPLAY': 'localhost:11.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '0', 'LOCAL_RANK': '0'})

environ({'XDG_SESSION_ID': '40164', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51550 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'OLDPWD': '/home/zhangwd', 'SSH_TTY': '/dev/pts/80', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/usr/local/cuda/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd/code/demo/self-supervised/pytorch_dist', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51550 10.100.37.21 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1013', 'DISPLAY': 'localhost:11.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '2', 'LOCAL_RANK': '2'})


environ({'XDG_SESSION_ID': '40164', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51550 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'OLDPWD': '/home/zhangwd', 'SSH_TTY': '/dev/pts/80', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/usr/local/cuda/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd/code/demo/self-supervised/pytorch_dist', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51550 10.100.37.21 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1013', 'DISPLAY': 'localhost:11.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '1', 'LOCAL_RANK': '1'})


environ({'XDG_SESSION_ID': '40164', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51550 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'OLDPWD': '/home/zhangwd', 'SSH_TTY': '/dev/pts/80', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/usr/local/cuda/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd/code/demo/self-supervised/pytorch_dist', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51550 10.100.37.21 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1013', 'DISPLAY': 'localhost:11.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '3', 'LOCAL_RANK': '3'})

</code></pre> 
<p>机器B的显示信息如下：</p> 
<pre><code>python -m torch.distributed.launch --nproc_per_node 4 --nnodes 2 --node_rank 1 --master_addr='10.100.37.21' --master_port='29500' train1.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************
environ({'XDG_SESSION_ID': '4475', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51380 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'SSH_TTY': '/dev/pts/4', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', 'MKL_INTERFACE_LAYER': 'LP64,GNU', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_MKL_INTERFACE_LAYER_BACKUP': '', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51380 10.100.37.100 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1005', 'DISPLAY': 'localhost:12.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '4', 'LOCAL_RANK': '0'})


environ({'XDG_SESSION_ID': '4475', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51380 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'SSH_TTY': '/dev/pts/4', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', 'MKL_INTERFACE_LAYER': 'LP64,GNU', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_MKL_INTERFACE_LAYER_BACKUP': '', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51380 10.100.37.100 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1005', 'DISPLAY': 'localhost:12.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '7', 'LOCAL_RANK': '3'})


environ({'XDG_SESSION_ID': '4475', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51380 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'SSH_TTY': '/dev/pts/4', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', 'MKL_INTERFACE_LAYER': 'LP64,GNU', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_MKL_INTERFACE_LAYER_BACKUP': '', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51380 10.100.37.100 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1005', 'DISPLAY': 'localhost:12.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '5', 'LOCAL_RANK': '1'})


environ({'XDG_SESSION_ID': '4475', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'SSH_CLIENT': '172.16.104.139 51380 22', 'CONDA_SHLVL': '2', 'CONDA_PROMPT_MODIFIER': '(pytorch1.8) ', 'SSH_TTY': '/dev/pts/4', 'http_proxy': 'http://172.16.17.164:3128', 'USER': 'zhangwd', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/zhangwd/anaconda3/bin/conda', 'MKL_INTERFACE_LAYER': 'LP64,GNU', '_CE_CONDA': '', 'CONDA_PREFIX_1': '/home/zhangwd/anaconda3', 'MAIL': '/var/mail/zhangwd', 'PATH': '/home/zhangwd/bin:/home/zhangwd/.local/bin:/home/zhangwd/anaconda3/envs/pytorch1.8/bin:/home/zhangwd/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_MKL_INTERFACE_LAYER_BACKUP': '', 'CONDA_PREFIX': '/home/zhangwd/anaconda3/envs/pytorch1.8', 'PWD': '/home/zhangwd', 'LANG': 'en_US.UTF-8', 'https_proxy': 'http://172.16.17.164:3128', '_CE_M': '', 'SHLVL': '1', 'HOME': '/home/zhangwd', 'LANGUAGE': 'en_US:en', 'CONDA_PYTHON_EXE': '/home/zhangwd/anaconda3/bin/python', 'LOGNAME': 'zhangwd', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SSH_CONNECTION': '172.16.104.139 51380 10.100.37.100 22', 'CONDA_DEFAULT_ENV': 'pytorch1.8', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'XDG_RUNTIME_DIR': '/run/user/1005', 'DISPLAY': 'localhost:12.0', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', '_': '/home/zhangwd/anaconda3/envs/pytorch1.8/bin/python', 'MASTER_ADDR': '10.100.37.21', 'MASTER_PORT': '29500', 'WORLD_SIZE': '8', 'OMP_NUM_THREADS': '1', 'RANK': '6', 'LOCAL_RANK': '2'})

</code></pre> 
<p>环境变量中’MASTER_ADDR’: ‘10.100.37.21’, ‘MASTER_PORT’: ‘29500’, ‘WORLD_SIZE’: ‘8’, ‘OMP_NUM_THREADS’: ‘1’, ‘RANK’: ‘6’, ‘LOCAL_RANK’: ‘2’,是跟我们的分布式训练相关的几个变量。</p> 
<h6><a id="2_DistributedDataParallel_229"></a>（2） DistributedDataParallel是如何做同步的</h6> 
<ul><li>在开始试验之前我们先说明<code>DataParallel</code>，当我们使用<code>DataParallel</code>去做分布式训练时，假设我们使用四块显卡去做训练，数据的<code>batch_size</code>设置为<code>8</code>，则程序启动时只启动一个进程，每块卡会分配<code>batch_size=2</code>的资源进行<code>forward</code>操作，当4快卡的<code>forward</code>操作做完之后，主进程会收集所有显卡的结果进行<code>loss</code>运算和梯度回传以及参数更新，这些都在主进程中完成，也就是说主进程看到看到的<code>forward</code>运算的结果是<code>batch_size=8</code>的。</li><li>当我们用<code>DistributedDataParallel</code>去做分布式训练时，假设我们使用4块显卡训练，总的<code>batch_size</code>设置为<code>8</code>，程序启动时会同时启动<strong>四个进程</strong>，<strong>每个进程会负责一块显卡</strong>，每块显卡对<strong>batch_size=2</strong>的数据进行<strong>forward</strong>操作，<strong>在每个进程中，程序都会进行的loss的运算、梯度回传以及参数更新，</strong> <strong>与DataParallel的区别是，每个进程都会进行loss的计算、梯度回传以及参数更新</strong>。这里抛出我们的问题，既然每个进程都会进行loss计算与梯度回传是怎么保证模型训练的同步的呢？</li></ul> 
<p>试验：</p> 
<ol><li>数据类<code>datasets.py</code>: 这个数据类随机生成<code>224x224</code>大小的图像和其对应的随机标签<code>0-8</code></li></ol> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">RandomClsDS</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token number">10000</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>
        image <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>
        label <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> image<span class="token punctuation">,</span> label
</code></pre> 
<ol start="2"><li>训练类train.py</li></ol> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> sys
sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>__file__<span class="token punctuation">)</span><span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>pardir<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">from</span> datasets<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> XRrayDS<span class="token punctuation">,</span> RandomClsDS
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm

<span class="token keyword">def</span> <span class="token function">reduce_val</span><span class="token punctuation">(</span>val<span class="token punctuation">)</span><span class="token punctuation">:</span>
    world_size <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>val<span class="token punctuation">,</span> async_op<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        val <span class="token operator">/=</span> world_size
    <span class="token keyword">return</span> val

local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
world_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'WORLD_SIZE'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">'nccl'</span><span class="token punctuation">,</span>world_size<span class="token operator">=</span>world_size<span class="token punctuation">,</span> rank<span class="token operator">=</span>rank<span class="token punctuation">)</span>


torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>


train_ds <span class="token operator">=</span> RandomClsDS<span class="token punctuation">(</span><span class="token punctuation">)</span>
train_dataloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

train_dataloader <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span>

model <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">9</span><span class="token punctuation">)</span>

model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span>local_rank<span class="token punctuation">)</span>

criterion <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>



<span class="token keyword">for</span> index<span class="token punctuation">,</span> <span class="token punctuation">(</span>images<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> index <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token comment"># print(model.module.fc.weight.grad[0][0])</span>
        <span class="token keyword">pass</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> labels<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> reduce_val<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>module<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">break</span>
</code></pre> 
<p><strong>试验过程1</strong><br> 执行<code>CUDA_VISIBLE_DEVICES=2,3,6,7 python -m torch.distributed.launch --nproc_per_node=4 --nnodes=1 --node_rank=0 --master_addr='10.100.37.21' --master_port='29500' train.py</code>命令，显示结果如下：</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">4</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> train<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token number">2.5341</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.0336</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:2'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.3375</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.5774</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:3'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:2'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:3'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p><strong>解析：</strong><br> 试验中程序通过<code>DistributedDataParallel</code>并行启动了4个进程，过程中只象征性的计算了一次<code>forward</code>操作并针对结果计算了<code>loss</code>，结果如图</p> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token number">2.5341</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.0336</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:2'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.3375</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.5774</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:3'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<p>过程中，每个进程的loss都不相同，但是当做了loss.backward()操作时，4块显卡上的模型的梯度更新却是相同的，如图</p> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:2'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:3'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.0832</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p>猜测：<code>DistributedDataParallel</code>在做loss回传时，内部应该有同步机制，不同进程中的<code>loss</code>**会先计算完，然后内部可能做了reduce操作，然后回传计算更新梯度，这样也解释了4个进程中的loss是不同的，但是更新之后模型参数的梯度是相同的，然后调用优化器的参数更新步骤，那么每个进程中的模型在一次迭代后，参数的更新结果也是相同的。**上述的过程，我重复了很多遍会发现不同进程间<code>loss</code>的打印一定在<code>grad</code>的打印之前出现，猜测在<code>loss.backward()</code>操作的前或后应该有同步机制，个人猜测这个同步机制应该在loss.backward()中？？？,待求证。</p> 
<p><strong>试验过程2</strong><br> 上述的代码改为如下启动，即模拟在两个节点上（即两台机器上，本例中还是同一台机器）启动训练过程：</p> 
<p>命令1： <code>CUDA_VISIBLE_DEVICES=2,3 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr='10.100.37.21' --master_port='29500' train.py</code></p> 
<p>命令2： <code>CUDA_VISIBLE_DEVICES=6,7 python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=1 --master_addr='10.100.37.21' --master_port='29500' train.py</code></p> 
<p>只启动命令1时，程序会被堵塞：</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> train<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>

</code></pre> 
<p>直到命令2同时启动时, 程序才能正常执行：</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> train<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token number">1.9137</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">2.3257</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">0.0404</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">0.0404</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p><a href="https://blog.csdn.net/searobbers_duck/article/details/115247025#comments_24407893">这应该也是torch.distributed自带的同步机制。<br> https://blog.csdn.net/searobbers_duck/article/details/115247025#comments_24407893</a></p> 
<h6><a id="3_torchutilsdatadistributedDistributedSampler_379"></a>（3） 分布式训练时，torch.utils.data.distributed.DistributedSampler做了什么？</h6> 
<p>试验用到的code</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> sys

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torchvision

<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader

<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">class</span> <span class="token class-name">InnerDS</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>n <span class="token operator">=</span> n

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>n

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>
        np_img <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">)</span>
        image <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_img<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        label <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> image<span class="token punctuation">,</span> label<span class="token punctuation">,</span> item


local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
world_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'WORLD_SIZE'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">'nccl'</span><span class="token punctuation">,</span>world_size<span class="token operator">=</span>world_size<span class="token punctuation">,</span> rank<span class="token operator">=</span>rank<span class="token punctuation">)</span>


torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>


<span class="token comment"># case 1</span>
<span class="token comment"># ds = InnerDS(8)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, drop_last=True)</span>

<span class="token comment"># case 2</span>
<span class="token comment"># ds = InnerDS(8)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, sampler=sampler, drop_last=True)</span>

<span class="token comment"># case 3</span>
<span class="token comment"># ds = InnerDS(8)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, sampler=sampler, drop_last=True)</span>

<span class="token comment"># case 4</span>
<span class="token comment"># ds = InnerDS(6)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, sampler=sampler, drop_last=False)</span>


<span class="token comment"># case 5</span>
<span class="token comment"># ds = InnerDS(5)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, sampler=sampler, drop_last=False)</span>

<span class="token comment"># case 6</span>
<span class="token comment"># ds = InnerDS(10)</span>
<span class="token comment"># sampler = torch.utils.data.distributed.DistributedSampler(ds)</span>
<span class="token comment"># dataloader = DataLoader(ds, batch_size=4, sampler=sampler, drop_last=False)</span>

<span class="token comment"># case 7</span>
ds <span class="token operator">=</span> InnerDS<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>ds<span class="token punctuation">)</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>sampler<span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>


<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># case 3+</span>
    <span class="token comment"># sampler.set_epoch(epoch)</span>
    <span class="token keyword">for</span> index<span class="token punctuation">,</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span>labels<span class="token punctuation">,</span> items<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>items<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># print('epoch:\t', epoch)</span>
        dist<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p><strong>试验过程</strong>：<br> 执行code： case 1, 不使用<code>torch.utils.data.distributed.DistributedSampler</code>, 结果显示，每块卡上（每个进程）每个epoch中都迭代了所有的数据。</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p>执行code: case 2, 使用<code>torch.utils.data.distributed.DistributedSampler,</code> 结果显示，数据被平分到两块卡上，每个epoch被分配到每块卡上的数据都一样。</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>

</code></pre> 
<p>为了解决case 2中每块卡上分配的数据相同的问题，执行code: case 3, 在每个epoch中加入<code>sampler.set_epoch(epoch)</code></p> 
<pre><code class="prism language-python"> CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p>执行code: case 4, 数据集里有6例数据，在两张卡，<code>batch_size=4, drop_last=False</code>时，每张卡上平均分配了3例数据；当<code>drop_last=True</code>时，不足4例数据的会被丢掉，在数据集只有6例数据时，每张卡上分配的3例数据都会被丢掉；</p> 
<pre><code class="prism language-python"> CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>

</code></pre> 
<p>执行code: case 5， 数据集里有5例数据，两张卡，<code>batch_size=4, drop_last=False</code>时，每张卡上平均分配了2.5例数据， 会向上补齐到6例数据，每张卡上三张，补齐的标准是把数据集的第一例数据（在本例1中index=4）用来补齐；如果将<code>sampler.set_epoch(epoch)</code>加入其中，补齐标准不变，在本例2中，第一个epoch补齐的是index=4，第二个epoch补齐的是index=0</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>

</code></pre> 
<p>当多进程同时工作时，执行case 6时，有的迭代中，会出现<code>batch_size=1</code>的情况，如果模型中存在<code>BatchNormalize</code>这样的模块时，运行可能报错。</p> 
<pre><code class="prism language-python"> CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>

</code></pre> 
<p>为了避免case 6这种情况，可以引入<code>BatchSampler</code>这样的模块，运行case 7, 将<code>drop_last=True</code></p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span> python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>nproc_per_node<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">-</span><span class="token operator">-</span>nnodes<span class="token operator">=</span><span class="token number">1</span> <span class="token operator">-</span><span class="token operator">-</span>node_rank<span class="token operator">=</span><span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>master_addr<span class="token operator">=</span><span class="token string">'10.100.37.21'</span> <span class="token operator">-</span><span class="token operator">-</span>master_port<span class="token operator">=</span><span class="token string">'29500'</span> exp_ds_dist_sampler<span class="token punctuation">.</span>py
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
Setting OMP_NUM_THREADS environment variable <span class="token keyword">for</span> each process to be <span class="token number">1</span> <span class="token keyword">in</span> default<span class="token punctuation">,</span> to avoid your system being overloaded<span class="token punctuation">,</span> please further tune the variable <span class="token keyword">for</span> optimal performance <span class="token keyword">in</span> your application <span class="token keyword">as</span> needed<span class="token punctuation">.</span>
<span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">**</span><span class="token operator">*</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>

</code></pre> 
<p><a href="https://blog.csdn.net/searobbers_duck/article/details/115299691">https://blog.csdn.net/searobbers_duck/article/details/115299691</a></p> 
<p><img src="https://images2.imgbox.com/ae/a7/DEhsH4kZ_o.png" alt="在这里插入图片描述"><br> <a href="https://blog.csdn.net/shenjianhua005/article/details/121485522">https://blog.csdn.net/shenjianhua005/article/details/121485522</a></p> 
<h3><a id="_torchmultiprocessing__581"></a>使用 torch.multiprocessing 取代启动器</h3> 
<blockquote> 
 <p>有的同学可能比较熟悉 torch.multiprocessing，也可以手动使用 torch.multiprocessing<br> 进行多进程控制。绕开 torch.distributed.launch 自动控制开启和退出进程的一些小毛病～</p> 
</blockquote> 
<p>使用时，只需要调用 torch.multiprocessing.spawn，torch.multiprocessing 就会帮助我们自动创建进程。如下面的代码所示，spawn 开启了 nprocs=4 个进程，每个进程执行 main_worker 并向其中传入 local_rank（当前进程 index）和 args（即 4 和 myargs）作为参数：</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> torch.multiprocessing as mp

mp.spawn<span class="token punctuation">(</span>main_worker, <span class="token assign-left variable">nprocs</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">args</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span>, myargs<span class="token punctuation">))</span>
</code></pre> 
<p>这里，我们直接将原本需要 torch.distributed.launch 管理的执行内容，封装进 main_worker 函数中，其中 proc 对应 local_rank（当前进程 index），进程数 nprocs 对应 4， args 对应 myargs：</p> 
<pre><code class="prism language-bash">def main_worker<span class="token punctuation">(</span>proc, nprocs, args<span class="token punctuation">)</span>:

   dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span>, <span class="token assign-left variable">init_method</span><span class="token operator">=</span><span class="token string">'tcp://127.0.0.1:23456'</span>, <span class="token assign-left variable">world_size</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">rank</span><span class="token operator">=</span>gpu<span class="token punctuation">)</span>
   torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

   train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.
   train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

   train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>

   model <span class="token operator">=</span> <span class="token punctuation">..</span>.
   model <span class="token operator">=</span> torch.nn.parallel.DistributedDataParallel<span class="token punctuation">(</span>model, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span><span class="token punctuation">[</span>args.local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

   optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

   <span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
      <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
          images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
          target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
          <span class="token punctuation">..</span>.
          output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
          loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
          <span class="token punctuation">..</span>.
          optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
          loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
          optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在上面的代码中值得注意的是，由于没有 torch.distributed.launch 读取的默认环境变量作为配置，我们需要手动为 init_process_group 指定参数：</p> 
<pre><code class="prism language-bash">dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span>, <span class="token assign-left variable">init_method</span><span class="token operator">=</span><span class="token string">'tcp://127.0.0.1:23456'</span>, <span class="token assign-left variable">world_size</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">rank</span><span class="token operator">=</span>gpu<span class="token punctuation">)</span>
</code></pre> 
<p>汇总一下，添加 multiprocessing 后并行训练部分主要与如下代码段有关：</p> 
<pre><code class="prism language-bash"><span class="token comment"># main.py</span>
<span class="token function">import</span> torch
<span class="token function">import</span> torch.distributed as dist
<span class="token function">import</span> torch.multiprocessing as mp

mp.spawn<span class="token punctuation">(</span>main_worker, <span class="token assign-left variable">nprocs</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">args</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span>, myargs<span class="token punctuation">))</span>

def main_worker<span class="token punctuation">(</span>proc, nprocs, args<span class="token punctuation">)</span>:

   dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span>, <span class="token assign-left variable">init_method</span><span class="token operator">=</span><span class="token string">'tcp://127.0.0.1:23456'</span>, <span class="token assign-left variable">world_size</span><span class="token operator">=</span><span class="token number">4</span>, <span class="token assign-left variable">rank</span><span class="token operator">=</span>gpu<span class="token punctuation">)</span>
   torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

   train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.
   train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

   train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>

   model <span class="token operator">=</span> <span class="token punctuation">..</span>.
   model <span class="token operator">=</span> torch.nn.parallel.DistributedDataParallel<span class="token punctuation">(</span>model, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span><span class="token punctuation">[</span>args.local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

   optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

   <span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
      <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
          images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
          target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
          <span class="token punctuation">..</span>.
          output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
          loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
          <span class="token punctuation">..</span>.
          optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
          loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
          optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在使用时，直接使用 python 运行就可以了：</p> 
<pre><code class="prism language-bash">python main.py
</code></pre> 
<h3><a id="_Apex__671"></a>使用 Apex 再加速</h3> 
<blockquote> 
 <p>Apex 是 NVIDIA 开源的用于混合精度训练和分布式训练库。Apex 对混合精度训练的过程进行了封装，改两三行配置就可以进行混合精度的训练，从而大幅度降低显存占用，节约运算时间。此外，Apex 也提供了对分布式训练的封装，针对 NVIDIA 的 NCCL 通信库进行了优化。</p> 
</blockquote> 
<pre><code class="prism language-bash">from apex <span class="token function">import</span> amp

model, optimizer <span class="token operator">=</span> amp.initialize<span class="token punctuation">(</span>model, optimizer<span class="token punctuation">)</span>
</code></pre> 
<p>在分布式训练的封装上，Apex 在胶水层的改动并不大，主要是优化了 NCCL 的通信。因此，大部分代码仍与 torch.distributed 保持一致。使用的时候只需要将 torch.nn.parallel.DistributedDataParallel 替换为 apex.parallel.DistributedDataParallel 用于包装模型。在 API 层面，相对于 torch.distributed ，它可以自动管理一些参数（可以少传一点）：</p> 
<pre><code class="prism language-bash">from apex.parallel <span class="token function">import</span> DistributedDataParallel

model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
<span class="token comment"># # torch.distributed</span>
<span class="token comment"># model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])</span>
<span class="token comment"># model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)</span>
</code></pre> 
<p>在正向传播计算 loss 时，Apex 需要使用 amp.scale_loss 包装，用于根据 loss 值自动对精度进行缩放：</p> 
<pre><code class="prism language-bash">with amp.scale_loss<span class="token punctuation">(</span>loss, optimizer<span class="token punctuation">)</span> as scaled_loss:
   scaled_loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>汇总一下，Apex 的并行训练部分主要与如下代码段有关：</p> 
<pre><code class="prism language-bash"><span class="token comment"># main.py</span>
<span class="token function">import</span> torch
<span class="token function">import</span> argparse
<span class="token function">import</span> torch.distributed as dist

from apex.parallel <span class="token function">import</span> DistributedDataParallel

parser <span class="token operator">=</span> argparse.ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser.add_argument<span class="token punctuation">(</span><span class="token string">'--local_rank'</span>, <span class="token assign-left variable">default</span><span class="token operator">=</span>-1, <span class="token assign-left variable">type</span><span class="token operator">=</span>int,
                    <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'node rank for distributed training'</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser.parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

dist.init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">)</span>
torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.
train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>

model <span class="token operator">=</span> <span class="token punctuation">..</span>.
model, optimizer <span class="token operator">=</span> amp.initialize<span class="token punctuation">(</span>model, optimizer<span class="token punctuation">)</span>
model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model, <span class="token assign-left variable">device_ids</span><span class="token operator">=</span><span class="token punctuation">[</span>args.local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      with amp.scale_loss<span class="token punctuation">(</span>loss, optimizer<span class="token punctuation">)</span> as scaled_loss:
         scaled_loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在使用时，调用 torch.distributed.launch 启动器启动：</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">UDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 python <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> main.py
</code></pre> 
<h3><a id="Horovod__741"></a>Horovod 的优雅实现</h3> 
<blockquote> 
 <p>Horovod 是 Uber 开源的深度学习工具，它的发展吸取了 Facebook “Training ImageNet In 1 Hour” 与百度 “Ring Allreduce” 的优点，可以无痛与 PyTorch/Tensorflow 等深度学习框架结合，实现并行训练。</p> 
</blockquote> 
<p>在 API 层面，Horovod 和 torch.distributed 十分相似。在 mpirun 的基础上，Horovod 提供了自己封装的 horovodrun 作为启动器。</p> 
<p>与 torch.distributed.launch 相似，我们只需要编写一份代码，horovodrun 启动器就会自动将其分配给n个进程，分别在n 个 GPU 上运行。在执行过程中，启动器会将当前进程的（其实就是 GPU的）index 注入 hvd，我们可以这样获得当前进程的 index：</p> 
<pre><code class="prism language-bash"><span class="token function">import</span> horovod.torch as hvd

hvd.local_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>与 init_process_group 相似，Horovod 使用 init 设置GPU 之间通信使用的后端和端口:</p> 
<pre><code class="prism language-bash">hvd.init<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>接着，使用 DistributedSampler 对数据集进行划分。如此前我们介绍的那样，它能帮助我们将每个 batch 划分成几个 partition，在当前进程中只需要获取和 rank 对应的那个 partition 进行训练：</p> 
<pre><code class="prism language-bash">train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>
</code></pre> 
<p>之后，使用 broadcast_parameters 包装模型参数，将模型参数从编号为 root_rank 的 GPU 复制到所有其他 GPU 中：</p> 
<pre><code class="prism language-bash">hvd.broadcast_parameters<span class="token punctuation">(</span>model.state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">root_rank</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<p>然后，使用 DistributedOptimizer 包装优化器。它能帮助我们为不同 GPU 上求得的梯度进行 all reduce（即汇总不同 GPU 计算所得的梯度，并同步计算结果）。all reduce 后不同 GPU 中模型的梯度均为 all reduce 之前各 GPU 梯度的均值：</p> 
<pre><code class="prism language-bash">hvd.DistributedOptimizer<span class="token punctuation">(</span>optimizer, <span class="token assign-left variable">named_parameters</span><span class="token operator">=</span>model.named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">compression</span><span class="token operator">=</span>hvd.Compression.fp16<span class="token punctuation">)</span>
</code></pre> 
<p>最后，把数据加载到当前 GPU 中。在编写代码时，我们只需要关注正常进行正向传播和反向传播：</p> 
<pre><code class="prism language-bash">torch.cuda.set_device<span class="token punctuation">(</span>args.local_rank<span class="token punctuation">)</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
      images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
      loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
      <span class="token punctuation">..</span>.
      optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
      loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
      optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>汇总一下，Horovod 的并行训练部分主要与如下代码段有关：</p> 
<pre><code class="prism language-bash"><span class="token comment"># main.py</span>
<span class="token function">import</span> torch
<span class="token function">import</span> horovod.torch as hvd

hvd.init<span class="token punctuation">(</span><span class="token punctuation">)</span>
torch.cuda.set_device<span class="token punctuation">(</span>hvd.local_rank<span class="token punctuation">(</span><span class="token punctuation">))</span>

train_dataset <span class="token operator">=</span> <span class="token punctuation">..</span>.
train_sampler <span class="token operator">=</span> torch.utils.data.distributed.DistributedSampler<span class="token punctuation">(</span>
    train_dataset, <span class="token assign-left variable">num_replicas</span><span class="token operator">=</span>hvd.size<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">rank</span><span class="token operator">=</span>hvd.rank<span class="token punctuation">(</span><span class="token punctuation">))</span>

train_loader <span class="token operator">=</span> torch.utils.data.DataLoader<span class="token punctuation">(</span>train_dataset, <span class="token assign-left variable">batch_size</span><span class="token operator">=</span><span class="token punctuation">..</span>., <span class="token assign-left variable">sampler</span><span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>

model <span class="token operator">=</span> <span class="token punctuation">..</span>.
model.cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

optimizer <span class="token operator">=</span> optim.SGD<span class="token punctuation">(</span>model.parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>

optimizer <span class="token operator">=</span> hvd.DistributedOptimizer<span class="token punctuation">(</span>optimizer, <span class="token assign-left variable">named_parameters</span><span class="token operator">=</span>model.named_parameters<span class="token punctuation">(</span><span class="token punctuation">))</span>
hvd.broadcast_parameters<span class="token punctuation">(</span>model.state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>, <span class="token assign-left variable">root_rank</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> <span class="token for-or-select variable">epoch</span> <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>:
   <span class="token keyword">for</span> batch_idx, <span class="token punctuation">(</span>data, target<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span>:
       images <span class="token operator">=</span> images.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
       target <span class="token operator">=</span> target.cuda<span class="token punctuation">(</span>non_blocking<span class="token operator">=</span>True<span class="token punctuation">)</span>
       <span class="token punctuation">..</span>.
       output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
       loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
       <span class="token punctuation">..</span>.
       optimizer.zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
       loss.backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
       optimizer.step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在使用时，调用 horovodrun 启动器启动：</p> 
<pre><code class="prism language-bash"><span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">0,1</span>,2,3 horovodrun <span class="token parameter variable">-np</span> <span class="token number">4</span> <span class="token parameter variable">-H</span> localhost:4 <span class="token parameter variable">--verbose</span> python main.py
</code></pre> 
<h3><a id="_evaluation_834"></a>分布式 evaluation</h3> 
<blockquote> 
 <p>all_reduce, barrier 等 API 是 distributed 中更为基础和底层的 API。这些 API<br> 可以帮助我们控制进程之间的交互，控制 GPU 数据的传输。在自定义 GPU 协作逻辑，汇总 GPU<br> 间少量的统计信息时，大有用处。熟练掌握这些 API 也可以帮助我们自己设计、优化分布式训练、测试流程。</p> 
</blockquote> 
<ol><li>训练样本被切分成了若干个部分，被若干个进程分别控制运行在若干个 GPU 上，如何在进程间进行通信汇总这些（GPU 上的）信息？</li><li>使用一张卡进行推理、测试太慢了，如何使用 Distributed 进行分布式地推理和测试，并将结果汇总在一起？</li></ol> 
<p>要解决这些问题，我们需要一个更为基础的 API，<strong>汇总记录不同 GPU 上生成的准确率、损失函数等指标信息</strong>。这个 API 就是 <code>torch.distributed.all_reduce</code>。<br> <img src="https://images2.imgbox.com/48/c3/gZynyGOW_o.jpg" alt="all_reduce 示意图"><br> 如上图所示，它的工作过程包含以下三步：</p> 
<ol><li>在调用 <code>all_reduce(tensor, op=...)</code>后，当前进程会向其他进程发送 <code>tensor</code>（例如 rank 0 会发送rank 0 的 tensor 到 rank 1、2、3）。</li><li>同时，当前进程接受其他进程发来的 tensor（例如 rank 0 会接收 rank 1 的 tensor、rank 2 的 tensor、rank 3 的 tensor）。</li><li>在全部接收完成后，当前进程（例如rank 0）会对当前进程的和接收到的 <code>tensor</code> （例如 rank 0 的 tensor、rank 1 的 tensor、rank 2 的 tensor、rank 3 的 tensor）进行 <code>op</code> （例如求和）操作。</li></ol> 
<p>使用 <code>torch.distributed.all_reduce(loss, op=torch.distributed.reduce_op.SUM)</code>，我们就能够对不同数据切片（不同 GPU 上的训练数据）的损失函数进行求和了。接着，我们只要再将其除以进程（GPU）数量 <code>world_size</code>就可以得到损失函数的平均值。正确率也能够通过同样方法进行计算：</p> 
<pre><code class="prism language-bash"><span class="token comment"># 原始代码</span>
output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
        
acc1, acc5 <span class="token operator">=</span> accuracy<span class="token punctuation">(</span>output, target, <span class="token assign-left variable">topk</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">5</span><span class="token punctuation">))</span>
losses.update<span class="token punctuation">(</span>loss.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top1.update<span class="token punctuation">(</span>acc1.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top5.update<span class="token punctuation">(</span>acc5.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
​
<span class="token comment"># 修改后，同步各 GPU 中数据切片的统计信息，用于分布式的 evaluation</span>
def reduce_tensor<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>:
    rt <span class="token operator">=</span> tensor.clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dist.all_reduce<span class="token punctuation">(</span>rt, <span class="token assign-left variable">op</span><span class="token operator">=</span>dist.reduce_op.SUM<span class="token punctuation">)</span>
    rt /<span class="token operator">=</span> args.world_size
    <span class="token builtin class-name">return</span> rt
​
output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
acc1, acc5 <span class="token operator">=</span> accuracy<span class="token punctuation">(</span>output, target, <span class="token assign-left variable">topk</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">5</span><span class="token punctuation">))</span>
​
torch.distributed.barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
​
reduced_loss <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>loss.data<span class="token punctuation">)</span>
reduced_acc1 <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>acc1<span class="token punctuation">)</span>
reduced_acc5 <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>acc5<span class="token punctuation">)</span>
​
losses.update<span class="token punctuation">(</span>loss.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top1.update<span class="token punctuation">(</span>acc1.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top5.update<span class="token punctuation">(</span>acc5.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
</code></pre> 
<p>值得注意的是，为了同步各进程的计算进度，我们在 reduce 之前插入了一个同步 API <code>torch.distributed.barrier()</code>。在所有进程运行到这一步之前，先完成此前代码的进程会等待其他进程。这使得我们能够得到准确、有序的输出。在 Horovod 中，我们无法使用 <code>torch.distributed.barrier()</code>，取而代之的是，我们可以在 allreduce 过程中指明：</p> 
<pre><code class="prism language-bash">def reduce_mean<span class="token punctuation">(</span>tensor, world_size<span class="token punctuation">)</span>:
    rt <span class="token operator">=</span> tensor.clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
    hvd.allreduce<span class="token punctuation">(</span>rt, <span class="token assign-left variable">name</span><span class="token operator">=</span><span class="token string">'barrier'</span><span class="token punctuation">)</span>
    rt /<span class="token operator">=</span> world_size
    <span class="token builtin class-name">return</span> rt
    
output <span class="token operator">=</span> model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output, target<span class="token punctuation">)</span>
acc1, acc5 <span class="token operator">=</span> accuracy<span class="token punctuation">(</span>output, target, <span class="token assign-left variable">topk</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span>, <span class="token number">5</span><span class="token punctuation">))</span>

reduced_loss <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>loss.data<span class="token punctuation">)</span>
reduced_acc1 <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>acc1<span class="token punctuation">)</span>
reduced_acc5 <span class="token operator">=</span> reduce_tensor<span class="token punctuation">(</span>acc5<span class="token punctuation">)</span>

losses.update<span class="token punctuation">(</span>loss.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top1.update<span class="token punctuation">(</span>acc1.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
top5.update<span class="token punctuation">(</span>acc5.item<span class="token punctuation">(</span><span class="token punctuation">)</span>, images.size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">))</span>
</code></pre> 
<blockquote> 
 <p>使用 4 块 Tesla V100-PICE 在 ImageNet 进行了运行时间的测试，测试结果发现 Apex 的加速效果最好，但与<br> Horovod/Distributed 差别不大，平时可以直接使用内置的 Distributed。Dataparallel<br> 较慢，不推荐使用。</p> 
</blockquote> 
<p><a href="https://zhuanlan.zhihu.com/p/98535650" rel="nofollow">全部来自知乎</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/858982058cef8817ab7b1c4115b69980/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">二分查找各种类型例题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/68d1aae1fad45e9b7a0ef9331c844586/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">一个vue项目配置访问两个服务器地址</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>