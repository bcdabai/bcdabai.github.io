<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习：pytorch nn.Embedding详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习：pytorch nn.Embedding详解" />
<meta property="og:description" content="目录
1 nn.Embedding介绍
1.1 nn.Embedding作用
1.2 nn.Embedding函数描述
1.3 nn.Embedding词向量转化
2 nn.Embedding实战
2.1 embedding如何处理文本
2.2 embedding使用示例
2.3 nn.Embedding的可学习性
1 nn.Embedding介绍 1.1 nn.Embedding作用 nn.Embedding是PyTorch中的一个常用模块，其主要作用是将输入的整数序列转换为密集向量表示。在自然语言处理（NLP）任务中，可以将每个单词表示成一个向量，从而方便进行下一步的计算和处理。
1.2 nn.Embedding函数描述 nn.Embedding是将输入向量化，定义如下：
torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False, device=None, dtype=None) 参数说明：
num_embeddings ：字典中词的个数embedding_dim：embedding的维度padding_idx（索引指定填充）：如果给定，则遇到padding_idx中的索引，则将其位置填0（0是默认值，事实上随便填充什么值都可以）。 注：embeddings中的值是正态分布N(0,1)中随机取值。
1.3 nn.Embedding词向量转化 在PyTorch中，nn.Embedding用来实现词与词向量的映射。nn.Embedding具有一个权重（.weight），形状是(num_words, embedding_dim)。例如一共有100个词，每个词用16维向量表征，对应的权重就是一个100×16的矩阵。
Embedding的输入形状N×W，N是batch size，W是序列的长度，输出的形状是N×W×embedding_dim。
Embedding输入必须是LongTensor，FloatTensor需通过tensor.long()方法转成LongTensor。
Embedding的权重是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。
2 nn.Embedding实战 2.1 embedding如何处理文本 在NLP任务中，首先要对文本进行处理，将文本进行编码转换，形成向量表达，embedding处理文本的流程如下：
（1）输入一段文本，中文会先分词（如jieba分词），英文会按照空格提取词
（2）首先将单词转成字典的形式，由于英语中以空格为词的分割，所以可以直接建立词典索引结构。类似于：word2id = {&#39;i&#39; : 1, &#39;like&#39; : 2, &#39;you&#39; : 3, &#39;want&#39; : 4, &#39;an&#39; : 5, &#39;apple&#39; : 6} 这样的形式。如果是中文的话，首先进行分词操作。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d37186d923f85cb46ca4810a7a57f0cf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-18T20:48:41+08:00" />
<meta property="article:modified_time" content="2023-09-18T20:48:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习：pytorch nn.Embedding详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="1%20nn.Embedding%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;"><a href="#1%20nn.Embedding%E4%BB%8B%E7%BB%8D" rel="nofollow">1 nn.Embedding介绍</a></p> 
<p id="1.1%20nn.Embedding%E4%BD%9C%E7%94%A8-toc" style="margin-left:40px;"><a href="#1.1%20nn.Embedding%E4%BD%9C%E7%94%A8" rel="nofollow">1.1 nn.Embedding作用</a></p> 
<p id="1.2%20nn.Embedding%E5%87%BD%E6%95%B0%E6%8F%8F%E8%BF%B0-toc" style="margin-left:40px;"><a href="#1.2%20nn.Embedding%E5%87%BD%E6%95%B0%E6%8F%8F%E8%BF%B0" rel="nofollow">1.2 nn.Embedding函数描述</a></p> 
<p id="1.3%20nn.Embedding%E8%AF%8D%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96-toc" style="margin-left:40px;"><a href="#1.3%20nn.Embedding%E8%AF%8D%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96" rel="nofollow">1.3 nn.Embedding词向量转化</a></p> 
<p id="2%20nn.Embedding%E5%AE%9E%E6%88%98-toc" style="margin-left:0px;"><a href="#2%20nn.Embedding%E5%AE%9E%E6%88%98" rel="nofollow">2 nn.Embedding实战</a></p> 
<p id="2.1%20embedding%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC-toc" style="margin-left:40px;"><a href="#2.1%20embedding%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC" rel="nofollow">2.1 embedding如何处理文本</a></p> 
<p id="2.2%20embedding%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-toc" style="margin-left:40px;"><a href="#2.2%20embedding%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B" rel="nofollow">2.2 embedding使用示例</a></p> 
<p id="2.3%20nn.Embedding%E7%9A%84%E5%8F%AF%E5%AD%A6%E4%B9%A0%E6%80%A7-toc" style="margin-left:40px;"><a href="#2.3%20nn.Embedding%E7%9A%84%E5%8F%AF%E5%AD%A6%E4%B9%A0%E6%80%A7" rel="nofollow">2.3 nn.Embedding的可学习性</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="1%20nn.Embedding%E4%BB%8B%E7%BB%8D">1 nn.Embedding介绍</h2> 
<h3 id="1.1%20nn.Embedding%E4%BD%9C%E7%94%A8">1.1 nn.Embedding作用</h3> 
<p>nn.Embedding是PyTorch中的一个常用模块，其主要作用是将输入的整数序列转换为密集向量表示。在自然语言处理（NLP）任务中，可以将每个单词表示成一个向量，从而方便进行下一步的计算和处理。</p> 
<h3 id="1.2%20nn.Embedding%E5%87%BD%E6%95%B0%E6%8F%8F%E8%BF%B0">1.2 nn.Embedding函数描述</h3> 
<p>nn.Embedding是将输入向量化，定义如下：</p> 
<pre><code class="language-python">torch.nn.Embedding(num_embeddings, 
                   embedding_dim, 
                   padding_idx=None, 
                   max_norm=None, 
                   norm_type=2.0, 
                   scale_grad_by_freq=False, 
                   sparse=False, 
                   _weight=None, 
                   _freeze=False, 
                   device=None, 
                   dtype=None)</code></pre> 
<p>参数说明：</p> 
<blockquote> 
 <ul><li><strong>num_embeddings </strong>：字典中词的个数</li><li><strong>embedding_dim</strong>：embedding的维度</li><li><strong>padding_idx</strong>（索引指定填充）：如果给定，则遇到padding_idx中的索引，则将其位置填0（0是默认值，事实上随便填充什么值都可以）。</li></ul> 
</blockquote> 
<p><strong>注：embeddings中的值是正态分布N(0,1)中随机取值。</strong></p> 
<h3 id="1.3%20nn.Embedding%E8%AF%8D%E5%90%91%E9%87%8F%E8%BD%AC%E5%8C%96">1.3 nn.Embedding词向量转化</h3> 
<p>在PyTorch中，nn.Embedding用来实现词与词向量的映射。nn.Embedding具有一个权重（.weight），形状是(num_words, embedding_dim)。例如一共有100个词，每个词用16维向量表征，对应的权重就是一个100×16的矩阵。</p> 
<p>Embedding的输入形状N×W，N是batch size，W是序列的长度，输出的形状是N×W×embedding_dim。</p> 
<p>Embedding输入必须是LongTensor，FloatTensor需通过tensor.long()方法转成LongTensor。</p> 
<p>Embedding的权重是可以训练的，既可以采用随机初始化，也可以采用预训练好的词向量初始化。</p> 
<h2 id="2%20nn.Embedding%E5%AE%9E%E6%88%98">2 nn.Embedding实战</h2> 
<h3 id="2.1%20embedding%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC">2.1 embedding如何处理文本</h3> 
<p>在NLP任务中，首先要对文本进行处理，将文本进行编码转换，形成向量表达，embedding处理文本的流程如下：</p> 
<p>（1）输入一段文本，中文会先分词（如jieba分词），英文会按照空格提取词</p> 
<p>（2）首先将单词转成字典的形式，由于英语中以空格为词的分割，所以可以直接建立词典索引结构。类似于：<strong>word2id = {'i' : 1, 'like' : 2, 'you' : 3, 'want' : 4, 'an' : 5, 'apple' : 6} </strong>这样的形式。如果是中文的话，首先进行分词操作。</p> 
<p>（3）然后再以句子为list，为每个句子建立索引结构，<strong>list [ [ sentence1 ] , [ sentence2 ] ] </strong>。以上面字典的索引来说，最终建立的就是 <strong>[ [ 1 , 2 , 3 ] , [ 1 , 4 , 5 , 6 ] ] </strong>。这样长短不一的句子</p> 
<p>（4）接下来要进行padding的操作。由于tensor结构中都是等长的，所以要对上面那样的句子做padding操作后再利用 nn.Embedding 来进行词的初始化。padding后的可能是这样的结构</p> 
<p><strong>[ [ 1 , 2 , 3, 0 ] , [ 1 , 4 , 5 , 6 ] ] </strong>。其中0作为填充。（注意：由于在NMT任务中肯定存在着填充问题，所以在embedding时一定存在着第三个参数，让某些索引下的值为0，代表无实际意义的填充）</p> 
<h3 id="2.2%20embedding%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B">2.2 embedding使用示例</h3> 
<p>比如有两个句子：</p> 
<ul><li> <p>I want a plane</p> </li><li> <p>I want to travel to Beijing</p> </li></ul> 
<p>将两个句子转化为ID映射：</p> 
<p>{I：1，want：2，a：3，plane：4，to：5，travel：6，Beijing：7}</p> 
<p>转化成ID表示的两个句子如下：</p> 
<ul><li> <p>1,2,3,4</p> </li><li> <p>1,2,5,6,5,7</p> </li></ul> 
<pre><code class="language-python">import torch
from torch import nn

# 创建最大词个数为10，每个词用维度为4表示
embedding = nn.Embedding(10, 4)

# 将第一个句子填充0，与第二个句子长度对齐
in_vector = torch.LongTensor([[1, 2, 3, 4, 0, 0], [1, 2, 5, 6, 5, 7]])
out_emb = embedding(in_vector)
print(in_vector.shape)
print((out_emb.shape))
print(out_emb)
print(embedding.weight)
</code></pre> 
<p>运行结果显示如下：</p> 
<pre><code class="language-python">torch.Size([2, 6])
torch.Size([2, 6, 4])
tensor([[[-0.6642, -0.6263,  1.2333, -0.6055],
         [ 0.9950, -0.2912,  1.0008,  0.1202],
         [ 1.2501,  0.1923,  0.5791, -1.4586],
         [-0.6935,  2.1906,  1.0595,  0.2089],
         [ 0.7359, -0.1194, -0.2195,  0.9161],
         [ 0.7359, -0.1194, -0.2195,  0.9161]],

        [[-0.6642, -0.6263,  1.2333, -0.6055],
         [ 0.9950, -0.2912,  1.0008,  0.1202],
         [-0.3216,  1.2407,  0.2542,  0.8630],
         [ 0.6886, -0.6119,  1.5270,  0.1228],
         [-0.3216,  1.2407,  0.2542,  0.8630],
         [ 0.0048,  1.8500,  1.4381,  0.3675]]], grad_fn=&lt;EmbeddingBackward0&gt;)
Parameter containing:
tensor([[ 0.7359, -0.1194, -0.2195,  0.9161],
        [-0.6642, -0.6263,  1.2333, -0.6055],
        [ 0.9950, -0.2912,  1.0008,  0.1202],
        [ 1.2501,  0.1923,  0.5791, -1.4586],
        [-0.6935,  2.1906,  1.0595,  0.2089],
        [-0.3216,  1.2407,  0.2542,  0.8630],
        [ 0.6886, -0.6119,  1.5270,  0.1228],
        [ 0.0048,  1.8500,  1.4381,  0.3675],
        [ 0.3810, -0.7594, -0.1821,  0.5859],
        [-1.4029,  1.2243,  0.0374, -1.0549]], requires_grad=True)

</code></pre> 
<p><strong>注意：</strong></p> 
<ul><li>句子中的ID不能大于最大词的index（上面例子中，不能大于10）</li><li>embeding的输入必须是维度对齐的，如果长度不够，需要预先做填充</li></ul> 
<h3 id="2.3%20nn.Embedding%E7%9A%84%E5%8F%AF%E5%AD%A6%E4%B9%A0%E6%80%A7">2.3 nn.Embedding的可学习性</h3> 
<p>nn.Embedding中的参数并不是一成不变的，它也是会参与梯度下降的。也就是更新模型参数也会更新nn.Embedding的参数，或者说nn.Embedding的参数本身也是模型参数的一部分。</p> 
<pre><code class="language-python">import torch
from torch import nn

# 创建最大词个数为10，每个词用维度为4表示
embedding = nn.Embedding(10, 4)

# 将第一个句子填充0，与第二个句子长度对齐
in_vector = torch.LongTensor([[1, 2, 3, 4, 0, 0], [1, 2, 5, 6, 5, 7]])

optimizer = torch.optim.SGD(embedding.parameters(), lr=0.01)
criteria = nn.MSELoss()

for i in range(1000):
    outputs = embedding(torch.LongTensor([1, 2, 3, 4]))
    loss = criteria(outputs, torch.ones(4, 4))
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

print(embedding.weight)
new_output = embedding(in_vector)
print(new_output)</code></pre> 
<p>经过1000epochs的训练后，查看新的编码结果，显示如下：</p> 
<pre><code class="language-python">Parameter containing:
tensor([[-0.2475, -1.3436, -0.0449,  0.2093],
        [ 0.4831,  0.5887,  1.2278,  1.1106],
        [ 1.1809,  0.7451,  0.2049,  1.3053],
        [ 0.7369,  1.1276,  1.0066,  0.4399],
        [ 1.3064,  0.3979,  0.8753,  0.9410],
        [-0.6222,  0.2574,  1.1211,  0.1801],
        [-0.5072,  0.2564,  0.5500,  0.3136],
        [-1.7473,  0.0504, -0.0633, -0.3138],
        [-2.4507, -0.6092,  0.0348, -0.4384],
        [ 0.9458, -0.2867, -0.0285,  1.1842]], requires_grad=True)
tensor([[[ 0.4831,  0.5887,  1.2278,  1.1106],
         [ 1.1809,  0.7451,  0.2049,  1.3053],
         [ 0.7369,  1.1276,  1.0066,  0.4399],
         [ 1.3064,  0.3979,  0.8753,  0.9410],
         [-0.2475, -1.3436, -0.0449,  0.2093],
         [-0.2475, -1.3436, -0.0449,  0.2093]],

        [[ 0.4831,  0.5887,  1.2278,  1.1106],
         [ 1.1809,  0.7451,  0.2049,  1.3053],
         [-0.6222,  0.2574,  1.1211,  0.1801],
         [-0.5072,  0.2564,  0.5500,  0.3136],
         [-0.6222,  0.2574,  1.1211,  0.1801],
         [-1.7473,  0.0504, -0.0633, -0.3138]]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre> 
<p>权重参数和编码结果都发生了很大变化，所以nn.Embedding在构建模型过程中，可以作为模型的一部分，进行共同训练。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8d83d2bb95e16dabfd9f72cac37149d4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Oracle mysql 达梦 大金仓 hive 区别</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b5b91c3b548f11227c4157d44781f153/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">通过Sealos 180秒部署一套K8S集群</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>