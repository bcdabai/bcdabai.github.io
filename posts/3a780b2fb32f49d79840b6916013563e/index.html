<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【机器学习】强化学习（五）深度强化学习理论 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【机器学习】强化学习（五）深度强化学习理论" />
<meta property="og:description" content="强化学习算法如Q学习的确有一些局限性，比如状态和动作空间过大或过复杂的问题。针对这些问题，有一些解决方案，比如：
使用函数逼近来近似Q函数，而不是用表格存储。函数逼近可以是线性的，也可以是非线性的，比如神经网络。这样可以减少存储空间，也可以处理连续的状态和动作空间。
使用分层强化学习来将复杂的任务分解为子任务，每个子任务有自己的状态和动作空间，以及奖励函数。这样可以降低问题的复杂度，也可以提高学习效率。
使用深度强化学习来结合深度学习和强化学习，利用深度神经网络来表示策略或值函数，从高维的原始输入（比如图像）中提取特征，学习复杂的环境和任务。深度强化学习已经在许多领域取得了令人瞩目的成果，比如AlphaGo，Atari游戏，机器人控制等。
五、深度强化学习
5.1 深度 Q 网络
图：深度 Q 学习中，使用神经网络来逼近 Q 值函数
（2个卷积层，2个全连接层）
DQN的网络结构
深度 Q 网络的网络结构是指用于近似 Q 函数的深度神经网络的结构。深度 Q 网络的网络结构可以根据不同的输入和输出的类型和维度来设计，但是一般都包括以下几个部分：
输入层：输入层是用于接收环境的状态信息的，它可以是一个向量，一个矩阵，或者一个张量。例如，如果输入是图像，那么输入层可以是一个三维的张量，表示图像的高度，宽度和通道数。
隐藏层：隐藏层是用于提取状态信息的特征的，它可以有多个，每个隐藏层都由若干个神经元组成。每个神经元都有一个激活函数，用于增加网络的非线性。隐藏层可以是全连接层，卷积层，循环层，或者其他类型的层。例如，如果输入是图像，那么隐藏层可以是若干个卷积层，用于提取图像的局部特征。
输出层：输出层是用于输出每个动作的 Q 值的，它的神经元的个数等于动作空间的大小。输出层一般是一个全连接层，没有激活函数。输出层的每个神经元都对应一个动作，其输出的值就是该动作的 Q 值。例如，如果动作空间是离散的，有四个动作，那么输出层就有四个神经元，分别表示上，下，左，右的 Q 值。
DQN概述1
DQN概述2
算法： 具有经验回放的深度 Q 学习
DQN的改进算法
5.2 策略梯度算法
策略梯度算法处理离散动作和连续动作的区别
策略梯度算法与深度Q网络算法的区别
目标函数的构造方法
目标函数对策略参数的梯度形式
策略梯度定理的证明
REINFORCE 算法流程
对于连续动作空间强化学习路径规划问题，有哪些求解方法
参考网址：
- [DQN论文](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
- [DQN 的改进算法的综述](https://arxiv.org/abs/1710.02298)
- [DQN 的改进算法的论文解读](https://zhuanlan.zhihu.com/p/32817711)
- [DQN 的改进算法的代码实现](https://github.com/higgsfield/RL-Adventure-2)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3a780b2fb32f49d79840b6916013563e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-21T20:45:38+08:00" />
<meta property="article:modified_time" content="2024-01-21T20:45:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【机器学习】强化学习（五）深度强化学习理论</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>强化学习算法如Q学习的确有一些局限性，比如状态和动作空间过大或过复杂的问题。针对这些问题，有一些解决方案，比如：</p> 
 <ul><li><p>使用<strong>函数逼近</strong>来近似Q函数，而不是用表格存储。函数逼近可以是线性的，也可以是非线性的，比如神经网络。这样可以减少存储空间，也可以处理连续的状态和动作空间。</p></li><li><p>使用<strong>分层强化学习</strong>来将复杂的任务分解为子任务，每个子任务有自己的状态和动作空间，以及奖励函数。这样可以降低问题的复杂度，也可以提高学习效率。</p></li><li><p>使用<strong>深度强化学习</strong>来结合深度学习和强化学习，利用深度神经网络来表示策略或值函数，从高维的原始输入（比如图像）中提取特征，学习复杂的环境和任务。深度强化学习已经在许多领域取得了令人瞩目的成果，比如AlphaGo，Atari游戏，机器人控制等。</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c8/3f/hRh7GFIu_o.png" alt="273a487cb3d412547d7f98c73ad40b80.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9b/10/8JV8n885_o.png" alt="4f815e00dc3fdabdc50060582cc1a75c.png"></p> 
 <p><strong>五、深度强化学习</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b7/1d/8neZRlsa_o.png" alt="b0d907592453df1d0b85acfdb65174ea.png"></p> 
 <p style="text-align:left;"><strong>5.1 深度 Q 网络</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/38/c9/wnA6nmZp_o.png" alt="d7a76b363e2c1cef0b3c9ee27a16b1db.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4e/60/0OAGwBwV_o.png" alt="7547d0a066c4c00ebc6a4d37e093408e.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/21/7c/0gpOMRHa_o.png" alt="56f9604ab5985f0c7045fbd55d3593b5.png"></p> 
 <p style="text-align:center;">图：深度 Q 学习中，使用神经网络来逼近 Q 值函数</p> 
 <p style="text-align:center;">（2个卷积层，2个全连接层）</p> 
 <p style="text-align:center;"><strong>DQN的网络结构</strong></p> 
 <p>深度 Q 网络的网络结构是指用于近似 Q 函数的深度神经网络的结构。深度 Q 网络的网络结构可以根据不同的输入和输出的类型和维度来设计，但是一般都包括以下几个部分：</p> 
 <ul><li><p>输入层：输入层是用于接收环境的状态信息的，它可以是一个向量，一个矩阵，或者一个张量。例如，如果输入是图像，那么输入层可以是一个三维的张量，表示图像的高度，宽度和通道数。</p></li><li><p>隐藏层：隐藏层是用于提取状态信息的特征的，它可以有多个，每个隐藏层都由若干个神经元组成。每个神经元都有一个激活函数，用于增加网络的非线性。隐藏层可以是全连接层，卷积层，循环层，或者其他类型的层。例如，如果输入是图像，那么隐藏层可以是若干个卷积层，用于提取图像的局部特征。</p></li><li><p>输出层：输出层是用于输出每个动作的 Q 值的，它的神经元的个数等于动作空间的大小。输出层一般是一个全连接层，没有激活函数。输出层的每个神经元都对应一个动作，其输出的值就是该动作的 Q 值。例如，如果动作空间是离散的，有四个动作，那么输出层就有四个神经元，分别表示上，下，左，右的 Q 值。</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fc/ac/OGmjiW8y_o.png" alt="c1f64a958a0517d2807d7bca593b5dbe.png"><br></p> 
 <p style="text-align:center;"><strong>DQN概述1</strong></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/05/a5/VoW87RUb_o.png" alt="77cd862e61b1eaaa1aab53af778eeb59.png"></p> 
 <p style="text-align:center;"><strong>DQN概述2</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/f3/96/JsG0ExkE_o.png" alt="6424eaab151d6c951820ac988f2bf128.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fc/fc/jLU06ApL_o.png" alt="01fc41c3a422c8dbd34ff1a178438342.png"></p> 
 <p style="text-align:center;"><strong>算法： 具有经验回放的深度 Q 学习<br></strong></p> 
 <p style="text-align:left;"><img src="https://images2.imgbox.com/57/2a/m2rYs3Be_o.png" alt="0d61c470db11384645c227d58eba3c1d.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/c3/76/cw8tQkRC_o.png" alt="eea9a4c8867204720026a3cd900e779a.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3c/01/s45FO9e3_o.png" alt="55ddaf827dc20ae05247fb0abdc9f4cc.png"></p> 
 <p style="text-align:center;"><strong>DQN的改进算法<br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/84/d3/qfIbC0Ga_o.png" alt="8e375729d42a691752e39aae583dd180.png"></p> 
 <p style="text-align:left;"><strong>5.2 策略梯度算法</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d8/d5/vZkWZeC5_o.png" alt="12edfdeb94d55aef7aaf28c67971b55a.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/0b/96/q7RQS1ip_o.png" alt="a9185e36db426564297f7a51745c5bc6.png"></p> 
 <p><strong>策略梯度算法处理离散动作和连续动作的区别</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9c/13/XuoZSB1O_o.png" alt="502fba0e942552851f39cb7f38141fc1.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b7/3e/BlzJ1J2O_o.png" alt="8dfedac594171d3242bf0329698a1803.png"></p> 
 <p><strong>策略梯度算法与深度Q网络算法的区别</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5b/bf/nLqRllu8_o.png" alt="43c20dfcc4d3c6645e58e2119ab9f0a5.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5c/18/oGg4jxB7_o.png" alt="0040324b74f06ff9d8264bd54e92707f.png"></p> 
 <p><strong>目标函数的构造方法</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/2a/d8/YTuk9vf0_o.png" alt="32d1912072d935ce506a11f930e819b5.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/03/36/PlVikWvm_o.png" alt="7339c0123c70e062d95776174dcef569.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ff/3c/pCxz9G7A_o.png" alt="e3291e12994e3d8abaf7717d3042e00d.png"></p> 
 <p><strong>目标函数对策略参数的梯度形式</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9f/ce/euTWXWox_o.png" alt="1294de95057675344119aa58c5a37e1d.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/3c/ae/z0Zlr6Zf_o.png" alt="0fe171599c79ed579b5b314eb0772f6d.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ef/0f/tzLhERQF_o.png" alt="e4f1e78b11eaf584448f3a293d6c9c5f.png"></p> 
 <p><strong>策略梯度定理的证明</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bc/49/Fulrooem_o.png" alt="85cb489958d3dd2f44620d419fcdb45f.png"></p> 
 <p><strong>REINFORCE 算法流程</strong><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/36/a0/h0cSSumP_o.png" alt="c7b9b452a454a2efff15175cfc280403.png"></p> 
 <p style="text-align:center;"><strong>对于连续动作空间强化学习路径规划问题，有哪些求解方法</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/48/9d/4EExCS4Q_o.png" alt="2d1accd1947bee70021e5ce5c951de8a.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cf/b0/fx99bEZ3_o.png" alt="7f9fe68797b9cacbabdfd098294e6a4f.png"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b9/60/Vz72s9k0_o.png" alt="1ad833694a63be192451f91d12adb075.png"></p> 
 <p><strong>参考网址：</strong></p> 
 <p>- [DQN论文](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)</p> 
 <p>- [DQN 的改进算法的综述](https://arxiv.org/abs/1710.02298)</p> 
 <p>- [DQN 的改进算法的论文解读](https://zhuanlan.zhihu.com/p/32817711)</p> 
 <p>- [DQN 的改进算法的代码实现](https://github.com/higgsfield/RL-Adventure-2)</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/227e2ec6b19c676a0e48a89992379c0e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ubuntu 使用 git 能够 clone 但不能 push 的参考解决方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e7d72d936b1907c75f6000d751c5a726/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Kotlin协程的JVM实现源码分析（下）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>