<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【AI】一文读懂大模型套壳——神仙打架？软饭硬吃？ - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【AI】一文读懂大模型套壳——神仙打架？软饭硬吃？" />
<meta property="og:description" content="目录
一、套壳的风波此起彼伏
二、到底什么是大模型的壳
2.1 大模型的3部分，壳指的是哪里
大模型的内核
预训练（Pre-training）
调优（Fine-tuning）
2.2 内核的发展历程和万流归宗
2.3 套壳不是借壳
三、软饭硬吃，套壳真的不行吗
四、神仙打架，百姓吃瓜
4.1 自研的佼佼者
4.2 模仿也不丢人
4.3 读书人偷书不算偷
模仿学习（Imitation Learning）
知识蒸馏（Knowledge Distillation）
五、我们还要再硬一点
一、套壳的风波此起彼伏 国内“百模大战”，我形容是“群模乱舞”，具体国内有哪些著名的大模型，请参考我的文章——
群模乱舞，AI大模型盛开，国内大模型盘点_ai大模型有哪些-CSDN博客
里面列出了大厂的模型，以及很多学院派的大模型。
大模型一夜之间，如同AIGAI，自体繁殖一样多起来，很多媒体的声音，也此起彼伏，说国内的大模型，很多都是开源大模型的套壳版本。
这里面动静最大的，可能是李开复先生的零一大模型张量命名事件。其推出的“Yi”大模型，这款模型被揭露只是对LLaMA进行了表面上的修改——仅仅改变了两个张量的名称。这种创新，确实在AI界并非孤例，而是一个普遍现象。
有人说，开源就该这样利用，不然开源干什么？有人说，闭源才是自主研发，参考开源就是套壳。
2023年2月，Meta首次发布了Llama羊驼系列模型。在这个初始版本中，羊驼系列包括了四种不同规模的模型：参数量分别为7亿、13亿、33亿和65亿。7月，Meta公布最新大模型 Llama 2(羊驼 2),包含 7B、13B 和 70B 三种参数变体,可免费用于商业或者研究。这引起不小的轰动，不光是国内，很多国外的大模型，基本都是复用了Llama2，后面我们会讲，为什么选择这个大模型，因为确实不用重复发明轮子了。
有想了解开源和闭源生态的，可以参考我的文章：【AI】马斯克说大模型要开源，我们缺的是源代码？（附一图看懂6大开源协议）_马斯克说大模型在技术上的突破-CSDN博客
非 AI 从业者，视套壳如洪水猛兽，吃瓜者认为套壳就是抄袭的代名词；真正的 AI 从业者，对套壳讳莫如深，需要借鉴，又狠怕惹锅上身。但由于“套壳”本身并没有清晰、准确的定义，导致行业对套壳的理解也是一千个读者有一千个哈姆雷特。
那么，问题来了——
二、到底什么是大模型的壳 2.1 大模型的3部分，壳指的是哪里 要想知道什么是大模型的壳，我们要先知道，大模型包括哪几个部分。
大模型的内核 大模型的内核通常指的是模型的核心架构和算法，这些设计决定了模型如何处理输入数据并生成输出。在大模型中，内核往往包含了大量的计算单元（如神经元、层等），以及它们之间的连接方式和权重。这些计算单元通过特定的数学运算（如矩阵乘法、激活函数等）共同工作，以提取输入数据的特征并做出预测。
大模型的内核设计通常基于深度学习理论，尤其是神经网络。近年来，如上所述，Transformer架构因其出色的性能成为了大模型内核的热门选择。Transformer利用自注意力机制来处理序列数据，能够捕获长距离依赖关系，并在各种NLP任务中取得了显著成果。
也就是说，大家的内核，基本都来自相同的老祖宗。
预训练（Pre-training） 预训练是指在大规模数据上对模型进行初步的训练。这个过程通常是无监督的，意味着模型不需要人工标注的数据就可以学习。预训练的目标是让模型学习到通用的知识和表示方法，这样它就能够更好地适应各种下游任务。
在大模型中，预训练尤为重要，因为庞大的参数量需要大量的数据来有效训练。预训练不仅可以提高模型的泛化能力，还可以加速后续任务的学习过程。例如，在NLP领域，BERT、GPT等模型就是通过在大规模文本语料库上进行预训练来获得强大的语言理解能力的。
调优（Fine-tuning） 调优，也叫做“微调”，是指在特定任务的数据上对已经预训练过的模型进行进一步的训练。这个过程通常是有监督的，需要使用标注好的数据来指导模型的学习。调优的目标是调整模型参数，使其更好地适应特定任务的需求。
在大模型中，调优通常比从头开始训练要高效得多，因为预训练已经为模型提供了一个很好的起点。通过调优，模型可以在较少的迭代次数和较小的数据集上达到较好的性能。此外，调优还可以使模型更加灵活地适应各种场景和任务需求。
在漫长的预训练之后会得到一个基座模型（Base Model），在基座模型的基础上加入特定行业的数据集做进一步的微调，就会得到一个微调模型（Fine-tuning Model），或者称为行业模型、垂直模型。
2.2 内核的发展历程和万流归宗 我们都知道，是大模型让AI达到如此的地位，实际上，AI经历了一段低迷期。
关于AI复兴推进器的自然语言处理、神经网络、遗传算法，我都分别写过文章去介绍。还有AI爆发的推进器之卷积神经网络、生成对抗网络、变分自动编码器、迁移学习、知识图谱、注意力机制与深度学习模型等，也可以参考我的之前的文章。
在 2020 年之前，NLP 的模型研究基本都是围绕算法展开，基于 BERT、T5 与 GPT 架构的模型百花齐放。这一时期模型参数较小，基本都在 10 亿以内量级。其中，谷歌 BERT 的表现独领风骚，基于 BERT 架构的模型一度在阅读理解的竞赛排行榜中屠榜。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d13f11783465fb40560e87d8ff252a49/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-03T20:43:51+08:00" />
<meta property="article:modified_time" content="2024-01-03T20:43:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【AI】一文读懂大模型套壳——神仙打架？软饭硬吃？</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%A5%97%E5%A3%B3%E7%9A%84%E9%A3%8E%E6%B3%A2%E6%AD%A4%E8%B5%B7%E5%BD%BC%E4%BC%8F-toc" style="margin-left:40px;"><a href="#%E4%B8%80%E3%80%81%E5%A5%97%E5%A3%B3%E7%9A%84%E9%A3%8E%E6%B3%A2%E6%AD%A4%E8%B5%B7%E5%BD%BC%E4%BC%8F" rel="nofollow">一、套壳的风波此起彼伏</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%88%B0%E5%BA%95%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A3%B3-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E3%80%81%E5%88%B0%E5%BA%95%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A3%B3" rel="nofollow">二、到底什么是大模型的壳</a></p> 
<p id="2.1%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%843%E9%83%A8%E5%88%86%EF%BC%8C%E5%A3%B3%E6%8C%87%E7%9A%84%E6%98%AF%E5%93%AA%E9%87%8C-toc" style="margin-left:80px;"><a href="#2.1%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%843%E9%83%A8%E5%88%86%EF%BC%8C%E5%A3%B3%E6%8C%87%E7%9A%84%E6%98%AF%E5%93%AA%E9%87%8C" rel="nofollow">2.1 大模型的3部分，壳指的是哪里</a></p> 
<p id="%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E6%A0%B8-toc" style="margin-left:120px;"><a href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E6%A0%B8" rel="nofollow">大模型的内核</a></p> 
<p id="%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88Pre-training%EF%BC%89-toc" style="margin-left:120px;"><a href="#%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88Pre-training%EF%BC%89" rel="nofollow">预训练（Pre-training）</a></p> 
<p id="%E8%B0%83%E4%BC%98%EF%BC%88Fine-tuning%EF%BC%89-toc" style="margin-left:120px;"><a href="#%E8%B0%83%E4%BC%98%EF%BC%88Fine-tuning%EF%BC%89" rel="nofollow">调优（Fine-tuning）</a></p> 
<p id="2.2%20%E5%86%85%E6%A0%B8%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%E5%92%8C%E4%B8%87%E6%B5%81%E5%BD%92%E5%AE%97-toc" style="margin-left:80px;"><a href="#2.2%20%E5%86%85%E6%A0%B8%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%E5%92%8C%E4%B8%87%E6%B5%81%E5%BD%92%E5%AE%97" rel="nofollow">2.2 内核的发展历程和万流归宗</a></p> 
<p id="2.3%20%E5%A5%97%E5%A3%B3%E4%B8%8D%E6%98%AF%E5%80%9F%E5%A3%B3-toc" style="margin-left:80px;"><a href="#2.3%20%E5%A5%97%E5%A3%B3%E4%B8%8D%E6%98%AF%E5%80%9F%E5%A3%B3" rel="nofollow">2.3 套壳不是借壳</a></p> 
<p id="%E4%B8%89%E3%80%81%E8%BD%AF%E9%A5%AD%E7%A1%AC%E5%90%83%EF%BC%8C%E5%A5%97%E5%A3%B3%E7%9C%9F%E7%9A%84%E4%B8%8D%E8%A1%8C%E5%90%97-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E3%80%81%E8%BD%AF%E9%A5%AD%E7%A1%AC%E5%90%83%EF%BC%8C%E5%A5%97%E5%A3%B3%E7%9C%9F%E7%9A%84%E4%B8%8D%E8%A1%8C%E5%90%97" rel="nofollow">三、软饭硬吃，套壳真的不行吗</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%EF%BC%8C%E7%99%BE%E5%A7%93%E5%90%83%E7%93%9C-toc" style="margin-left:40px;"><a href="#%E5%9B%9B%E3%80%81%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%EF%BC%8C%E7%99%BE%E5%A7%93%E5%90%83%E7%93%9C" rel="nofollow">四、神仙打架，百姓吃瓜</a></p> 
<p id="4.1%20%E8%87%AA%E7%A0%94%E7%9A%84%E4%BD%BC%E4%BD%BC%E8%80%85-toc" style="margin-left:80px;"><a href="#4.1%20%E8%87%AA%E7%A0%94%E7%9A%84%E4%BD%BC%E4%BD%BC%E8%80%85" rel="nofollow">4.1 自研的佼佼者</a></p> 
<p id="4.2%20%E6%A8%A1%E4%BB%BF%E4%B9%9F%E4%B8%8D%E4%B8%A2%E4%BA%BA-toc" style="margin-left:80px;"><a href="#4.2%20%E6%A8%A1%E4%BB%BF%E4%B9%9F%E4%B8%8D%E4%B8%A2%E4%BA%BA" rel="nofollow">4.2 模仿也不丢人</a></p> 
<p id="4.3%20%E8%AF%BB%E4%B9%A6%E4%BA%BA%E5%81%B7%E4%B9%A6%E4%B8%8D%E7%AE%97%E5%81%B7-toc" style="margin-left:80px;"><a href="#4.3%20%E8%AF%BB%E4%B9%A6%E4%BA%BA%E5%81%B7%E4%B9%A6%E4%B8%8D%E7%AE%97%E5%81%B7" rel="nofollow">4.3 读书人偷书不算偷</a></p> 
<p id="%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%EF%BC%88Imitation%20Learning%EF%BC%89-toc" style="margin-left:120px;"><a href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%EF%BC%88Imitation%20Learning%EF%BC%89" rel="nofollow">模仿学习（Imitation Learning）</a></p> 
<p id="%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Knowledge%20Distillation%EF%BC%89-toc" style="margin-left:120px;"><a href="#%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Knowledge%20Distillation%EF%BC%89" rel="nofollow">知识蒸馏（Knowledge Distillation）</a></p> 
<p id="%E4%BA%94%E3%80%81%E6%88%91%E4%BB%AC%E8%BF%98%E8%A6%81%E5%86%8D%E7%A1%AC%E4%B8%80%E7%82%B9-toc" style="margin-left:40px;"><a href="#%E4%BA%94%E3%80%81%E6%88%91%E4%BB%AC%E8%BF%98%E8%A6%81%E5%86%8D%E7%A1%AC%E4%B8%80%E7%82%B9" rel="nofollow">五、我们还要再硬一点</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h3 id="%E4%B8%80%E3%80%81%E5%A5%97%E5%A3%B3%E7%9A%84%E9%A3%8E%E6%B3%A2%E6%AD%A4%E8%B5%B7%E5%BD%BC%E4%BC%8F">一、套壳的风波此起彼伏</h3> 
<p>国内“百模大战”，我形容是“群模乱舞”，具体国内有哪些著名的大模型，请参考我的文章——</p> 
<p><a href="https://blog.csdn.net/giszz/article/details/134340176" title="群模乱舞，AI大模型盛开，国内大模型盘点_ai大模型有哪些-CSDN博客">群模乱舞，AI大模型盛开，国内大模型盘点_ai大模型有哪些-CSDN博客</a></p> 
<p>里面列出了大厂的模型，以及很多学院派的大模型。</p> 
<p>大模型一夜之间，如同AIGAI，自体繁殖一样多起来，很多媒体的声音，也此起彼伏，说国内的大模型，很多都是开源大模型的套壳版本。</p> 
<p>这里面动静最大的，可能是李开复先生的零一大模型张量命名事件。其推出的“Yi”大模型，这款模型被揭露只是对LLaMA进行了表面上的修改——仅仅改变了两个张量的名称。这种创新，确实在AI界并非孤例，而是一个普遍现象。</p> 
<p>有人说，开源就该这样利用，不然开源干什么？有人说，闭源才是自主研发，参考开源就是套壳。</p> 
<p>2023年2月，Meta首次发布了Llama羊驼系列模型。在这个初始版本中，羊驼系列包括了四种不同规模的模型：参数量分别为7亿、13亿、33亿和65亿。7月，Meta公布最新大模型 Llama 2(羊驼 2),包含 7B、13B 和 70B 三种参数变体,可免费用于商业或者研究。这引起不小的轰动，不光是国内，很多国外的大模型，基本都是复用了Llama2，后面我们会讲，为什么选择这个大模型，因为确实不用重复发明轮子了。</p> 
<p>有想了解开源和闭源生态的，可以参考我的文章：<a href="https://blog.csdn.net/giszz/article/details/134460066" title="【AI】马斯克说大模型要开源，我们缺的是源代码？（附一图看懂6大开源协议）_马斯克说大模型在技术上的突破-CSDN博客">【AI】马斯克说大模型要开源，我们缺的是源代码？（附一图看懂6大开源协议）_马斯克说大模型在技术上的突破-CSDN博客</a></p> 
<p>非 AI 从业者，视套壳如洪水猛兽，吃瓜者认为套壳就是抄袭的代名词；真正的 AI 从业者，对套壳讳莫如深，需要借鉴，又狠怕惹锅上身。但由于“套壳”本身并没有清晰、准确的定义，导致行业对套壳的理解也是一千个读者有一千个哈姆雷特。</p> 
<p>那么，问题来了——</p> 
<h3 id="%E4%BA%8C%E3%80%81%E5%88%B0%E5%BA%95%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A3%B3">二、到底什么是大模型的壳</h3> 
<h4 id="2.1%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%843%E9%83%A8%E5%88%86%EF%BC%8C%E5%A3%B3%E6%8C%87%E7%9A%84%E6%98%AF%E5%93%AA%E9%87%8C">2.1 大模型的3部分，壳指的是哪里</h4> 
<p>要想知道什么是大模型的壳，我们要先知道，大模型包括哪几个部分。</p> 
<h5 id="%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%86%85%E6%A0%B8" style="background-color:transparent;">大模型的内核</h5> 
<p>大模型的内核通常指的是模型的核心架构和算法，这些设计决定了模型如何处理输入数据并生成输出。在大模型中，内核往往包含了大量的计算单元（如神经元、层等），以及它们之间的连接方式和权重。这些计算单元通过特定的数学运算（如矩阵乘法、激活函数等）共同工作，以提取输入数据的特征并做出预测。</p> 
<p>大模型的内核设计通常基于深度学习理论，尤其是神经网络。近年来，如上所述，Transformer架构因其出色的性能成为了大模型内核的热门选择。Transformer利用自注意力机制来处理序列数据，能够捕获长距离依赖关系，并在各种NLP任务中取得了显著成果。</p> 
<p>也就是说，大家的内核，基本都来自相同的老祖宗。</p> 
<h5 id="%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88Pre-training%EF%BC%89" style="background-color:transparent;">预训练（Pre-training）</h5> 
<p>预训练是指在大规模数据上对模型进行初步的训练。这个过程通常是无监督的，意味着模型不需要人工标注的数据就可以学习。预训练的目标是让模型学习到通用的知识和表示方法，这样它就能够更好地适应各种下游任务。</p> 
<p>在大模型中，预训练尤为重要，因为庞大的参数量需要大量的数据来有效训练。预训练不仅可以提高模型的泛化能力，还可以加速后续任务的学习过程。例如，在NLP领域，BERT、GPT等模型就是通过在大规模文本语料库上进行预训练来获得强大的语言理解能力的。</p> 
<h5 id="%E8%B0%83%E4%BC%98%EF%BC%88Fine-tuning%EF%BC%89" style="background-color:transparent;">调优（Fine-tuning）</h5> 
<p>调优，也叫做“微调”，是指在特定任务的数据上对已经预训练过的模型进行进一步的训练。这个过程通常是有监督的，需要使用标注好的数据来指导模型的学习。调优的目标是调整模型参数，使其更好地适应特定任务的需求。</p> 
<p>在大模型中，调优通常比从头开始训练要高效得多，因为预训练已经为模型提供了一个很好的起点。通过调优，模型可以在较少的迭代次数和较小的数据集上达到较好的性能。此外，调优还可以使模型更加灵活地适应各种场景和任务需求。</p> 
<p>在漫长的预训练之后会得到一个基座模型（Base Model），在基座模型的基础上加入特定行业的数据集做进一步的微调，就会得到一个微调模型（Fine-tuning Model），或者称为行业模型、垂直模型。</p> 
<h4 id="2.2%20%E5%86%85%E6%A0%B8%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%E5%92%8C%E4%B8%87%E6%B5%81%E5%BD%92%E5%AE%97" style="background-color:transparent;">2.2 内核的发展历程和万流归宗</h4> 
<p>我们都知道，是大模型让AI达到如此的地位，实际上，AI经历了一段低迷期。</p> 
<p>关于AI复兴推进器的自然语言处理、神经网络、遗传算法，我都分别写过文章去介绍。还有AI爆发的推进器之卷积神经网络、生成对抗网络、变分自动编码器、迁移学习、知识图谱、注意力机制与深度学习模型等，也可以参考我的之前的文章。</p> 
<p>在 2020 年之前，NLP 的模型研究基本都是围绕算法展开，基于 BERT、T5 与 GPT 架构的模型百花齐放。这一时期模型参数较小，基本都在 10 亿以内量级。其中，谷歌 BERT 的表现独领风骚，基于 BERT 架构的模型一度在阅读理解的竞赛排行榜中屠榜。</p> 
<p>唯有不同的是，2017 年谷歌大脑团队发布的Transformer 神经网络架构。"Transformer" 的核心思想是利用自注意力（self-attention）机制来处理序列数据，如文本或音频。这种架构避免了传统循环神经网络（RNN）、卷积神经网络或长短期记忆网络（LSTM）中顺序计算的限制，从而能够并行处理整个输入序列，大大提高了计算效率。</p> 
<p>到 2020 年，OpenAI 发布一篇论文，首次提出了 Scaling Laws（尺度定律），NLP 的研究才正式进入大模型时代——大模型基于“大算力、大参数、大数据”，模型性能就会像摩尔定律一样持续提升，直到“智能涌现”的时刻。</p> 
<p>OpenAI公司从此进入公众视野，并且一发不可收拾，除了谷歌最新发布的 Gemini 是基于 T5 架构，几乎清一色都是从 GPT 架构衍生而来。可以说，GPT 完成了一场大模型架构内核的大一统。</p> 
<p>可以参考下图：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/50/ab/uRSfK082_o.png"></p> 
<p>如此看来，别说什么套壳不套壳，人类的发源地就是那么几个。内核（DNA）算法框架，几乎都是一样的。</p> 
<p>总结下，内核，就是3种：BERT、T5 和 GPT 。其中GPT是最枝繁叶茂的！</p> 
<p>这3种框架内核，都广泛应用于自然语言处理（NLP）领域。</p> 
<ol><li> <p><strong>BERT（Bidirectional Encoder Representations from Transformers）</strong>：BERT 是由 Google 在 2018 年提出的一种基于 Transformer 的预训练模型。它采用双向 Transformer 结构，能够在预测单词时同时利用该单词前后的上下文信息。BERT 在多项 NLP 任务中取得了显著的效果提升，并成为了许多后续研究的基础。</p> </li><li> <p><strong>T5（Text-to-Text Transfer Transformer）</strong>：T5 是由 Google 在 2019 年提出的另一种基于 Transformer 的模型。与 BERT 不同，T5 采用统一的文本到文本（text-to-text）框架，将所有 NLP 任务都转化为文本生成任务。这意味着无论是分类、问答还是摘要等任务，都可以使用相同的模型结构和训练方式来处理。</p> </li><li> <p><strong>GPT（Generative Pre-trained Transformer）</strong>：GPT 是由 OpenAI 在 2018 年提出的一种基于 Transformer 的自回归语言模型。它采用单向的 Transformer 结构，只能利用上文信息来预测下一个单词。GPT 在生成式任务（如文本生成、对话生成等）中表现出色，并且随着版本的迭代（如 GPT-2、GPT-3），其模型规模和性能也在不断提升。</p> </li></ol> 
<p>这三种模型框架各有特点，但都是基于 Transformer 架构的深度学习模型，并在 NLP 领域取得了显著的成果。它们通常都需要大量的数据进行预训练，以学习到通用的语言表示和知识，然后再通过微调（fine-tuning）来适应特定的 NLP 任务。</p> 
<h4 id="2.3%20%E5%A5%97%E5%A3%B3%E4%B8%8D%E6%98%AF%E5%80%9F%E5%A3%B3" style="background-color:transparent;">2.3 套壳不是借壳</h4> 
<p>按照大家的现在普遍理解，实际上，内核我们往往是核心框架+预训练，因为这两个非常难，预训练的费用，动辄上亿，有实力的企业，一年做一次就不错了，文心一言4.0，如今最新的数据还是2023年4月份的。常理说，壳指的是调优，而并不能认为是“改个名字”，特别是改个变量名吧！如果真是直接套开源，改个名字，那才是我们理解的传统意义上的套壳。</p> 
<p>除了那些直接用API蒙人的，其实大部分的大模型，都做了调优，也就是很多人说的套壳。当然，也有只做了提示词优化或者注入个性化知识库的大模型产品。</p> 
<p>所以在大模型行业，套壳，是个中性词。</p> 
<p>照搬开源，或者盗用别人API号称自研的，才是贬义词，那是借壳了。</p> 
<p>为什么这么宽容？请往下看。</p> 
<h3 id="%E4%B8%89%E3%80%81%E8%BD%AF%E9%A5%AD%E7%A1%AC%E5%90%83%EF%BC%8C%E5%A5%97%E5%A3%B3%E7%9C%9F%E7%9A%84%E4%B8%8D%E8%A1%8C%E5%90%97" style="background-color:transparent;">三、软饭硬吃，套壳真的不行吗</h3> 
<p>如上，本文内，我们把调优，叫做套壳，没有把改个大模型的名字，或者改个张量的名字，叫做套壳，那是借壳。外人也不懂，忽悠一些普通的资本，上市公司老板，还是绰绰有余的。那个我们是要坚决鄙视。</p> 
<p>相反，套壳是个正常的模式，针对某个垂直领域，用几天调优一下，真的是个常见模式。</p> 
<p>现在调优的论文非常多，可以这么说，堂而皇之的，软饭硬吃了。</p> 
<p>为什么这么多软饭硬吃呢，是不套真的不行。上述我们知道，做一个大模型，包括内核、预悬链和调优三个部分，其中自己做预训练，是比开发一个大模型框架还要难太多的事。把很多公司都拦在了门外，大模型实际是一个高端局，门票太贵。</p> 
<p>比如ChatGPT这样的大模型，预训练的花费、数据量和时间都是相当巨大的。</p> 
<ol><li>预训练的花费：ChatGPT等模型的训练成本非常高昂。根据OpenAI发布的报告，ChatGPT的训练成本大约在4700万美元左右。这包括了硬件、人力、电力等多方面的开支。对于更大的模型，训练成本可能会更高，甚至可能达到数亿美元。</li><li>预训练的数据量：ChatGPT等模型在预训练阶段需要处理的数据量也是巨大的。它们通常需要在数十亿甚至上百亿的文本数据上进行学习，以理解语言表达和实现人工智能语言处理任务。这些数据可能来自于各种来源，如网页、书籍、新闻文章等。</li><li>预训练的时间：训练一个大模型需要的时间也是非常长的。在预训练阶段，处理数十亿甚至上百亿的文本数据可能需要数天甚至数周的时间，具体取决于计算资源的多寡和模型规模的大小。此外，在微调阶段，虽然时间相对较短，但也可能需要几个小时到几天的时间，这取决于特定任务的数据量和复杂性。</li></ol> 
<p>因此，大部分的企业，一年，或者几个月，做一次预训练，就是很好的了。</p> 
<p>正因如此，只有充足的算力、财力的大公司与资本支持的创业公司，才会涉足基座模型（预训练之后得到的模型）。“群模乱舞”中的国产大模型数量虽然多，但只有大约 10% 的模型是基座模型，90% 的模型是在开源模型基础上加入特定数据集做微调的行业模型、垂直模型。其中，应用最广、性能最好的开源基座模型，目前就是 Meta 的 Llama 2。</p> 
<p>Llama2已经达到了GPT-3.5的水平，除非有能力自研一个达到GPT-4、甚至下一代 GPT-5 能力的模型，否则用Llama2是最好的选择。这里的能力指的是有技术能力，且有足够的资金持续投入，因为目前预期是 GPT-5 的训练可能需要 3-5 万张 H100，成本在 10-20 亿美金。</p> 
<blockquote> 
 <p>Tips：H100是Nvidia推出的一款面向计算的GPU。它采用Hopper架构，建立在一个巨大的814mm²芯片上，使用台积电的4N工艺和800亿个晶体管。这款GPU在算力上的FP16、TF32以及FP64性能都是其前代产品A100的3倍，并且具有强大的能力，据英伟达CEO黄仁勋表示，20个H100 GPU便可承托相当于全球互联网的流量。</p> 
</blockquote> 
<h3 id="%E5%9B%9B%E3%80%81%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%EF%BC%8C%E7%99%BE%E5%A7%93%E5%90%83%E7%93%9C">四、神仙打架，百姓吃瓜</h3> 
<h4 id="4.1%20%E8%87%AA%E7%A0%94%E7%9A%84%E4%BD%BC%E4%BD%BC%E8%80%85">4.1 自研的佼佼者</h4> 
<p>有人会觉得奇怪，那么多国产的大模型，我为什么更青睐百度的文心一言，是不是收取了代言费。确实是因为百度在AIGC上，确实赢了一分。</p> 
<p>2019年，百度就发布了自研的预训练框架 ERNIE，也就是今天的文心大模型，今天已经更新到ERNIE-4.0（4.0比3.5免费版有较大的跨越，聪明了很多）。值得一提的是，谷歌 BERT 与百度 ERNIE 名字取材于美国著名儿童节目《芝麻街》中的角色，两者是一对好友。</p> 
<h4 id="4.2%20%E6%A8%A1%E4%BB%BF%E4%B9%9F%E4%B8%8D%E4%B8%A2%E4%BA%BA">4.2 模仿也不丢人</h4> 
<p>对于开源社区而言，这是一套非常正常的做法，开源的意义就是公开自己的研究成果，促进技术的交流与共享，让开源社区内更多的研究者受益。</p> 
<p>Llama 2 也是站在过去开源模型的肩膀上一步步发展而来。比如，Llama 2 的模型架构中， Pre-normalization（预归一化）受 GPT-3 启发，SwiGLU（激活函数）受 PaLM 的启发，Rotary Embeddings（位置编码）受 GPT-Neo 的启发。其他模型也经常魔改这几个参数来做预训练。</p> 
<p>羊驼2这么牛，但是也就是3.5的水平，下一步，就上不去了。这是开源的必然，很多开源软件，都符合这个特征，大模型，当然也不例外。羊驼做到这一步，已经很牛了。</p> 
<h4 id="4.3%20%E8%AF%BB%E4%B9%A6%E4%BA%BA%E5%81%B7%E4%B9%A6%E4%B8%8D%E7%AE%97%E5%81%B7">4.3 读书人偷书不算偷</h4> 
<p>读书人偷书不算偷，还有一句话，叫书非借不能读也。数据当然也可以借一借。</p> 
<p>预训练的数据，在于多，不在精。</p> 
<p>调优的数据，在于精，不在多。</p> 
<p>预训练的数据在于多，对于互联网时代，难的好像就是钱的门槛，数据其实不缺。</p> 
<p>调优的数据，要想高质量，或者对齐竞对，那就要下大功夫了，而且要注入商业逻辑。</p> 
<p>说到“借书”，这里要提到一个模仿学习，或者知识蒸馏的概念。</p> 
<p>模仿学习（Imitation Learning）和知识蒸馏（Knowledge Distillation）是两种机器学习中的技术，它们在不同的场景下被用来提升模型的性能或效率。</p> 
<h5 id="%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%EF%BC%88Imitation%20Learning%EF%BC%89">模仿学习（Imitation Learning）</h5> 
<p>模仿学习，又称为学习从示范（Learning from Demonstration, LfD），是一种机器学习的方法，其中模型通过观察专家的行为来学习如何执行任务。专家的示范可以是人类的操作，也可以是一个已经训练好的模型的行为。模仿学习的目标是让模型能够模仿专家的行为，并在没有专家指导的情况下自主地完成任务。</p> 
<p>在模仿学习中，有两种主要的方法：行为克隆（Behavior Cloning）和逆强化学习（Inverse Reinforcement Learning, IRL）。</p> 
<ul><li><strong>行为克隆</strong>：这种方法直接将专家的行为映射到模型的行为上。通过收集专家的示范数据，并使用监督学习的方法来训练模型，使其尽可能地模仿专家的行为。</li><li><strong>逆强化学习</strong>：与行为克隆不同，逆强化学习旨在从专家的示范中推断出奖励函数，然后使用强化学习的方法在这个奖励函数下训练模型。</li></ul> 
<h5 id="%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Knowledge%20Distillation%EF%BC%89">知识蒸馏（Knowledge Distillation）</h5> 
<p>知识蒸馏是一种模型压缩技术，它允许我们将一个大型、复杂的模型（通常称为“教师模型”）的知识转移到一个较小、较简单的模型（通常称为“学生模型”）中。知识蒸馏的核心思想是利用教师模型的输出来指导学生模型的训练，而不仅仅是使用原始的标签数据。</p> 
<p>在知识蒸馏中，除了使用传统的分类损失（如交叉熵损失）来训练学生模型外，还会引入一种额外的损失，这种损失衡量了学生模型的输出与教师模型的输出之间的差异。通过这种方式，学生模型能够学习到教师模型的“暗知识”，即在训练数据中没有明确给出但有助于提升性能的信息。</p> 
<p>知识蒸馏可以用于多种应用场景，包括模型压缩、加速推理、以及在不牺牲太多性能的情况下将大型模型部署到资源受限的设备上。</p> 
<p><span style="color:#fe2c24;">这就是了，偷书是有理论模型滴！</span></p> 
<p>刚刚过去的2023 年 12 月，字节跳动的“疑似套壳”事件正是来源于此。根据字节跳动的回应，2023 年初部分工程师曾将 OpenAI 的 API 服务应用于实验性的模型研究，但并未上线，后来已经禁止该行为。其实，这是很多大模型或者AI公司都心照不宣的事情，是个男人都会犯的那种错误。</p> 
<p>其实很多大模型，特别是高校的，都是在当学生，OpenAI，肯定是很好的老师了。值得一提的是，OpenAI 在服务条款中明确禁止使用 ChatGPT 生成的数据开发与 OpenAI 竞争的模型。所以，上述模仿模型不能用于商业用途。</p> 
<p>很多国外的大模型，为了加强中文的能力，把文心一言当老师，也是很正常的。</p> 
<p>神仙打架，我们老百姓，就是吃个瓜好了。</p> 
<h3 id="%E4%BA%94%E3%80%81%E6%88%91%E4%BB%AC%E8%BF%98%E8%A6%81%E5%86%8D%E7%A1%AC%E4%B8%80%E7%82%B9">五、我们还要再硬一点</h3> 
<p>在预训练阶段模仿 Llama 2、在微调阶段学习ChatGPT 的数据，是个比较不错的技术路线。</p> 
<p>当然，我们内心还是渴望自己能再硬一点。</p> 
<p>一些模型做prompt的调优，类似文心一言的提示词应用，客户模糊的提示词，根据一定的规律，内置更好的逻辑。</p> 
<p>想了解更多提示词的朋友，可以参考我的系列文章：</p> 
<p><a href="https://blog.csdn.net/giszz/article/details/134193286" title="【AIGC】一起学习prompt提示词（1/4）-CSDN博客">【AIGC】一起学习prompt提示词（1/4）-CSDN博客</a></p> 
<p>再进一步，一些模型做类似插件，或者植入个性化的数据，行业知识库等。实现更好的表现。</p> 
<p>还有一些模型，能做到我们今天说的，调优的大模型，真心是不错的了。</p> 
<p>起码没有去偷人家的API，说是自己的产品，或者类似以前的盗图、盗链，说是自己的作品吧！</p> 
<p>再次重申，如果是上面的情况，那就是绝对贬义的借壳了。</p> 
<p>目前大部分的大模型，都可以说，是站在巨人的肩膀上吧！</p> 
<p>大模型的应用，实际还任重而道远。</p> 
<p>随着人工智能技术的不断发展，大模型已经在自然语言处理、计算机视觉、语音识别等领域取得了显著的成果。然而，要实现大模型在更多领域的广泛应用，需要将其与具体行业的需求相结合，进行垂直化开发和应用。</p> 
<p>行业垂直应用是指将大模型的技术和算法应用到特定行业中，解决该行业中的实际问题。例如，在医疗领域，可以利用大模型进行疾病诊断、药物研发等工作；在金融领域，可以利用大模型进行风险评估、投资决策等工作；在制造领域，可以利用大模型进行智能制造、质量控制等工作。</p> 
<p>通过行业垂直应用，大模型可以更好地满足各行业的需求，提高行业的智能化水平和工作效率。同时，由于不同行业的数据和场景具有独特性，这也为大模型的发展提供了更多的挑战和机遇。</p> 
<p>然而，要实现大模型在行业垂直应用中的广泛应用，还需要克服一些技术和社会方面的挑战。例如，需要解决大模型的可解释性和鲁棒性问题，以确保其在各种复杂场景下的稳定性和可靠性；同时还需要考虑数据隐私和安全问题，以保护用户的数据安全和隐私权益。</p> 
<p>toC领域也有巨大的场景，充满着想象的空间，我们拭目以待吧！</p> 
<p>（giszz出品，感谢关注）</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4495a8915c570af1a25e69499921a9ac/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">面试经典题---14.最长公共前缀</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/90c5c426a5401002a2552d6c9f9e3096/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">STM32 CubeMX 中断NVIC 实战 (超详细配30张高清图，附源码)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>