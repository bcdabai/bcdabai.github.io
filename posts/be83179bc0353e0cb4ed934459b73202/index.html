<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>scrapy框架实战 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="scrapy框架实战" />
<meta property="og:description" content="👨‍💻更多精彩尽在博主首页：i新木优子👀
🎉欢迎关注🔍点赞👍收藏⭐留言📝
🧚‍♂️寄语:当你将信心放在自己身上时，你将永远充满力量👣
✨有任何疑问欢迎评论探讨
什么是全站数据crawling呢，顾名思义就是将一个网站的全部数据都crawling下来，这里我采用scrapy框架，这里我提供了很多方式，可以挑选自己喜欢的玩一玩
接下来有请我们的幸运儿：不能说的网站名，我怕不过审🚗
0️⃣1️⃣创建scrapy项目
scrapy startproject 文件名 cd 文件名 scrapy genspider 名称 要crawling网站的域名 0️⃣2️⃣更改settings配置文件
USER_AGENT ----------&gt;设置UA ROBOTSTXT_OBEY ----------&gt;君子协议（我们爬虫当然不会遵守啦😎） LOG_LEVEL ----------&gt;日志等级（建议设置为WARNING） ❗❗❗一定要记得设置·DOWNLOAD_DELAY·限制访问频率
因为scrapy的底层是协程，速度非常快，如果不设置可能用不了几分钟就会弹出安全验证无法抓取网页。有些网站如果不设置，抓取的数据量够多几分钟就会把网站跑die了,毕竟我们是善良的spider，不要破坏网站哦😄
首先，我们进入网页按键盘上的F12进入开发者模式，在Elements中做参考，Elements可以做参考不能作为依据，因为Elements是经过css和js渲染之后形成的，作为依据的只能是页面源代码（Sources）。可以观察到每一个li标签就是一条数据（这里我们先不考虑分页，先抓取一页的数据，一页搞定了分页就很简单了）
我们要是只抓取首页上的数据就很没意思，更多的是想点击进入详情页,抓取详情页中的数据，详情页的数据才更全面
0️⃣3️⃣解析首页数据拿到详情页的url
li_list = resp.xpath(&#34;//ul[@class=&#39;viewlist_ul&#39;]/li&#34;) # 拿到每一个li for li in li_list: href = li.xpath(&#34;./a/@href&#34;).extract_first() print(href) 0️⃣4️⃣拿到的url如上图，发现这并不是我们想要的url它不完整，所以我们要将href进行拼接，得到真正的url
href = resp.urljoin(href) 0️⃣5️⃣这样我们就拿到了真正的url，仔细观察发现最后一条数据并不是我们想要的（最后一条url是广告），加一个if判断就可以个将没用的url去除
if &#34;topicm&#34; in href: continue 0️⃣6️⃣到此为止，我们拿到了每一条详情页的url，只需再一次发送请求进入详情页，解析详情页拿到我们要的数据即可
⚠⚠⚠我们的目的是为了实现全站数据crawling,数据量是非常大的，所以我们要提前预估风险，就像上图中的数据可能某一条或某几条会缺失，这就涉及到缺省值的处理
0️⃣7️⃣💎缺省值的处理：
方式一：
可以用if条件判断，通过判断小标题拿对应的内容（这种比较麻烦，数据越多难度越大）方式二（推荐）：
自己定义一种数据结构作为映射（简便且数据规整） 代码和运行图片如下：
car_tag = { &#34;表显里程&#34;: &#34;mileage&#34;, &#34;上牌时间&#34;: &#34;time&#34;, &#34;挡位/排量&#34;: &#34;displace&#34;, &#34;车辆所在地&#34;: &#34;location&#34;, &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/be83179bc0353e0cb4ed934459b73202/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-14T21:09:49+08:00" />
<meta property="article:modified_time" content="2022-05-14T21:09:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">scrapy框架实战</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>👨‍💻更多精彩尽在博主首页：<a href="https://blog.csdn.net/weixin_56382303?spm=1011.2419.3001.5343">i新木优子</a>👀<br> 🎉欢迎关注🔍点赞👍收藏⭐留言📝<br> 🧚‍♂️寄语:当你将信心放在自己身上时，你将永远充满力量👣<br> ✨有任何疑问欢迎评论探讨</p> 
</blockquote> 
<hr> 
<p><code>什么是全站数据crawling呢，顾名思义就是将一个网站的全部数据都crawling下来，这里我采用scrapy框架，这里我提供了很多方式，可以挑选自己喜欢的玩一玩</code><br> 接下来有请我们的幸运儿：<a href="https://www.che168.com/china/a0_0msdgscncgpi1ltocsp1exx0/" rel="nofollow">不能说的网站名，我怕不过审🚗</a><br> <img src="https://images2.imgbox.com/4e/87/eKXdEwia_o.gif" alt="在这里插入图片描述"></p> 
<hr> 
<blockquote> 
 <p>0️⃣1️⃣<code>创建scrapy项目</code></p> 
 <pre><code class="prism language-python">scrapy startproject 文件名
cd 文件名
scrapy genspider 名称 要crawling网站的域名
</code></pre> 
 <p><img src="https://images2.imgbox.com/b6/fa/su6KPeay_o.png" alt=""><br> 0️⃣2️⃣<code>更改settings配置文件</code></p> 
 <pre><code class="prism language-python">USER_AGENT       <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span>设置UA
ROBOTSTXT_OBEY   <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span>君子协议（我们爬虫当然不会遵守啦😎）
LOG_LEVEL        <span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">&gt;</span>日志等级（建议设置为WARNING）
</code></pre> 
 <p><img src="https://images2.imgbox.com/6a/7c/HUQNUskn_o.png" alt="在这里插入图片描述"><br> ❗❗❗<code>一定要记得设置·DOWNLOAD_DELAY·限制访问频率</code><br> <mark>因为scrapy的底层是协程，速度非常快，如果不设置可能用不了几分钟就会弹出安全验证无法抓取网页。<font color="red">有些网站如果不设置，抓取的数据量够多几分钟就会把网站跑die了,毕竟我们是<strong>善良的spider</strong>，不要破坏网站哦😄</font></mark><br> <img src="https://images2.imgbox.com/ab/26/tgVYgvc9_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<blockquote> 
 <p>首先，我们进入网页按键盘上的<code>F12</code>进入开发者模式，在<code>Elements</code>中做<mark>参考</mark>，<mark>Elements可以做参考不能作为依据</mark>，因为Elements是经过css和js渲染之后形成的，作为依据的只能是<code>页面源代码</code>（Sources）。可以观察到每一个<code>li标签</code>就是一条数据（<mark><font color="red">这里我们先不考虑分页，先抓取一页的数据，一页搞定了分页就很简单了</font></mark>）<br> 我们要是只抓取首页上的数据就很没意思，更多的是想点击进入<code>详情页</code>,抓取详情页中的数据，详情页的数据才更全面</p> 
</blockquote> 
<hr> 
<p>0️⃣3️⃣<font color="#944bb1">解析首页数据拿到详情页的url</font></p> 
<pre><code class="prism language-python">li_list <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//ul[@class='viewlist_ul']/li"</span><span class="token punctuation">)</span>  <span class="token comment"># 拿到每一个li</span>
<span class="token keyword">for</span> li <span class="token keyword">in</span> li_list<span class="token punctuation">:</span>
	href <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./a/@href"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span>href<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/18/9b/ql2GjIwX_o.png" alt="拿到的href"><br> 0️⃣4️⃣<font color="#944bb1">拿到的url如上图，发现这并不是我们想要的url它不完整，所以我们要将href进行拼接，得到真正的url</font></p> 
<pre><code class="prism language-python">href <span class="token operator">=</span> resp<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>href<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/83/47/S8n27u8A_o.png" alt="在这里插入图片描述"><br> 0️⃣5️⃣<font color="#944bb1">这样我们就拿到了真正的url，仔细观察发现最后一条数据并不是我们想要的（最后一条url是广告），加一个if判断就可以个将没用的url去除</font></p> 
<pre><code class="prism language-python"><span class="token keyword">if</span> <span class="token string">"topicm"</span> <span class="token keyword">in</span> href<span class="token punctuation">:</span>
		<span class="token keyword">continue</span>
</code></pre> 
<p>0️⃣6️⃣<font color="#944bb1">到此为止，我们拿到了每一条详情页的url，只需再一次发送请求进入详情页，解析详情页拿到我们要的数据即可</font></p> 
<hr> 
<blockquote> 
 <p>⚠⚠⚠<code>我们的目的是为了实现全站数据crawling,数据量是非常大的，所以我们要提前预估风险，就像上图中的数据可能某一条或某几条会缺失，这就涉及到缺省值的处理</code><br> 0️⃣7️⃣💎<strong>缺省值的处理</strong>：</p> 
 <ul><li>方式一：<br> 可以用if条件判断，通过判断小标题拿对应的内容（这种比较麻烦，数据越多难度越大）</li><li>方式二（推荐）：<br> 自己定义一种数据结构作为映射（简便且数据规整）</li></ul> 
</blockquote> 
<blockquote> 
 <p>代码和运行图片如下：</p> 
 <pre><code class="prism language-python">car_tag <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
	<span class="token string">"表显里程"</span><span class="token punctuation">:</span> <span class="token string">"mileage"</span><span class="token punctuation">,</span>
	<span class="token string">"上牌时间"</span><span class="token punctuation">:</span> <span class="token string">"time"</span><span class="token punctuation">,</span>
	<span class="token string">"挡位/排量"</span><span class="token punctuation">:</span> <span class="token string">"displace"</span><span class="token punctuation">,</span>
	<span class="token string">"车辆所在地"</span><span class="token punctuation">:</span> <span class="token string">"location"</span><span class="token punctuation">,</span>
	<span class="token string">"查看限迁地"</span><span class="token punctuation">:</span> <span class="token string">"standard"</span>
<span class="token punctuation">}</span>  <span class="token comment"># 映射</span>
dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
   <span class="token string">'name'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
   <span class="token string">'mileage'</span><span class="token punctuation">:</span> <span class="token string">'0公里'</span><span class="token punctuation">,</span>
   <span class="token string">'time'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
   <span class="token string">'displace'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
   <span class="token string">'location'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
   <span class="token string">'standard'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span>
<span class="token punctuation">}</span>  <span class="token comment"># 承载最终的数据</span>
name <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@class='car-box']/h3/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>
dic<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span> <span class="token operator">=</span> name
lis <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@class='car-box']/ul/li"</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> li <span class="token keyword">in</span> lis<span class="token punctuation">:</span>
   p_name <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./p//text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
   p_value <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h4/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
   p_name <span class="token operator">=</span> p_name<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
   p_value <span class="token operator">=</span> p_value<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

   data_key <span class="token operator">=</span> self<span class="token punctuation">.</span>car_tag<span class="token punctuation">[</span>p_name<span class="token punctuation">]</span>
   dic<span class="token punctuation">[</span>data_key<span class="token punctuation">]</span> <span class="token operator">=</span> p_value

<span class="token keyword">print</span><span class="token punctuation">(</span>dic<span class="token punctuation">)</span>
</code></pre> 
 <p><img src="https://images2.imgbox.com/29/3d/3I5NyEuT_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<blockquote> 
 <p>0️⃣8️⃣一页的数据拿到了，接下来就是分页这里我也提供两种方式：</p> 
 <ul><li>方式一：<br> 仔细的观察网址：<br> https://www.che168.com/china/a0_0msdgscncgpi1ltocsp<code>1</code>exx0/?pvareaid=102179#currengpostion<br> https://www.che168.com/china/a0_0msdgscncgpi1ltocsp<code>2</code>exx0/?pvareaid=102179#currengpostion<br> https://www.che168.com/china/a0_0msdgscncgpi1ltocsp<code>3</code>exx0/?pvareaid=102179#currengpostion<br> 只要我们将数字依次替换就可以实现翻页<br> <strong>我们只需要写一个for循环就可以搞定哦</strong>🐍</li><li>方式二：<br> 方式一是最基本的翻页逻辑，但是我们用的是scrapy，<strong>scrapy有自己的方式</strong><br> 只需要拿到翻页的url发送请求即可实现翻页（<mark><font color="red">这里不需要担心有重复的url,scrapy框架中有一个调度器（scheduler）会自动的帮助我们实现去重，这样就可以将100页的数据全部抓取到</font></mark>）</li></ul> 
 <pre><code class="prism language-python">hrefs <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@id='listpagination']/a/@href"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> href <span class="token keyword">in</span> hrefs<span class="token punctuation">:</span>
	 <span class="token keyword">if</span> href<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"javascript"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token keyword">continue</span>
	 href <span class="token operator">=</span> resp<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>href<span class="token punctuation">)</span>
     <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>
               url<span class="token operator">=</span>href<span class="token punctuation">,</span>
               callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse
     <span class="token punctuation">)</span>
</code></pre> 
</blockquote> 
<blockquote> 
 <p>0️⃣9️⃣<code>数据全部crawling到了，就剩下存储了</code><br> <font color="hotpink">存储之前一定要记得在配置文件中打开管道<br> <img src="https://images2.imgbox.com/6b/2a/MIHxEmVL_o.png" alt="在这里插入图片描述"><br> 存储数据就要在管道（pipeline）中写代码，这里我选择存储在<strong>csv文件</strong>，当然也可以选择<strong>Mysql</strong>、<strong>MongoDB</strong>等等</font></p> 
 <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
   self<span class="token punctuation">.</span>f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"car.csv"</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
   self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">print</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span>
   self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'mileage'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'time'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'displace'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'standard'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">\n"</span></span><span class="token punctuation">)</span>
   <span class="token keyword">return</span> item
</code></pre> 
 <p><img src="https://images2.imgbox.com/dc/a9/Dv5YWMLC_o.png" alt="在这里插入图片描述"><br> <font color="blue">只要程序一直跑下去就可以将数据全部获取到，只需耐心等待即可（全站数据crawling的时间可能很长），这样我们就实现了全站数据crawling，是不是很简单呢😼</font></p> 
</blockquote> 
<p>1️⃣0️⃣接下来就是小伙伴们最喜欢的源代码环节😀<br> <code>jia.py</code></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">JiaSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'jia'</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'che168.com'</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'https://www.che168.com/china/a0_0msdgscncgpi1ltocsp1exx0/'</span><span class="token punctuation">]</span>

    car_tag <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">"表显里程"</span><span class="token punctuation">:</span> <span class="token string">"mileage"</span><span class="token punctuation">,</span>
        <span class="token string">"上牌时间"</span><span class="token punctuation">:</span> <span class="token string">"time"</span><span class="token punctuation">,</span>
        <span class="token string">"挡位/排量"</span><span class="token punctuation">:</span> <span class="token string">"displace"</span><span class="token punctuation">,</span>
        <span class="token string">"车辆所在地"</span><span class="token punctuation">:</span> <span class="token string">"location"</span><span class="token punctuation">,</span>
        <span class="token string">"查看限迁地"</span><span class="token punctuation">:</span> <span class="token string">"standard"</span>
    <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> resp<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># print(resp.url)</span>
        li_list <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//ul[@class='viewlist_ul']/li"</span><span class="token punctuation">)</span>  <span class="token comment"># 拿到每一个li</span>
        <span class="token keyword">for</span> li <span class="token keyword">in</span> li_list<span class="token punctuation">:</span>
            href <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./a/@href"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            href <span class="token operator">=</span> resp<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>href<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token string">"topicm"</span> <span class="token keyword">in</span> href<span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            <span class="token comment"># print(href)</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>
                url<span class="token operator">=</span>href<span class="token punctuation">,</span>
                callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_detail
            <span class="token punctuation">)</span>

        <span class="token comment"># 分页</span>
        hrefs <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@id='listpagination']/a/@href"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> href <span class="token keyword">in</span> hrefs<span class="token punctuation">:</span>
            <span class="token keyword">if</span> href<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"javascript"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            href <span class="token operator">=</span> resp<span class="token punctuation">.</span>urljoin<span class="token punctuation">(</span>href<span class="token punctuation">)</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>
                url<span class="token operator">=</span>href<span class="token punctuation">,</span>
                callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse
            <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_detail</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> resp<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dic <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">'name'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
            <span class="token string">'mileage'</span><span class="token punctuation">:</span> <span class="token string">'0公里'</span><span class="token punctuation">,</span>
            <span class="token string">'time'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
            <span class="token string">'displace'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
            <span class="token string">'location'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span><span class="token punctuation">,</span>
            <span class="token string">'standard'</span><span class="token punctuation">:</span> <span class="token string">'未知'</span>
        <span class="token punctuation">}</span>  <span class="token comment"># 最终的数据</span>
        name <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@class='car-box']/h3/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span>
        dic<span class="token punctuation">[</span><span class="token string">"name"</span><span class="token punctuation">]</span> <span class="token operator">=</span> name
        lis <span class="token operator">=</span> resp<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//div[@class='car-box']/ul/li"</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> li <span class="token keyword">in</span> lis<span class="token punctuation">:</span>
            p_name <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./p//text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            p_value <span class="token operator">=</span> li<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./h4/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
            p_name <span class="token operator">=</span> p_name<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
            p_value <span class="token operator">=</span> p_value<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

            data_key <span class="token operator">=</span> self<span class="token punctuation">.</span>car_tag<span class="token punctuation">[</span>p_name<span class="token punctuation">]</span>
            dic<span class="token punctuation">[</span>data_key<span class="token punctuation">]</span> <span class="token operator">=</span> p_value

        <span class="token keyword">yield</span> dic
</code></pre> 
<p><code>settings.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># Scrapy settings for car project</span>
<span class="token comment">#</span>
<span class="token comment"># For simplicity, this file contains only settings considered important or</span>
<span class="token comment"># commonly used. You can find more settings consulting the documentation:</span>
<span class="token comment">#</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/settings.html</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="token comment">#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>

BOT_NAME <span class="token operator">=</span> <span class="token string">'car'</span>

SPIDER_MODULES <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'car.spiders'</span><span class="token punctuation">]</span>
NEWSPIDER_MODULE <span class="token operator">=</span> <span class="token string">'car.spiders'</span>

<span class="token comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
USER_AGENT <span class="token operator">=</span> <span class="token string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36'</span>

<span class="token comment"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY <span class="token operator">=</span> <span class="token boolean">False</span>
LOG_LEVEL <span class="token operator">=</span> <span class="token string">"WARNING"</span>

<span class="token comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span>
<span class="token comment"># CONCURRENT_REQUESTS = 32</span>

<span class="token comment"># Configure a delay for requests for the same website (default: 0)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay</span>
<span class="token comment"># See also autothrottle settings and docs</span>
DOWNLOAD_DELAY <span class="token operator">=</span> <span class="token number">3</span>
<span class="token comment"># The download delay setting will honor only one of:</span>
<span class="token comment"># CONCURRENT_REQUESTS_PER_DOMAIN = 16</span>
<span class="token comment"># CONCURRENT_REQUESTS_PER_IP = 16</span>

<span class="token comment"># Disable cookies (enabled by default)</span>
COOKIES_ENABLED <span class="token operator">=</span> <span class="token boolean">False</span>

<span class="token comment"># Disable Telnet Console (enabled by default)</span>
<span class="token comment"># TELNETCONSOLE_ENABLED = False</span>

<span class="token comment"># Override the default request headers:</span>
DEFAULT_REQUEST_HEADERS <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">'Accept'</span><span class="token punctuation">:</span> <span class="token string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span><span class="token punctuation">,</span>
    <span class="token string">'Accept-Language'</span><span class="token punctuation">:</span> <span class="token string">'en'</span><span class="token punctuation">,</span>
    <span class="token string">"Cookie"</span><span class="token punctuation">:</span> <span class="token string">"listuserarea=0; fvlid=1652426719616u4Xrt5cvwi40; Hm_lvt_d381ec2f88158113b9b76f14c497ed48=1652426720; sessionid=8f823055-9b43-44fd-96a7-6ebeaabb8c5f; sessionip=39.154.171.103; area=150699; sessionvisit=0ac66484-306b-41b1-a2b1-caae86be6f16; sessionvisitInfo=8f823055-9b43-44fd-96a7-6ebeaabb8c5f||0; che_sessionid=1CC9EB24-B4F9-4B9D-B2F7-389ED89C1BB9%7C%7C2022-05-13+15%3A25%3A20.567%7C%7C0; che_sessionvid=FA117386-76D5-4F08-B102-914CDFD4E4F6; userarea=110100; UsedCarBrowseHistory=0%3A43635729; carDownPrice=1; ahpvno=3; Hm_lpvt_d381ec2f88158113b9b76f14c497ed48=1652427405; ahuuid=3EC672D4-DFF9-4DA7-956D-F9D7A2B89915; v_no=3; visit_info_ad=1CC9EB24-B4F9-4B9D-B2F7-389ED89C1BB9||FA117386-76D5-4F08-B102-914CDFD4E4F6||-1||-1||3; che_ref=0%7C0%7C0%7C0%7C2022-05-13+15%3A36%3A45.754%7C2022-05-13+15%3A25%3A20.567; showNum=3; sessionuid=8f823055-9b43-44fd-96a7-6ebeaabb8c5f"</span>
<span class="token punctuation">}</span>

<span class="token comment"># Enable or disable spider middlewares</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/spider-middleware.html</span>
<span class="token comment"># SPIDER_MIDDLEWARES = {<!-- --></span>
<span class="token comment">#    'car.middlewares.CarSpiderMiddleware': 543,</span>
<span class="token comment"># }</span>

<span class="token comment"># Enable or disable downloader middlewares</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span>
<span class="token comment"># DOWNLOADER_MIDDLEWARES = {<!-- --></span>
<span class="token comment">#    'car.middlewares.CarDownloaderMiddleware': 543,</span>
<span class="token comment"># }</span>

<span class="token comment"># Enable or disable extensions</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span>
<span class="token comment"># EXTENSIONS = {<!-- --></span>
<span class="token comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span>
<span class="token comment"># }</span>

<span class="token comment"># Configure item pipelines</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
   <span class="token string">'car.pipelines.CarPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>

<span class="token comment"># Enable and configure the AutoThrottle extension (disabled by default)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span>
<span class="token comment"># AUTOTHROTTLE_ENABLED = True</span>
<span class="token comment"># The initial download delay</span>
<span class="token comment"># AUTOTHROTTLE_START_DELAY = 5</span>
<span class="token comment"># The maximum download delay to be set in case of high latencies</span>
<span class="token comment"># AUTOTHROTTLE_MAX_DELAY = 60</span>
<span class="token comment"># The average number of requests Scrapy should be sending in parallel to</span>
<span class="token comment"># each remote server</span>
<span class="token comment"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span>
<span class="token comment"># Enable showing throttling stats for every response received:</span>
<span class="token comment"># AUTOTHROTTLE_DEBUG = False</span>

<span class="token comment"># Enable and configure HTTP caching (disabled by default)</span>
<span class="token comment"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span>
<span class="token comment"># HTTPCACHE_ENABLED = True</span>
<span class="token comment"># HTTPCACHE_EXPIRATION_SECS = 0</span>
<span class="token comment"># HTTPCACHE_DIR = 'httpcache'</span>
<span class="token comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span>
<span class="token comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span>
</code></pre> 
<p><code>pipelines.py</code></p> 
<pre><code class="prism language-python"><span class="token comment"># Define your item pipelines here</span>
<span class="token comment">#</span>
<span class="token comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span>
<span class="token comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span>


<span class="token comment"># useful for handling different item types with a single interface</span>
<span class="token keyword">from</span> itemadapter <span class="token keyword">import</span> ItemAdapter


<span class="token keyword">class</span> <span class="token class-name">CarPipeline</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"car.csv"</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"w"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'mileage'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'time'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'displace'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'location'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">,</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>item<span class="token punctuation">[</span><span class="token string">'standard'</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">\n"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> item
</code></pre> 
<p><code>runner.py</code></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>cmdline <span class="token keyword">import</span> execute

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    execute<span class="token punctuation">(</span><span class="token string">"scrapy crawl jia"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><code>上面的只是一种方式，除此之外，还有一种方式简单粗暴的方式也可以实现全站数据crawling</code><br> <mark>只抓一个倒霉蛋往die里搞，终究太过于残忍😃，下面有请第二个倒霉蛋：</mark><a href="https://www.shicimingjv.com/tangshi/index_1.html" rel="nofollow">某诗词网站📚</a></p> 
<blockquote> 
 <p><code>创建CrawlSpider项目</code></p> 
 <pre><code class="prism language-python">scrapy startproject 文件名
cd 文件名
scrapy genspider <span class="token operator">-</span>t crawl 名称 要爬取网站的域名
</code></pre> 
 <p><code>配置文件还和之前一样</code><br> <mark>这个网站和上述网站的结构一模一样，也需要进入到详情页crawling数据，并且分页</mark></p> 
 <p><font color="green">我们知道我们在网页上点击的都是超链接，那我们只要能拿到每一个超链接，就可以实现页面数据的crawling，所以CrawlSpider就为我们准备好了链接提取器</font></p> 
</blockquote> 
<p>🐲<mark>让我们以代码为例，讲一下链接提取器是怎么工作的：</mark></p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>linkextractors <span class="token keyword">import</span> LinkExtractor  <span class="token comment"># 导入链接提取器</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>spiders <span class="token keyword">import</span> CrawlSpider<span class="token punctuation">,</span> Rule


<span class="token keyword">class</span> <span class="token class-name">TangSpider</span><span class="token punctuation">(</span>CrawlSpider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'tang'</span>  <span class="token comment"># 名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'shicimingjv.com'</span><span class="token punctuation">]</span>  <span class="token comment"># 域名</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'https://www.shicimingjv.com/tangshi/index_1.html'</span><span class="token punctuation">]</span>  <span class="token comment"># 首页的网址</span>

	<span class="token comment"># lk1 = LinkExtractor()  表示造一个链接提取器，括号中的表示提取规则，下图有源码的详细说明</span>
    <span class="token comment"># 详情页的url地址</span>
    lk1 <span class="token operator">=</span> LinkExtractor<span class="token punctuation">(</span>restrict_xpaths<span class="token operator">=</span><span class="token string">"//div[@class='sec-panel-body']/ul/li/div[1]/h3/a"</span><span class="token punctuation">)</span>
    <span class="token comment"># 分页的url地址</span>
    lk2 <span class="token operator">=</span> LinkExtractor<span class="token punctuation">(</span>restrict_xpaths<span class="token operator">=</span><span class="token string">"//ul[@class='pagination']/li/a"</span><span class="token punctuation">)</span>

    rules <span class="token operator">=</span> <span class="token punctuation">(</span>
        Rule<span class="token punctuation">(</span>lk1<span class="token punctuation">,</span> callback<span class="token operator">=</span><span class="token string">'parse_item'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># callback表示请求回来要执行的函数</span>
        Rule<span class="token punctuation">(</span>lk2<span class="token punctuation">,</span> follow<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># follow=True表示是否要重新执行一次rules</span>
    <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 解析详情页的内容</span>
        title <span class="token operator">=</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//h1[@class='mp3']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>title<span class="token punctuation">)</span>
</code></pre> 
<p><mark>通过对比之前的全站提取方式，我们可以发现CrawlSpider就是省略了parse这个函数，因为CrawlSpider是高度分装的，所以他的灵活性不如之前的高</mark></p> 
<blockquote> 
 <p>🏆让我们进入<code>LinkExtractor源码</code>中一探究竟，看看提取器的提取方式是什么样子的</p> 
 <p><img src="https://images2.imgbox.com/f2/db/1l6zxjZC_o.png" alt="在这里插入图片描述"></p> 
</blockquote> 
<p>只要是页面上的链接，用连接提取器都能提取到，找到对应的链接发送请求进行解析，就可以实现真正的全站数据crawling<br> <mark><font color="red">这些小伙伴们都可以自由发挥，切记不要太过分，把人家网站往die里搞，毕竟我们是抱着学习的目的去的，做<strong>善良的spider</strong></font></mark></p> 
<hr> 
<blockquote> 
 <p>讲到最后，有些小伙伴可能不会运行scrapy，这里我们说两种方式：</p> 
 <ul><li>方式一：<br> 进入terminal输入<code>scrapy crawl 名称</code></li><li>方式二（推荐）：<br> 建一个py文件runner,右击即可运行</li></ul> 
 <pre><code class="prism language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>cmdline <span class="token keyword">import</span> execute

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
   execute<span class="token punctuation">(</span><span class="token string">"scrapy crawl 名称"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
</blockquote> 
<p>🙏因审核的原因，有些细节没有办法说明，做了很多的删减，望谅解</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/638619bec63428b0bb2f5766ee004490/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">linux下安装yum步骤</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/378e9a026ef9f06b85293fe384deb558/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">转录组-差异基因热图</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>