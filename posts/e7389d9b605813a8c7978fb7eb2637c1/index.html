<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Attention Mechanisms in Computer Vision: A Survey(三) - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Attention Mechanisms in Computer Vision: A Survey(三)" />
<meta property="og:description" content="接上一篇博客：Attention Mechanisms in Computer Vision: A Survey(二)
我又回来了，这篇综述简直信息量巨大，这是我第三天消化这篇综述了，有兴趣的同学可以读一下原文。
一、Branch Attention 分支注意可以被看作是一种动态的分支选择机制：注意哪个分支，与多分支结构一起使用。
（一）、Highway networks 受长短期记忆网络的启发，Srivastava等人提出了Highway networks，该网络采用自适应选通机制，使信息能够跨层流动，以解决训练非常深层网络的问题。 假设一个普通的神经网络由L层组成，并且 H l （ X ） H_l（X） Hl​（X）表示第L层上的非线性变换，那么Highway networks可以表示为
T l （ X ） T_l（X） Tl​（X）表示调节第l层的信息流的变换门。 X l X_l Xl​和 Y l Y_l Yl​是第l层的输入和输出。
选通机制和跳接结构使得使用简单的梯度下降方法直接训练非常深的Highway networks成为可能。与固定跳过连接不同，选通机制适应输入，这有助于跨层路由信息。Highway networks可以并入任何CNN。
(二)SKNet 见博文
（三）、CondConv CNN中的一个基本假设是所有卷积核都是相同的。一般情况下增强网络表现力的典型方法是增加其深度或宽度，这会带来大量额外的计算成本。为了更有效地提高卷积神经网络的容量，Yang等人提出了一种新的多分支算子CondConv，可以定义一个普通的卷积 ：
∗ 表示卷积。所有样本的可学习参数W都相同。CondConv自适应地组合了多个卷积核，可以写成：
α是一个可学习的权重向量
这个过程相当于多个experts的集合，如下图所示：
CondConv充分利用了多分支结构的优点，采用了一种计算量小的分支注意方法。它提供了一种有效提高网络容量的新方法。
(四)、Dynamic Convolution 轻量级CNN极低的计算成本限制了网络的深度和宽度，进一步降低了其代表性。为了解决上述问题，Chen等人提出了动态卷积，这是一种新的算子设计，可以增加表征能力，但额外的计算成本可以忽略不计，并且不会与CondConv并行改变网络的宽度或深度。 动态卷积使用K个大小和输入/输出维度相同的并行卷积核，而不是每层一个核。与SE块一样，它采用挤压和激励机制为不同的卷积核生成注意权重。这些卷积核通过加权求和动态聚合，并应用于输入特征映射X：
卷积通过卷积核的权重和偏差之和进行组合。
与将卷积应用于特征映射相比，压缩、激励和加权求和的计算成本极低。因此，动态卷积提供了一种有效的操作来提高表示能力，并且可以轻松地用作任何卷积的替代品。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e7389d9b605813a8c7978fb7eb2637c1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-01T18:21:31+08:00" />
<meta property="article:modified_time" content="2022-05-01T18:21:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Attention Mechanisms in Computer Vision: A Survey(三)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>接上一篇博客：<a href="https://blog.csdn.net/qq_52302919/article/details/124513045">Attention Mechanisms in Computer Vision: A Survey(二)</a><br> 我又回来了，这篇综述简直信息量巨大，这是我第三天消化这篇综述了，有兴趣的同学可以读一下原文。</p> 
<h2><a id="Branch_Attention_2"></a>一、Branch Attention</h2> 
<p>分支注意可以被看作是一种动态的分支选择机制：注意哪个分支，与多分支结构一起使用。</p> 
<h3><a id="Highway_networks_4"></a>（一）、Highway networks</h3> 
<p>受长短期记忆网络的启发，Srivastava等人提出了Highway networks，该网络采用自适应选通机制，使信息能够跨层流动，以解决训练非常深层网络的问题。 假设一个普通的神经网络由L层组成，并且<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          H 
         
        
          l 
         
        
       
         （ 
        
       
         X 
        
       
         ） 
        
       
      
        H_l（X） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.08125em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord cjk_fallback">（</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mord cjk_fallback">）</span></span></span></span></span>表示第L层上的非线性变换，那么Highway networks可以表示为<br> <img src="https://images2.imgbox.com/25/0a/Npew24oS_o.png" alt="在这里插入图片描述"><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          T 
         
        
          l 
         
        
       
         （ 
        
       
         X 
        
       
         ） 
        
       
      
        T_l（X） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord cjk_fallback">（</span><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="mord cjk_fallback">）</span></span></span></span></span>表示调节第l层的信息流的变换门。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          X 
         
        
          l 
         
        
       
      
        X_l 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.07847em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Y 
         
        
          l 
         
        
       
      
        Y_l 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>是第l层的输入和输出。<br> 选通机制和跳接结构使得使用简单的梯度下降方法直接训练非常深的Highway networks成为可能。与固定跳过连接不同，选通机制适应输入，这有助于跨层路由信息。Highway networks可以并入任何CNN。</p> 
<h3><a id="SKNet_9"></a>(二)SKNet</h3> 
<p><a href="https://blog.csdn.net/qq_52302919/article/details/123078832">见博文</a></p> 
<h3><a id="CondConv_11"></a>（三）、CondConv</h3> 
<p>CNN中的一个基本假设是所有卷积核都是相同的。一般情况下增强网络表现力的典型方法是增加其深度或宽度，这会带来大量额外的计算成本。为了更有效地提高卷积神经网络的容量，Yang等人提出了一种新的多分支算子CondConv，可以定义一个普通的卷积 ：</p> 
<p><img src="https://images2.imgbox.com/d8/a8/naYdXa54_o.png" alt="在这里插入图片描述"><br> ∗ 表示卷积。所有样本的可学习参数W都相同。CondConv自适应地组合了多个卷积核，可以写成：<br> <img src="https://images2.imgbox.com/bc/94/IRhdk39v_o.png" alt="在这里插入图片描述"></p> 
<p>α是一个可学习的权重向量<br> <img src="https://images2.imgbox.com/54/81/s40TkwEl_o.png" alt="在这里插入图片描述"><br> 这个过程相当于多个experts的集合，如下图所示：<br> <img src="https://images2.imgbox.com/8e/b8/QbgIkABO_o.png" alt="在这里插入图片描述"><br> CondConv充分利用了多分支结构的优点，采用了一种计算量小的分支注意方法。它提供了一种有效提高网络容量的新方法。</p> 
<h3><a id="Dynamic_Convolution_23"></a>(四)、Dynamic Convolution</h3> 
<p>轻量级CNN极低的计算成本限制了网络的深度和宽度，进一步降低了其代表性。为了解决上述问题，Chen等人提出了动态卷积，这是一种新的算子设计，可以增加表征能力，但额外的计算成本可以忽略不计，并且不会与CondConv并行改变网络的宽度或深度。 动态卷积使用K个大小和输入/输出维度相同的并行卷积核，而不是每层一个核。与SE块一样，它采用挤压和激励机制为不同的卷积核生成注意权重。这些卷积核通过加权求和动态聚合，并应用于输入特征映射X：<br> <img src="https://images2.imgbox.com/fd/f4/9T294xuL_o.png" alt="在这里插入图片描述"><br> 卷积通过卷积核的权重和偏差之和进行组合。<br> 与将卷积应用于特征映射相比，压缩、激励和加权求和的计算成本极低。因此，动态卷积提供了一种有效的操作来提高表示能力，并且可以轻松地用作任何卷积的替代品。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ab82f5f300897f8f703a961362f7697d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">从0到1的熟悉掌握——Kerberos协议</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/45ef0d7ac45fea43e6f593f952624ad2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">RunTime.getRunTime().addShutdownHook的用法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>