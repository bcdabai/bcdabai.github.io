<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>在C &#43;&#43;中加载TORCHSCRIPT模型 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="在C &#43;&#43;中加载TORCHSCRIPT模型" />
<meta property="og:description" content="顾名思义，PyTorch的主要接口是Python编程语言。尽管Python是许多需要动态性和易于迭代的场景的合适且首选的语言，但在同样许多情况下，Python的这些属性恰恰是不利的。后者通常适用的一种环境是生产-低延迟和严格部署要求的土地。对于生产场景，即使只将C &#43;&#43;绑定到Java，Rust或Go之类的另一种语言中，它通常也是首选语言。以下段落将概述PyTorch提供的从现有Python模型到可以加载和执行的序列化表示形式的路径 完全来自C &#43;&#43;，不依赖Python。
步骤1：将PyTorch模型转换为Torch脚本
Step 1: Converting Your PyTorch Model to Torch Script
PyTorch模型从Python到C &#43;&#43;的旅程由Torch Script启用，Torch Script是PyTorch模型的表示形式，可以由Torch Script编译器理解，编译和序列化。如果从使用香草“渴望” API编写的现有PyTorch模型开始，则必须首先将模型转换为Torch脚本。在最常见的情况下（如下所述），只需很少的努力。如果已经有了Torch脚本模块，则可以跳到本教程的下一部分。
有两种将PyTorch模型转换为Torch脚本的方法。第一种称为跟踪，一种机制，通过使用示例输入对模型的结构进行一次评估，并记录这些输入在模型中的流动，从而捕获模型的结构。这适用于有限使用控制流的模型。第二种方法是在模型中添加显式批注，以告知Torch Script编译器可以根据Torch Script语言施加的约束直接解析和编译模型代码。
TIP
大家可以在官方的Torch脚本参考中找到有关这两种方法的完整文档，以及使用方法的进一步指导。
Converting to Torch Script via Tracing
要将PyTorch模型通过跟踪转换为Torch脚本，必须将模型的实例以及示例输入传递给torch.jit.trace 函数。这将产生一个torch.jit.ScriptModule对象，该对象的模型评估跟踪将嵌入在模块的forward方法中：
import torchimport torchvision# An instance of your model.model = torchvision.models.resnet18()# An example input you would normally provide to your model&#39;s forward() method.example = torch.rand(1, 3, 224, 224)# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.traced_script_module = torch." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3d62f53fdd4e16b11a71f0b250966d61/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-25T16:24:44+08:00" />
<meta property="article:modified_time" content="2020-09-25T16:24:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">在C &#43;&#43;中加载TORCHSCRIPT模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>顾名思义，PyTorch的主要接口是Python编程语言。尽管Python是许多需要动态性和易于迭代的场景的合适且首选的语言，但在同样许多情况下，Python的这些属性恰恰是不利的。后者通常适用的一种环境是<em>生产</em>-低延迟和严格部署要求的土地。对于生产场景，即使只将C ++绑定到Java，Rust或Go之类的另一种语言中，它通常也是首选语言。以下段落将概述PyTorch提供的从现有Python模型到可以<em>加载</em>和<em>执行</em>的序列化表示形式的路径 完全来自C ++，不依赖Python。</p> 
<h4> </h4> 
<p><strong>步骤1：将PyTorch模型转换为Torch脚本</strong></p> 
<h4> </h4> 
<p><strong>Step 1: Converting Your PyTorch Model to Torch Script</strong></p> 
<p>PyTorch模型从Python到C ++的旅程由<a href="https://pytorch.org/docs/master/jit.html" rel="nofollow">Torch Script</a>启用，<a href="https://pytorch.org/docs/master/jit.html" rel="nofollow">Torch Script</a>是PyTorch模型的表示形式，可以由Torch Script编译器理解，编译和序列化。如果从使用香草“渴望” API编写的现有PyTorch模型开始，则必须首先将模型转换为Torch脚本。在最常见的情况下（如下所述），只需很少的努力。如果已经有了Torch脚本模块，则可以跳到本教程的下一部分。</p> 
<p>有两种将PyTorch模型转换为Torch脚本的方法。第一种称为<em>跟踪</em>，一种机制，通过使用示例输入对模型的结构进行一次评估，并记录这些输入在模型中的流动，从而捕获模型的结构。这适用于有限使用控制流的模型。第二种方法是在模型中添加显式批注，以告知Torch Script编译器可以根据Torch Script语言施加的约束直接解析和编译模型代码。</p> 
<p>TIP</p> 
<p>大家可以在官方的Torch脚本参考中找到有关这两种方法的完整文档，以及使用方法的进一步指导。</p> 
<h4> </h4> 
<p><strong>Converting to Torch Script via Tracing</strong></p> 
<p>要将PyTorch模型通过跟踪转换为Torch脚本，必须将模型的实例以及示例输入传递给<code>torch.jit.trace</code> 函数。这将产生一个<code>torch.jit.ScriptModule</code>对象，该对象的模型评估跟踪将嵌入在模块的<code>forward</code>方法中：</p> 
<pre>import torchimport torchvision# An instance of your model.model = torchvision.models.resnet18()# An example input you would normally provide to your model's forward() method.example = torch.rand(1, 3, 224, 224)# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.traced_script_module = torch.jit.trace(model, example)</pre> 
<p><code>ScriptModule</code>现在可以与常规PyTorch模块相同地评估被跟踪的对象：</p> 
<pre>In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))In[2]: output[0, :5]Out[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)</pre> 
<p> </p> 
<h4> </h4> 
<p><strong>Converting to Torch Script via Annotation</strong></p> 
<p>在某些情况下，例如，如果模型采用特定形式的控制流，则可能需要直接在Torch脚本中编写模型并相应地注释模型。例如，假设大家有以下香草Pytorch模型：</p> 
<pre>import torchclass MyModule(torch.nn.Module):    def __init__(self, N, M):        super(MyModule, self).__init__()        self.weight = torch.nn.Parameter(torch.rand(N, M))    def forward(self, input):        if input.sum() &gt; 0:          output = self.weight.mv(input)        else:          output = self.weight + input        return output</pre> 
<p>因为<code>forward</code>此模块的方法使用取决于输入的控制流，所以它不适合跟踪。相反，我们可以将其转换为<code>ScriptModule</code>。为了将模块转换为<code>ScriptModule</code>，需要使用以下代码进行编译<code>torch.jit.script</code>：</p> 
<pre>class MyModule(torch.nn.Module):    def __init__(self, N, M):        super(MyModule, self).__init__()        self.weight = torch.nn.Parameter(torch.rand(N, M))    def forward(self, input):        if input.sum() &gt; 0:          output = self.weight.mv(input)        else:          output = self.weight + input        return outputmy_module = MyModule(10,20)sm = torch.jit.script(my_module)</pre> 
<p>如果大家需要排除某些方法，<code>nn.Module</code> 因为它们使用的是TorchScript不支持的Python功能，则可以使用以下方法注释这些方法<code>@torch.jit.ignore</code></p> 
<p><code>my_module</code>是<code>ScriptModule</code>已经准备好进行序列化的实例 。</p> 
<h4> </h4> 
<p><strong>步骤2：将脚本模块序列化为文件</strong></p> 
<h4> </h4> 
<p><strong>Step 2: Serializing Your Script Module to a File</strong></p> 
<p> </p> 
<p>一旦有了对<code>ScriptModule</code>PyTorch模型的跟踪或注释，就可以将其序列化为文件了。稍后，大家将能够使用C ++从此文件加载模块并执行它，而无需依赖Python。假设我们要序列化<code>ResNet18</code>先前在跟踪示例中显示的模型。要执行此序列化，只需 在模块上调用 <a href="https://pytorch.org/docs/master/jit.html#torch.jit.ScriptModule.save" rel="nofollow">save</a>并传递一个文件名即可：</p> 
<pre>traced_script_module.save("traced_resnet_model.pt")</pre> 
<p>这将<code>traced_resnet_model.pt</code>在大家的工作目录中生成一个文件。如果大家还想序列化<code>my_module</code>，请致电<code>my_module.save("my_module_model.pt")</code> 我们现在已经正式离开Python领域，并准备跨入C ++领域。</p> 
<h4> </h4> 
<p><strong>步骤3：在C ++中加载脚本模块</strong></p> 
<h4> </h4> 
<p><strong>Step 3: Loading Your Script Module in C++</strong></p> 
<p>要在C ++中加载序列化的PyTorch模型，大家的应用程序必须依赖于PyTorch C ++ API（也称为<em>LibTorch）</em>。LibTorch发行版包含共享库，头文件和CMake构建配置文件的集合。虽然CMake不是依赖LibTorch的要求，但它是推荐的方法，并且在将来会得到很好的支持。对于本教程，我们将使用CMake和LibTorch构建一个最小的C ++应用程序，该应用程序简单地加载并执行序列化的PyTorch模型。</p> 
<h4> </h4> 
<p><strong>最小的C ++应用程序</strong></p> 
<p>让我们从讨论加载模块的代码开始。以下将已经做：</p> 
<pre>#include &lt;torch/script.h&gt; // One-stop header.#include &lt;iostream&gt;#include &lt;memory&gt;int main(int argc, const char* argv[]) {  if (argc != 2) {    std::cerr &lt;&lt; "usage: example-app &lt;path-to-exported-script-module&gt;\n";    return -1;  }  torch::jit::script::Module module;  try {    // Deserialize the ScriptModule from a file using torch::jit::load().    module = torch::jit::load(argv[1]);  }  catch (const c10::Error&amp; e) {    std::cerr &lt;&lt; "error loading the model\n";    return -1;  }  std::cout &lt;&lt; "ok\n";}</pre> 
<p>该<code>&lt;torch/script.h&gt;</code>首标包括由运行示例所必需的库LibTorch所有相关包括。我们的应用程序接受序列化的PyTorch的文件路径<code>ScriptModule</code>作为其唯一的命令行参数，然后使用该<code>torch::jit::load()</code> 函数继续反序列化该模块，该函数将该文件路径作为输入。作为回报，我们收到一个<code>torch::jit::script::Module</code> 对象。我们将稍后讨论如何执行它。</p> 
<h4> </h4> 
<p><strong>取决于LibTorch和构建应用程序</strong></p> 
<p>假设我们将上述代码存储到名为的文件中<code>example-app.cpp</code>。最小<code>CMakeLists.txt</code>的构建看起来可能很简单：</p> 
<pre>cmake_minimum_required(VERSION 3.0 FATAL_ERROR)project(custom_ops)find_package(Torch REQUIRED)add_executable(example-app example-app.cpp)target_link_libraries(example-app "${TORCH_LIBRARIES}")set_property(TARGET example-app PROPERTY CXX_STANDARD 14)</pre> 
<p>建立示例应用程序的最后一件事是LibTorch发行版。大家可以随时从PyTorch网站的<a href="https://pytorch.org/" rel="nofollow">下载页面</a>上获取最新的稳定版本。如果下载并解压缩最新的归档文件，则应该收到具有以下目录结构的文件夹：</p> 
<pre>libtorch/  bin/  include/  lib/  share/</pre> 
<ul><li>The <code>lib/</code> folder contains the shared libraries you must link against,</li><li>The <code>include/</code> folder contains header files your program will need to include,</li><li>The <code>share/</code> folder contains the necessary CMake configuration to enable the simple <code>find_package(Torch)</code> command above.</li></ul> 
<p>TIP</p> 
<p>在Windows上，调试和发行版本不兼容ABI。如果计划以调试模式构建项目，请尝试使用LibTorch的调试版本。另外，请确保在 下面的行中指定正确的配置。<code>cmake</code><code> </code><code>--build</code><code> </code><code>.</code></p> 
<p>最后一步是构建应用程序。为此，假定示例目录的布局如下：</p> 
<pre>example-app/  CMakeLists.txt  example-app.cpp</pre> 
<p>现在，我们可以运行以下命令从<code>example-app/</code>文件夹中构建应用程序 ：</p> 
<pre>mkdir buildcd buildcmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..cmake --build . --config Release</pre> 
<p>这里<code>/path/to/libtorch</code>应该是解压的LibTorch分布的完整路径。如果一切顺利，它将看起来像这样：</p> 
<pre>root@4b5a67132e81:/example-app# mkdir buildroot@4b5a67132e81:/example-app# cd buildroot@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..-- The C compiler identification is GNU 5.4.0-- The CXX compiler identification is GNU 5.4.0-- Check for working C compiler: /usr/bin/cc-- Check for working C compiler: /usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done-- Check for working CXX compiler: /usr/bin/c++-- Check for working CXX compiler: /usr/bin/c++ -- works-- Detecting CXX compiler ABI info-- Detecting CXX compiler ABI info - done-- Detecting CXX compile features-- Detecting CXX compile features - done-- Looking for pthread.h-- Looking for pthread.h - found-- Looking for pthread_create-- Looking for pthread_create - not found-- Looking for pthread_create in pthreads-- Looking for pthread_create in pthreads - not found-- Looking for pthread_create in pthread-- Looking for pthread_create in pthread - found-- Found Threads: TRUE-- Configuring done-- Generating done-- Build files have been written to: /example-app/buildroot@4b5a67132e81:/example-app/build# makeScanning dependencies of target example-app[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o[100%] Linking CXX executable example-app[100%] Built target example-app</pre> 
<p>如果将生成的跟踪<code>ResNet18</code>模型的路径提供给<code>traced_resnet_model.pt</code> 生成的<code>example-app</code>二进制文件，我们应该得到友好的“确定”。请注意，如果尝试与<code>my_module_model.pt</code>一起运行此示例，则会收到一条错误消息，提示你输入的形状不兼容。<code>my_module_model.pt</code>期望是1D而不是4D。</p> 
<pre>root@4b5a67132e81:/example-app/build# ./example-app &lt;path_to_model&gt;/traced_resnet_model.ptok</pre> 
<h4> </h4> 
<p><strong>步骤4：在C ++中执行脚本模块</strong></p> 
<h4> </h4> 
<p><strong>Step 4: Executing the Script Module in C++</strong></p> 
<p>成功加载了<code>ResNet18</code>用C ++编写的序列化代码后，现在离执行它仅几行代码了！让我们将这些行添加到C ++应用程序的<code>main()</code>函数中：</p> 
<pre>// Create a vector of inputs.std::vector&lt;torch::jit::IValue&gt; inputs;inputs.push_back(torch::ones({1, 3, 224, 224}));// Execute the model and turn its output into a tensor.at::Tensor output = module.forward(inputs).toTensor();std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) &lt;&lt; '\n';</pre> 
<p>前两行设置了模型的输入。我们创建一个向量 <code>torch::jit::IValue</code>（类型擦除的值类型<code>script::Module</code>方法接受并返回），并添加单个输入。要创建输入张量，我们使用 <code>torch::ones()</code>，等效<code>torch.ones</code>于C ++ API。然后，我们运行<code>script::Module</code>的<code>forward</code>方法，并将创建的输入向量传递给它。作为回报，我们得到一个新的<code>IValue</code>，通过调用将其转换为张量<code>toTensor()</code>。</p> 
<p>TIP</p> 
<p>要大致了解诸如<code>torch::ones</code>PyTorch C ++ API之类的功能，请参阅<a href="https://pytorch.org/cppdocs" rel="nofollow">https://pytorch.org/cppdocs</a>上的文档。PyTorch C ++ API提供了与Python API几乎相同的功能，使大家可以像在Python中一样进一步操纵和处理张量。</p> 
<p>在最后一行，我们打印输出的前五个条目。由于在本教程前面的部分中，我们为Python中的模型提供了相同的输入，因此理想情况下，我们应该看到相同的输出。让我们通过重新编译应用程序并使用相同的序列化模型运行它来进行尝试：</p> 
<pre>root@4b5a67132e81:/example-app/build# makeScanning dependencies of target example-app[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o[100%] Linking CXX executable example-app[100%] Built target example-approot@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt-0.2698 -0.0381  0.4023 -0.3010 -0.0448[ Variable[CPUFloatType]{1,5} ]</pre> 
<p>作为参考，Python以前的输出为：</p> 
<pre>tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)</pre> 
<p>Looks like a good match!</p> 
<p>TIP</p> 
<p>要将模型移至GPU内存，可以编写<code>model.to(at::kCUDA);</code>。通过调用来确保模型的输入也位于CUDA内存中<code>tensor.to(at::kCUDA)</code>，这将在CUDA内存中返回新的张量。</p> 
<h4> </h4> 
<p><strong>第5步：获取帮助并探索API</strong></p> 
<h4> </h4> 
<p><strong>Step 5: Getting Help and Exploring the API</strong></p> 
<p>希望本教程对PyTorch模型从Python到C ++的路径有一个大致的了解。使用本教程中描述的概念，应该能够从原始的“急切”的PyTorch模型，<code>ScriptModule</code>用Python编译，在磁盘上序列化的文件，再到关闭循环，再到<code>script::Module</code>C ++的可执行文件。</p> 
<p>当然，有许多我们没有介绍的概念。例如，大家可能会发现自己想要扩展<code>ScriptModule</code>一个使用C ++或CUDA实现的自定义运算符，并在已<code>ScriptModule</code>加载到纯C ++生产环境中的内部执行此自定义运算符 。好消息是：这是可能的，并且得到了很好的支持！现在，大家可以浏览<a href="https://github.com/pytorch/pytorch/tree/master/test/custom_operator">此</a>文件夹中的示例，我们将很快提供一个教程。目前，以下链接通常可能会有所帮助：</p> 
<ul><li>火炬脚本参考：<a href="https://pytorch.org/docs/master/jit.html" rel="nofollow">https</a> : <a href="https://pytorch.org/docs/master/jit.html" rel="nofollow">//pytorch.org/docs/master/jit.html</a></li><li>PyTorch C ++ API文档：<a href="https://pytorch.org/cppdocs/" rel="nofollow">https</a> ://pytorch.org/cppdocs/</li><li>PyTorch Python API文档：<a href="https://pytorch.org/docs/" rel="nofollow">https</a> ://pytorch.org/docs/</li></ul> 
<p>​</p> 
<p> </p> 
<p>接下来，给大家介绍一下租用GPU做实验的方法，我们是在智星云租用的GPU，使用体验很好。具体大家可以参考：智星云官网： <a href="http://www.ai-galaxy.cn/" rel="nofollow">http://www.ai-galaxy.cn/</a>，淘宝店：<a href="https://shop36573300.taobao.com/" rel="nofollow">https://shop36573300.taobao.com/</a>公众号: 智星AI</p> 
<p>                <img alt="" src="https://images2.imgbox.com/33/26/g5sGLXGW_o.png" width="367"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/eef4d50177097743ca59f16daa076648/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">gitee&#43;picgo 上传图片失败 显示404 project not found 的解决。</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2cd420e2b4f62d69e3504a9242df869a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">LWN：GCC也支持BPF了！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>