<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>scrapy-redis 安装 及使用 结合例子解释 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="scrapy-redis 安装 及使用 结合例子解释" />
<meta property="og:description" content="scrapy-redis安装及配置
scrapy-redis 的安装 pip install scrapy-redis
easy_install scrapy-redis
下载
http://redis.io/download
版本推荐
stable 3.0.2
运行redis
redis-server redis.conf
清空缓存
redis-cli flushdb
scrapy配置redis settings.py配置redis（在scrapy-redis 自带的例子中已经配置好）
SCHEDULER = &#34;scrapy_redis.scheduler.Scheduler&#34;
SCHEDULER_PERSIST = True
SCHEDULER_QUEUE_CLASS = &#39;scrapy_redis.queue.SpiderPriorityQueue&#39;
REDIS_URL = None # 一般情况可以省去
REDIS_HOST = &#39;127.0.0.1&#39; # 也可以根据情况改成 localhost
REDIS_PORT = 6379
在scrapy中使用scrapy-redis spider 继承RedisSpider
class tempSpider(RedisSpider) name = &#34;temp&#34;
redis_key = &#39;&#39;temp:start_url&#34;
启动redis 在redis的src目录下，执行 ./redis-server启动服务器
执行 ./redis-cli 启动客户端
设置好setting.py的redis 的ip和端口 启动scropy-redis的代码；
如启动name= &#34;lhy&#34;，start_urls=&#34;lhy:start_urls&#34; 的spider。
如果在redis中没有 主键为lhy:start_urls 的list，则爬虫已只监听等待。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/858d7130572afff399a5a60ad0d6d8f4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-01-04T14:12:00+08:00" />
<meta property="article:modified_time" content="2017-01-04T14:12:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">scrapy-redis 安装 及使用 结合例子解释</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="content-detail markdown-body"> 
 <h2>scrapy-redis安装及配置<br></h2> 
 <h3>scrapy-redis 的安装</h3> 
 <p>pip install scrapy-redis</p> 
 <p>easy_install scrapy-redis</p> 
 <p>下载</p> 
 <p>http://redis.io/download</p> 
 <p>版本推荐</p> 
 <p>stable 3.0.2</p> 
 <p>运行redis</p> 
 <p>redis-server redis.conf</p> 
 <p>清空缓存</p> 
 <p>redis-cli flushdb</p> 
 <h3>scrapy配置redis</h3> 
 <p>settings.py配置redis（在scrapy-redis 自带的例子中已经配置好）</p> 
 <p>SCHEDULER = "scrapy_redis.scheduler.Scheduler"</p> 
 <p>SCHEDULER_PERSIST = True<br></p> 
 <p>SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.SpiderPriorityQueue'<br></p> 
 <p>REDIS_URL = None # 一般情况可以省去</p> 
 <p>REDIS_HOST = '127.0.0.1' # 也可以根据情况改成 localhost</p> 
 <p>REDIS_PORT = 6379</p> 
 <h3>在scrapy中使用scrapy-redis</h3> 
 <p>spider 继承RedisSpider</p> 
 <p>class tempSpider(RedisSpider)  </p> 
 <p>name = "temp"</p> 
 <p>redis_key  = ''temp:start_url"</p> 
 <h3>启动redis</h3> 
 <p>在redis的src目录下，执行 ./redis-server启动服务器</p> 
 <p>执行 ./redis-cli 启动客户端</p> 
 <p><br></p> 
 <h3>设置好setting.py的redis 的ip和端口</h3> 
 <p><strong>启动scropy-redis的代码；</strong></p> 
 <p><span style="color:#FF0000;"><strong>如启动name= "lhy"，start_urls="lhy:start_urls" 的spider。</strong></span></p> 
 <p><strong>如果在redis中没有 主键为lhy:start_urls 的list，则爬虫已只监听等待。</strong></p> 
 <p><strong>此时，在redis客户端执行：lpush lhy:start_urls   http://blog.csdn.net/u013378306/article/details/53888173 <br></strong></p> 
 <p><strong>可以看到爬虫开始抓取。在 redis客户端下输入 keys *，查看所有主键<br></strong></p> 
 <p><span style="color:#FF0000;"><strong>原来的 lhy:start_urls 已经被自动删除，并新建了 一个lhy:dupefilter (set),一个 lhy:items (list), 一个 lhy:requests（zset） </strong></span></p> 
 <p><strong>lhy:dupefilter用来存储 已经requests 过的url的hash值，分布式去重时使用到,  lhy:items是分布式生成的items，lhy:requests是新生成的 url封装后的requests。</strong></p> 
 <p><strong>理论上，lhy:dupefilter 等于已经request的数量，一直增加</strong></p> 
 <p><strong>lhy:items 是经过 spider prase生成的</strong></p> 
 <p><strong>lhy:requests 是有序集合ZSET，scrapy-redis 重新吧他封装成了一个队列，requests是spider 解析生成新url后重新封装，如果不载有新的url产生，则随着spider的prase，一直减少。总之取request时出队列，新的url会重新封装成request后增加进来，入队列。</strong><br></p> 
 <h2>scrapy-redis 原理及解释</h2> 
 <p>scrapy-redis 重写了scrapy的多个类，具体请看http://blog.csdn.net/u013378306/article/details/53992707</p> 
 <p>并且在setting.py中 配置了这些类，所以当运行scrapy-redis例子时,自动使用了scrapy-redis 重写的类。</p> 
 <p><br></p> 
 <p>下载scrapy-redis源代码 https://github.com/rolando/scrapy-Redis</p> 
 <p>文档结构如下：</p> 
 <p><img src="https://images2.imgbox.com/93/8b/XBincPm5_o.png" alt=""></p> 
 <p>其中，src 中是scrapy-redis的源代码。</p> 
 <p>example-redis 是写好的例子，其中有三个例子</p> 
 <p><img src="https://images2.imgbox.com/b1/2a/wb5PGGqR_o.png" alt=""></p> 
 <p><span style="color:#FF0000;"><strong>（1）domz.py   （2） mycrawler_redis.py   （3） myspider.py</strong></span></p> 
 <p><span style="color:#FF0000;"><strong>文档中队者三个例子的解释如下</strong></span></p> 
 <p></p> 
 <pre><code class="language-python">* **dmoz**

  This spider simply scrapes dmoz.org.

* **myspider_redis**

  This spider uses redis as a shared requests queue and uses
  ``myspider:start_urls`` as start URLs seed. For each URL, the spider outputs
  one item.

* **mycrawler_redis**

  This spider uses redis as a shared requests queue and uses
  ``mycrawler:start_urls`` as start URLs seed. For each URL, the spider follows
  are links.</code></pre> 
 <br> 
 <br> 
 <p><span style="color:#FF0000;">domz.py :此例子仅仅是抓取一个网站下的数据，没有用分布式</span><br></p> 
 <p></p> 
 <pre><code class="language-python">from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class DmozSpider(CrawlSpider):
    """Follow categories and extract links."""
    name = 'dmoz'
    allowed_domains = ['dmoz.org']
    start_urls = ['http://www.dmoz.org/']

    rules = [
        Rule(LinkExtractor(
            restrict_css=('.top-cat', '.sub-cat', '.cat-item')
        ), callback='parse_directory', follow=True),
    ]

    def parse_directory(self, response):
        for div in response.css('.title-and-desc'):
            yield {
                'name': div.css('.site-title::text').extract_first(),
                'description': div.css('.site-descr::text').extract_first().strip(),
                'link': div.css('a::attr(href)').extract_first(),
            }</code></pre> 
 <p><span style="color:#FF0000;">mycrawler_redis.py <br> 此例子 使用 RedisCrawlSpider类，支持分布式去重爬取，并且 可以定义抓取连接的rules </span><br></p> 
 <p></p> 
 <pre><code class="language-python">from scrapy.spiders import Rule
from scrapy.linkextractors import LinkExtractor

from scrapy_redis.spiders import RedisCrawlSpider


class MyCrawler(RedisCrawlSpider):
    """Spider that reads urls from redis queue (myspider:start_urls)."""
    name = 'mycrawler_redis'
    redis_key = 'mycrawler:start_urls'

    rules = (
        # follow all links
        Rule(LinkExtractor(), callback='parse_page', follow=True),
    )

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(MyCrawler, self).__init__(*args, **kwargs)

    def parse_page(self, response):
        return {
            'name': response.css('title::text').extract_first(),
            'url': response.url,
        }</code></pre> 
 <br> 
 <span style="color:#FF0000;">myspider.py 此例子支持分布式去重爬取，但不支持定义规则抓取url</span> 
 <p></p> 
 <pre><code class="language-python">from scrapy_redis.spiders import RedisSpider


class MySpider(RedisSpider):
    """Spider that reads urls from redis queue (myspider:start_urls)."""
    name = 'myspider_redis'
    redis_key = 'myspider:start_urls'

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(MySpider, self).__init__(*args, **kwargs)

    def parse(self, response):
        return {
            'name': response.css('title::text').extract_first(),
            'url': response.url,
        }</code></pre> 
 <br> 
 <br> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/33e7f8b137dc4d6a4f9c137a8e35dad9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">让VS支持jQuery代码智能提示</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/244b36a36b000b8d0ac3d208be8e7200/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">jquery中ajax下设置‘正在加载’的方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>