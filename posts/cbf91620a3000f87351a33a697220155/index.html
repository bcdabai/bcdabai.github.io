<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>挑战更高难度的多目标跟踪，MOT20数据集使用指南 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="挑战更高难度的多目标跟踪，MOT20数据集使用指南" />
<meta property="og:description" content="上一期为大家介绍了多目标跟踪任务及其常用的数据集：多目标跟踪（MOT）数据集资源整理分享。其中最新发布的行人数据集MOT20，环境更复杂、人群更密集，任务难度更大。
这一期，给想挑战的朋友，详细介绍一下。
目录指引
1. 数据集简介
2. 数据集详细信息
3. 数据集任务定义及介绍
4. 数据集结构解读
5. 下载链接及可视化脚本
一、数据集简介 发布方：Dynamic Vision and Learning Group at TUM Munich, Germany
发布时间：2020
发布版本：MOT20
背景：相比于此前的多目标跟踪（multi-object tracking）数据集，MOT20关注人群密集的场景，其视频最多可达单帧 246 人。
二、数据集详细信息 1. 数据量及标注情况 数据集共包含 8 个视频片段，分别来自三个不同的场景，4 个视频片段用于训练，4 个视频片段用于测试。每个视频片段均以视频帧的形式提供，8 个视频片段总共包含 13410 帧，其中训练视频 8931 帧，测试视频 4479 帧。
​
​
数据集提供每个视频帧上的矩形框标注，其中测试数据的标注不公开。训练数据平均每帧包含 127.04 个行人（pedestrian）矩形框标注，测试数据平均每帧包含 115.52 个行人矩形框标注。
​
除了行人，数据集还包含其他类型的矩形框标注，如非移动交通工具（non motorized vehicle）等。在评测时仅考虑行人类型的标注，不考虑其他类型。
除了标注信息外，数据集作者在训练数据上训练了一个以 ResNet101 为 backbone 的 Faster R-CNN 作为 baseline，并提供训练和测试数据上逐帧检测行人的结果（注：在竞赛中，如果参赛者要使用基于检测的跟踪方法，只能使用官方提供的检测结果）。
2. 标注类别 数据集的标注包含以下五种标注类别：
● Pedestrian" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/cbf91620a3000f87351a33a697220155/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-11T11:15:52+08:00" />
<meta property="article:modified_time" content="2022-07-11T11:15:52+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">挑战更高难度的多目标跟踪，MOT20数据集使用指南</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>上一期为大家介绍了多目标跟踪任务及其常用的数据集：<a class="link-info" href="https://blog.csdn.net/OpenDataLab/article/details/125329004?spm=1001.2014.3001.5501" title="多目标跟踪（MOT）数据集资源整理分享">多目标跟踪（MOT）数据集资源整理分享</a>。其中最新发布的行人数据集MOT20，环境更复杂、人群更密集，任务难度更大。</p> 
<p></p> 
<p>这一期，给想挑战的朋友，详细介绍一下。</p> 
<p></p> 
<p><strong>目录指引</strong></p> 
<blockquote> 
 <p>1. 数据集简介</p> 
 <p>2. 数据集详细信息</p> 
 <p>3. 数据集任务定义及介绍</p> 
 <p>4. 数据集结构解读</p> 
 <p>5. 下载链接及可视化脚本</p> 
</blockquote> 
<p></p> 
<h2><strong>一、数据集简介</strong></h2> 
<p><strong>发布方：</strong>Dynamic Vision and Learning Group at TUM Munich, Germany</p> 
<p><strong>发布时间：</strong>2020</p> 
<p><strong>发布版本：</strong>MOT20</p> 
<p><strong>背景：</strong>相比于此前的多目标跟踪（multi-object tracking）数据集，MOT20关注人群密集的场景，其视频最多可达单帧 246 人。</p> 
<p></p> 
<p></p> 
<h2><strong>二、数据集详细信息</strong></h2> 
<h3><strong>1. 数据量及标注情况</strong></h3> 
<p>数据集共包含 8 个视频片段，分别来自三个不同的场景，4 个视频片段用于训练，4 个视频片段用于测试。每个视频片段均以视频帧的形式提供，8 个视频片段总共包含 13410 帧，其中训练视频 8931 帧，测试视频 4479 帧。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/5f/fe/QqHLmwJt_o.png">​</p> 
</div> 
<p></p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/0c/91/eRpP833W_o.png">​</p> 
</div> 
<p></p> 
<p>数据集提供每个视频帧上的矩形框标注，其中测试数据的标注不公开。训练数据平均每帧包含 127.04 个行人（pedestrian）矩形框标注，测试数据平均每帧包含 115.52 个行人矩形框标注。</p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/55/2d/8pOgwnMX_o.png">​</p> 
</div> 
<p></p> 
<p>除了行人，数据集还包含其他类型的矩形框标注，如非移动交通工具（non motorized vehicle）等。在评测时仅考虑行人类型的标注，不考虑其他类型。</p> 
<p></p> 
<p>除了标注信息外，数据集作者在训练数据上训练了一个以 ResNet101 为 backbone 的 Faster R-CNN 作为 baseline，并提供训练和测试数据上逐帧检测行人的结果（注：在竞赛中，如果参赛者要使用基于检测的跟踪方法，只能使用官方提供的检测结果）。</p> 
<p></p> 
<h3><strong>2. 标注类别</strong></h3> 
<p>数据集的标注包含以下五种标注类别：</p> 
<p>● Pedestrian</p> 
<p>● Non motorized vehicle</p> 
<p>● Static person</p> 
<p>● Occluder on the ground</p> 
<p>● crowd</p> 
<h3><br><strong>3. 可视化</strong></h3> 
<p>下面是 MOT20-03 的原视频、标注结果、检测结果，平均每帧 130.42 个行人。</p> 
<div> 
 <div class="csdn-video-box"> 
  <iframe id="bSkg0INK-1655878798847" frameborder="0" src="https://live.csdn.net/v/embed/218265" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
  <p>MOT20-03-raw（原视频）</p> 
 </div> 
</div> 
<p></p> 
<div> 
 <div class="csdn-video-box"> 
  <iframe id="NVC0Yhwx-1655880926832" frameborder="0" src="https://live.csdn.net/v/embed/218266" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
  <p>MOT20-03（标注结果）</p> 
 </div> 
</div> 
<p></p> 
<div> 
 <div class="csdn-video-box"> 
  <iframe id="1WOXU9N5-1655880940167" frameborder="0" src="https://live.csdn.net/v/embed/218268" allowfullscreen="true" data-mediaembed="csdn"></iframe> 
  <p>MOT20-03-det（检测结果）</p> 
 </div> 
</div> 
<p></p> 
<p></p> 
<h2><strong>三、数据集任务定义及介绍</strong></h2> 
<h3><strong>多目标跟踪</strong></h3> 
<p><strong>● 任务定义</strong></p> 
<p>在给定的一段视频中识别与跟踪多个目标。（具体介绍可见：<a class="link-info" href="https://blog.csdn.net/OpenDataLab/article/details/125329004?spm=1001.2014.3001.5501" title="多目标跟踪（MOT）数据集资源整理分享">多目标跟踪（MOT）数据集资源整理分享</a>）</p> 
<p></p> 
<p><strong>● 评估</strong></p> 
<p><strong>评估过程</strong></p> 
<p>在第 <img alt="t" class="mathcode" src="https://images2.imgbox.com/f3/7c/PmcotIT6_o.png"> 帧，ground truth 为 <img alt="o_{1}, o_{2}, \ldots, o_{n}" class="mathcode" src="https://images2.imgbox.com/1d/74/UqTADeMt_o.png"> ，预测结果 hypothesis 为 <img alt="h_{1}, h_{2}, \ldots, h_{m}" class="mathcode" src="https://images2.imgbox.com/9e/27/kJrIqsqK_o.png">，其中每个 <img alt="o_{i} (1 \le i \le n)" class="mathcode" src="https://images2.imgbox.com/5f/39/NDd0hgg2_o.png"> 与每个 <img alt="h_{j} (1 \le j \le m)" class="mathcode" src="https://images2.imgbox.com/fa/09/OQRW4uAq_o.png"> 都属于一个特定的轨迹，即可能已经出现在此前的若干帧中。</p> 
<p></p> 
<p>对于 <img alt="h_{j} (1 \le j \le m)" class="mathcode" src="https://images2.imgbox.com/82/fb/GMXcZAGp_o.png"> ，若交并比 <img alt="\textrm{IOU}(o_i, h_j) \ge t_{d}" class="mathcode" src="https://images2.imgbox.com/7d/db/phj5OXQy_o.png"> ，则二者可建立关联（relationship），其中 <img alt="t_{d}" class="mathcode" src="https://images2.imgbox.com/fa/ee/vJVlrGvp_o.png"> 为预设的阈值，在 MOT20 中为 <img alt="t_{d} = 0.5" class="mathcode" src="https://images2.imgbox.com/8d/49/bAStgPK1_o.png"> 。</p> 
<p></p> 
<p>若 <img alt="o_{i}, h_{j}" class="mathcode" src="https://images2.imgbox.com/fb/6c/J6D2EEeQ_o.png"> 在第 <img alt="t-1" class="mathcode" src="https://images2.imgbox.com/82/76/Kw6IQJHy_o.png"> 帧匹配（matched），且在第 <img alt="t" class="mathcode" src="https://images2.imgbox.com/02/e1/aqbftPqz_o.png"> 帧可建立关联，则在第 <img alt="t" class="mathcode" src="https://images2.imgbox.com/67/50/6xu3eNOs_o.png"> 帧直接进行匹配。</p> 
<p></p> 
<p>对于剩余未匹配的 ground truth 和 hypothesis，基于建立的关联关系，计算最大二分匹配。</p> 
<p></p> 
<p>仍未匹配的 ground truth 为 False Negative，仍未匹配的 hypothesis 为 False Positive。</p> 
<p></p> 
<p>若 <img alt="o_{i}" class="mathcode" src="https://images2.imgbox.com/a5/c9/KMDsfHXt_o.png"> 与 <img alt="h_{j}" class="mathcode" src="https://images2.imgbox.com/f5/4c/8Y2d5efz_o.png"> 匹配，且此前最近的一次匹配对象为 <img alt="h_{k} \neq h_{j}" class="mathcode" src="https://images2.imgbox.com/a5/86/eTiFRpPY_o.png"> ，则 <img alt="o_{i}" class="mathcode" src="https://images2.imgbox.com/59/2c/bDQMaGrP_o.png"> 在第 <img alt="t" class="mathcode" src="https://images2.imgbox.com/bb/b7/QjbB8OyU_o.png"> 帧发生一次 identity switch (IDSW)。</p> 
<p></p> 
<p><strong>指标</strong></p> 
<p>MOT20 竞赛所用的评价指标：</p> 
<p><a class="link-info" href="https://motchallenge.net/results/MOT20/" rel="nofollow" title="https://motchallenge.net/results/MOT20/">https://motchallenge.net/results/MOT20/</a></p> 
<div> 
 <p><img alt="" src="https://images2.imgbox.com/be/24/DpxcD4bh_o.png">​</p> 
</div> 
<p></p> 
<h2><strong>四、数据集文件结构解读</strong></h2> 
<p><strong>1. 目录结构</strong></p> 
<pre><code>dataset_root</code><code>├── test                      # 测试数据</code><code>│   ├── MOT20-04              # MOT20-04: 视频文件夹，存放一个视频的所有信息</code><code>│   │   ├── det               # Faster R-CNN 检测结果</code><code>│   │   │   └── det.txt       # Faster R-CNN 检测结果，每行一个行人矩形框检测结果</code><code>│   │   ├── img1              # 媒体文件</code><code>│   │   │   ├── 000001.jpg    # 视频逐帧图片，图片名字为 6 位数字</code><code>│   │   │   ├── 000002.jpg</code><code>│   │   │   └── ...</code><code>│   │   └── seqinfo.ini       # 视频信息，包含时长、帧率、宽度、高度等</code><code>│   ├── MOT20-06</code><code>│   │   └── ...</code><code>│   ├── MOT20-07</code><code>│   │   └── ...</code><code>│   └── MOT20-08</code><code>│       └── ...</code><code>└── train                     # 训练数据</code><code>    ├── MOT20-01              # MOT20-01: 视频文件夹，存放一个视频的所有信息</code><code>    │   ├── det               # Faster R-CNN 检测结果</code><code>    │   │   └── det.txt       # Faster R-CNN 检测结果，每行一个行人矩形框检测结果</code><code>    │   ├── gt                # 标注信息</code><code>    │   │   └── gt.txt        # 标注信息，每行一个矩形框标注</code><code>    │   ├── img1              # 媒体文件</code><code>    │   │   ├── 000001.jpg    # 视频逐帧图片，图片名字为 6 位数字</code><code>    │   │   ├── 000002.jpg</code><code>    │   │   └── ...</code><code>    │   └── seqinfo.ini       # 视频信息，包含时长、帧率、宽度、高度等</code><code>    ├── MOT20-02</code><code>    │   └── ...</code><code>    ├── MOT20-03</code><code>    │   └── ...</code><code>    └── MOT20-05</code><code>        └── ...</code></pre> 
<p></p> 
<p><strong>2. 元信息格式</strong></p> 
<p><strong>● seqinfo.ini 文件内容如下：</strong></p> 
<pre><code>[Sequence]</code><code>name=MOT20-01</code><code>imDir=img1</code><code>frameRate=25</code><code>seqLength=429</code><code>imWidth=1920</code><code>imHeight=1080</code><code>imExt=.jpg</code>
</pre> 
<p><strong>3. 标注格式</strong></p> 
<p><strong>● gt.txt 标注格式如下：</strong></p> 
<pre><code>1,1,199,813,140,268,1,1,0.83643</code><code>2,1,201,812,140,268,1,1,0.84015</code><code>3,1,203,812,140,268,1,1,0.84015</code><code>4,1,206,812,140,268,1,1,0.84015</code><code>5,1,208,812,140,268,1,1,0.84015</code></pre> 
<p>每行包含九个字段，用逗号分隔，含义如下：</p> 
<p><strong>frame number</strong>：帧号</p> 
<p><strong>identity number</strong>：物体编号，同一物体在整个视频片段中具有唯一的编号</p> 
<p><strong>x_min</strong>：2D 矩形框左上角横坐标</p> 
<p><strong>y_min</strong>：2D 矩形框左上角纵坐标</p> 
<p><strong>width</strong>：2D 矩形框宽度</p> 
<p><strong>height</strong>：2D 矩形框高度</p> 
<p><strong>flag</strong>：当前标注在评估中是否被考虑，若 flag = 1，则考虑当前标注，若 flag = 0，则忽略。</p> 
<p><strong>category</strong>：标注物体类别 ID，数据集中出现的 ID 及对应类别包括：</p> 
<p>    1: Pedestrian</p> 
<p>    6: Non motorized vehicle</p> 
<p>    7: Static person</p> 
<p>    11: Occluder on the ground</p> 
<p>    13: crowd</p> 
<p><strong>visibility</strong>：物体可见程度，取值在 0~1 之间，有些物体会被遮挡，取值越低，遮挡越严重。</p> 
<p></p> 
<p><strong>● det.txt 标注格式如下：</strong></p> 
<pre><code>1,-1,757,692,96,209,1,-1,-1,-1</code><code>1,-1,667,682,100,222,1,-1,-1,-1</code><code>1,-1,343,818,127,258,1,-1,-1,-1</code><code>1,-1,806,524,71,172,1,-1,-1,-1</code><code>1,-1,196,814,141,265,1,-1,-1,-1</code></pre> 
<p>每行包含十个字段，用逗号分隔，前六个主要字段含义如下：</p> 
<p><strong>frame number：</strong>帧号</p> 
<p><strong>此字段</strong>恒为 -1</p> 
<p><strong>x_min</strong>：2D 矩形框左上角横坐标</p> 
<p><strong>y_min</strong>：2D 矩形框左上角纵坐标</p> 
<p><strong>width</strong>：2D 矩形框宽度</p> 
<p><strong>height</strong>：2D 矩形框高度</p> 
<p></p> 
<h2><strong>五、下载链接及可视化脚本</strong></h2> 
<p>OpenDataHub平台已经上架了MOT20数据集，为大家提供了完整的数据集信息、流畅的下载速度、可视化脚本、快来体验吧！</p> 
<p><strong>1. MOT20数据集资源</strong></p> 
<p><a class="link-info" href="https://opendatalab.com/70" rel="nofollow" title="https://opendatalab.com/70">https://opendatalab.com/70</a></p> 
<p><strong>2. MOT20数据集可视化</strong>​​​​​​​</p> 
<pre><code>import argparse
import json
import os

import cv2
from itertools import count
from tqdm import tqdm


parse = argparse.ArgumentParser(description="Visualize MOT20 dataset of standard format")
parse.add_argument("--input_path", type=str, help="path of standard format dataset")
parse.add_argument("--output_path", type=str, help="path of output")
parse.add_argument("--fps", type=int, default=25, help="frames per second")
args = parse.parse_args()


def visualize_sub_dataset(sub_dataset_name, media, labels, ann_source):
    for i, sequence in tqdm(enumerate(media), total=len(media), desc=f"{sub_dataset_name}-{ann_source}"):
        visualize_sequence(sub_dataset_name, i, sequence, labels, ann_source)


def visualize_sequence(sub_dataset_name, seq_id, sequence, labels, ann_source):
    frames = []
    colors = ((243, 129, 129), (252, 227, 138), (234, 255, 208), (149, 225, 211))
    color_id_gen = count()
    instance2color = {}
    for media_path in sequence:
        media_label = labels[media_path]
        frame = cv2.imread(os.path.join(args.input_path, media_path))
        if "ground_truth" in media_label and "box2d" in media_label["ground_truth"]:
            for bbox in media_label["ground_truth"]["box2d"]:
                if bbox["attributes"]["source"] == ann_source:
                    x, y, w, h = list(map(int, bbox["bounding_box"]))
                    instance_id = bbox.get("instance_id", "0")
                    if instance_id not in instance2color:
                        instance2color[instance_id] = colors[next(color_id_gen) % len(colors)]
                    cv2.rectangle(frame, (x, y), (x + w, y + h), instance2color[instance_id])
        frames.append(frame)
    video_path = os.path.join(args.output_path, f"{sub_dataset_name}-{seq_id}-{ann_source}.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video = cv2.VideoWriter(video_path, fourcc, args.fps, (frames[0].shape[1], frames[0].shape[0]))
    for frame in frames:
        video.write(frame)


def main():
    if not os.path.isdir(args.output_path):
        os.makedirs(args.output_path)
    elif os.listdir(args.output_path):
        raise OSError(f"Directory is not empty: '{args.output_path}'")

    annotated_sub_datasets = ["train"]
    with open(os.path.join(args.input_path, "dataset_info.json"), "r", encoding="utf8") as f:
        dataset_info = json.load(f)
    for sub_dataset_name, sub_dataset in dataset_info["data"].items():
        media = sub_dataset["media"]
        with open(os.path.join(args.input_path, sub_dataset["label"]), "r", encoding="utf8") as f:
            labels = json.load(f)
        labels = {ann["media"]: ann for ann in labels[sub_dataset_name]}

        if sub_dataset_name in annotated_sub_datasets:
            visualize_sub_dataset(sub_dataset_name, media, labels, "gt")
        visualize_sub_dataset(sub_dataset_name, media, labels, "det")


if __name__ == "__main__":
    main()</code></pre> 
<p></p> 
<p><strong>参考资料：</strong></p> 
<p>[1]Dendorfer, P., Rezatofighi, H., Milan, A., Shi, J., Cremers, D., Reid, I., Roth, S., Schindler, K. &amp; Leal-Taixé, L. MOT20: A benchmark for multi object tracking in crowded scenes. arXiv:2003.09003[cs], 2020., (arXiv: 2003.09003).</p> 
<p>[2]https://motchallenge.net/data/MOT20/</p> 
<p></p> 
<p>更多数据集上架动态、更全面的数据集内容解读、最牛大佬在线答疑、最活跃的同行圈子……欢迎添加微信<strong>opendatalab_yunying </strong>加入OpenDataLab官方交流群。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2e1cbb2ab61cb088cc52aef9d5861b25/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">element el-dialog组件嵌套时遮罩层显示异常</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a0dcc7defe867ec25d2097f2ace9dbd9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">收藏丨8个常用中文OCR数据集，附下载链接</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>