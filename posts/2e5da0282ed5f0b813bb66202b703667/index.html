<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Kubernetes单主集群的部署（一） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Kubernetes单主集群的部署（一）" />
<meta property="og:description" content="目录
一、k8s单主架构集群的部署 1.操作系统初始化配置
2.部署 etcd 集群 3.部署docker引擎
4.部署 Master 组件
5.部署 Worker Node 组件
6.部署 CNI 网络组件（使用 flannel）
一、k8s单主架构集群的部署 k8s集群master01：192.168.116.10
k8s集群master02：192.168.116.20（第二部分高可用架构会加入）
k8s集群node01：192.168.116.30
k8s集群node02：192.168.116.40
etcd集群节点1：192.168.116.10（etcd集群原则上单独部署，此实验为方便部署在节点上）
etcd集群节点2：192.168.116.30
etcd集群节点3：192.168.116.40
负载均衡nginx&#43;keepalive01（master）：192.168.116.50
负载均衡nginx&#43;keepalive02（backup）：192.168.116.60
1.操作系统初始化配置 #关闭防火墙 systemctl disable --now firewalld iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X #关闭selinux setenforce 0 sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config #关闭swap swapoff -a sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab #根据规划设置主机名 hostnamectl set-hostname master01 hostnamectl set-hostname node01 hostnamectl set-hostname node02 su #添加主机名映射 vim /etc/hosts #添加 192." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2e5da0282ed5f0b813bb66202b703667/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-05T08:37:07+08:00" />
<meta property="article:modified_time" content="2023-09-05T08:37:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Kubernetes单主集群的部署（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81k8s%E5%8D%95%E4%B8%BB%E6%9E%B6%E6%9E%84%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81k8s%E5%8D%95%E4%B8%BB%E6%9E%B6%E6%9E%84%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%C2%A0" rel="nofollow"> </a></p> 
<p id="%E4%B8%80%E3%80%81k8s%E5%8D%95%E4%B8%BB%E6%9E%B6%E6%9E%84%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%C2%A0-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81k8s%E5%8D%95%E4%B8%BB%E6%9E%B6%E6%9E%84%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%C2%A0" rel="nofollow">一、k8s单主架构集群的部署 </a></p> 
<p id="1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE-toc" style="margin-left:40px;"><a href="#1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE" rel="nofollow">1.操作系统初始化配置</a></p> 
<p id="2.%E9%83%A8%E7%BD%B2%20etcd%20%E9%9B%86%E7%BE%A4%C2%A0-toc" style="margin-left:40px;"><a href="#2.%E9%83%A8%E7%BD%B2%20etcd%20%E9%9B%86%E7%BE%A4%C2%A0" rel="nofollow">2.部署 etcd 集群 </a></p> 
<p id="3.%E9%83%A8%E7%BD%B2docker%E5%BC%95%E6%93%8E-toc" style="margin-left:40px;"><a href="#3.%E9%83%A8%E7%BD%B2docker%E5%BC%95%E6%93%8E" rel="nofollow">3.部署docker引擎</a></p> 
<p id="4.%E9%83%A8%E7%BD%B2%20Master%20%E7%BB%84%E4%BB%B6-toc" style="margin-left:40px;"><a href="#4.%E9%83%A8%E7%BD%B2%20Master%20%E7%BB%84%E4%BB%B6" rel="nofollow">4.部署 Master 组件</a></p> 
<p id="5.%E9%83%A8%E7%BD%B2%C2%A0Worker%20Node%20%E7%BB%84%E4%BB%B6-toc" style="margin-left:40px;"><a href="#5.%E9%83%A8%E7%BD%B2%C2%A0Worker%20Node%20%E7%BB%84%E4%BB%B6" rel="nofollow">5.部署 Worker Node 组件</a></p> 
<p id="6.%E9%83%A8%E7%BD%B2%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%EF%BC%88%E4%BD%BF%E7%94%A8%20flannel%EF%BC%89-toc" style="margin-left:40px;"><a href="#6.%E9%83%A8%E7%BD%B2%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%EF%BC%88%E4%BD%BF%E7%94%A8%20flannel%EF%BC%89" rel="nofollow">6.部署 CNI 网络组件（使用 flannel）</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2> </h2> 
<h2 id="%E4%B8%80%E3%80%81k8s%E5%8D%95%E4%B8%BB%E6%9E%B6%E6%9E%84%E9%9B%86%E7%BE%A4%E7%9A%84%E9%83%A8%E7%BD%B2%C2%A0">一、k8s单主架构集群的部署 </h2> 
<p><img alt="" height="625" src="https://images2.imgbox.com/fb/e8/EoLHznoC_o.png" width="1200"></p> 
<blockquote> 
 <p>k8s集群master01：192.168.116.10<br> k8s集群master02：192.168.116.20（第二部分高可用架构会加入）</p> 
 <p>k8s集群node01：192.168.116.30<br> k8s集群node02：192.168.116.40</p> 
 <p>etcd集群节点1：192.168.116.10（etcd集群原则上单独部署，此实验为方便部署在节点上）<br> etcd集群节点2：192.168.116.30<br> etcd集群节点3：192.168.116.40</p> 
 <p>负载均衡nginx+keepalive01（master）：192.168.116.50<br> 负载均衡nginx+keepalive02（backup）：192.168.116.60</p> 
</blockquote> 
<h3 id="1.%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE">1.操作系统初始化配置</h3> 
<pre><code class="language-bash">#关闭防火墙
systemctl disable --now firewalld
iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X

#关闭selinux
setenforce 0
sed -i 's/enforcing/disabled/' /etc/selinux/config

#关闭swap
swapoff -a
sed -ri 's/.*swap.*/#&amp;/' /etc/fstab 

#根据规划设置主机名
hostnamectl set-hostname master01
hostnamectl set-hostname node01
hostnamectl set-hostname node02
su

#添加主机名映射
vim /etc/hosts
#添加
192.168.116.10 master01
192.168.116.30 node01
192.168.116.40 node02

vim /etc/sysctl.d/k8s.conf
#开启网桥模式，可将网桥的流量传递给iptables链
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
#开启路由转发
net.ipv4.ip_forward=1
#关闭ipv6（可选）
#net.ipv6.conf.all.disable_ipv6=1

#加载系统配置
sysctl --system

#时间同步（可以做计划任务）
yum install ntpdate -y
ntpdate ntp.aliyun.com
crontab -e
*/10 * * * * /usr/sbin/ntpdate ntp.aliyun.com &amp;&gt; /dev/null

</code></pre> 
<h3 id="2.%E9%83%A8%E7%BD%B2%20etcd%20%E9%9B%86%E7%BE%A4%C2%A0">2.部署 etcd 集群 </h3> 
<p><strong>在 master01 节点上操作 </strong></p> 
<p><strong>先准备签发证书的环境</strong></p> 
<blockquote> 
 <p></p> 
 <p>CFSSL是CloudFlare公司开源的一款PKI/TLS工具。CFSSL包含一个命令行工具和一个用于签名、验证和捆绑TLS证书的HTTPAPI服务。使用Go语言编写。</p> 
 <p>CFSSIL使用配置文件生成证书，因此自签之前，需要生成它识别的json格式的配置文件，CFSSL提供了方便的命令行生成配置文件。</p> 
 <p></p> 
 <p>CFSSL用来为etcd提供TLS证书，它支持签三种类型的证书：</p> 
 <p><strong>client 证书：</strong>服务端连接客户端时携带的证书，用于客户端验证服务端身份，如kube-apiserver 访问 etcd；<br><strong>server证书：</strong>客户端连接服务端时携带的证书，用于服务端验证客户端身份，如etcd对外提供服务；<br><strong>peer证书：</strong>相互之间连接时使用的证书，如etcd节点之间进行验证和通信。</p> 
 <p></p> 
 <p>这里为了方便全部都使用同一套证书认证，生产环境中一般不这样使用。</p> 
</blockquote> 
<pre><code class="language-bash">#准备cfssl证书生成工具
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfo

chmod +x /usr/local/bin/cfssl*
</code></pre> 
<blockquote> 
 <p><strong>cfssl：</strong>证书签发的工具命令<br><strong>cfssljson：</strong>将cfssl生成的证书（json格式）变为文件承载式证书</p> 
 <p><strong>cfssl-certinfo：</strong>验证证书的信息</p> 
 <p><br> cfssl-certinfo -cert &lt;证书名称&gt;        #可以使用此命令查看证书的信息</p> 
</blockquote> 
<p><strong>编写etcd-cert.sh用于生成CA证书、etcd 服务器证书以及私钥</strong></p> 
<pre><code class="language-bash">mkdir -p /opt/k8s/etcd-cert
cd /opt/k8s/etcd-cert
vim /opt/k8s/etcd-cert/etcd-cert.sh
#!/bin/bash
#配置证书生成策略，让 CA 软件知道颁发有什么功能的证书，生成用来签发其他组件证书的根证书
cat &gt; ca-config.json &lt;&lt;EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "www": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

#ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；
#后续在签名证书时会使用某个 profile；此实例只有一个 www 模板。
#expiry：指定了证书的有效期，87600h 为10年，如果用默认值一年的话，证书到期后集群会立即宕掉
#signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；
#key encipherment：表示使用非对称密钥加密，如 RSA 加密；
#server auth：表示client可以用该 CA 对 server 提供的证书进行验证；
#client auth：表示server可以用该 CA 对 client 提供的证书进行验证；
#注意标点符号，最后一个字段一般是没有逗号的。


#-----------------------
#生成CA证书和私钥（根证书和私钥）
#特别说明： cfssl和openssl有一些区别，openssl需要先生成私钥，然后用私钥生成请求文件，最后生成签名的证书和私钥等，但是cfssl可以直接得到请求文件。
cat &gt; ca-csr.json &lt;&lt;EOF
{
    "CN": "etcd",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing"
        }
    ]
}
EOF

#CN：Common Name，浏览器使用该字段验证网站或机构是否合法，一般写的是域名
#key：指定了加密算法，一般使用rsa（size：2048）
#C：Country，国家
#ST：State，州，省
#L：Locality，地区,城市
#O: Organization Name，组织名称，公司名称
#OU: Organization Unit Name，组织单位名称，公司部门

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

#生成的文件：
#ca-key.pem：根证书私钥
#ca.pem：根证书
#ca.csr：根证书签发请求文件

#cfssl gencert -initca &lt;CSRJSON&gt;：使用 CSRJSON 文件生成生成新的证书和私钥。如果不添加管道符号，会直接把所有证书内容输出到屏幕。
#注意：CSRJSON 文件用的是相对路径，所以 cfssl 的时候需要 csr 文件的路径下执行，也可以指定为绝对路径。
#cfssljson 将 cfssl 生成的证书（json格式）变为文件承载式证书，-bare 用于命名生成的证书文件。


#-----------------------
#生成 etcd 服务器证书和私钥
cat &gt; server-csr.json &lt;&lt;EOF
{
    "CN": "etcd",
    "hosts": [
    "192.168.116.10",
    "192.168.116.30",
    "192.168.116.40"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
EOF

#hosts：将所有 etcd 集群节点添加到 host 列表，需要指定所有 etcd 集群的节点 ip 或主机名不能使用网段，新增 etcd 服务器需要重新签发证书。

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server

#生成的文件：
#server.csr：服务器的证书请求文件
#server-key.pem：服务器的私钥
#server.pem：服务器的数字签名证书

#-config：引用证书生成策略文件 ca-config.json
#-profile：指定证书生成策略文件中的的使用场景，比如 ca-config.json 中的 www


#编写完后给上执行权限并执行
chmod +x etcd-cert.sh
./etcd-cert.sh</code></pre> 
<p><img alt="" height="106" src="https://images2.imgbox.com/d9/da/cTBrjAPc_o.png" width="1146"></p> 
<p><strong>编写etcd服务脚本，添加集群配置</strong></p> 
<pre><code class="language-bash">vim /opt/k8s/etcd.sh
#!/bin/bash
#example: ./etcd.sh etcd01 192.168.80.10 etcd02=https://192.168.80.11:2380,etcd03=https://192.168.80.12:2380

#创建etcd配置文件/opt/etcd/cfg/etcd
ETCD_NAME=$1
ETCD_IP=$2
ETCD_CLUSTER=$3

WORK_DIR=/opt/etcd

cat &gt; $WORK_DIR/cfg/etcd  &lt;&lt;EOF
#[Member]
ETCD_NAME="${ETCD_NAME}"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://${ETCD_IP}:2380"
ETCD_LISTEN_CLIENT_URLS="https://${ETCD_IP}:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://${ETCD_IP}:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://${ETCD_IP}:2379"
ETCD_INITIAL_CLUSTER="etcd01=https://${ETCD_IP}:2380,${ETCD_CLUSTER}"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF

#Member:成员配置
#ETCD_NAME：节点名称，集群中唯一。成员名字，集群中必须具备唯一性，如etcd01
#ETCD_DATA_DIR：数据目录。指定节点的数据存储目录，这些数据包括节点ID，集群ID，集群初始化配置，Snapshot文件，若未指定-wal-dir，还会存储WAL文件；如果不指定会用缺省目录
#ETCD_LISTEN_PEER_URLS：集群通信监听地址。用于监听其他member发送信息的地址。ip为全0代表监听本机所有接口
#ETCD_LISTEN_CLIENT_URLS：客户端访问监听地址。用于监听etcd客户发送信息的地址。ip为全0代表监听本机所有接口

#Clustering：集群配置
#ETCD_INITIAL_ADVERTISE_PEER_URLS：集群通告地址。其他member使用，其他member通过该地址与本member交互信息。一定要保证从其他member能可访问该地址。静态配置方式下，该参数的value一定要同时在--initial-cluster参数中存在
#ETCD_ADVERTISE_CLIENT_URLS：客户端通告地址。etcd客户端使用，客户端通过该地址与本member交互信息。一定要保证从客户侧能可访问该地址
#ETCD_INITIAL_CLUSTER：集群节点地址。本member使用。描述集群中所有节点的信息，本member根据此信息去联系其他member
#ETCD_INITIAL_CLUSTER_TOKEN：集群Token。用于区分不同集群。本地如有多个集群要设为不同
#ETCD_INITIAL_CLUSTER_STATE：加入集群的当前状态，new是新集群，existing表示加入已有集群。
#创建etcd.service服务管理文件
cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=${WORK_DIR}/cfg/etcd
ExecStart=${WORK_DIR}/bin/etcd \
--cert-file=${WORK_DIR}/ssl/server.pem \
--key-file=${WORK_DIR}/ssl/server-key.pem \
--trusted-ca-file=${WORK_DIR}/ssl/ca.pem \
--peer-cert-file=${WORK_DIR}/ssl/server.pem \
--peer-key-file=${WORK_DIR}/ssl/server-key.pem \
--peer-trusted-ca-file=${WORK_DIR}/ssl/ca.pem \
--logger=zap \
--enable-v2
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

#--enable-v2：开启 etcd v2 API 接口。当前 flannel 版本不支持 etcd v3 通信
#--logger=zap：使用 zap 日志框架。zap.Logger 是go语言中相对日志库中性能最高的
#--peer开头的配置项用于指定集群内部TLS相关证书（peer 证书），这里全部都使用同一套证书认证
#不带--peer开头的的参数是指定 etcd 服务器TLS相关证书（server 证书），这里全部都使用同一套证书认证
systemctl daemon-reload
systemctl enable etcd
systemctl restart etcd


#编写完后给上执行权限，在下一步需要执行
chmod +x /opt/k8s/etcd.sh</code></pre> 
<p><strong>安装etcd服务，并启动集群</strong></p> 
<pre><code class="language-bash">#上传 etcd-v3.4.9-linux-amd64.tar.gz 到 /opt/k8s 目录中，并尝试启动服务
cd /opt/k8s/
tar zxvf etcd-v3.4.9-linux-amd64.tar.gz
mkdir -p /opt/etcd/{cfg,bin,ssl}

#etcdctl管理文件，etcd服务文件
cd /opt/k8s/etcd-v3.4.9-linux-amd64/
mv etcd etcdctl /opt/etcd/bin/
cp /opt/k8s/etcd-cert/*.pem /opt/etcd/ssl/

#此脚本可以启动本地etcd服务，添加集群配置（第一次启动会卡住，是因为需要其他节点etcd服务加入）
cd /opt/k8s/
./etcd.sh etcd01 192.168.116.10 etcd02=https://192.168.116.30:2380,etcd03=https://192.168.116.40:2380

#复制服务脚本到其他etcd节点，并修改配置（见下图）
scp -r /opt/etcd/ root@192.168.116.30:/opt/
scp -r /opt/etcd/ root@192.168.116.40:/opt/
scp /usr/lib/systemd/system/etcd.service root@192.168.116.30:/usr/lib/systemd/system/
scp /usr/lib/systemd/system/etcd.service root@192.168.116.40:/usr/lib/systemd/system/

#修改完后分别启动服务
systemctl start etcd

#然后在主etcd节点再次执行
cd /opt/k8s/
./etcd.sh etcd01 192.168.116.10 etcd02=https://192.168.116.30:2380,etcd03=https://192.168.116.40:2380
</code></pre> 
<p><img alt="" height="376" src="https://images2.imgbox.com/07/e1/b4daE62B_o.png" width="1147"></p> 
<p><strong>执行完后可以使用以下命令查看etcd集群状态（全为true则集群无误）</strong></p> 
<blockquote> 
 <p><strong>#使用 health 是查看健康状态；使用 status 是查看所有状态信息</strong></p> 
 <p>ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.116.10:2379,https://192.168.116.30:2379,https://192.168.116.40:2379" endpoint health --write-out=table </p> 
 <p><strong>--cert：</strong>识别HTTPS端使用SSL证书文件</p> 
 <p><strong>--cacert：</strong>使用此CA证书验证启用https的服务器的证书</p> 
 <p><strong>--key：</strong>使用此SSL密钥文件标识HTTPS客户端</p> 
 <p><strong>--endpoints：</strong>集群中以逗号分隔的机器地址列表cluster-health: 检查etcd集群的运行状况</p> 
 <p><strong>--write-out=table 或 -wtable：</strong>以表格形式输出</p> 
</blockquote> 
<p><img alt="" height="163" src="https://images2.imgbox.com/c5/bb/q3slpiOZ_o.png" width="685"></p> 
<p><img alt="" height="220" src="https://images2.imgbox.com/a5/6a/xTRiZRBT_o.png" width="1200"></p> 
<p><strong>查看etcd集群成员信息</strong></p> 
<blockquote> 
 <p>ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.116.10:2379,https://192.168.116.30:2379,https://192.168.116.40:2379" member list --write-out=table</p> 
</blockquote> 
<p><img alt="" height="244" src="https://images2.imgbox.com/cc/cb/2SYUifqx_o.png" width="1200"></p> 
<p><strong>补充 etcd 备份和还原方式</strong></p> 
<p><strong>注：备份时 endpoint 只需要指定etcd中的一个节点</strong></p> 
<blockquote> 
 <p><strong>备份</strong></p> 
 <p>ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.116.10:2379" snapshot save 备份文件路径</p> 
</blockquote> 
<blockquote> 
 <p><strong>查看</strong></p> 
 <p>ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.116.10:2379" snapshot status 备份文件路径 -wtable</p> 
</blockquote> 
<blockquote> 
 <p><strong>恢复</strong></p> 
 <p>ETCDCTL_API=3 /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.116.10:2379" snapshot restore 备份文件路径</p> 
</blockquote> 
<h3 id="3.%E9%83%A8%E7%BD%B2docker%E5%BC%95%E6%93%8E">3.部署docker引擎</h3> 
<p><strong>所有 node 节点部署docker引擎</strong></p> 
<pre><code class="language-bash">yum install -y yum-utils device-mapper-persistent-data lvm2 
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 
yum install -y docker-ce docker-ce-cli containerd.io

systemctl start docker.service
systemctl enable docker.service </code></pre> 
<h3 id="4.%E9%83%A8%E7%BD%B2%20Master%20%E7%BB%84%E4%BB%B6">4.部署 Master 组件</h3> 
<p><strong>编写master各组件认证的脚本</strong></p> 
<pre><code class="language-bash">mkdir /opt/k8s/k8s-cert
vim k8s-cert.sh
#!/bin/bash
#配置证书生成策略，让 CA 软件知道颁发有什么功能的证书，生成用来签发其他组件证书的根证书
cat &gt; ca-config.json &lt;&lt;EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF

#生成CA证书和私钥（根证书和私钥）
cat &gt; ca-csr.json &lt;&lt;EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

#-----------------------
#生成 apiserver 的证书和私钥（apiserver和其它k8s组件通信使用）
#hosts中将所有可能作为 apiserver 的 ip 添加进去，后面 keepalived 使用的 VIP 也要加入
cat &gt; apiserver-csr.json &lt;&lt;EOF
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "192.168.116.10",
      "192.168.116.20",
      "192.168.116.100",
      "192.168.116.50",
      "192.168.116.60",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes apiserver-csr.json | cfssljson -bare apiserver

#-----------------------
#生成 kubectl 连接集群的证书和私钥，具有admin权限
cat &gt; admin-csr.json &lt;&lt;EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin


#-----------------------
#生成 kube-proxy 的证书和私钥
cat &gt; kube-proxy-csr.json &lt;&lt;EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

#给上执行权限并执行
chmod +x k8s-cert.sh
./k8s-cert.sh

#再将所需证书存放到/opt/kubernetes/ssl/
cp ca*pem apiserver*pem /opt/kubernetes/ssl/
</code></pre> 
<p><strong>安装k8s服务，准备master需要的工具</strong></p> 
<pre><code class="language-bash">#上传 kubernetes-server-linux-amd64.tar.gz 到 /opt/k8s/ 目录中，解压 kubernetes 压缩包
cd /opt/k8s/
tar zxvf kubernetes-server-linux-amd64.tar.gz

#将master的组件和命令工具存放到/opt/kubernetes/bin/，做软连接能被系统识别
cd /opt/k8s/kubernetes/server/bin
cp kube-apiserver kubectl kube-controller-manager kube-scheduler /opt/kubernetes/bin/
ln -s /opt/kubernetes/bin/* /usr/local/bin/

#上传master.zip到/opt/k8s目录并解压
cd /opt/k8s/
unzip master.zip -d master/
cd master/
chmod +x *.sh
</code></pre> 
<p><strong>创建 bootstrap token 认证文件，apiserver 启动时会调用，然后就相当于在集群内创建了一个这个用户，接下来就可以用 RBAC 给他授权</strong></p> 
<pre><code class="language-bash">#创建 bootstrap token 认证文件，apiserver 启动时会调用，然后就相当于在集群内创建了一个这个用户，接下来就可以用 RBAC 给他授权
cd /opt/kubernetes
vim token.sh
#!/bin/bash
#获取随机数前16个字节内容，以十六进制格式输出，并删除其中空格
BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
#生成 token.csv 文件，按照 Token序列号,用户名,UID,用户组 的格式生成
cat &gt; /opt/kubernetes/cfg/token.csv &lt;&lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

#执行 
chmod +x token.sh
./token.sh</code></pre> 
<p><img alt="" height="53" src="https://images2.imgbox.com/4f/d7/SSFvZj7c_o.png" width="1020"></p> 
<pre><code class="language-bash">#执行apiserver脚本 后面跟master的ip 和 etcd集群的ip
./apiserver.sh 192.168.116.10 https://192.168.116.10:2379,https://192.168.116.30:2379,https://192.168.116.40:2379

#再执行另外三个脚本
./controller-manager.sh
./scheduler.sh
./admin</code></pre> 
<p><strong>检查master组件和etcd集群状态</strong></p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/b1/db/07weyGzp_o.png" width="1200"></p> 
<h3 id="5.%E9%83%A8%E7%BD%B2%C2%A0Worker%20Node%20%E7%BB%84%E4%BB%B6">5.部署 Worker Node 组件</h3> 
<p><strong>在所有 node 节点上操作</strong></p> 
<pre><code class="language-bash">#创建kubernetes工作目录
mkdir -p /opt/kubernetes/{bin,cfg,ssl,logs}

#上传 node.zip 到 /opt 目录中，解压 node.zip 压缩包，获得kubelet.sh、proxy.sh
cd /opt/
unzip node.zip
chmod +x kubelet.sh proxy.sh
</code></pre> 
<p><strong>在 master01 节点上操作</strong></p> 
<pre><code class="language-bash">#把 kubelet、kube-proxy 拷贝到 node 节点
cd /opt/k8s/kubernetes/server/bin
scp kubelet kube-proxy root@192.168.116.30:/opt/kubernetes/bin/
scp kubelet kube-proxy root@192.168.116.40:/opt/kubernetes/bin/

#上传 kubeconfig.sh 文件到 /opt/k8s/kubeconfig 目录中，生成 kubeconfig 的配置文件
mkdir /opt/k8s/kubeconfig

cd /opt/k8s/kubeconfig
chmod +x kubeconfig.sh
./kubeconfig.sh 192.168.116.10 /opt/k8s/k8s-cert/

scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.116.30:/opt/kubernetes/cfg/
scp bootstrap.kubeconfig kube-proxy.kubeconfig root@192.168.116.40:/opt/kubernetes/cfg/

#RBAC授权，使用户 kubelet-bootstrap 能够有权限发起 CSR 请求
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
</code></pre> 
<blockquote> 
 <p><strong>kubelet采用TLS Bootstrapping 机制，自动完成到kube-apiserver的注册，在node节点量较大或者后期自动扩容时非常有用。</strong></p> 
 <p></p> 
 <p>Master apiserver 启用TLS 认证后，node 节点kubelet 组件想要加入集群，必须使用CA签发的有效证书才能与apiserver 通信，当node节点很多时，签署证书是一件很繁琐的事情。因此Kubernetes引入了TLS bootstraping 机制来自动颁发客户端证书，kubelet会以一个低权限用户自动向 apiserver申请证书，kubelet的证书由apiserver 动态签署。</p> 
 <p></p> 
 <p>kubelet首次启动通过加载bootstrap.kubeconfig 中的用户Token和apiserver CA证书发起首次CSR请求，这个Token 被预先内置在apiserver节点的token.csv中，其身份为kubelet-bootstrap用户和system:kubelet-bootstrap 用户组；想要首次CSR请求能成功（即不会被 apiserver 401拒绝），则需要先创建一个ClusterRoleBinding，将kubelet-bootstrap 用户和system:node-bootstrapper 内置ClusterRole 绑定（通过 kubectl get clusterroles可查询），使其能够发起CSR认证请求。</p> 
 <p></p> 
 <p>TLS bootstrapping 时的证书实际是由kube-controller-manager组件来签署的，也就是说证书有效期是kube-controller-manager组件控制的：kube-controller-manager组件提供了一个–experimental-cluster-signing-duration 参数来设置签署的证书有效时间：默认为8760h0m0s，将其改87600h0m0s，即10年后再进行TLS bootstrapping 签者证书即可。</p> 
 <p></p> 
 <p><strong>也就是说kubelet 首次访问API Server时，是使用token做认证，通过后，Controller Manager会为kubelet生成一个证书，以后的访问都是用证书做认证了。</strong></p> 
</blockquote> 
<p><strong>在两个 node 节点上操作</strong></p> 
<pre><code class="language-bash">#启动 kubelet 服务
cd /opt/
./kubelet.sh 192.168.116.30</code></pre> 
<p><img alt="" height="74" src="https://images2.imgbox.com/32/59/jkjIM3ko_o.png" width="1003"></p> 
<p><strong>在 master01 节点上操作，通过 CSR 请求</strong></p> 
<blockquote> 
 <p>#检查到 node01 节点的 kubelet 发起的 CSR 请求，Pending表示等待集群给该节点签发证书<br> kubectl get csr</p> 
</blockquote> 
<p><img alt="" height="128" src="https://images2.imgbox.com/71/3d/brFNbGEQ_o.png" width="1200"></p> 
<blockquote> 
 <p>kubectl certificate approve 请求名        #通过 CSR 请求</p> 
</blockquote> 
<p><img alt="" height="61" src="https://images2.imgbox.com/ee/37/7SHLSRyR_o.png" width="1200"></p> 
<blockquote> 
 <p>#查看节点，由于网络插件还没有部署，节点会没有准备就绪 NotReady<br> kubectl get node</p> 
</blockquote> 
<p><img alt="" height="83" src="https://images2.imgbox.com/bd/d4/LehpJ8gj_o.png" width="691"></p> 
<p><strong>在两个 node 节点上操作 </strong></p> 
<pre><code class="language-bash">#加载 ip_vs 模块
for i in $(ls /usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs|grep -o "^[^.]*");do echo $i; /sbin/modinfo -F filename $i &gt;/dev/null 2&gt;&amp;1 &amp;&amp; /sbin/modprobe $i;done

#启动proxy服务
cd /opt/
./proxy.sh 192.168.116.30</code></pre> 
<p><img alt="" height="439" src="https://images2.imgbox.com/65/fa/EIiPm3BC_o.png" width="1200"></p> 
<h3 id="6.%E9%83%A8%E7%BD%B2%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%EF%BC%88%E4%BD%BF%E7%94%A8%20flannel%EF%BC%89">6.部署 CNI 网络组件（使用 flannel）</h3> 
<p><strong>在 node01 节点上操作</strong></p> 
<pre><code class="language-bash">#上传 cni-plugins-linux-amd64-v0.8.6.tgz 和 flannel.tar 到 /opt 目录中
cd /opt/
docker load -i flannel.tar

mkdir /opt/cni/bin
tar zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin
</code></pre> 
<p><strong>在 master01 节点上操作</strong></p> 
<pre><code class="language-bash">#上传 kube-flannel.yml 文件到 /opt/k8s 目录中，部署 CNI 网络
cd /opt/k8s
kubectl apply -f kube-flannel.yml </code></pre> 
<blockquote> 
 <p>kubectl get pods -n kube-system        #查看pods指定命名空间</p> 
</blockquote> 
<p><img alt="" height="99" src="https://images2.imgbox.com/fc/a9/IieQCsLU_o.png" width="907"></p> 
<blockquote> 
 <p>kubectl get nodes        #查看节点状态（Ready即成功）</p> 
</blockquote> 
<p><img alt="" height="103" src="https://images2.imgbox.com/ab/8c/jPQ9Pf3u_o.png" width="907"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7f5e06436647dae963674da018eb23a6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">windows查看端口占用，通过端口找进程号（查找进程号），通过进程号定位应用名（查找应用）（netstat、tasklist）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a556d7c3f7bf17302b64ace337e4ad90/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数据结构与算法基础-学习-25-图之MST（最小代价生成树）之Prim（普利姆）算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>