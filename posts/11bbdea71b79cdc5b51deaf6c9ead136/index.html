<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch中.detach()与.data()的用法 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch中.detach()与.data()的用法" />
<meta property="og:description" content="这里是官方文档对detach的定义
实际上，detach()就是返回一个新的tensor，并且这个tensor是从当前的计算图中分离出来的。但是返回的tensor和原来的tensor是共享内存空间的。
import torch a = torch.tensor([1.0, 2.0, 3.0], requires_grad = True) a = a.detach() # 会将requires_grad 属性设置为False print(a.requires_grad) 举个例子来说明一下detach有什么用。 如果A网络的输出被喂给B网络作为输入， 如果我们希望在梯度反传的时候只更新B中参数的值，而不更新A中的参数值，这时候就可以使用detach()
a = A(input) a = a.deatch() # 或者a.detach_()进行in_place操作 out = B(a) loss = criterion(out, labels) loss.backward() 如果希望修改A的参数， 而不希望修改B的参数， 那么就需要手动将B中参数的requires_grad属性设置为False
for param in B.parameters(): param.requires_grad = False 还有一点需要注意的是Tensor.detach()和Tensor.data()的区别
Tensor.data()和Tensor.detach()一样， 都会返回一个新的Tensor， 这个Tensor和原来的Tensor共享内存空间，一个改变，另一个也会随着改变，且都会设置新的Tensor的requires_grad属性为False。这两个方法只取出原来Tensor的tensor数据， 丢弃了grad、grad_fn等额外的信息。区别在于Tensor.data()方法不能被autograd追踪到，如果你修改了Tensor.data()返回的新Tensor，原来的Tensor也会改变， 但是这时候的微分并没有被追踪到，那么当你执行loss.backward()的时候并不会报错，但是求的梯度就是错误的！因此， 如果你使用了Tensor.data()方法， 那么切记一定不要随便修改返回的新Tensor的值。如果你使用的是Tensor.detach()方法， 当你修改他的返回值并进行求导操作，会报错。 因此，Tensor.detach()是安全的。
转载链接：https://zhuanlan.zhihu.com/p/410199046" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/11bbdea71b79cdc5b51deaf6c9ead136/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-06T17:25:30+08:00" />
<meta property="article:modified_time" content="2022-04-06T17:25:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch中.detach()与.data()的用法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>这里是官方文档对detach的定义</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/37/ba/Wvs0bYz3_o.png"></p> 
<p>实际上，detach()就是返回一个新的tensor，并且这个tensor是从当前的计算图中分离出来的。但是返回的tensor和原来的tensor是共享内存空间的。</p> 
<pre><code>import torch

a = torch.tensor([1.0, 2.0, 3.0], requires_grad = True)
a = a.detach() # 会将requires_grad 属性设置为False
print(a.requires_grad)</code></pre> 
<p>举个例子来说明一下detach有什么用。 如果A网络的输出被喂给B网络作为输入， 如果我们希望在梯度反传的时候只更新B中参数的值，而不更新A中的参数值，这时候就可以使用detach()</p> 
<pre><code>a = A(input)
a = a.deatch() # 或者a.detach_()进行in_place操作
out = B(a)
loss = criterion(out, labels)
loss.backward()</code></pre> 
<p>如果希望修改A的参数， 而不希望修改B的参数， 那么就需要手动将B中参数的requires_grad属性设置为False</p> 
<pre><code>for param in B.parameters():
    param.requires_grad = False</code></pre> 
<p>还有一点需要注意的是Tensor.detach()和Tensor.data()的区别</p> 
<p>Tensor.data()和Tensor.detach()一样， 都会返回一个新的Tensor， 这个Tensor和原来的Tensor共享内存空间，一个改变，另一个也会随着改变，且都会设置新的Tensor的requires_grad属性为False。这两个方法只取出原来Tensor的tensor数据， 丢弃了grad、grad_fn等额外的信息。区别在于Tensor.data()方法不能被autograd追踪到，如果你修改了Tensor.data()返回的新Tensor，原来的Tensor也会改变， 但是这时候的微分并没有被追踪到，那么当你执行loss.backward()的时候并不会报错，但是求的梯度就是错误的！因此， 如果你使用了Tensor.data()方法， 那么切记一定不要随便修改返回的新Tensor的值。如果你使用的是Tensor.detach()方法， 当你修改他的返回值并进行<a href="https://www.zhihu.com/search?q=%E6%B1%82%E5%AF%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22410199046%22%7D" rel="nofollow" title="求导">求导</a>操作，会报错。 因此，Tensor.detach()是安全的。</p> 
<p><br> 转载链接：https://zhuanlan.zhihu.com/p/410199046<br>  </p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/230912d6fb1a4a065a842cf8fa5e38eb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Spring远程命令执行漏洞（CVE-2022-22965）原理分析和思考</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c2d1f8cfac3e9a9a241ddbedc6fef36e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">安全狗西部网络安全运营中心 护航“东数西算”工程安全</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>