<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习之卷积神经网络 ZF Net - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习之卷积神经网络 ZF Net" />
<meta property="og:description" content="ZFNet出自论文《 Visualizing and Understanding Convolutional Networks》，作者Matthew D. Zeiler和Rob Fergus——显然ZFNet是以两位作者名字的首字母命名的。ZFNet通常被认为是ILSVRC 2013的冠军方法，但实际上ZFNet排在第3名，前两名分别是Clarifai和NUS，不过Clarifai和ZFNet都出自Matthew D. Zeiler之手，见ILSVRC2013 results。
ZFNet的网络架构是在AlexNet基础上修改而来，与AlexNet相比，差异不大：
第1个卷积层，kernel size从11减小为7，将stride从4减小为2（这将导致feature map增大1倍）为了让后续feature map的尺寸保持一致，第2个卷积层的stride从1变为2 仅这2项修改，就获得了几个点的性能提升。所以，重要的是为什么这样修改？这样修改的动机是什么？文中这样叙述：
通过对AlexNet的特征进行可视化，文章作者发现第2层出现了aliasing。在数字信号处理中，aliasing是指在采样频率过低时出现的不同信号混淆的现象，作者认为这是第1个卷积层stride过大引起的，为了解决这个问题，可以提高采样频率，所以将stride从4调整为2，与之相应的将kernel size也缩小（可以认为stride变小了，kernel没有必要看那么大范围了），这样修改前后，特征的变化情况如下图所示，第1层呈现了更多更具区分力的特征，第二2层的特征也更加清晰，没有aliasing现象。
这就引出了另外一个问题，如何将特征可视化？正如论文标题Visualizing and Understanding Convolutional Networks所显示的那样，与提出一个性能更好的网络结构相比，这篇论文更大的贡献在于提出一种将卷积神经网络深层特征可视化的方法。
可视化操作，针对的是已经训练好的网络，或者训练过程中的网络快照，可视化操作不会改变网络的权重，只是用于分析和理解在给定输入图像时网络观察到了什么样的特征，以及训练过程中特征发生了什么变化。
给定1张输入图像，先前向传播，得到每一层的feature map，如果想可视化第i层学到的特征，保留该层feature map的最大值，将其他位置和其他feature map置0，将其反向映射回原始输入所在的像素空间。对于一般的卷积神经网络，前向传播时不断经历 input image→conv → rectification → pooling →……，可视化时，则从某一层的feature map开始，依次反向经历 unpooling → rectification → deconv → …… → input space，如下图所示，上方对应更深层，下方对应更浅层，前向传播过程在右半侧从下至上，特征可视化过程在左半侧从上至下：
可视化时每一层的操作如下：
Unpooling：在前向传播时，记录相应max pooling层每个最大值来自的位置，在unpooling时，根据来自上层的map直接填在相应位置上，如上图所示，Max Locations “Switches”是一个与pooling层输入等大小的二值map，标记了每个局部极值的位置。Rectification：因为使用的ReLU激活函数，前向传播时只将正值原封不动输出，负值置0，“反激活”过程与激活过程没什么分别，直接将来自上层的map通过ReLU。Deconvolution：可能称为transposed convolution更合适，卷积操作output map的尺寸一般小于等于input map的尺寸，transposed convolution可以将尺寸恢复到与输入相同，相当于上采样过程，该操作的做法是，与convolution共享同样的卷积核，但需要将其左右上下翻转（即中心对称），然后作用在来自上层的feature map进行卷积，结果继续向下传递。 不断经历上述过程，将特征映射回输入所在的像素空间，就可以呈现出人眼可以理解的特征。给定不同的输入图像，看看每一层关注到最显著的特征是什么，如下图所示：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8a25d06aad52e1f3b761229c615a250c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-08T18:30:22+08:00" />
<meta property="article:modified_time" content="2021-06-08T18:30:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习之卷积神经网络 ZF Net</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>ZFNet出自论文《 <a href="https://arxiv.org/abs/1311.2901" rel="nofollow">Visualizing and Understanding Convolutional Networks</a>》，作者Matthew D. Zeiler和Rob Fergus——显然ZFNet是以两位作者名字的首字母命名的。ZFNet通常被认为是ILSVRC 2013的冠军方法，但实际上ZFNet排在第3名，前两名分别是Clarifai和NUS，不过Clarifai和ZFNet都出自Matthew D. Zeiler之手，见<a href="http://www.image-net.org/challenges/LSVRC/2013/results.php#cls" rel="nofollow">ILSVRC2013 results</a>。<br> <img src="https://images2.imgbox.com/c8/40/I0ynNbK0_o.png" alt="在这里插入图片描述"></p> 
<p>ZFNet的网络架构是在AlexNet基础上修改而来，与AlexNet相比，差异不大：</p> 
<ul><li>第1个卷积层，kernel size从11减小为7，将stride从4减小为2（这将导致feature map增大1倍）</li><li>为了让后续feature map的尺寸保持一致，第2个卷积层的stride从1变为2</li></ul> 
<p>仅这2项修改，就获得了几个点的性能提升。所以，重要的是为什么这样修改？这样修改的动机是什么？文中这样叙述：<br> <img src="https://images2.imgbox.com/27/de/6Nn07raw_o.png" alt="在这里插入图片描述"><br> 通过对AlexNet的特征进行可视化，文章作者发现第2层出现了aliasing。在数字信号处理中，aliasing是指在采样频率过低时出现的不同信号混淆的现象，作者认为这是第1个卷积层stride过大引起的，为了解决这个问题，可以提高采样频率，所以将stride从4调整为2，与之相应的将kernel size也缩小（可以认为stride变小了，kernel没有必要看那么大范围了），这样修改前后，特征的变化情况如下图所示，第1层呈现了更多更具区分力的特征，第二2层的特征也更加清晰，没有aliasing现象。<br> <img src="https://images2.imgbox.com/14/b2/qYJJPz09_o.png" alt="在这里插入图片描述"><br> 这就引出了另外一个问题，如何将特征可视化？正如论文标题Visualizing and Understanding Convolutional Networks所显示的那样，与提出一个性能更好的网络结构相比，这篇论文更大的贡献在于提出一种将卷积神经网络深层特征可视化的方法。</p> 
<p>可视化操作，针对的是已经训练好的网络，或者训练过程中的网络快照，可视化操作不会改变网络的权重，只是用于分析和理解在给定输入图像时网络观察到了什么样的特征，以及训练过程中特征发生了什么变化。</p> 
<p>给定1张输入图像，先前向传播，得到每一层的feature map，如果想可视化第i层学到的特征，保留该层feature map的最大值，将其他位置和其他feature map置0，将其反向映射回原始输入所在的像素空间。对于一般的卷积神经网络，前向传播时不断经历 input image→conv → rectification → pooling →……，可视化时，则从某一层的feature map开始，依次反向经历 unpooling → rectification → deconv → …… → input space，如下图所示，上方对应更深层，下方对应更浅层，前向传播过程在右半侧从下至上，特征可视化过程在左半侧从上至下：</p> 
<p><img src="https://images2.imgbox.com/fa/7c/ZYz9FLaN_o.png" alt="在这里插入图片描述"></p> 
<p>可视化时每一层的操作如下：</p> 
<ul><li>Unpooling：在前向传播时，记录相应max pooling层每个最大值来自的位置，在unpooling时，根据来自上层的map直接填在相应位置上，如上图所示，Max Locations “Switches”是一个与pooling层输入等大小的二值map，标记了每个局部极值的位置。</li><li>Rectification：因为使用的ReLU激活函数，前向传播时只将正值原封不动输出，负值置0，“反激活”过程与激活过程没什么分别，直接将来自上层的map通过ReLU。</li><li>Deconvolution：可能称为transposed convolution更合适，卷积操作output map的尺寸一般小于等于input map的尺寸，transposed convolution可以将尺寸恢复到与输入相同，相当于上采样过程，该操作的做法是，与convolution共享同样的卷积核，但需要将其左右上下翻转（即中心对称），然后作用在来自上层的feature map进行卷积，结果继续向下传递。</li></ul> 
<p>不断经历上述过程，将特征映射回输入所在的像素空间，就可以呈现出人眼可以理解的特征。给定不同的输入图像，看看每一层关注到最显著的特征是什么，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/23/ae/qCaJCuIw_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/08237355cd2769db432d819b32c03fb1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">在Eclipse中安装SpringTools插件</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0468b2df6ba622b5a0bbe54b0a20c3c2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端vue 跨域问题 报错Access to XMLHttpRequest at ‘‘ from origin ‘‘has been blocked by CORS policy</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>