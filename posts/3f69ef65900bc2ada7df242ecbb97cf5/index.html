<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>SMOTE过采样算法 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="SMOTE过采样算法" />
<meta property="og:description" content="为什么类别不平衡是不好的 从模型的训练过程来看 从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。 使用经验风险（模型在训练集上的平均损失）最小化作为模型的学习准则。设损失函数为0-1 loss（这是一种典型的均等代价的损失函数），那么优化目标就等价于错误率最小化（也就是accuracy最大化）。考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%，于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，ok，到此为止，训练结束！ 于是这个模型没有学习到如何去判别出少数类。
从模型的预测过程来看 考虑二项Logistic回归模型。输入一个样本 x ，模型输出的是其属于正类的概率 y’ 。当 y’&gt;0.5时，模型判定该样本属于正类，否则就是属于反类。 为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？ 从几率（odds）的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为 y’/(1-y’) 。 模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 θ 等于正类样本数除以全部样本数，那么样本的真实几率为 θ/(1−θ) 。当观测几率大于真实几率时，也就是 y’&gt;θ 时，那么就判定这个样本属于正类。 虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个假设：训练集是真实样本总体的无偏采样。正是因为这个假设，所以认为训练集的观测几率 θ/(1−θ) 就代表了真实几率 θ/(1−θ) 。 所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。
三种解决方案 目前主要有三种办法： 1. 调整 θ值（也叫再缩放、再平衡、阈值移动、是代价敏感学习的基础） 根据训练集的正负样本比例，调整 θ 值。 这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。 2. 过采样 对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。 下面将介绍一种经典的过采样算法：SMOTE。 3. 欠采样 对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。 SMOTE过采样算法 SMOTE全称是Synthetic Minority Oversampling Technique即合成少数类过采样技术，它是基于随机过采样算法的一种改进方案，由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使得模型学习到的信息过于特别(Specific)而不够泛化(General)，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中，算法流程如下。
1、对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻。
2、根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn。
3、对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本 xnew=x&#43;rand(0,1)∗|x−xn| smote算法的伪代码如下： 因此，smote算法的思想是合成新的少数类样本，合成的策略是对每个少数类样本a，从它的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 下面具体介绍如何合成新的样本。 设训练集的一个少数类的样本数为 T ，那么SMOTE算法将为这个少数类合成 NT 个新样本。这里要求 N 必须是正整数，如果给定的 N&lt;1 那么算法将“认为”少数类的样本数 T=NT ，并将强制 N=1 。 考虑该少数类的一个样本 ii ，其特征向量为 xi,i∈{1,…,T}： 首先从该少数类的全部 T 个样本中找到样本 xi 的 k个近邻（例如用欧氏距离），记为 xi(near),near∈{1,…,k}； 然后从这 k 个近邻中随机选择一个样本 xi(nn) ，再生成一个0 到 1 之间的随机数 ζ1 ，从而合成一个新样本 xi1 ： xi1=xi&#43;ζ1⋅(xi(nn)−xi) 3." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3f69ef65900bc2ada7df242ecbb97cf5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-01-28T22:28:29+08:00" />
<meta property="article:modified_time" content="2018-01-28T22:28:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SMOTE过采样算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2 id="为什么类别不平衡是不好的">为什么类别不平衡是不好的</h2> 
<p><strong>从模型的训练过程来看</strong> <br>         从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。 <br>         使用经验风险（模型在训练集上的平均损失）最小化作为模型的学习准则。设损失函数为0-1 loss（这是一种典型的均等代价的损失函数），那么优化目标就等价于错误率最小化（也就是accuracy最大化）。考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%，于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，ok，到此为止，训练结束！ <br>         于是这个模型没有学习到如何去判别出少数类。</p> 
<p><strong>从模型的预测过程来看</strong> <br>         考虑二项Logistic回归模型。输入一个样本 x ，模型输出的是其属于正类的概率 y’ 。当 y’&gt;0.5时，模型判定该样本属于正类，否则就是属于反类。 <br>         为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型估计的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？ <br>         从几率（odds）的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的预测几率为 y’/(1-y’) 。 <br>          模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 θ 等于正类样本数除以全部样本数，那么样本的真实几率为 θ/(1−θ) 。当观测几率大于真实几率时，也就是 y’&gt;θ 时，那么就判定这个样本属于正类。 <br>         虽然我们无法获悉真实样本总体，但之于训练集，<strong>存在这样一个假设：训练集是真实样本总体的无偏采样</strong>。正是因为这个假设，所以认为训练集的观测几率 θ/(1−θ) 就代表了真实几率 θ/(1−θ) 。 <br>         所以，在这个假设下，当一个样本的预测几率大于观测几率时，就应该将样本判断为正类。</p> 
<h2 id="三种解决方案">三种解决方案</h2> 
<pre><code>  目前主要有三种办法：

  1. 调整 θ值（也叫再缩放、再平衡、阈值移动、是代价敏感学习的基础）

  根据训练集的正负样本比例，调整 θ 值。

  这样做的依据是上面所述的对训练集的假设。但在给定任务中，这个假设是否成立，还有待讨论。

  2. 过采样

  对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡。

  下面将介绍一种经典的过采样算法：SMOTE。

  3. 欠采样

  对训练集里面样本数量较多的类别（多数类）进行欠采样，抛弃一些样本来缓解类不平衡。
</code></pre> 
<h2 id="smote过采样算法">SMOTE过采样算法</h2> 
<p>        SMOTE全称是Synthetic Minority Oversampling Technique即合成少数类过采样技术，它是基于随机过采样算法的一种改进方案，<strong>由于随机过采样采取简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使得模型学习到的信息过于特别(Specific)而不够泛化(General)</strong>，SMOTE算法的基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中，算法流程如下。</p> 
<p>1、对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻。</p> 
<p>2、根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn。</p> 
<p>3、对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本 <br>                                    xnew=x+rand(0,1)∗|x−xn| <br> <img src="https://images2.imgbox.com/97/19/PyLdxSLS_o.png" alt="这里写图片描述" title=""></p> 
<p><strong>smote算法的伪代码如下：</strong> <br> <img src="https://images2.imgbox.com/bc/1d/hpRuOrDf_o.png" alt="这里写图片描述" title=""></p> 
<p>       因此，smote算法的思想是合成新的少数类样本，合成的策略是对每个少数类样本a，从它的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 </p> 
<p>  <strong>下面具体介绍如何合成新的样本。</strong> <br>    <br>        设训练集的一个少数类的样本数为 T ，那么SMOTE算法将为这个少数类合成 NT 个新样本。这里要求 N 必须是正整数，如果给定的 N&lt;1 那么算法将“认为”少数类的样本数 T=NT ，并将强制 N=1 。 考虑该少数类的一个样本 ii ，其特征向量为 xi,i∈{1,…,T}： </p> 
<ol><li>首先从该少数类的全部 T 个样本中找到样本 xi 的 k个近邻（例如用欧氏距离），记为 xi(near),near∈{1,…,k}； </li><li>然后从这 k 个近邻中随机选择一个样本 xi(nn) ，再生成一个0 到 1 之间的随机数 ζ1 ，从而合成一个新样本 xi1 ：</li></ol> 
<p>                                                        xi1=xi+ζ1⋅(xi(nn)−xi) <br>      3. 将步骤2重复进行 N 次，从而可以合成 NN 个新样本：xinew,new∈1,…,N。 </p> 
<p>       那么，对全部的 T 个少数类样本进行上述操作，便可为该少数类合成 NT个新样本。</p> 
<p>       如果样本的特征维数是 2维，那么每个样本都可以用二维平面上的一个点来表示。SMOTE算法所合成出的一个新样本 xi1 相当于是表示样本 xi的点和表示样本 xi(nn) 的点之间所连线段上的一个点。所以说该算法是基于“插值”来合成新样本。</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#SMOTE算法及其python实现</span>
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> NearestNeighbors
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Smote</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,samples,N=<span class="hljs-number">10</span>,k=<span class="hljs-number">5</span>)</span>:</span>
        self.n_samples,self.n_attrs=samples.shape
        self.N=N
        self.k=k
        self.samples=samples
        self.newindex=<span class="hljs-number">0</span>
       <span class="hljs-comment"># self.synthetic=np.zeros((self.n_samples*N,self.n_attrs))</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">over_sampling</span><span class="hljs-params">(self)</span>:</span>
        N=int(self.N/<span class="hljs-number">100</span>)
        self.synthetic = np.zeros((self.n_samples * N, self.n_attrs))
        neighbors=NearestNeighbors(n_neighbors=self.k).fit(self.samples)
        <span class="hljs-keyword">print</span> <span class="hljs-string">'neighbors'</span>,neighbors
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(self.samples)):
            nnarray=neighbors.kneighbors(self.samples[i].reshape(<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>),return_distance=<span class="hljs-keyword">False</span>)[<span class="hljs-number">0</span>]
            <span class="hljs-comment">#print nnarray</span>
            self._populate(N,i,nnarray)
        <span class="hljs-keyword">return</span> self.synthetic


    <span class="hljs-comment"># for each minority class samples,choose N of the k nearest neighbors and generate N synthetic samples.</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_populate</span><span class="hljs-params">(self,N,i,nnarray)</span>:</span>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(N):
            nn=random.randint(<span class="hljs-number">0</span>,self.k-<span class="hljs-number">1</span>)
            dif=self.samples[nnarray[nn]]-self.samples[i]
            gap=random.random()
            self.synthetic[self.newindex]=self.samples[i]+gap*dif
            self.newindex+=<span class="hljs-number">1</span>
a=np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
s=Smote(a,N=<span class="hljs-number">100</span>)
<span class="hljs-keyword">print</span> s.over_sampling()</code></pre> 
<h2 id="smote算法的缺陷">SMOTE算法的缺陷</h2> 
<p>       该算法主要存在两方面的问题:一是在近邻选择时,存在一定的盲目性。从上面的算法流程可以看出,在算法执行过程中,需要确定K值,即选择多少个近邻样本,这需要用户自行解决。从K值的定义可以看出,K值的下限是M值(M值为从K个近邻中随机挑选出的近邻样本的个数,且有M&lt; K),M的大小可以根据负类样本数量、正类样本数量和数据集最后需要达到的平衡率决定。但K值的上限没有办法确定,只能根据具体的数据集去反复测试。因此如何确定K值,才能使算法达到最优这是未知的。 <br>        另外,该算法无法克服非平衡数据集的数据分布问题,容易产生分布边缘化问题。由于负类样本的分布决定了其可选择的近邻,如果一个负类样本处在负类样本集的分布边缘,则由此负类样本和相邻样本产生的“人造”样本也会处在这个边缘,且会越来越边缘化,从而模糊了正类样本和负类样本的边界,而且使边界变得越来越模糊。这种边界模糊性,虽然使数据集的平衡性得到了改善,但加大了分类算法进行分类的难度．</p> 
<h2 id="针对smote算法的进一步改进">针对SMOTE算法的进一步改进</h2> 
<p>       针对SMOTE算法存在的边缘化和盲目性等问题,很多人纷纷提出了新的改进办法,在一定程度上改进了算法的性能,但还存在许多需要解决的问题。</p> 
<p>       Han等人<a href="http://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf" rel="nofollow">Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning </a>在SMOTE算法基础上进行了改进,提出了Borderhne.SMOTE算法,解决了生成样本重叠(Overlapping)的问题该算法在运行的过程中,查找一个适当的区域,该区域可以较好地反应数据集的性质,然后在该区域内进行插值,以使新增加的“人造”样本更有效。这个适当的区域一般由经验给定,因此算法在执行的过程中有一定的局限性。</p> 
<h2 id="进一步阅读">进一步阅读</h2> 
<p>有两篇翻译自国外博客的文章：</p> 
<p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650718717&amp;idx=1&amp;sn=85038d7c906c135120a8e1a2f7e565ad&amp;scene=0#wechat_redirect" rel="nofollow">解决真实世界问题：如何在不平衡类上使用机器学习?</a></p> 
<p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650724464&amp;idx=1&amp;sn=1f34358862bacfb4c7ea17c864d8c44d&amp;chksm=871b1c0eb06c95180e717d8316b0380602f638a764530b4b9e35ac812c7c33799d3357d46f00&amp;scene=0&amp;key=0f5e635eeb6bf20a076ad60d7f11c6ef5c5c1c8f02873bc8b458381b629a1e2ae76174d0d4ba34331c71d095e3b3b92aa7fff5e1e11badeaf6c87ff90fd264f3dc6b1eb074eaccb2ac46e8f2d440cefd&amp;ascene=0&amp;uin=MTU1NTY3MTA0Mg==&amp;devicetype=iMac%20MacBookPro12,1%20OSX%20OSX%2010.11.6%20build%2815G1217%29&amp;version=12010310&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=csWk%2bJXfpl7rA8r527fLqF%2bF3EZEeBKpFRjI%2bWMXoPf2PEtPt/LMrscLX4GBl7gg" rel="nofollow">从重采样到数据合成：如何处理机器学习中的不平衡分类问题？</a></p> 
<p>smote算法的论文地址：<a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" rel="nofollow">https://www.jair.org/media/953/live-953-2037-jair.pdf</a></p> 
<p>SMOTE相关论文: <a href="http://blog.csdn.net/yaphat/article/details/60347968">http://blog.csdn.net/yaphat/article/details/60347968</a></p> 
<h2 id="本文参考">本文参考：</h2> 
<p><a href="https://www.cnblogs.com/Determined22/p/5772538.html" rel="nofollow">https://www.cnblogs.com/Determined22/p/5772538.html</a></p> 
<p><a href="http://blog.csdn.net/Yaphat/article/details/52463304?locationNum=7">http://blog.csdn.net/Yaphat/article/details/52463304?locationNum=7</a></p> 
<p>《机器学习》，周志华</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b64239398c03b2e96fefe959525b52d7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vi的使用与基本命令</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/701b3a373fa069563bd80f1db45a65f5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">git 首次切换到已经存在的分支</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>