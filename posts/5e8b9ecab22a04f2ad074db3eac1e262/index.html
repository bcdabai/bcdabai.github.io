<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>transformer与视觉 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="transformer与视觉" />
<meta property="og:description" content="目录 综述优秀网文基本transformer视觉transformer原理具体的transformer一般方法ViT ：一张图等于 16x16 个字，计算机视觉也用上 Transformer 了DeiT：[Facebook开源高效图像Transformer，速度、准确率与泛化性能媲美SOTA CNN](https://mp.weixin.qq.com/s/FmlCX1okXxOiJeBPQ6wuOQ)T2T ViT：Tokens-to-Token ViT: Training Vision Transformers from Scratch on Imagenet (图像patch之间有部分重叠)DeepViT: Towards Deeper Vision Transformer(多个注意力头之间重新生成注意力，增加transformer深度) transformer与CNN结合Bottleneck Transformers for Visual Recognition：resnet的卷积用多头注意力模块替代PiT: Rethinking Spatial Dimensions of Vision Transformers（ 结合池化层的视觉Transformer网络）.Scalable Visual Transformers with Hierarchical PoolingCvT: Introducing Convolutions to Vision TransformersCeiT: Incorporating Convolution Designs into Visual TransformersConViT: Improving Vision Transformers with Soft Convolutional Inductive BiasesTransFuse：融合Transformers和CNN用于医学图像分割CoTr：基于CNN和Transformer进行3D医学图像分割 多尺度，高分辨率PVT：Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without ConvolutionsDPT: Vision Transformers for Dense PredictionCrossViT: Cross-Attention Multi-Scale Vision Transformer for Image ClassificationMulti-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image EncodingSwin Transformer: Hierarchical Vision Transformer using Shifted WindowsM2TR: multi-modal multi-scale transformers for deepfake detection (多模多尺度) Transformer内部机制的探究充分挖掘patch内部信息：Transformer in Transformer：TNT探究位置编码的必要性：Do We Really Need Explicit Position Encodings for Vision Transformers?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5e8b9ecab22a04f2ad074db3eac1e262/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-11T21:22:04+08:00" />
<meta property="article:modified_time" content="2022-12-11T21:22:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">transformer与视觉</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">综述</a></li><li><a href="#_8" rel="nofollow">优秀网文</a></li><li><a href="#transformer_9" rel="nofollow">基本transformer</a></li><li><a href="#transformer_16" rel="nofollow">视觉transformer原理</a></li><li><a href="#transformer_25" rel="nofollow">具体的transformer</a></li><li><ul><li><a href="#_26" rel="nofollow">一般方法</a></li><li><ul><li><a href="#ViT__16x16__Transformer__27" rel="nofollow">ViT ：一张图等于 16x16 个字，计算机视觉也用上 Transformer 了</a></li><li><a href="#DeiTFacebookTransformerSOTA_CNNhttpsmpweixinqqcomsFmlCX1okXxOiJeBPQ6wuOQ_33" rel="nofollow">DeiT：[Facebook开源高效图像Transformer，速度、准确率与泛化性能媲美SOTA CNN](https://mp.weixin.qq.com/s/FmlCX1okXxOiJeBPQ6wuOQ)</a></li><li><a href="#T2T_ViTTokenstoToken_ViT_Training_Vision_Transformers_from_Scratch_on_Imagenet_patch_42" rel="nofollow">T2T ViT：Tokens-to-Token ViT: Training Vision Transformers from Scratch on Imagenet (图像patch之间有部分重叠)</a></li><li><a href="#DeepViT_Towards_Deeper_Vision_Transformertransformer_58" rel="nofollow">DeepViT: Towards Deeper Vision Transformer(多个注意力头之间重新生成注意力，增加transformer深度)</a></li></ul> 
   </li><li><a href="#transformerCNN_68" rel="nofollow">transformer与CNN结合</a></li><li><ul><li><a href="#Bottleneck_Transformers_for_Visual_Recognitionresnet_69" rel="nofollow">Bottleneck Transformers for Visual Recognition：resnet的卷积用多头注意力模块替代</a></li><li><a href="#PiT___Rethinking_Spatial_Dimensions_of_Vision_Transformers_Transformer_80" rel="nofollow">PiT: Rethinking Spatial Dimensions of Vision Transformers（ 结合池化层的视觉Transformer网络）.</a></li><li><a href="#Scalable_Visual_Transformers_with_Hierarchical_Pooling_86" rel="nofollow">Scalable Visual Transformers with Hierarchical Pooling</a></li><li><a href="#CvT_Introducing_Convolutions_to_Vision_Transformers_91" rel="nofollow">CvT: Introducing Convolutions to Vision Transformers</a></li><li><a href="#CeiT_Incorporating_Convolution_Designs_into_Visual_Transformers_98" rel="nofollow">CeiT: Incorporating Convolution Designs into Visual Transformers</a></li><li><a href="#ConViT_Improving_Vision_Transformers_with_Soft_Convolutional_Inductive_Biases_106" rel="nofollow">ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</a></li><li><a href="#TransFuseTransformersCNNhttpszhuanlanzhihucomp351064594_116" rel="nofollow">TransFuse：融合Transformers和CNN用于医学图像分割</a></li><li><a href="#CoTrCNNTransformer3Dhttpszhuanlanzhihucomp354761580_122" rel="nofollow">CoTr：基于CNN和Transformer进行3D医学图像分割</a></li></ul> 
   </li><li><a href="#_129" rel="nofollow">多尺度，高分辨率</a></li><li><ul><li><a href="#PVTPyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_without_Convolutions_130" rel="nofollow">PVT：Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</a></li><li><a href="#DPT_Vision_Transformers_for_Dense_Prediction_144" rel="nofollow">DPT: Vision Transformers for Dense Prediction</a></li><li><a href="#CrossViT_CrossAttention_MultiScale_Vision_Transformer_for_Image_Classification_152" rel="nofollow">CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</a></li><li><a href="#MultiScale_Vision_Longformer_A_New_Vision_Transformer_for_HighResolution_Image_Encoding_157" rel="nofollow">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</a></li><li><a href="#Swin_Transformer_Hierarchical_Vision_Transformer_using_Shifted_Windows_164" rel="nofollow">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></li><li><a href="#M2TR_multimodal_multiscale_transformers_for_deepfake_detection__177" rel="nofollow">M2TR: multi-modal multi-scale transformers for deepfake detection (多模多尺度)</a></li></ul> 
   </li><li><a href="#Transformer_196" rel="nofollow">Transformer内部机制的探究</a></li><li><ul><li><a href="#patchTransformer_in_TransformerTNT_197" rel="nofollow">充分挖掘patch内部信息：Transformer in Transformer：TNT</a></li><li><a href="#Do_We_Really_Need_Explicit_Position_Encodings_for_Vision_Transformers_206" rel="nofollow">探究位置编码的必要性：Do We Really Need Explicit Position Encodings for Vision Transformers?</a></li><li><a href="#LocalViTBringing_Locality_to_Vision_Transformers_209" rel="nofollow">LocalViT---Bringing Locality to Vision Transformers（引入局部性）</a></li><li><a href="#Transformer_217" rel="nofollow">Transformer升级之路：博采众长的旋转式位置编码</a></li></ul> 
   </li><li><a href="#_222" rel="nofollow">预训练</a></li><li><a href="#_226" rel="nofollow">统一语言与视觉</a></li><li><ul><li><a href="#Facebook_AITransformer_227" rel="nofollow">来自Facebook AI的多任务多模态的统一Transformer：向更通用的智能迈出了一步</a></li></ul> 
   </li><li><a href="#_232" rel="nofollow">应用</a></li><li><ul><li><a href="#Lifting_Transformer_Transformer_233" rel="nofollow">Lifting Transformer: 基于跨步卷积Transformer的高效三维人体姿态估计</a></li><li><a href="#SpecTr_Spectral_Transformer_for_Hyperspectral_Pathology_Image_Segmentation_239" rel="nofollow">SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation</a></li><li><a href="#_244" rel="nofollow">其它</a></li></ul> 
  </li></ul> 
  </li><li><a href="#transformer_292" rel="nofollow">高效transformer</a></li><li><a href="#_297" rel="nofollow">设计</a></li><li><a href="#_300" rel="nofollow">源代码</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>综述</h2> 
<p>论文与代码的大合集：<a href="https://github.com/dk-liang/Awesome-Visual-Transformer">Awesome Visual-Transformer</a><br> <a href="https://zhuanlan.zhihu.com/p/348593638" rel="nofollow">Vision Transformer 超详细解读 (原理分析+代码解读) (目录)</a></p> 
<p><a href="https://mp.weixin.qq.com/s/N2PAgp-epq4i9CLll1nzJA" rel="nofollow">华为联合北大、悉尼大学对 Visual Transformer 的最新综述</a><br> 又一篇视觉Transformer综述来了！ <a href="https://arxiv.org/abs/2101.01169" rel="nofollow">Transformers in Vision: A Survey</a></p> 
<h2><a id="_8"></a>优秀网文</h2> 
<h2><a id="transformer_9"></a>基本transformer</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/348593638" rel="nofollow">Vision Transformer , 通用 Vision Backbone 超详细解读 (原理分析+代码解读) (目录)</a>（xys：有各种类型的视觉transformer，长系列的持续改进）</p> 
<p><a href="https://blog.csdn.net/lgzlgz3102/article/details/108989314">Transformer 超详细解读，一图胜千言</a><br> <a href="https://mp.weixin.qq.com/s/k2vP7ZDgn9pE4_s5iIhwYg" rel="nofollow">这么多年，终于有人讲清楚 Transformer 了！</a><br> <a href="https://mp.weixin.qq.com/s/BIt7J4Y60PmBzNLYPNZJ8g" rel="nofollow">硬核 | 9 种Transformer结构详细解读</a></p> 
<h2><a id="transformer_16"></a>视觉transformer原理</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/308301901" rel="nofollow">3W字长文带你轻松入门视觉transformer</a><br> <a href="https://mp.weixin.qq.com/s/j3bhzY2aUWhFwMAgRTgotg" rel="nofollow">搞懂 Vision Transformer 原理和代码，看这篇技术综述就够了</a><br> <a href="https://mp.weixin.qq.com/s/cylzbtRJ9uXJVaE5n3qlug" rel="nofollow">搞懂 Vision Transformer 原理和代码，看这篇技术综述就够了（二）</a></p> 
<p><a href="https://blog.csdn.net/weixin_44966641/article/details/118733341">Vision Transformer（ViT）PyTorch代码全解析（附图解）</a></p> 
<h2><a id="transformer_25"></a>具体的transformer</h2> 
<h3><a id="_26"></a>一般方法</h3> 
<h4><a id="ViT__16x16__Transformer__27"></a>ViT ：一张图等于 16x16 个字，计算机视觉也用上 Transformer 了</h4> 
<p>源码1、(https://github.com/lucidrains/vit-pytorch)（有Vit及其很多改进版本的源码）<br> 源码2、https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py （.py文件内有预训练模型的地址）<br> 网文：<br> 1、<a href="https://blog.csdn.net/lgzlgz3102/article/details/109140622">一张图等于 16x16 个字，计算机视觉也用上 Transformer 了</a></p> 
<h4><a id="DeiTFacebookTransformerSOTA_CNNhttpsmpweixinqqcomsFmlCX1okXxOiJeBPQ6wuOQ_33"></a>DeiT：<a href="https://mp.weixin.qq.com/s/FmlCX1okXxOiJeBPQ6wuOQ" rel="nofollow">Facebook开源高效图像Transformer，速度、准确率与泛化性能媲美SOTA CNN</a></h4> 
<p>论文地址：https://arxiv.org/pdf/2012.12877.pdf<br> GitHub 地址：https://github.com/facebookresearch/deit（有预训练模型）</p> 
<p>Facebook 提出了一项新技术 Data-efficient image Transformers（DeiT），需要更少的数据和更少的计算资源就能生成高性能的图像分类模型。研究人员仅用一台 8-GPU 的服务器对 DeiT 模型进行 3 天训练，该方法就在 ImageNet 基准测试中达到了 84.2% 的 top-1 准确率，并且训练阶段未使用任何外部数据，该结果可以与顶尖的卷积神经网络（CNN）媲美。</p> 
<p>DeiT 首个重要的组件是其训练策略。研究者在最初用于卷积神经网络的现有研究基础上进行了调整与改进，并提出了一种基于蒸馏 token 的新型蒸馏流程，它的作用与 class token 相同，不过其目的在于复制教师网络估计的标签。实验结果表明，这种特定 transformer 策略大幅度优于 vanilla 蒸馏方法。</p> 
<h4><a id="T2T_ViTTokenstoToken_ViT_Training_Vision_Transformers_from_Scratch_on_Imagenet_patch_42"></a>T2T ViT：Tokens-to-Token ViT: Training Vision Transformers from Scratch on Imagenet (图像patch之间有部分重叠)</h4> 
<p>论文：Tokens-to-Token ViT- Training Vision Transformers from Scratch on ImageNet<br> <a href="https://github.com/yitu-opensource/T2T-ViT">源码（官方pytorch，带预训练模型）</a><br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/359930253" rel="nofollow">Tokens-to-Token ViT: Training Vision Transformers from Scratch on Imagenet 学习笔记</a><br> 2、 <a href="https://zhuanlan.zhihu.com/p/354522966" rel="nofollow">Tokens-to-Token ViT:真正意义上击败了CNN</a></p> 
<p>依图科技出品<br> 前面提到过ViT，但是ViT在数据量不够巨大的情况下是打不过ResNet的。于是ViT的升级版T2T-ViT横空出世了，速度更快性能更强。T2T-ViT相比于ViT，参数量和MACs(Multi-Adds)减少了200%，性能在ImageNet上又有2.5%的提升。T2T-ViT在和ResNet50模型大小差不多的情况下，在ImageNet上达到了80.7%的准确率。论文的贡献：</p> 
<ul><li>证明了通过精心设计的Transformer-based的网络(T2T module and efficient backbone)，是可以打败CNN-based的模型的，而且不需要在巨型的训练集(如JFT-300M)上预训练。</li><li>提出了一种可以编码局部信息的结构T2T module，并证明了T2T的有效性（图像patch之间有部分重叠）。</li><li>展示了在设计CNNs backbone时用到的architecture engineering经验同样适用于设计Transformer-based的模型，通过大量的实验证明深且窄(deep-narrow)的网络能够增加feature的丰富性和减少冗余。</li></ul> 
<h4><a id="DeepViT_Towards_Deeper_Vision_Transformertransformer_58"></a>DeepViT: Towards Deeper Vision Transformer(多个注意力头之间重新生成注意力，增加transformer深度)</h4> 
<p>论文：https://arxiv.org/abs/2103.11886<br> 源码：https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/deepvit.py<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/359191305" rel="nofollow">DeepViT：迈向更深的视觉Transformer</a><br> 随着Transformers变深，注意力图在某些层之后逐渐变得相似甚至几乎相同。换句话说，在深度ViT模型的顶层中，特征图趋于相同。这一事实表明，在更深层次的ViT中，自注意力机制无法学习有效的表示学习概念，并且阻碍了模型获得预期的性能提升。</p> 
<p>基于以上观察，我们提出了一种简单而有效的方法，称为Re-attention，以可忽略的计算和存储成本重新生成注意图以增加其在不同层的多样性。<br> <img src="https://images2.imgbox.com/d0/a6/RXFLgh7c_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="transformerCNN_68"></a>transformer与CNN结合</h3> 
<h4><a id="Bottleneck_Transformers_for_Visual_Recognitionresnet_69"></a>Bottleneck Transformers for Visual Recognition：resnet的卷积用多头注意力模块替代</h4> 
<p>论文：https://arxiv.org/abs/2101.11605<br> <a href="https://github.com/leaderj1001/BottleneckTransformers">源码1</a>，<a href="https://github.com/CandiceD17/Bottleneck-Transformers-for-Visual-Recognition">源码2</a>—暂时没有预训练模型<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/347849929" rel="nofollow">CNN与Transformer的强强联合！谷歌开源BoTNet，ImageNet达84.7%准确率</a><br> 2、<a href="https://zhuanlan.zhihu.com/p/347742822" rel="nofollow">CNN+Transformer！谷歌提出BoTNet：新主干网络！在ImageNet上达84.7%</a></p> 
<p>BoTNet：一种简单却功能强大的backbone，该架构将自注意力纳入了多种计算机视觉任务，包括图像分类，目标检测和实例分割。该方法在实例分割和目标检测方面显著改善了基线，同时还减少了参数，从而使延迟最小化。</p> 
<p>通过仅在ResNet中，用Multi-Head Self-Attention (MHSA)来替换3 × 3 convolution，并且不进行其他任何更改（如图1所示）。<br> <img src="https://images2.imgbox.com/3a/39/6g2AWjFD_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="PiT___Rethinking_Spatial_Dimensions_of_Vision_Transformers_Transformer_80"></a>PiT: Rethinking Spatial Dimensions of Vision Transformers（ 结合池化层的视觉Transformer网络）.</h4> 
<p>论文：http://arxiv.org/pdf/2103.16302v1<br> 源码：https://github.com/naver-ai/pit（有预训练模型）<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/361442695" rel="nofollow">加入池化！PiT：重新思考视觉Transformer的空间尺寸</a></p> 
<h4><a id="Scalable_Visual_Transformers_with_Hierarchical_Pooling_86"></a>Scalable Visual Transformers with Hierarchical Pooling</h4> 
<p>a Hierarchical Vision Transformer which progressively pools visual tokens to reduce computational costs.<br> 论文：https://arxiv.org/abs/2103.10619</p> 
<h4><a id="CvT_Introducing_Convolutions_to_Vision_Transformers_91"></a>CvT: Introducing Convolutions to Vision Transformers</h4> 
<p>introduces convolutions for ViT to get the best of both worlds.<br> 论文：https://arxiv.org/abs/2103.15808<br> 源码：https://github.com/rishikksh20/convolution-vision-transformers<br> 网文：<br> <a href="https://zhuanlan.zhihu.com/p/362938251" rel="nofollow">CNN+Transformer = CvT | ImageNet Top-1 精度突破 87.7%</a></p> 
<h4><a id="CeiT_Incorporating_Convolution_Designs_into_Visual_Transformers_98"></a>CeiT: Incorporating Convolution Designs into Visual Transformers</h4> 
<p>modifies the Transformer to introduce CNN-like modules that are better at modellng locality.<br> 论文：https://arxiv.org/abs/2103.11816<br> 源码：<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/359671238" rel="nofollow">商汤提出CeiT：将卷积设计整合到视觉Transformers中</a><br> 本文提出一个新的卷积增强图像Transformer（CeiT），它结合了CNN在提取low-level特征，增强局部性以及Transformer在建立远程依赖项的优势</p> 
<h4><a id="ConViT_Improving_Vision_Transformers_with_Soft_Convolutional_Inductive_Biases_106"></a>ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</h4> 
<p>论文：https://arxiv.org/abs/2103.10697<br> <a href="https://zhuanlan.zhihu.com/p/358902555" rel="nofollow">[论文阅读]ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases</a><br> 我们创造了GPSA(gated positional self-attention),将CNN的归纳偏置带到transformer中</p> 
<p>我们的工作分为以下三步：</p> 
<ul><li>引入了一个新的self-attention层叫做gated positional self-attention(GPSA),其可以像卷积层一样被初始化,每个attention head有一个gating参数来调整表达性，可以在focus on局部特征和恢复全局特征间调节</li><li>基于DeiT进行了实验，将其的一部分SA(self-attention)层替换为了GPSA层,结果模型在样本利用率和表现上超过了DeiT</li><li>定量研究了ViT中attention的local特性，使用消融实验等实验研究了ConViT work的深层原因</li></ul> 
<h4><a id="TransFuseTransformersCNNhttpszhuanlanzhihucomp351064594_116"></a><a href="https://zhuanlan.zhihu.com/p/351064594" rel="nofollow">TransFuse：融合Transformers和CNN用于医学图像分割</a></h4> 
<p>论文：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2102.08005" rel="nofollow">TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation</a></p> 
<p>使用TransFuse，可以以较浅的方式有效地捕获全局依赖性和low-level空间细节。<br> 此外，提出了一种新颖的融合技术-BiFusion模块，用于融合每个分支的多级特征。</p> 
<h4><a id="CoTrCNNTransformer3Dhttpszhuanlanzhihucomp354761580_122"></a><a href="https://zhuanlan.zhihu.com/p/354761580" rel="nofollow">CoTr：基于CNN和Transformer进行3D医学图像分割</a></h4> 
<p>论文：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2103.03024" rel="nofollow">CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation</a></p> 
<p>卷积运算不可避免地在建模远程依赖项方面存在局限性，这是由于它们的局部性和权重归纳性偏差。尽管Transformer就是为解决这个问题而诞生的，但是在处理高分辨率3D特征图时，它遭受了极大的计算和空间复杂性的困扰。<br> 在本文中，我们提出了一个新颖的框架，该框架可以有效地桥接卷积神经网络和Transformer，以进行精确的3D医学图像分割。在此框架下，构建CNN来提取特征表示，并构建有效的可变形Transformer（DeTrans）来建模对提取的特征图的远程依赖性。与原Transformer（均等地对待所有图像位置）不同，我们的DeTrans通过引入可变形的自注意力机制，仅关注少数关键位置。<br> 因此，大大降低了DeTrans的计算和空间复杂度，从而可以处理通常对于图像分割至关重要的多尺度和高分辨率特征图。</p> 
<h3><a id="_129"></a>多尺度，高分辨率</h3> 
<h4><a id="PVTPyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_without_Convolutions_130"></a>PVT：Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h4> 
<p>论文：https://arxiv.org/abs/2102.12122v1<br> 源码:https://github.com/whai362/PVT(带预训练模型)<br> 网文：<br> 1、<a href="https://mp.weixin.qq.com/s/oJHZWmStQYYzEEOveiuQrQ" rel="nofollow">大白话Pyramid Vision Transformer</a><br> 2、<a href="https://www.jianshu.com/p/d2a878723af4" rel="nofollow">阅读笔记-PVT-Pyramid Vision Transformer_A versatile backbone for dense prediction without convolutions</a></p> 
<p>文章内容用一句话概括就是给ViT方法装上金字塔结构处理密集预测问题。主要创新点包括两点：1. progressive shrinking strategy 能够实现金字塔结构；2. spatial reduction attention减少self-attention的计算量。</p> 
<p>本文方法相对于传统CNN的优势：传统CNN通过层数增加来增加感受野，但相对来说还是局部信息，而transformer机制刻画的是全局的关联关系。<br> 相对于ViT这类方法的优势：ViT一般而言通过将图像划分成不同的patch之后，每个patch提取特征，在后面的若干层transformer layer中还是针对于相同patch区域的特征，两方面劣势1）划分的patch较粗糙难以应用到dense prediction任务中；2）没有金字塔结构对不同尺寸目标同等对待。而本文的方式通过金字塔结构在开始层patch划分相对精细的多，其次通过金字塔的构造很容易作为backbone结构嵌入到已有的结构，比如替换resnet backbone等。<br> <img src="https://images2.imgbox.com/53/97/A3zVC0fJ_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="DPT_Vision_Transformers_for_Dense_Prediction_144"></a>DPT: Vision Transformers for Dense Prediction</h4> 
<p>论文：https://arxiv.org/abs/2103.13413<br> 源码： https://github.com/intel-isl/DPT（有预训练模型）<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/360158974" rel="nofollow">英特尔提出DPT：基于视觉Transformer的密集预测</a></p> 
<p>我们将视觉Transformer各个阶段的token组装成各种分辨率的图像表示形式，并使用卷积解码器将它们逐步组合为全分辨率预测。</p> 
<h4><a id="CrossViT_CrossAttention_MultiScale_Vision_Transformer_for_Image_Classification_152"></a>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</h4> 
<p>论文：https://arxiv.org/abs/2103.14899<br> 源码：https://github.com/rishikksh20/CrossViT-pytorch（无预训练模型）<br> 网文：</p> 
<h4><a id="MultiScale_Vision_Longformer_A_New_Vision_Transformer_for_HighResolution_Image_Encoding_157"></a>Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</h4> 
<p>用于高分辨率图像编码，代码将开源<br> 论文：https://arxiv.org/abs/2103.15358<br> 源码：<br> 网文：</p> 
<h4><a id="Swin_Transformer_Hierarchical_Vision_Transformer_using_Shifted_Windows_164"></a>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</h4> 
<p>论文：https://arxiv.org/pdf/2103.14030v1.pdf<br> 源码：https://github.com/microsoft/Swin-Transformer(预训练模型)<br> 网文：<br> <a href="https://blog.csdn.net/Q1u1NG/article/details/115330307">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows论文阅读</a><br> <a href="https://zhuanlan.zhihu.com/p/362690149" rel="nofollow">Swin Transformer: 用CNN的方式打败CNN</a>(描述了shift windows是怎么回事)<br> <a href="https://zhuanlan.zhihu.com/p/361366090" rel="nofollow">CV+Transformer之Swin Transformer</a>（对照代码讲解）</p> 
<p>作者在Transformer基础上做了改进，提出了Swin Transformer，目的是解决上面提到的两个问题：</p> 
<ul><li>Swin Transformer可以产生层级feature map，能够更好的建模不同尺寸的物体</li><li>Swin Transformer在self-attention阶段具有线性复杂度</li></ul> 
<h4><a id="M2TR_multimodal_multiscale_transformers_for_deepfake_detection__177"></a>M2TR: multi-modal multi-scale transformers for deepfake detection (多模多尺度)</h4> 
<p>论文：https://arxiv.org/pdf/2104.09770.pdf</p> 
<p>网文：<br> <a href="https://mp.weixin.qq.com/s/6hZ13fjT_Bk7f_9TxHzvEQ" rel="nofollow">M2TR: 复旦提出首个多模态多尺度Transformer</a></p> 
<p>我们首先利用EfficientNet作为backbone提取了网络的浅层特征，然后利用Multi-scale Transformer进行多尺度特征提取和Cross Modality Fusion进行双模态融合。<br> 区别于ViT的地方有三点：<br> 1.不是在图像取patch而是在feature map上<br> 2.<strong>不同head里取的patch size不同</strong><br> 3.patch被flatten成一维经过了self-attention之后会被重新reassemble成二维得到3D feature map</p> 
<blockquote> 
 <p>Cross Modality Fusion<br> 首先我们对图像做<strong>DCT变换，得到了频率分布图</strong>，DCT变换具有良好的性质：高频集中在左上角而低频在右下角，因此我们手动对其进行划分，得到了高、中、低三个频段的频率分量，但是频率图不具有RGB图像的视觉特征，也没有办法利用CNN进行特征提取，为此我们<strong>对三个分量进行逆DCT变换，重新得到了RGB域的表示，但是这个表示是frequency-aware的</strong>，然后利用卷积层进行特征提取。<br> 双模态融合（<strong>本文指的是：RGB与频域的融合</strong>）是一个长久的问题，如何良好的嵌入到transformer更是关键，一种最简单的做法、也是现在很多利用频率信息的做法就是对RGB和Frequency采用一种two-stream的双流结结构，平行操作然后在中间或者输出的地方进行融合，这种做法我不是非常赞同，因为并行结构的一个重要前提是两个模态的地位或者信息量是均衡的，但是对于Deepfake来说，频率的信息其实还是具有噪声的，比如人脸的一些部分如头发等在频率也可能集中在高频区域里，因此合适的做法是把频域作为一种辅助模态。<br> 基于这种想法、同时受启发于transformer的query-key-value的设计，我们提出了一种QKV的方法来融合两个模态：把RGB模态作为query、frequency模态作为memory，这样的设计一方面能突出RGB的作用，另一方面能更好地发掘frequency分布里的异常区域。</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/c1/e8/OWlThUuY_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="Transformer_196"></a>Transformer内部机制的探究</h3> 
<h4><a id="patchTransformer_in_TransformerTNT_197"></a>充分挖掘patch内部信息：Transformer in Transformer：TNT</h4> 
<p>(来自北京华为诺亚方舟实验室)<br> 论文：https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2103.00112</p> 
<p>源码：https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/tnt.py（有预训练模型，下载路径见 tnt.py文件里边）<br> https://github.com/lucidrains/transformer-in-transformer<br> 网文：<br> 1、<a href="https://zhuanlan.zhihu.com/p/354683871" rel="nofollow">TNT | 致敬Network in Network，华为诺亚提出Transformer-in-Transformer</a></p> 
<h4><a id="Do_We_Really_Need_Explicit_Position_Encodings_for_Vision_Transformers_206"></a>探究位置编码的必要性：Do We Really Need Explicit Position Encodings for Vision Transformers?</h4> 
<p>（美团）</p> 
<h4><a id="LocalViTBringing_Locality_to_Vision_Transformers_209"></a>LocalViT—Bringing Locality to Vision Transformers（引入局部性）</h4> 
<p>论文： LocalViT: Bringing Locality to Vision Transformers<br> 源码：https://github.com/ofsoundof/LocalViT<br> 网文： <a href="https://zhuanlan.zhihu.com/p/364442111" rel="nofollow">CNN再助力！LocalViT：将Locality带入视觉Transformer</a></p> 
<p>本文引入depth-wise卷积，给视觉Transformer带来locality机制，经过增强的Transformer网络性能涨点明显，表现SOTA！性能优于TNT、PVT、DeiT等网络，代码即将开源！</p> 
<h4><a id="Transformer_217"></a>Transformer升级之路：博采众长的旋转式位置编码</h4> 
<p>RoFormer：https://github.com/ZhuiyiTechnology/roformer<br> 网文：<br> <a href="https://mp.weixin.qq.com/s/NN5skAwIhuwIMgSbsu6Guw" rel="nofollow">Transformer升级之路：博采众长的旋转式位置编码</a></p> 
<h3><a id="_222"></a>预训练</h3> 
<p><a href="https://arxiv.org/abs/2012.00364" rel="nofollow">Pre-Trained Image Processing Transformer（CVPR）</a><br> <a href="https://arxiv.org/abs/2011.09094" rel="nofollow">UP-DETR: Unsupervised Pre-training for Object Detection with Transformers (CVPR)</a></p> 
<h3><a id="_226"></a>统一语言与视觉</h3> 
<h4><a id="Facebook_AITransformer_227"></a>来自Facebook AI的多任务多模态的统一Transformer：向更通用的智能迈出了一步</h4> 
<p>论文链接：https://arxiv.org/pdf/2102.10772.pdf<br> 网文：<br> https://mp.weixin.qq.com/s/Dkdewm4PI72uh8JULdu1ug</p> 
<h3><a id="_232"></a>应用</h3> 
<h4><a id="Lifting_Transformer_Transformer_233"></a>Lifting Transformer: 基于跨步卷积Transformer的高效三维人体姿态估计</h4> 
<p>论文：Lifting Transformer for 3D Human Pose Estimation in Video<br> 地址：https://arxiv.org/pdf/2103.14304.pdf<br> 网文：<br> <a href="https://mp.weixin.qq.com/s/iGLti-h5ndMXaR4aEkw38Q" rel="nofollow">Lifting Transformer: 基于跨步卷积Transformer的高效三维人体姿态估计</a></p> 
<h4><a id="SpecTr_Spectral_Transformer_for_Hyperspectral_Pathology_Image_Segmentation_239"></a>SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation</h4> 
<p>源码：https://github.com/hfut-xc-yun/SpecTr（无需预训练）<br> 网文：<br> <a href="https://zhuanlan.zhihu.com/p/355430881" rel="nofollow">SpecTr：高光谱病理图像分割</a></p> 
<h4><a id="_244"></a>其它</h4> 
<ul><li><a href="https://mp.weixin.qq.com/s/_pS-h3pNpSGSzMUyLgqYMA" rel="nofollow">图像版GPT3问世，一句话就能变成图！打破语言与视觉界线，AI将更加聪明</a></li><li><a href="https://zhuanlan.zhihu.com/p/342759246?utm_source=wechat_session&amp;utm_medium=social&amp;s_r=0" rel="nofollow">Transformer 再下一城，Facebook 等提出多目标跟踪算法 TrackFormer</a></li><li><a href="https://mp.weixin.qq.com/s/h2SP30xMZNzg79iZJS5w4g" rel="nofollow">美团提出具有「位置编码」的Transformer，性能优于ViT和DeiT</a></li></ul> 
<p>1、Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding<br> 用于高分辨率图像编码，代码将开源<br> 微软<br> https://arxiv.org/abs/2103.15358</p> 
<p>2、TFPose: Direct Human Pose Estimation with Transformers<br> 用于人体姿态估计，代码开源：https://github.com/aim-uofa/AdelaiDet/<br> 阿德莱德大学和阿里<br> https://arxiv.org/abs/2103.15320</p> 
<p>3、TransCenter: Transformers with Dense Queries for Multiple-Object Tracking<br> 用于多目标跟踪，代码将开源<br> Inria 和麻省理工学院<br> https://arxiv.org/abs/2103.15145</p> 
<p>4、HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval<br> 用于视频文本检索<br> 北大、FAIR、中科院、快手<br> https://arxiv.org/abs/2103.15049</p> 
<p>6、Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers<br> 已开源：https://github.com/hila-chefer/Transformer-MM-Explainability<br> 以色列特拉维夫大学和 FAIR<br> https://arxiv.org/abs/2103.15679</p> 
<p>7、On the Adversarial Robustness of Visual Transformers<br> 论 Visual Transformers 的对抗鲁棒性<br> 西交、UCLA、京东、IBM<br> https://arxiv.org/abs/2103.15670</p> 
<p>8、Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers<br> 端到端的多目标跟踪，代码将开源<br> 蒙纳士大学、阿德莱德大学、澳大利亚机器人视觉中心<br> https://arxiv.org/abs/2103.14829</p> 
<p>9、Face Transformer for Recognition<br> 用于人脸识别<br> 北邮<br> https://arxiv.org/abs/2103.14803</p> 
<h2><a id="transformer_292"></a>高效transformer</h2> 
<p><a href="https://mp.weixin.qq.com/s/ziY7y9Lcc4jXf08UtJvKsQ" rel="nofollow">线性Attention的探索：Attention必须有个Softmax吗？</a><br> <a href="https://mp.weixin.qq.com/s/q2xbcW9r0rge_8RXabyWvw" rel="nofollow">提升Transformer效率又有新招？基于矩阵分解的线性化Attention方案</a></p> 
<h2><a id="_297"></a>设计</h2> 
<p><a href="https://blog.csdn.net/fengdu78/article/details/113805108">【深度学习】网络架构设计：CNN based和Transformer based</a><br> <a href="https://zhuanlan.zhihu.com/p/344170484" rel="nofollow">计算机视觉中的transformer</a></p> 
<h2><a id="_300"></a>源代码</h2> 
<p><a href="https://mp.weixin.qq.com/s/-qYYbW4DMdpxvQ0YbUwCig" rel="nofollow">用Pytorch轻松实现28个视觉Transformer，开源库 timm 了解一下！（附代码解读，所谓28个是指ViT和DeiT的28个不同配置版本）</a><br> 这个库中的每个模型，基本山都有预训练模型文件。<br> 在线使用方法：timm.create_model(‘mobilenetv3’, pretrained=True)<br> 离线下载方法：打开模型文件本身，如/timm/models/vgg.py，找到default_cfgs即可找到下载路径：<br> <img src="https://images2.imgbox.com/3c/80/dSfxQ3l9_o.png" alt="在这里插入图片描述"></p> 
<p><a href="https://github.com/lucidrains/vit-pytorch">vision transformer 源代码</a><br> <a href="https://github.com/whai362/PVT">Pyramid Vision Transformer源码</a> （有预训练模型）</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e0a32131f38cfbe9f1e4602ce42d525d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">解一元二次方程——Java</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/38cc710860f7a27f6766548540cfb5fb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【开发经验】为什么gateWay网关要用webFlux代替WebMvc</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>