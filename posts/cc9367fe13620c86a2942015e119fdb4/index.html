<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深入理解多层感知机（MLP）：原理与代码解析 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深入理解多层感知机（MLP）：原理与代码解析" />
<meta property="og:description" content="文章目录 1. MLP的原理1.1 结构1.2 激活函数1.3 前向传播1.4 反向传播算法 2.MLP分类任务应用3.参考文献： 多层感知机（MLP）是一种经典的神经网络模型，由多个神经元层组成。它的结构和功能使其成为深度学习中的重要组成部分。MLP在各种任务中表现出色，如图像分类、文本分类、预测和回归等。 1. MLP的原理 1.1 结构 MLP由输入层、隐藏层和输出层组成。输入层接收输入数据，隐藏层通过学习特征表示，输出层产生最终的预测结果。隐藏层和输出层的每个神经元都具有激活函数，用于引入非线性映射。
1.2 激活函数 常用的激活函数包括Sigmoid、ReLU、Tanh等。激活函数的作用是在神经网络中引入非线性性质，使其能够学习复杂的非线性关系。
1.3 前向传播 MLP的前向传播过程即从输入层到输出层的计算过程。它涉及到权重和偏置的计算、激活函数的应用等。通过一个简单的二分类任务为例来演示MLP的前向传播过程。
import numpy as np def sigmoid(x): return 1 / (1 &#43; np.exp(-x)) # MLP的前向传播 def forward_propagation(inputs, weights, biases): hidden_layer = sigmoid(np.dot(inputs, weights[0]) &#43; biases[0]) output_layer = sigmoid(np.dot(hidden_layer, weights[1]) &#43; biases[1]) return output_layer # 输入数据 inputs = np.array([1, 2, 3]) # 权重和偏置 weights = [np.array([[0.2, 0.3, 0.4], [0.3, 0.4, 0.5]]), np." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/cc9367fe13620c86a2942015e119fdb4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-14T11:53:36+08:00" />
<meta property="article:modified_time" content="2023-06-14T11:53:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深入理解多层感知机（MLP）：原理与代码解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#1_MLP_3" rel="nofollow">1. MLP的原理</a></li><li><ul><li><a href="#11__4" rel="nofollow">1.1 结构</a></li><li><a href="#12__8" rel="nofollow">1.2 激活函数</a></li><li><a href="#13__11" rel="nofollow">1.3 前向传播</a></li><li><a href="#14__39" rel="nofollow">1.4 反向传播算法</a></li></ul> 
  </li><li><a href="#2MLP_42" rel="nofollow">2.MLP分类任务应用</a></li><li><a href="#3_102" rel="nofollow">3.参考文献：</a></li></ul> 
</div> 
<br> 多层感知机（MLP）是一种经典的神经网络模型，由多个神经元层组成。它的结构和功能使其成为深度学习中的重要组成部分。MLP在各种任务中表现出色，如图像分类、文本分类、预测和回归等。 
<p></p> 
<h2><a id="1_MLP_3"></a>1. MLP的原理</h2> 
<h3><a id="11__4"></a>1.1 结构</h3> 
<p>MLP由输入层、隐藏层和输出层组成。输入层接收输入数据，隐藏层通过学习特征表示，输出层产生最终的预测结果。隐藏层和输出层的每个神经元都具有激活函数，用于引入非线性映射。<br> <img src="https://images2.imgbox.com/36/a6/4VTBE5hP_o.jpg" alt="在这里插入图片描述"></p> 
<h3><a id="12__8"></a>1.2 激活函数</h3> 
<p>常用的激活函数包括Sigmoid、ReLU、Tanh等。激活函数的作用是在神经网络中引入非线性性质，使其能够学习复杂的非线性关系。</p> 
<h3><a id="13__11"></a>1.3 前向传播</h3> 
<p>MLP的前向传播过程即从输入层到输出层的计算过程。它涉及到权重和偏置的计算、激活函数的应用等。通过一个简单的二分类任务为例来演示MLP的前向传播过程。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># MLP的前向传播</span>
<span class="token keyword">def</span> <span class="token function">forward_propagation</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> weights<span class="token punctuation">,</span> biases<span class="token punctuation">)</span><span class="token punctuation">:</span>
    hidden_layer <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> biases<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    output_layer <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>hidden_layer<span class="token punctuation">,</span> weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> biases<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> output_layer

<span class="token comment"># 输入数据</span>
inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 权重和偏置</span>
weights <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
biases <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token comment"># 前向传播计算</span>
output <span class="token operator">=</span> forward_propagation<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> weights<span class="token punctuation">,</span> biases<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测结果:"</span><span class="token punctuation">,</span> output<span class="token punctuation">)</span>
</code></pre> 
<p>代码中，定义了一个<code>forward_propagation</code>函数，该函数接收输入数据、权重和偏置作为参数，并通过矩阵乘法和激活函数进行前向传播计算。最终输出的<code>output_layer</code>即为MLP的预测结果。</p> 
<h3><a id="14__39"></a>1.4 反向传播算法</h3> 
<p>反向传播算法是用于训练MLP模型的关键步骤。通过计算梯度来调整权重和偏置，以最小化预测结果与真实结果之间的误差。在本文中，将使用反向传播算法来训练MLP模型并进行分类任务。</p> 
<h2><a id="2MLP_42"></a>2.MLP分类任务应用</h2> 
<p>使用Python和NumPy库来实现MLP模型，并使用一个简单的数据集进行训练和测试。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># MLP类</span>
<span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> num_hidden<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>num_hidden<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>biases <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hidden<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">)</span><span class="token punctuation">]</span>
    
    <span class="token keyword">def</span> <span class="token function">forward_propagation</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        hidden_layer <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>biases<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        output_layer <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>hidden_layer<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>biases<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output_layer
    
    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> learning_rate<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 前向传播</span>
            output <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_propagation<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
            
            <span class="token comment"># 反向传播</span>
            error <span class="token operator">=</span> targets <span class="token operator">-</span> output
            delta <span class="token operator">=</span> error <span class="token operator">*</span> output <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> output<span class="token punctuation">)</span>
            hidden_error <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>delta<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>T<span class="token punctuation">)</span>
            hidden_delta <span class="token operator">=</span> hidden_error <span class="token operator">*</span> hidden_layer <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> hidden_layer<span class="token punctuation">)</span>
            
            <span class="token comment"># 权重和偏置更新</span>
            self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> learning_rate <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>hidden_layer<span class="token punctuation">.</span>T<span class="token punctuation">,</span> delta<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>biases<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> learning_rate <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>delta<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> learning_rate <span class="token operator">*</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>inputs<span class="token punctuation">.</span>T<span class="token punctuation">,</span> hidden_delta<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>biases<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> learning_rate <span class="token operator">*</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>hidden_delta<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_propagation<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

<span class="token comment"># Sigmoid激活函数</span>
<span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 数据集</span>
inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
targets <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 创建MLP模型</span>
mlp <span class="token operator">=</span> MLP<span class="token punctuation">(</span>num_inputs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_hidden<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> num_outputs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 训练模型</span>
mlp<span class="token punctuation">.</span>train<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

<span class="token comment"># 测试模型</span>
test_inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
predictions <span class="token operator">=</span> mlp<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_inputs<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"预测结果:"</span><span class="token punctuation">,</span> predictions<span class="token punctuation">)</span>
</code></pre> 
<p>首先定义一个<code>MLP</code>类，其中包含MLP模型的初始化、前向传播、训练和预测方法。我使用一个简单的逻辑门数据集进行训练和测试，其中<code>inputs</code>表示输入数据，<code>targets</code>表示对应的目标结果。通过调用<code>train</code>方法进行训练，并通过调用<code>predict</code>方法进行预测。</p> 
<h2><a id="3_102"></a>3.参考文献：</h2> 
<ol><li>Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press.</li><li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/9a7a5eb6f6a5eb7b8853f81d5d13e547/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">（Appium_apk package）查询apk包名</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/389ad3270ea2acac91a88e0399556a15/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue-i18n插入变量，HTML等</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>