<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TensorFlow的卷积神经网络例子解析 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TensorFlow的卷积神经网络例子解析" />
<meta property="og:description" content="TensorFlow教程地址：https://www.tensorflow.org/tutorials/mnist/pros/ 讲的是经典的机器学习问题MNIST。 使用卷积神经网络进行训练。
载入MNIST数据 MNIST数据可以从这里下载
from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) 创建多层卷积网络 权重初始化 这里定义两个方法：
def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) tf.truncated_normal根据截断正态分布产生随机数 tf.constant产生常数
卷积(Convolution)和池化(Pooling) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 先说tf.nn.conv2d，它的参数strides代表切边移动的步长，4个方向，而padding是切片上是否可以越过边缘，有两种方式：”SAME”和”VALID”，”SAME”为越过，“VALID”为不越过，它的意义是决定切片中心是否经过图的边缘。
卷积过程例子如下： 再说tf.nn.max_pool，它是最大化池策略。 参数ksize是要执行取最值的切片在各个维度上的尺寸，四维数组意义为[batch, height, width, channels]。 参数strides是取切片的步长，四维数组意义为四个方向的步长，这里height和width方向都为2，例如原本8x8的矩阵，用2x2切片去pool，会获得5x5的矩阵输出（SAME模式），有效的减少特征维度。 参数Padding同conv2d。
第一个卷积层 W_conv1 = weight_variable([5, 5, 1, 32]) b_conv1 = bias_variable([32]) 第一层将以大小为5x5的切片来生成32个特征， [5,5,1,32] 前两位为patch size，再为in_channel，最后为out_channel。in_channel的意义可理解为一个图像RGB的三层，out_channel即生成的神经元数量，如图中的output volume。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f75f50b59351580b188047553f1c8940/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-12-22T15:19:41+08:00" />
<meta property="article:modified_time" content="2016-12-22T15:19:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TensorFlow的卷积神经网络例子解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>TensorFlow教程地址：<a href="https://www.tensorflow.org/tutorials/mnist/pros/" rel="nofollow">https://www.tensorflow.org/tutorials/mnist/pros/</a> <br> 讲的是经典的机器学习问题MNIST。 <br> 使用卷积神经网络进行训练。</p> 
<h3 id="载入mnist数据">载入MNIST数据</h3> 
<p>MNIST数据可以从<a href="http://yann.lecun.com/exdb/mnist/" rel="nofollow">这里</a>下载</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
mnist = input_data.read_data_sets(<span class="hljs-string">'MNIST_data'</span>, one_hot=<span class="hljs-keyword">True</span>)</code></pre> 
<h2 id="创建多层卷积网络">创建多层卷积网络</h2> 
<h3 id="权重初始化">权重初始化</h3> 
<p>这里定义两个方法：</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">weight_variable</span><span class="hljs-params">(shape)</span>:</span>
  initial = tf.truncated_normal(shape, stddev=<span class="hljs-number">0.1</span>)
  <span class="hljs-keyword">return</span> tf.Variable(initial)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bias_variable</span><span class="hljs-params">(shape)</span>:</span>
  initial = tf.constant(<span class="hljs-number">0.1</span>, shape=shape)
  <span class="hljs-keyword">return</span> tf.Variable(initial)</code></pre> 
<p>tf.truncated_normal根据截断正态分布产生随机数 <br> tf.constant产生常数</p> 
<h3 id="卷积convolution和池化pooling">卷积(Convolution)和池化(Pooling)</h3> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2d</span><span class="hljs-params">(x, W)</span>:</span>
  <span class="hljs-keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">max_pool_2x2</span><span class="hljs-params">(x)</span>:</span>
  <span class="hljs-keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
                        strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)</code></pre> 
<p>先说tf.nn.conv2d，它的参数strides代表切边移动的步长，4个方向，而padding是切片上是否可以越过边缘，有两种方式：”SAME”和”VALID”，”SAME”为越过，“VALID”为不越过，它的意义是决定切片中心是否经过图的边缘。</p> 
<p>卷积过程例子如下： <br> <img src="https://images2.imgbox.com/a5/33/HslZ83Z0_o.gif" alt="卷积过程" title=""></p> 
<p>再说tf.nn.max_pool，它是最大化池策略。 <br> 参数ksize是要执行取最值的切片在各个维度上的尺寸，四维数组意义为[batch, height, width, channels]。 <br> 参数strides是取切片的步长，四维数组意义为四个方向的步长，这里height和width方向都为2，例如原本8x8的矩阵，用2x2切片去pool，会获得5x5的矩阵输出（SAME模式），有效的减少特征维度。 <br> 参数Padding同conv2d。</p> 
<h3 id="第一个卷积层">第一个卷积层</h3> 
<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">W_conv1 = <span class="hljs-value">weight_variable([<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>])</span></span>
<span class="hljs-setting">b_conv1 = <span class="hljs-value">bias_variable([<span class="hljs-number">32</span>])</span></span></code></pre> 
<p>第一层将以大小为5x5的切片来生成32个特征，<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-1-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-1" style="width: 6.283em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.574em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.745em, 1000em, 2.881em, -0.35em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3" style="font-family: STIXGeneral-Regular;">[</span><span class="mn" id="MathJax-Span-4" style="font-family: STIXGeneral-Regular;">5</span><span class="mo" id="MathJax-Span-5" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-6" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">5</span><span class="mo" id="MathJax-Span-7" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-8" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">1</span><span class="mo" id="MathJax-Span-9" style="font-family: STIXGeneral-Regular;">,</span><span class="mn" id="MathJax-Span-10" style="font-family: STIXGeneral-Regular; padding-left: 0.188em;">32</span><span class="mo" id="MathJax-Span-11" style="font-family: STIXGeneral-Regular;">]</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.29em; vertical-align: -0.28em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-1">[5, 5, 1, 32]</script>前两位为patch size，再为in_channel，最后为out_channel。in_channel的意义可理解为一个图像RGB的三层，out_channel即生成的神经元数量，如图中的output volume。</p> 
<pre class="prettyprint"><code class=" hljs fix"><span class="hljs-attribute">x_image </span>=<span class="hljs-string"> tf.reshape(x, [-1,28,28,1])</span></code></pre> 
<p>-1代表任何维度，这里是样本数量，MNIST的图像大小为28*28，由于是黑白的，只有一个in_channel。</p> 
<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">h_conv1 = <span class="hljs-value">tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span></span>
<span class="hljs-setting">h_pool1 = <span class="hljs-value">max_pool_2x2(h_conv1)</span></span></code></pre> 
<p>这里使用relu为激活函数，conv2d过程如动图所示，生成32个output volumn，每个大小为28x28，因为<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-54-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-911" style="width: 31.217em; display: inline-block;"><span style="display: inline-block; position: relative; width: 22.774em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.745em, 1000em, 2.902em, -0.39em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-912"><span class="mo" id="MathJax-Span-913" style="font-family: STIXGeneral-Regular;">(</span><span class="mn" id="MathJax-Span-914" style="font-family: STIXGeneral-Regular;">2</span><span class="mo" id="MathJax-Span-915" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-916" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">28</span><span class="mo" id="MathJax-Span-917" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-918" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">2</span><span class="mo" id="MathJax-Span-919" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-920" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">1</span><span class="mo" id="MathJax-Span-921" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">−</span><span class="mn" id="MathJax-Span-922" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">5</span><span class="mo" id="MathJax-Span-923" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-924" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">×</span><span class="mo" id="MathJax-Span-925" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">(</span><span class="mn" id="MathJax-Span-926" style="font-family: STIXGeneral-Regular;">2</span><span class="mo" id="MathJax-Span-927" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-928" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">28</span><span class="mo" id="MathJax-Span-929" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-930" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">2</span><span class="mo" id="MathJax-Span-931" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-932" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">1</span><span class="mo" id="MathJax-Span-933" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">−</span><span class="mn" id="MathJax-Span-934" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">5</span><span class="mo" id="MathJax-Span-935" style="font-family: STIXGeneral-Regular;">)</span><span class="mo" id="MathJax-Span-936" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-937" style="font-family: STIXGeneral-Regular; padding-left: 0.313em;">28</span><span class="mo" id="MathJax-Span-938" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">×</span><span class="mn" id="MathJax-Span-939" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">28</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.318em; vertical-align: -0.309em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-54">(2+28+2+1-5)\times (2+28+2+1-5)=28\times 28</script>。 <br> max_pool后生成32个大小为14x14的矩阵，因为<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-55-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-940" style="width: 2.55em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.849em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.757em, 1000em, 2.739em, -0.409em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-941"><span class="mn" id="MathJax-Span-942" style="font-family: STIXGeneral-Regular;">28</span><span class="texatom" id="MathJax-Span-943"><span class="mrow" id="MathJax-Span-944"><span class="mo" id="MathJax-Span-945" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mn" id="MathJax-Span-946" style="font-family: STIXGeneral-Regular;">2</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.079em; vertical-align: -0.086em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-55">28/2</script>。</p> 
<h3 id="第二个卷积层">第二个卷积层</h3> 
<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">W_conv2 = <span class="hljs-value">weight_variable([<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>])</span></span>
<span class="hljs-setting">b_conv2 = <span class="hljs-value">bias_variable([<span class="hljs-number">64</span>])</span></span>

<span class="hljs-setting">h_conv2 = <span class="hljs-value">tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span></span>
<span class="hljs-setting">h_pool2 = <span class="hljs-value">max_pool_2x2(h_conv2)</span></span></code></pre> 
<p>过程逻辑同第一层，这里只说明输入输出的形式。 <br> 输入的是32个14x14的矩阵，权重体现了这层要输出的矩阵个数为64。 <br> 卷积输出64个14x14的矩阵，因为<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-52-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-887" style="width: 12.817em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.343em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.745em, 1000em, 2.902em, -0.39em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-888"><span class="mo" id="MathJax-Span-889" style="font-family: STIXGeneral-Regular;">(</span><span class="mn" id="MathJax-Span-890" style="font-family: STIXGeneral-Regular;">14</span><span class="mo" id="MathJax-Span-891" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-892" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">2</span><span class="mo" id="MathJax-Span-893" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-894" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">2</span><span class="mo" id="MathJax-Span-895" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">+</span><span class="mn" id="MathJax-Span-896" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">1</span><span class="mo" id="MathJax-Span-897" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">−</span><span class="mn" id="MathJax-Span-898" style="font-family: STIXGeneral-Regular; padding-left: 0.25em;">5</span><span class="mo" id="MathJax-Span-899" style="font-family: STIXGeneral-Regular;">)</span><span class="texatom" id="MathJax-Span-900"><span class="mrow" id="MathJax-Span-901"><span class="mo" id="MathJax-Span-902" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mn" id="MathJax-Span-903" style="font-family: STIXGeneral-Regular;">1</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.318em; vertical-align: -0.309em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-52">(14+2+2+1-5)/1</script> <br> 池化输出64个7x7的矩阵，因为<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-53-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-904" style="width: 2.55em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.849em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.757em, 1000em, 2.739em, -0.327em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-905"><span class="mn" id="MathJax-Span-906" style="font-family: STIXGeneral-Regular;">14</span><span class="texatom" id="MathJax-Span-907"><span class="mrow" id="MathJax-Span-908"><span class="mo" id="MathJax-Span-909" style="font-family: STIXGeneral-Regular;">/</span></span></span><span class="mn" id="MathJax-Span-910" style="font-family: STIXGeneral-Regular;">2</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.079em; vertical-align: -0.086em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-53">14/2</script></p> 
<h3 id="全连接层">全连接层</h3> 
<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">W_fc1 = <span class="hljs-value">weight_variable([<span class="hljs-number">7</span> * <span class="hljs-number">7</span> * <span class="hljs-number">64</span>, <span class="hljs-number">1024</span>])</span></span>
<span class="hljs-setting">b_fc1 = <span class="hljs-value">bias_variable([<span class="hljs-number">1024</span>])</span></span>

<span class="hljs-setting">h_pool2_flat = <span class="hljs-value">tf.reshape(h_pool2, [-<span class="hljs-number">1</span>, <span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>])</span></span>
<span class="hljs-setting">h_fc1 = <span class="hljs-value">tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span></span></code></pre> 
<p>全连接层就是普通的神经网络，输入参数维度为7x7x64，第一个隐层神经元数为1024。</p> 
<h3 id="dropout">Dropout</h3> 
<pre class="prettyprint"><code class=" hljs avrasm">keep_prob = tf<span class="hljs-preprocessor">.placeholder</span>(tf<span class="hljs-preprocessor">.float</span>32)
h_fc1_drop = tf<span class="hljs-preprocessor">.nn</span><span class="hljs-preprocessor">.dropout</span>(h_fc1, keep_prob)</code></pre> 
<p>为了减少过拟合，使用dropout策略来训练，<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-6-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-60" style="width: 6.017em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.38em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.75em, 1000em, 2.93em, -0.424em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-61"><span class="mi" id="MathJax-Span-62" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.017em;"></span></span><span class="mi" id="MathJax-Span-63" style="font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-64" style="font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-65" style="font-family: STIXGeneral-Italic;">p</span><span class="mi" id="MathJax-Span-66" style="font-family: STIXGeneral-Regular;">_</span><span class="mi" id="MathJax-Span-67" style="font-family: STIXGeneral-Italic;">p</span><span class="mi" id="MathJax-Span-68" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.023em;"></span></span><span class="mi" id="MathJax-Span-69" style="font-family: STIXGeneral-Italic;">o</span><span class="mi" id="MathJax-Span-70" style="font-family: STIXGeneral-Italic;">b</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.35em; vertical-align: -0.348em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-6">keep\_prob</script>为各神经元在dropout过程中保留的概率。dropout只在训练过程中开启，在测试过程中会关闭。</p> 
<h3 id="输出层">输出层</h3> 
<pre class="prettyprint"><code class=" hljs ini"><span class="hljs-setting">W_fc2 = <span class="hljs-value">weight_variable([<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>])</span></span>
<span class="hljs-setting">b_fc2 = <span class="hljs-value">bias_variable([<span class="hljs-number">10</span>])</span></span>

<span class="hljs-setting">y_conv = <span class="hljs-value">tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span></span></code></pre> 
<p>使用softmax作为输出层。这里暂时还没使用softmax函数，因为下面要使用tf.nn.softmax_cross_entropy_with_logits函数进行最后的计算，它在数值计算上比tf.nn.softmax稳定。</p> 
<h3 id="训练与评估">训练与评估</h3> 
<pre class="prettyprint"><code class=" hljs avrasm">cross_entropy = tf<span class="hljs-preprocessor">.reduce</span>_mean(tf<span class="hljs-preprocessor">.nn</span><span class="hljs-preprocessor">.softmax</span>_cross_entropy_with_logits(y_conv, y_))
train_step = tf<span class="hljs-preprocessor">.train</span><span class="hljs-preprocessor">.AdamOptimizer</span>(<span class="hljs-number">1e-4</span>)<span class="hljs-preprocessor">.minimize</span>(cross_entropy)
correct_prediction = tf<span class="hljs-preprocessor">.equal</span>(tf<span class="hljs-preprocessor">.argmax</span>(y_conv,<span class="hljs-number">1</span>), tf<span class="hljs-preprocessor">.argmax</span>(y_,<span class="hljs-number">1</span>))
accuracy = tf<span class="hljs-preprocessor">.reduce</span>_mean(tf<span class="hljs-preprocessor">.cast</span>(correct_prediction, tf<span class="hljs-preprocessor">.float</span>32))
sess<span class="hljs-preprocessor">.run</span>(tf<span class="hljs-preprocessor">.global</span>_variables_initializer())
for i <span class="hljs-keyword">in</span> range(<span class="hljs-number">20000</span>):
  batch = mnist<span class="hljs-preprocessor">.train</span><span class="hljs-preprocessor">.next</span>_batch(<span class="hljs-number">50</span>)
  if i%<span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
    train_accuracy = accuracy<span class="hljs-preprocessor">.eval</span>(feed_dict={
        <span class="hljs-built_in">x</span>:batch[<span class="hljs-number">0</span>], y_: batch[<span class="hljs-number">1</span>], keep_prob: <span class="hljs-number">1.0</span>})
    print(<span class="hljs-string">"step %d, training accuracy %g"</span>%(i, train_accuracy))
  train_step<span class="hljs-preprocessor">.run</span>(feed_dict={<!-- --><span class="hljs-built_in">x</span>: batch[<span class="hljs-number">0</span>], y_: batch[<span class="hljs-number">1</span>], keep_prob: <span class="hljs-number">0.5</span>})

print(<span class="hljs-string">"test accuracy %g"</span>%accuracy<span class="hljs-preprocessor">.eval</span>(feed_dict={
    <span class="hljs-built_in">x</span>: mnist<span class="hljs-preprocessor">.test</span><span class="hljs-preprocessor">.images</span>, y_: mnist<span class="hljs-preprocessor">.test</span><span class="hljs-preprocessor">.labels</span>, keep_prob: <span class="hljs-number">1.0</span>}))</code></pre> 
<p>这里有三个不同之处： <br> 1. 使用ADAM框架来替代steepest随机梯度下降框架。 <br> 2. 增加每次batch使用dropout的概率<span class="MathJax_Preview"></span><span class="MathJax" id="MathJax-Element-7-Frame" style=""> 
   
   <span class="math" id="MathJax-Span-71" style="width: 6.017em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.38em; height: 0px; font-size: 137%;"><span style="position: absolute; clip: rect(1.75em, 1000em, 2.93em, -0.424em); top: -2.579em; left: 0em;"><span class="mrow" id="MathJax-Span-72"><span class="mi" id="MathJax-Span-73" style="font-family: STIXGeneral-Italic;">k<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.017em;"></span></span><span class="mi" id="MathJax-Span-74" style="font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-75" style="font-family: STIXGeneral-Italic;">e</span><span class="mi" id="MathJax-Span-76" style="font-family: STIXGeneral-Italic;">p</span><span class="mi" id="MathJax-Span-77" style="font-family: STIXGeneral-Regular;">_</span><span class="mi" id="MathJax-Span-78" style="font-family: STIXGeneral-Italic;">p</span><span class="mi" id="MathJax-Span-79" style="font-family: STIXGeneral-Italic;">r<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.023em;"></span></span><span class="mi" id="MathJax-Span-80" style="font-family: STIXGeneral-Italic;">o</span><span class="mi" id="MathJax-Span-81" style="font-family: STIXGeneral-Italic;">b</span></span><span style="display: inline-block; width: 0px; height: 2.579em;"></span></span></span><span style="border-left-width: 0em; border-left-style: solid; display: inline-block; overflow: hidden; width: 0px; height: 1.35em; vertical-align: -0.348em;"></span></span> 
  </span><script type="math/tex" id="MathJax-Element-7">keep\_prob</script>。 <br> 3. 每100次迭代都进行记录。</p> 
<h2 id="reference">Reference</h2> 
<p><a href="https://www.tensorflow.org/tutorials/mnist/pros/#train_and_evaluate_the_model" rel="nofollow">https://www.tensorflow.org/tutorials/mnist/pros/#train_and_evaluate_the_model</a> <br> <a href="http://www.cnblogs.com/hellocwh/p/5564568.html" rel="nofollow">http://www.cnblogs.com/hellocwh/p/5564568.html</a> <br> <a href="http://blog.csdn.net/han_xiaoyang/article/details/50542880">http://blog.csdn.net/han_xiaoyang/article/details/50542880</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c83110058d3d182be7bcee38bb540c69/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">cocos2dx 3.12 各平台资源加密解密</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/131fcff8b174b5d39300419106e23047/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">TCP Server处理多Client请求的方法—非阻塞accept与select</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>