<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>关于调节学习率(learning rate)的几点建议 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="关于调节学习率(learning rate)的几点建议" />
<meta property="og:description" content="关于调节学习率的几点建议 1.对于不同大小的数据集，调节不同的学习率 根据我们选择的成本函数F(x)不同，问题会有区别。当平方误差和（Sum of Squared Errors）作为成本函数时， ∂ F ( ω j ) ∂ ω j \frac{∂F(ω_j)} { ∂ω_j} ∂ωj​∂F(ωj​)​ 会随着训练集数据的增多变得越来越大，因此学习率需要被设定在相应更小的值上。
解决此类问题的一个方法是将学习率λ 乘上1/N，N是训练集中数据量。这样每步更新的公式变成下面的形式：
ω j = ω j − ( λ N ∂ F ( ω j ) ∂ ω j ) ω_j=ω_j-(\frac{\lambda}{N} \frac{∂F(ω_j) }{∂ω_j}) ωj​=ωj​−(Nλ​∂ωj​∂F(ωj​)​)
相关内容可参考： Wilson et al. paper “The general inefﬁciency of batch training for gradient descent learning”
另外一种解决方法是：选择一个不被训练集样本个数影响的成本函数，如均值平方差（Mean Squared Errors）。
2. 在每次迭代中调节不同的学习率 在每次迭代中去调整学习率的值是另一种很好的学习率自适应方法。此类方法的基本思路是当你离最优值越远，你需要朝最优值移动的就越多，即学习率就应该越大；反之亦然。
但是这里有一个问题，就是我们并不知道实际上的最优值在哪里，我们也不知道每一步迭代中我们离最优值有多远。
解决办法是，我们在每次迭代的最后，使用估计的模型参数检查误差函数（error function）的值。如果相对于上一次迭代，错误率减少了，就可以增大学习率，以5%的幅度；如果相对于上一次迭代，错误率增大了（意味着跳过了最优值），那么应该重新设置上一轮迭代ωj 的值，并且减少学习率到之前的50%。这种方法叫做 Bold Driver." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/be3df0a24435519da40ca21e7371c6c1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-11-06T09:49:08+08:00" />
<meta property="article:modified_time" content="2017-11-06T09:49:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">关于调节学习率(learning rate)的几点建议</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>关于调节学习率的几点建议</h3> 
<h4><a id="1_1"></a>1.对于不同大小的数据集，调节不同的学习率</h4> 
<p>根据我们选择的成本函数F(x)不同，问题会有区别。当平方误差和（Sum of Squared Errors）作为成本函数时， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           ∂ 
          
         
           F 
          
         
           ( 
          
          
          
            ω 
           
          
            j 
           
          
         
           ) 
          
         
         
         
           ∂ 
          
          
          
            ω 
           
          
            j 
           
          
         
        
       
      
        \frac{∂F(ω_j)} { ∂ω_j} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.57464em; vertical-align: -0.54232em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.03232em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.281886em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.50732em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mord mathit mtight" style="margin-right: 0.13889em;">F</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.281886em;"><span class=""></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.54232em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> 会随着训练集数据的增多变得越来越大，因此学习率需要被设定在相应更小的值上。<br> 解决此类问题的一个方法是将学习率λ 乘上1/N，N是训练集中数据量。这样每步更新的公式变成下面的形式：<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ω 
         
        
          j 
         
        
       
         = 
        
        
        
          ω 
         
        
          j 
         
        
       
         − 
        
       
         ( 
        
        
        
          λ 
         
        
          N 
         
        
        
         
         
           ∂ 
          
         
           F 
          
         
           ( 
          
          
          
            ω 
           
          
            j 
           
          
         
           ) 
          
         
         
         
           ∂ 
          
          
          
            ω 
           
          
            j 
           
          
         
        
       
         ) 
        
       
      
        ω_j=ω_j-(\frac{\lambda}{N} \frac{∂F(ω_j) }{∂ω_j}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.869438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathit" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.57464em; vertical-align: -0.54232em;"></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.880108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.10903em;">N</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">λ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.03232em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.281886em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.50732em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mord mathit mtight" style="margin-right: 0.13889em;">F</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328086em;"><span class="" style="top: -2.357em; margin-left: -0.03588em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight" style="margin-right: 0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.281886em;"><span class=""></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.54232em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p> 
<p>相关内容可参考： Wilson et al. paper “The general inefﬁciency of batch training for gradient descent learning”</p> 
<p>另外一种解决方法是：选择一个不被训练集样本个数影响的成本函数，如均值平方差（Mean Squared Errors）。</p> 
<h4><a id="2	_9"></a>2. 在每次迭代中调节不同的学习率</h4> 
<p>在每次迭代中去调整学习率的值是另一种很好的学习率自适应方法。此类方法的基本思路是当你离最优值越远，你需要朝最优值移动的就越多，即学习率就应该越大；反之亦然。</p> 
<p>但是这里有一个问题，就是我们并不知道实际上的最优值在哪里，我们也不知道每一步迭代中我们离最优值有多远。<br> 解决办法是，我们在每次迭代的最后，使用估计的模型参数检查误差函数（error function）的值。如果相对于上一次迭代，错误率减少了，就可以增大学习率，以5%的幅度；如果相对于上一次迭代，错误率增大了（意味着跳过了最优值），那么应该重新设置上一轮迭代ωj 的值，并且减少学习率到之前的50%。这种方法叫做 Bold Driver.</p> 
<h4><a id="3	_15"></a>3. 建议：归一化输入向量</h4> 
<p>归一化输入向量在机器学习问题中是一个通用的方法。在一些应用中，由于使用距离或者特征方差，要求必须归一化输入向量，因为如果不归一化将导致结果会严重被具有大方差的特征和不同的尺度影响。归一化输入能够帮助数值最优方法（例如，梯度下降法）更快，更准确地收敛。</p> 
<p>尽管有一些不同的归一化变量的方法，[0,1]归一化（也叫做min-max）和z-score归一化是两种最为广泛应用的。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0d87ed6ffaa9266624b897d51cb79ec7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">家庭智能网关elinkc</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e39df32ea316a4b6b2b6bde4db49cd54/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">多线程和Socket套接字</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>