<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习基础 | RNN家族全面解析 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习基础 | RNN家族全面解析" />
<meta property="og:description" content="作者 | Chilia 整理 | NewBeeNLP
首先，请阅读先修知识：深度学习基础 | 从Language Model到RNN
1. 梯度消失和梯度爆炸 1.1 梯度消失 【定义】当很多的层都用特定的激活函数(尤其是sigmoid函数)，损失函数的梯度会趋近于0，因此模型更加不容易训练。(As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.)
以最简单的网络结构为例，假如有三个隐藏层，每层的神经元个数都是1，且对应的非线性函数为sigmoid:
每个节点的输出 , 那么
梯度消失的罪魁祸首是sigmoid函数，在sigmoid函数靠近0和1的位置，其导数很小。很多小的值相乘，导致最终的梯度很小。
sigmoid函数和其导数 由于我们初始化的网络权值通常都小于1，因此当层数增多时，小于0的值不断相乘，最后就导致梯度消失的情况出现。同理，当权值过大时，导致大于1的值不断相乘，就会产生梯度爆炸。
如果一个深层网络有很多层，梯度消失导致网络只等价于后面几层的浅层网络的学习，而前面的层不怎么更新了：
深层网络 在RNN中，也会出现梯度消失的问题，比如下面这个例子：
这里应该填&#34;ticket&#34;,但是如果梯度非常的小，RNN模型就不能够学习在很久之前出现的词语和现在要预测的词语的关联。也就是说，RNN模型也不能把握长期的信息。
「梯度消失有几种常见的解决方法：」
用下文提到的LSTM/GRU
加上一些skip-connection, 让梯度直接流过而不经过bottleneck。例如resnet：
用Relu、Leaky relu等激活函数
ReLu：让激活函数的导数为1
LeakyReLu：包含了ReLu的几乎所有优点，同时解决了ReLu中0区间带来的影响
1.2 梯度爆炸 回忆梯度更新的公式：
那么，如果梯度太大，则参数更新的过快。步子迈的太大就会导致训练非常不稳定(训飞了)，甚至最后loss变成「Inf」。
梯度爆炸的解决方法：
（1）gradient clipping
如果梯度大于某个阈值了，就对其进行裁剪，让它不要高于那个阈值。
(2) 权重正则化。如果发生梯度爆炸，那么权值的范数就会变的非常大。通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f3d45017829e58eaeeef521a1eb0cd0d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-19T11:05:00+08:00" />
<meta property="article:modified_time" content="2021-10-19T11:05:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习基础 | RNN家族全面解析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h3><img src="https://images2.imgbox.com/1b/11/qlp9YeJN_o.png" alt="72570dcf37431b32e7e7ee6b59bbe65d.png"></h3> 
 <blockquote> 
  <p>作者 | Chilia  </p> 
  <p>整理 | NewBeeNLP</p> 
 </blockquote> 
 <p style="text-align:justify;">首先，请阅读先修知识：<a href="https://blog.csdn.net/qq_33590580/article/details/114295714">深度学习基础 | 从Language Model到RNN</a><br></p> 
 <h3>1. 梯度消失和梯度爆炸</h3> 
 <h5>1.1 梯度消失</h5> 
 <p style="text-align:justify;">【定义】当很多的层都用特定的激活函数(尤其是sigmoid函数)，损失函数的梯度会趋近于0，因此模型更加不容易训练。(As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train.)</p> 
 <p style="text-align:justify;">以最简单的网络结构为例，假如有三个隐藏层，每层的神经元个数都是1，且对应的非线性函数为sigmoid:</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/fc/15/vx2Za04e_o.png" alt="7631c7cfe1968870d41a4c8f721a03df.png"></p> 
 <br> 
 <p style="text-align:justify;">每个节点的输出  , 那么</p> 
  
 <p style="text-align:justify;">梯度消失的罪魁祸首是sigmoid函数，在sigmoid函数靠近0和1的位置，其导数很小。很多小的值相乘，导致最终的梯度很小。</p> 
 <img src="https://images2.imgbox.com/6f/a2/7Yj45Pj0_o.png" alt="2ba607a04d5a607da3c57e1d36a489bd.png"> 
 <figcaption>
   sigmoid函数和其导数 
 </figcaption> 
 <p style="text-align:justify;">由于我们初始化的网络权值通常都小于1，因此当层数增多时，小于0的值不断相乘，最后就导致梯度消失的情况出现。同理，当权值过大时，导致大于1的值不断相乘，就会产生梯度爆炸。</p> 
 <p style="text-align:justify;">如果一个深层网络有很多层，梯度消失导致网络只等价于后面几层的浅层网络的学习，而前面的层不怎么更新了：</p> 
 <img src="https://images2.imgbox.com/a8/ba/JSiiccnI_o.png" alt="023808c6a96622f398399c0159511741.png"> 
 <figcaption>
   深层网络 
 </figcaption> 
 <p style="text-align:justify;">在RNN中，也会出现梯度消失的问题，比如下面这个例子：</p> 
 <img src="https://images2.imgbox.com/6d/59/KkfDuqVa_o.png" alt="338c0067d7f1690b1805993528005a9f.png"> 
 <p style="text-align:justify;">这里应该填"ticket",但是如果梯度非常的小，RNN模型就不能够学习在很久之前出现的词语和现在要预测的词语的关联。也就是说，RNN模型也不能把握长期的信息。</p> 
 <p style="text-align:justify;"><strong>「梯度消失有几种常见的解决方法：」</strong></p> 
 <ul><li><p>用下文提到的LSTM/GRU</p></li><li><p>加上一些skip-connection, 让梯度直接流过而不经过bottleneck。例如resnet：</p></li></ul> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6c/dd/IF1HhY0f_o.png" alt="c37890bedbf2c392a869ddb00986e98b.png"></p> 
 <br> 
 <ul><li><p>用Relu、Leaky relu等激活函数</p> 
   <ul><li><p>ReLu：让激活函数的导数为1</p></li><li><p>LeakyReLu：包含了ReLu的几乎所有优点，同时解决了ReLu中0区间带来的影响</p></li></ul></li></ul> 
 <h5>1.2 梯度爆炸</h5> 
 <p style="text-align:justify;">回忆梯度更新的公式：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/d2/fb/GJSVSlcO_o.png" alt="bb2801c2c35fb744abea4a25a106b626.png"></p> 
 <br> 
 <p style="text-align:justify;">那么，如果梯度太大，则参数更新的过快。步子迈的太大就会导致训练非常不稳定(训飞了)，甚至最后loss变成<strong>「Inf」</strong>。</p> 
 <p style="text-align:justify;">梯度爆炸的解决方法：</p> 
 <p style="text-align:justify;">（1）gradient clipping</p> 
 <img src="https://images2.imgbox.com/ff/d0/Hr8rizfr_o.png" alt="04944904d569fa9aa55bae8fd8f378d7.png"> 
 <p style="text-align:justify;">如果梯度大于某个阈值了，就对其进行裁剪，让它不要高于那个阈值。</p> 
 <img src="https://images2.imgbox.com/df/cf/LXLe22SK_o.png" alt="ade0ba93f0864c81ca3a1a39cb90952d.png"> 
 <p style="text-align:justify;">(2) 权重正则化。如果发生梯度爆炸，那么权值的范数就会变的非常大。通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。</p> 
 <h3>2. LSTM</h3> 
 <p style="text-align:justify;">Vanilla RNN最致命的问题就是，它不能够保留很久之前的信息(由于梯度消失)。这是因为它的隐藏状态在不停的被重写：</p> 
  
 <p style="text-align:justify;">所以，可不可以有一种RNN，能够有独立的记忆(separated memory)呢？</p> 
 <h5>2.1 LSTM 基本思想</h5> 
 <p style="text-align:justify;">对于任一时间 t，都有三个概念：</p> 
 <ul><li><p>hidden state: n维向量</p></li><li><p>cell state: n维向量，存储长期记忆。cell就像一个小小的计算机系统，可以<strong>「读、写、擦除」</strong>。</p></li><li><p>gates: <strong>「n维向量」</strong>，每个元素的大小都是0~1之间（之后做element-wise product）。决定哪些信息可以穿过，哪些需要被挡住。</p></li></ul> 
 <p style="text-align:justify;"><strong>「（1）三个gate的计算」</strong></p> 
 <p style="text-align:justify;">首先，计算三个gate，它们都由上一个hidden state的输出  和当前的input  计算得到。gate是n维向量：</p> 
 <img src="https://images2.imgbox.com/5a/d4/ybKAWilR_o.png" alt="96080a4a53bfa8eeece10d3ca61a5067.png"> 
 <p style="text-align:justify;"><strong>「(2) cell 和 hidden state 的更新」</strong></p> 
 <img src="https://images2.imgbox.com/80/b0/mbGmPeTB_o.png" alt="2741fe1dba17275b19fa9dfe860dcbd8.png"> 
 <p style="text-align:justify;"><strong>「cell」</strong>存放长期记忆，t时刻的长期记忆  由两部分组成：①旧信息  遗忘一部分；②新信息  写入一部分。</p> 
 <p style="text-align:justify;">t时刻的<strong>「hidden state」</strong>  就是选择一部分长期记忆  输出的结果。</p> 
 <p style="text-align:justify;">LSTM图示：</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/4b/59/lgO6JuDH_o.png" alt="177c003323c48a4ddefd4a1c74fc3221.png"></p> 
 <figcaption>
   LSTM图示 
 </figcaption> 
 <p style="text-align:justify;">图中，每一个绿色方块是一个timestep。和普通的RNN一样，LSTM也是每一步有输入  ，有隐藏状态  作为输出。</p> 
 <h5>2.2 为什么LSTM能够解决梯度消失</h5> 
 <p style="text-align:justify;">LSTM能够让RNN一直保留原来的信息(preserve information over many timesteps)。如果LSTM的遗忘门被设置成1，那么LSTM会一直记住原来每一步的旧信息。相比之下，RNN很难能够学习到一个参数矩阵  能够保留hidden state的全部信息。</p> 
 <p style="text-align:justify;">所以，可以说LSTM解决梯度消失的主要原因是因为它有<strong>「skip-connection」</strong>的结构，能够让信息直接流过。而vanilla RNN每一步backprop都要经过  这个bottleneck,导致梯度消失。</p> 
 <h3>3. GRU(gated recurrent unit)</h3> 
 <h5>3.1 GRU的基本思想</h5> 
 <p style="text-align:justify;">跟LSTM不同的是，GRU没有cell state，只有hidden state和两个gate。</p> 
 <p style="text-align:justify;"><strong>「（1）gate的计算：」</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/bc/3c/ZM12QDF0_o.png" alt="2e719b795052c3ac1f72891f950af8b5.png"></p> 
 <br> 
 <ul><li><p>update gate: 相当于LSTM中的forget gate(擦除旧信息)和input gate(写入新信息)</p></li><li><p>reset gate: 判断哪一部分的hidden state是有用的，哪些是无用的。</p></li></ul> 
 <p style="text-align:justify;"><strong>「（2）hidden state的计算」</strong></p> 
 <img src="https://images2.imgbox.com/19/22/esp4uEhP_o.png" alt="7e61efb0dddf3fa2e546eb5ad51edb77.png"> 
 <h5>3.2 为什么GRU能解决梯度消失？</h5> 
 <p style="text-align:justify;">就像LSTM一样，GRU也能够保持长期记忆(想象一下把update gate设置成0，则以前的信息全部被保留了)，也是一种增加skip-connection的方法。</p> 
 <h5>3.3 LSTM vs GRU</h5> 
 <ul><li><p>LSTM和GRU并没有明显的准确率上的区别</p></li><li><p>GRU比起LSTM来，参数更少，运算更快，仅此而已。</p></li><li><p>所以，在实际应用中，我们用LSTM做default方法，如果追求更高的性能，就换成GRU</p></li></ul> 
 <h3>4. Bidirectional RNN</h3> 
 <h5>4.1 单向RNN的局限性</h5> 
 <img src="https://images2.imgbox.com/fc/70/skLyTNGa_o.png" alt="bc60c276952abebdb9e9f7242154a6f6.png"> 
 <h5>4.2 双向RNN</h5> 
 <img src="https://images2.imgbox.com/63/5f/LYWN7Li7_o.png" alt="f9b760d034ab89a2fdfdd88efa92088d.png"> 
 <p style="text-align:justify;">把forward RNN和backward RNN的hidden state都拼接在一起，就可以得到包含双向信息的hidden state。</p> 
 <img src="https://images2.imgbox.com/8a/1c/EHaM4UdH_o.png" alt="703cc3d48d223a563aeaa55eafef92c4.png"> 
 <p style="text-align:justify;">【注意】只有当我们有<strong>「整句话」</strong>的时候才能用双向RNN。对于language model问题，就不能用双向RNN，因为只有左边的信息。</p> 
 <h3>5. Multi-layer RNNs</h3> 
 <p style="text-align:justify;">多层RNN也叫 stacked RNNs .</p> 
 <h5>5.1 多层RNN结构</h5> 
 <p style="text-align:justify;">下一层的hidden state作为上一层的输入：</p> 
 <img src="https://images2.imgbox.com/af/87/mnG2JZ28_o.png" alt="daf3a682dcd7b165e61028645e307481.png"> 
 <h5>5.2 多层RNN的好处</h5> 
 <p style="text-align:justify;">多层RNN可以让RNN网络得到词语序列更加复杂的表示（more complex representations）</p> 
 <ul><li><p>下面的RNN层可以得到低阶特征(lower-level features)</p></li><li><p>上面的RNN层可以得到高阶特征(higher-level features)</p></li></ul> 
 <h5>5.3 多层RNN的应用</h5> 
 <img src="https://images2.imgbox.com/d1/24/5ZYdOZDz_o.png" alt="0905aa550a0889b5f66e10f7b5bb4db8.png"> 
 <p style="text-align:justify;">【注意】如果multi-layer RNN深度很大，最好用一些skip connection</p> 
 <h3>一起交流</h3> 
 <p style="text-align:left;">想和你一起学习进步！『<strong>NewBeeNLP』</strong>目前已经建立了多个不同方向交流群（<strong>机器学习 / 深度学习 / 自然语言处理 / 搜索推荐 / 图网络 / 面试交流 / </strong>等），名额有限，赶紧添加下方微信加入一起讨论交流吧！（注意一定要<strong>备注信息</strong>才能通过）</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/e1/6b/oxvcr2qT_o.png" alt="ec367809b6f5b88ae31958761cfbdbfd.png"></p> 
 <h4>本文参考资料<br></h4> 
 <p>[1]</p> 
 <p style="text-align:justify;">cs224n-2019-lecture07: https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture07-fancy-rnn.pdf</p> 
 <p style="text-align:center;">- <strong>END </strong>-</p> 
 <p><img src="https://images2.imgbox.com/5b/d9/bS3I2reg_o.png" alt="0978a74f17b5cfd37044acef31320f1c.png"><br></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/13/76/6q0lVXEP_o.png" alt="11156916ffae2fa87e66c6865ddd3663.png"><br></p> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247505271&amp;idx=1&amp;sn=426f4799c1d39122289c6d966273c5a9&amp;chksm=97ad33a5a0dabab3474ebabe05315fbd17c8e69e94a93cf570ff928fa56f9b2819df82612792&amp;scene=21#wechat_redirect" rel="nofollow"></a></p> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247505271&amp;idx=1&amp;sn=426f4799c1d39122289c6d966273c5a9&amp;chksm=97ad33a5a0dabab3474ebabe05315fbd17c8e69e94a93cf570ff928fa56f9b2819df82612792&amp;scene=21#wechat_redirect" rel="nofollow">2021 年各家大厂的 AI Lab 现状如何？</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247505271&amp;idx=1&amp;sn=426f4799c1d39122289c6d966273c5a9&amp;chksm=97ad33a5a0dabab3474ebabe05315fbd17c8e69e94a93cf570ff928fa56f9b2819df82612792&amp;scene=21#wechat_redirect" rel="nofollow"></a> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247505271&amp;idx=1&amp;sn=426f4799c1d39122289c6d966273c5a9&amp;chksm=97ad33a5a0dabab3474ebabe05315fbd17c8e69e94a93cf570ff928fa56f9b2819df82612792&amp;scene=21#wechat_redirect" rel="nofollow">2021-09-01</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247505271&amp;idx=1&amp;sn=426f4799c1d39122289c6d966273c5a9&amp;chksm=97ad33a5a0dabab3474ebabe05315fbd17c8e69e94a93cf570ff928fa56f9b2819df82612792&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/b5/96/bM97UHx2_o.png" alt="989a417af79b7a9e3c7a5bccb21d54c2.png"> </a> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504946&amp;idx=1&amp;sn=0f2caa772a2168dec8cf6b0fde4184bb&amp;chksm=97ad32e0a0dabbf6e0f39ee452d1f4ef74595fe23ddb137ea952ed729840875261aaf2a6e2f2&amp;scene=21#wechat_redirect" rel="nofollow"></a></p> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504946&amp;idx=1&amp;sn=0f2caa772a2168dec8cf6b0fde4184bb&amp;chksm=97ad32e0a0dabbf6e0f39ee452d1f4ef74595fe23ddb137ea952ed729840875261aaf2a6e2f2&amp;scene=21#wechat_redirect" rel="nofollow">NLP预训练家族 | 自成一派的GPT！</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504946&amp;idx=1&amp;sn=0f2caa772a2168dec8cf6b0fde4184bb&amp;chksm=97ad32e0a0dabbf6e0f39ee452d1f4ef74595fe23ddb137ea952ed729840875261aaf2a6e2f2&amp;scene=21#wechat_redirect" rel="nofollow"></a> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504946&amp;idx=1&amp;sn=0f2caa772a2168dec8cf6b0fde4184bb&amp;chksm=97ad32e0a0dabbf6e0f39ee452d1f4ef74595fe23ddb137ea952ed729840875261aaf2a6e2f2&amp;scene=21#wechat_redirect" rel="nofollow">2021-08-26</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504946&amp;idx=1&amp;sn=0f2caa772a2168dec8cf6b0fde4184bb&amp;chksm=97ad32e0a0dabbf6e0f39ee452d1f4ef74595fe23ddb137ea952ed729840875261aaf2a6e2f2&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/0c/c7/hFhxdsxq_o.png" alt="25c703ea39b5e4458ee67a6fefec02ec.png"> </a> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504279&amp;idx=1&amp;sn=8d02ecbde4076932d4d5260379d32780&amp;chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&amp;scene=21#wechat_redirect" rel="nofollow"></a></p> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504279&amp;idx=1&amp;sn=8d02ecbde4076932d4d5260379d32780&amp;chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&amp;scene=21#wechat_redirect" rel="nofollow">大规模搜索+预训练，百度是如何落地的？</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504279&amp;idx=1&amp;sn=8d02ecbde4076932d4d5260379d32780&amp;chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&amp;scene=21#wechat_redirect" rel="nofollow"></a> 
 <p><a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504279&amp;idx=1&amp;sn=8d02ecbde4076932d4d5260379d32780&amp;chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&amp;scene=21#wechat_redirect" rel="nofollow">2021-08-17</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504279&amp;idx=1&amp;sn=8d02ecbde4076932d4d5260379d32780&amp;chksm=97ad3745a0dabe532b697b4407c4f947ce94a5a6a6cae0df534fd290a97fd2bdd2467bd81e86&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/b7/dc/cbJtQYKO_o.png" alt="b3fdc2dc358e359d9ecd0ebc12008c83.png"> </a> 
 <p><a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/119770080">万物皆可Graph | 当推荐系统遇上图神经网络（三）</a></p> 
 <p><a href="https://blog.csdn.net/qq_51651904/article/details/118616141">2021-08-16</a></p> 
 <a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247504278&amp;idx=1&amp;sn=87a551180989ff7c0b2111a5d2f25b8e&amp;chksm=97ad3744a0dabe5214f7bc538d8a81241b7f4fa48ba9c2641f5c2989337f6665cdf6736937f1&amp;scene=21#wechat_redirect" rel="nofollow"><img src="https://images2.imgbox.com/d6/15/zX8Hcevn_o.png" alt="2c6a007e25a20990c4d27f15b3c24136.png"></a> 
 <p style="text-align:center;"><img height="792" width="1068" src="https://images2.imgbox.com/34/9d/XyhOpjXh_o.gif" alt="1e288e80a0ff77e104d8d021976e54a3.gif"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b851dbabd1367b8fde08e20da21f4c60/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">resolve函数参数是个promise</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a9d57c8943fa094bbc912b4cdcbf21bc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Web资源服务器搭建之一samba服务搭建</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>