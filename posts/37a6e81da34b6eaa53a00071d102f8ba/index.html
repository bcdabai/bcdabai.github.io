<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>image caption 必看论文，模型整理 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="image caption 必看论文，模型整理" />
<meta property="og:description" content="基础模型 transformers- attention is all you need 细节笔记
论文笔记 几个重点 1. 架构图
2. attention 原理
attention机制中的query,key,value的概念解释
3. 为什么self-attention？为什么multi-head attention？
rnn 是ht的结果是根据ht-1的结果计算得到，无法并行的计算，这也是推出transfomers最根本的理由。
根据这个图，Self Attention就是让Attention中的q、k、v相等，所以self attention其实得到的是输入序列中每个部分与整个序列其他部分的关系，注意q、k、v来自于同一个向量但是经过了不同的投影矩阵，因此q、k、v是不一样的。
我们直接来看transformer中的Self Attention结构图，可以看到transformer是使用的Dot-Product Attention的变体（多除了个根号d，d是输入序列的通道数，目的是为了防止Q与K转置的内积过大或过小导致梯度较小），但因为d是设置好的超参数（常数），因此两者其实没有太大差别。
的结果就是Q和K的相似度，经过softmax使其变为0到1的数值后作为V的权重。
可以看到上述过程其实就是一系列的矩阵运算，并不包含任何参数，只有上图右侧中的Linear层是可学习的，而Linear层的作用是将Q、K、V投影到高维语义空间（也就是multi-head attention的作用）。所以上述的Self Attention操作实际上是在计算高维语义空间的Q和K的相似度作为V的权重进行了特征融合以获取全局信息。那么说明我们其实是通过喂数据给模型，指导其学会投影出这么一个语义空间——在此空间中我们期望相似的Q与K具有很高的相似度。
举个句子生成的例子来说，我们以Hello作为模型的输入，World作为下一个词的ground truth，只有当Hello和Word在投影出来的空间中具有较高的相似度，我们的attention机制才会给World较高的权重从而让模型输出World，那么通过自注意力的计算和梯度下降，我们就能让Linear学会让两个词投影到相近的位置
4. 为什么position encoding？
对于序列的seq / rnn 是有时序信息的。而对于Transformer来说，由于句子中的词语都是同时进入网络进行处理，顺序信息在输入网络时就已丢失。一句话概括，Positional Encoding就是句子中词语相对位置的编码，让Transformer保留词语的位置信息。
CV领域 vision transformer的笔记 DETR： End-to-End Object Detection with Transformers clip 论文 笔记
论文笔记
多模态鸿沟
数据集：四亿的图文对，进行对比学习。
对比学习，就是需要正样本和负样本的定义。在这里的正样本就是已经配对好的，反之就是负样本。所以有N个正样本，有N2 -N个负样本
clip 做目标检测 Open-Vocabulary Object Detection
clip 做图像分割 CLIPSeg
Image Caption 领域 李飞飞经典 Deep Visual-Semantic Alignments for Generating Image Descriptions" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/37a6e81da34b6eaa53a00071d102f8ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-30T09:43:31+08:00" />
<meta property="article:modified_time" content="2023-11-30T09:43:31+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">image caption 必看论文，模型整理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2 id="QS5f3">基础模型</h2> 
<h3 id="F5soG">transformers- attention is all you need</h3> 
<p>细节笔记</p> 
<h4 id="klC5h"><a href="https://readpaper.com/pdf-annotate/note?pdfId=4557951319176912897&amp;noteId=1730115108941718016" rel="nofollow" title="论文笔记">论文笔记</a></h4> 
<h4 id="QVhAA">几个重点</h4> 
<p id="uef7f6cf9">1. 架构图</p> 
<p id="u090e4d60"></p> 
<p class="img-center"><img alt="" height="1200" id="DFEFb" src="https://images2.imgbox.com/a5/06/saa15nAR_o.png" width="964"></p> 
<p id="u4767e8c8">2. attention 原理</p> 
<p id="ua03235ba"></p> 
<p class="img-center"><img alt="" height="498" id="wv8IA" src="https://images2.imgbox.com/33/b5/p9Dj0aRD_o.png" width="1200"></p> 
<p id="u38a75695"><a href="https://zhuanlan.zhihu.com/p/148737297" rel="nofollow" title="attention机制中的query,key,value的概念解释">attention机制中的query,key,value的概念解释</a></p> 
<p id="u537c6faa"></p> 
<p class="img-center"><img alt="" height="404" id="Vhiow" src="https://images2.imgbox.com/dd/ad/0SGcQ5an_o.png" width="1200"></p> 
<p id="uf89f1365">3. 为什么self-attention？为什么multi-head attention？</p> 
<p id="ub2f12152">rnn 是ht的结果是根据ht-1的结果计算得到，无法并行的计算，这也是推出transfomers最根本的理由。</p> 
<p id="u206b00d9"></p> 
<p class="img-center"><img alt="" height="648" id="OtC8w" src="https://images2.imgbox.com/7a/02/M2T1ceG2_o.png" width="1176"></p> 
<p id="u36561715">根据这个图，<strong>Self Attention就是让Attention中的q、k、v相等</strong>，所以self attention其实得到的是输入序列中每个部分与整个序列其他部分的关系，注意q、k、v来自于同一个向量但是经过了<strong>不同的投影矩阵</strong>，因此q、k、v是不一样的。</p> 
<p id="u537feef4">我们直接来看transformer中的Self Attention结构图，可以看到transformer是使用的Dot-Product Attention的变体（多除了个根号d，<strong>d是输入序列的通道数</strong>，目的是为了防止Q与K转置的内积过大或过小导致梯度较小），但因为d是设置好的超参数（常数），因此两者其实没有太大差别。</p> 
<p class="img-center"><img alt="" height="90" id="JR4Hy" src="https://images2.imgbox.com/1f/25/oxbuO17R_o.png" width="82"></p> 
<p id="u537feef4">的结果就是Q和K的相似度，经过softmax使其变为0到1的数值后作为V的权重。</p> 
<p id="ub4284965">可以看到上述过程其实就是一系列的矩阵运算，并不包含任何参数，只有上图右侧中的Linear层是可学习的，而Linear层的作用是将Q、K、V投影到<strong>高维语义空间（也就是multi-head attention的作用）</strong>。所以<strong>上述的Self Attention操作实际上是在计算高维语义空间的Q和K的相似度作为V的权重进行了特征融合以获取全局信息</strong>。那么说明我们其实是通过喂数据给模型，指导其学会投影出这么一个语义空间——在此空间中我们期望相似的Q与K具有很高的相似度。</p> 
<p id="uf248df5b">举个句子生成的例子来说，我们以Hello作为模型的输入，World作为下一个词的ground truth，只有当Hello和Word在投影出来的空间中具有较高的相似度，我们的attention机制才会给World较高的权重从而让模型输出World，那么通过自注意力的计算和梯度下降，我们就能让Linear学会让两个词投影到相近的位置</p> 
<p id="u4296d626">4. 为什么position encoding？</p> 
<p id="ub5a058d7">对于序列的seq / rnn 是有时序信息的。而对于Transformer来说，由于句子中的词语都是<strong>同时进入网络进行处理，顺序信息在输入网络时就已丢失。</strong>一句话概括，<strong>Positional Encoding就是句子中词语相对位置的编码，让Transformer保留词语的位置信息。</strong></p> 
<p id="u8e5b56e0"></p> 
<p id="u3bde3383"></p> 
<p id="u58e81f3a"></p> 
<h2 id="F5G2O">CV领域</h2> 
<h3 id="PuDZS"><a href="https://readpaper.com/paper/3094502228" rel="nofollow" title="vision transformer的笔记">vision transformer的笔记</a></h3> 
<h3 id="bia9o"><a href="https://readpaper.com/paper/3030520226" rel="nofollow" title="DETR： End-to-End Object Detection with Transformers">DETR： End-to-End Object Detection with Transformers</a></h3> 
<p id="u1b02cd90"></p> 
<h3 id="O6F2N">clip 论文</h3> 
<p>笔记</p> 
<p id="ue64972c2"><a href="https://readpaper.com/paper/3135367836" rel="nofollow" title="论文笔记">论文笔记</a></p> 
<p id="u1c036c37">多模态鸿沟</p> 
<p id="u8fc5e933">数据集：四亿的图文对，进行对比学习。</p> 
<p id="uc01d645c">对比学习，就是需要正样本和负样本的定义。在这里的正样本就是已经配对好的，反之就是负样本。所以有N个正样本，有N2 -N个负样本</p> 
<h4 id="AE0JI">clip 做目标检测</h4> 
<p id="ud4920ae5"><a href="https://zhuanlan.zhihu.com/p/595169030" rel="nofollow" title="Open-Vocabulary Object Detection">Open-Vocabulary Object Detection</a></p> 
<h4 id="c5fFc">clip 做图像分割</h4> 
<p id="ucd659fd7"><a href="https://zhuanlan.zhihu.com/p/538329106" rel="nofollow" title="CLIPSeg">CLIPSeg</a></p> 
<p id="ud82ed84d"></p> 
<h2 id="tAhz7">Image Caption 领域</h2> 
<h3 id="E0nl5">李飞飞经典</h3> 
<p>Deep Visual-Semantic Alignments for Generating Image Descriptions</p> 
<p id="u578f65c2">文章目标：</p> 
<ul><li id="uc9b41287">We develop a deep neural network model that infers the latent alignment between segments of sentences and the region of the image that they describe.</li></ul> 
<ol><li id="u94ef0d56">提出了一种深度神经网路模型，该模型用来将训练样本中图片中的一些重点部分与生成句中的词组相对应。 <strong>部分图片和词组做对应（1 图像分割 2. 句子分割 3. 目标对应 4. 解码） 匹配来做。</strong> <p class="img-center"><img alt="" height="1046" id="u9f80fae4" src="https://images2.imgbox.com/42/10/18DvoLQN_o.png" width="1200"></p> <p class="img-center"><img alt="" height="1200" id="ufd827cc0" src="https://images2.imgbox.com/c6/77/hgB3wBRo_o.png" width="1196"></p> </li></ol> 
<ul><li id="u79e21c9d">We introduce a multimodal Recurrent Neural Network architecture that takes an input image and generates its description in text.</li></ul> 
<ol><li id="ue59e68fd">提出一种多通道RNN框架来描述一张图片。生成</li></ol> 
<p id="u6f0dc34e"></p> 
<p id="u699c8264"></p> 
<p class="img-center"><img alt="" height="1200" id="u99031b51" src="https://images2.imgbox.com/a7/d3/rScN1iCn_o.png" width="1200"></p> 
<p id="uefc5516c"><a href="https://blog.csdn.net/Skin_Wolf/article/details/106068566" title="解读">解读</a></p> 
<p id="uf335e9e6"></p> 
<h3 id="xvbLo">show and tell 入门</h3> 
<p>Show and Tell: A Neural Image Caption Generator 2015</p> 
<p id="ucdcc2b48"><a href="https://arxiv.org/pdf/1411.4555.pdf" rel="nofollow" title="Show and Tell: A Neural Image Caption Generator 2015">Show and Tell: A Neural Image Caption Generator 2015</a></p> 
<p id="ua91960b5">这个模型的意思呢就是参考机器翻译那样，把图像编码成一种可以用来表示主要特征的机器语言，然后再讲该语言像中英文翻译的那种方法来“翻译”出来，从而形成图像描述。我们可以通过以下公式最大化生成单词的概率</p> 
<p id="ud28945a3">创新点：</p> 
<ol><li id="u316e0372">deep cnn - 把googlenet拿来做预训练，把网络往深了挖。</li><li id="u1cda4024">这篇论文则<strong>首次</strong>提出用一个模型来解决所有的问题，俗称一步到位（狗头）</li></ol> 
<p id="u8c7bc133"></p> 
<p id="u9c39e262">与上篇方法的对比，问题在于：这种方法训练会有一个问题，图像在转序列化时会丢失很多的信息。loss为生成的单词和groud truth的对比，<strong>关键点在于能否把文本生成的越来越好</strong>。</p> 
<p id="u8406cd5e"></p> 
<p class="img-center"><img alt="" height="446" id="uf94b418d" src="https://images2.imgbox.com/12/a6/lGpGsoqm_o.png" width="604"></p> 
<p id="uf32fb9a9"></p> 
<p id="u35574bb7"></p> 
<p class="img-center"><img alt="" height="874" id="ua42a21bf" src="https://images2.imgbox.com/a1/48/9Ek4pUcY_o.png" width="1200"></p> 
<h3 id="SE2r9">Show, Attend and Tell 引入注意力机制</h3> 
<p>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p> 
<p id="u5cacda29"></p> 
<p class="img-center"><img alt="" height="1160" id="u67ef72ee" src="https://images2.imgbox.com/7f/e5/srhdDSl1_o.png" width="1200"></p> 
<p id="u4bd627db">生成的文本应该关注到生成的单词是来自于图像 还是来自于文本，所以引入了注意力机制。</p> 
<p id="u1bbd8590"></p> 
<p id="u40050792"></p> 
<p class="img-center"><img alt="" height="550" id="uf5b74718" src="https://images2.imgbox.com/f4/f7/XOSF8sw5_o.png" width="722"></p> 
<p id="u4a8d09ab"><a href="https://blog.csdn.net/qq_41115379/article/details/111184782" title="解读">解读</a></p> 
<p id="u8a183b75">这篇论文更多关注的是图像的attention，把很多图像的具体信息，比如说颜色，背景 个数，但是也会有很多问题。在下面的论文也有提到，不应该单纯的依赖纯图像注意力。</p> 
<p id="ucd1b062e"></p> 
<h3 id="JhpOU">引入语义的attention</h3> 
<p>Image Captioning with Semantic Attention</p> 
<p id="u77b0772d"></p> 
<p class="img-center"><img alt="" height="1126" id="u5b46a025" src="https://images2.imgbox.com/22/ab/VUcSUvkU_o.png" width="1200"></p> 
<p id="u102f7a76">这篇论文更关注了语义的attention 比如说 on _ table 让文本来做，很容易预测是 on the table ，但是对于cv来说，生成冠词 会浪费很多计算量等等。<strong>词表的生成是出现五次以上的词才有</strong>。</p> 
<p id="u99277650">所以会以此有很多attention的paper：</p> 
<ul><li id="u2d8b5e57">channel attention</li><li id="u2b5caaea">个数的attention</li><li id="ud99925b3">location 的attention</li><li id="uc17dd147">冠词的 attention</li></ul> 
<p id="u6dc9257e"></p> 
<p id="u9e33fb00">例子：</p> 
<p id="uf1a3d683">Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning [<a href="https://arxiv.org/abs/1612.01887" rel="nofollow" title="Paper">Paper</a>] [<a href="https://github.com/yufengm/Papers/blob/master/reviews/lu2016knowing.md" title="Review">Review</a>]</p> 
<p id="u7eed04ca">相关代码：</p> 
<p id="u510102b6"><a href="https://github.com/yufengm/Adaptive" title="GitHub - yufengm/Adaptive: Pytorch Implementation of Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning">GitHub - yufengm/Adaptive: Pytorch Implementation of Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning</a></p> 
<p id="ua7de7122"><a href="https://github.com/jiasenlu/AdaptiveAttention" title='GitHub - jiasenlu/AdaptiveAttention: Implementation of "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning"'>GitHub - jiasenlu/AdaptiveAttention: Implementation of "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning"</a></p> 
<p id="u194bf725">架构图</p> 
<p id="u6bbf7e2f"></p> 
<p class="img-center"><img alt="" height="780" id="ufc062402" src="https://images2.imgbox.com/87/9f/d7aLZHAC_o.png" width="1200"></p> 
<p id="uab421588">结果：</p> 
<p id="u080814a1"></p> 
<p class="img-center"><img alt="" height="1200" id="ufe6ee523" src="https://images2.imgbox.com/2a/92/YTBmLdiw_o.png" width="1200"></p> 
<p id="u7eb769e2"></p> 
<h3 id="qvgFR">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering 2018</h3> 
<p id="u66a73502"></p> 
<p id="u3e83a2b7"></p> 
<p class="img-center"><img alt="" height="1200" id="u54da3ada" src="https://images2.imgbox.com/75/07/Ieh3INjr_o.png" width="1200"></p> 
<h3 id="V1Rly">多任务辅助合作</h3> 
<p>多任务辅助合作</p> 
<p id="u1351cdb7">检索和生成</p> 
<p id="ue6663a50"></p> 
<p class="img-center"><img alt="" height="1090" id="u3274db5d" src="https://images2.imgbox.com/1a/8d/jMS2BOdg_o.png" width="1200"></p> 
<p id="uf89b2ec4"><a href="https://arxiv.org/pdf/2302.04858.pdf" rel="nofollow" title="Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning">Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning</a></p> 
<p id="u696538b3">检索增强的</p> 
<p id="u1b901433"></p> 
<h3 id="QAYuw">细粒度可控</h3> 
<p>淘宝对于此做了很多的工作</p> 
<p id="u2ae88bd9">对于淘宝的搜索，比如说品牌的细粒度。</p> 
<p id="u9ac5e514"></p> 
<p class="img-center"><img alt="" height="1020" id="u903ea571" src="https://images2.imgbox.com/61/68/T9VWlyj6_o.png" width="1200"></p> 
<p id="u5b622212"><a href="https://arxiv.org/pdf/2003.00387.pdf" rel="nofollow" title="论文">论文</a></p> 
<p id="u377afd10"><a href="https://github.com/cshizhe/asg2cap" title="代码">代码</a></p> 
<p id="u5beecb75">clip 对于细粒度的捕获并不足，这篇论文的关注点在于提高的细粒度的信息。</p> 
<p id="uc25445b5">提出了一个三元组的概念，其中某个单词的是可以影响最后一句话的输出的。</p> 
<p id="u92ce7be1">比如说clip 能否 检测到一些位置信息，然后根据位置信息，区域的细粒度，文本的细粒度，生成更多的描述。</p> 
<p id="ua71dc64b"></p> 
<p id="u6b37f672"><strong>vqa的</strong></p> 
<p id="u933911f8"></p> 
<p class="img-center"><img alt="" height="1076" id="u65ff3e5d" src="https://images2.imgbox.com/bf/d3/ZhUnZSE9_o.png" width="1200"></p> 
<p id="u933911f8"><a href="https://arxiv.org/pdf/1612.00837.pdf" rel="nofollow" title="论文">论文</a></p> 
<p id="u5d35178e">nlp的结构化信息 表格 三元组 把这些信息利用好。</p> 
<p id="u867638b1"></p> 
<h3 id="t7Gks">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</h3> 
<p id="u6180db34"></p> 
<p class="img-center"><img alt="" height="1200" id="u1623b249" src="https://images2.imgbox.com/41/f2/SocIgLYi_o.png" width="1200"></p> 
<p id="u392d135c"><a href="https://arxiv.org/pdf/1511.07571.pdf" rel="nofollow" title="论文">论文</a></p> 
<p id="u2c4bba30"></p> 
<p id="u42be2d60"></p> 
<p id="uebbd5848"></p> 
<p id="u3007cef5"></p> 
<h3 id="eqjJL">预训练模型结合</h3> 
<p>与clip结合</p> 
<p id="u18de73f4"><a href="https://arxiv.org/pdf/2211.00575.pdf" rel="nofollow" title="论文">论文</a></p> 
<p id="u8ffe045f"><a href="https://github.com/DavidHuji/CapDec" title="代码">代码</a></p> 
<p id="uf50f9883"></p> 
<p class="img-center"><img alt="" height="850" id="u44543e9d" src="https://images2.imgbox.com/a1/24/HhzzbhrG_o.png" width="1200"></p> 
<p id="uc9ec22ed"></p> 
<p id="u970a1a0f">llm和clip都有着超强的knowledge。都能够把open domain的领域进行一个表达。记忆力机制。</p> 
<p id="u56a42e98">MemCan: Memorizing Style Knowledge for Image Captioning</p> 
<p id="u3f35e335"></p> 
<p>一些问题的思考</p> 
<p id="u4d1241cc"><strong>Image caption = NLG?</strong><br> 1. 跟图像相关性判断？<br> 2. 文本风格自由度？<br> 3. 预训练词汇的相似度？ 有一个paper是根据template来做，比如说第一个为冠词，这样的计算量速度。比如说gpt出现低频词的时候 概率是比较低的<br> 4. <strong>因果推理的源头？不应该是简单的encode-decode。是应该有一些reason的过程。而整个过程是应该有个推理链的。用途是vqa。需要更多统计学习的。</strong></p> 
<p id="u381f4d15"></p> 
<p id="u40899ac0"></p> 
<p class="img-center"><img alt="" height="944" id="u83434514" src="https://images2.imgbox.com/9d/80/HKar9HbW_o.png" width="1056"></p> 
<h3 id="ZNlyA"></h3> 
<h2 id="y2qGr"></h2> 
<h2 id="jd8au"></h2> 
<p id="u4f873bf2"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a246512de612ebcfbf8ed5b4815758d9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">修改git仓库地址</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/147f890dc301a06a0d627e5e7a06d1b5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">在mac中删除本地 Maven 仓库中所有的 .lastUpdated 文件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>