<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>行业前瞻｜Segment Anything 都发布了，耗时、耗力的人工数据标注还有意义吗？附SA-1B（Segment Anything ）数据集国内免费高速下载资源 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="行业前瞻｜Segment Anything 都发布了，耗时、耗力的人工数据标注还有意义吗？附SA-1B（Segment Anything ）数据集国内免费高速下载资源" />
<meta property="og:description" content="本文已获授权，部分有删改。来源：Xtreme1
自 2012 年以来，深度学习技术变革引起的人工智能热潮，这股势头已经持续十年。在去年底 ChatGPT 的出现，大模型的超能力完全展现在大众的视线中，将人工智能行业又推向了一个全新的发展阶段，许多研究者更是惊呼“ChatGPT 爆火后，NLP 技术不存在了” [1]。因为过去的自然语言专家，有着擅长于自己的领域，有人专门做文本分类、有人专门做信息抽取、有人做问答、有人做阅读理解，而在大模型范式下，一个大语言模型就能实现多种NLP任务的完美统一。
计算机视觉（CV）领域，大家也都密切关注着“大一统”模型，所谓的“ImageGPT”以及“多模态 GPT”的发展。
4 月 5 日 Meta 发布了Segment Anything Model（SAM）——第一个图像分割基础模型，可以称得上是当前最先进的一种图像分割模型，其将NLP领域的prompt范式引进CV，让模型通过prompt一键抠图，在照片或视频中对任意对象实现一键分割，并且能够零样本迁移到其他任务[2]，这意味着图片大模型时代已经来临。
同时，让人不禁发问，学术界和商用落地场景使用的、耗时耗力的人工标注的标准答案（Ground truth）是否还有存在的必要？
（文末有图像分割开源数据集推荐）
一、 什么是图像分割？ 图像分割（Image Segmentation）是图像处理中的一种技术，也是计算机视觉领域核心任务之一。它是预测图像中每一个像素所属的类别或者物体，输出不同类别的像素级掩码。简单来说，就是将图像中的每个像素标注为属于哪一个对象，比如人、车、树等等，并精细地标注出每个物体的具体位置和形状。
大体上，图像分割可以分为三个子任务: 实例分割 (instance segmentation) 、语义分割 (semantic segmentation) 、全景分割 (panoptic segmentation)，这三个子任务都有着大量的算法与模型。他们在计算机视觉、医学影像处理、数字艺术等领域都有广泛的应用。
目标检测与语义分割标注
二、“Segment Anything”项目发布了什么？ Meta 发布了“Segment Anything Model（SAM）”和相应的数据集 SA-1B（segment anything），这是一项新的图像分割任务、模型和数据集。
核心亮点：
1. 该模型被设计和训练为可提示性（promptable），支持文本、关键点、边界框等多模态提示。你可以用一个点、一个框、一句话等方式轻松分割出指定物体；甚至接受其他系统的输入提示，比如根据AR/VR头显传来的视觉焦点信息，来选择对应的物体；
2. 可以非常灵活地泛化到新任务和新领域。积累了大量学习经验的SAM 已经能够理解对象的一般概念，不要额外训练，即可对不熟悉的物体和图像进行全自分割标注，可以为任意图像或视频中的任何对象生成掩码；
3. 对于稠密的图片，仍然有非常好的分割效率和效果；
4. 使用高效的SAM模型构建了迄今为止最大的分割数据集SA-1B（segment anything），包括超过 1 亿个 Mask 图和 1100 万张符合许可证的图片。为模型提供了充足的训练数据，有望成为未来计算机视觉分割模型训练和评测的经典数据集。
要知道，以往创建准确的分割模型“需要技术专家通过 AI 模型训练和大量人工精细标注数据进行高度专业化的工作”。而Meta 创建 SAM，旨在减少对专业培训和专业知识的需求，让这个过程更加“平等化”，以求推动计算机视觉研究的进一步发展。
三、 Segment Anything Model 的效果如何？ Meta 表示，SAM 已经掌握了对物体的一般概念，能为任何图像或视频中的任何物体生成 Mask，即使在训练过程中没有遇到过这些物体和图像类型。SAM 足够通用，覆盖了广泛的用例，并可在新的图像“领域”（例如水下照片或细胞显微镜图像）上直接使用，无需额外训练（这种能力通常称为零样本迁移）。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/7f8d8bc8636fd0ca18c3ba88ef68cf84/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-09T21:14:02+08:00" />
<meta property="article:modified_time" content="2023-05-09T21:14:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">行业前瞻｜Segment Anything 都发布了，耗时、耗力的人工数据标注还有意义吗？附SA-1B（Segment Anything ）数据集国内免费高速下载资源</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>本文已获授权，部分有删改。来源：Xtreme1</p> 
<blockquote> 
 <p>自 2012 年以来，<strong>深度学习技术</strong>变革引起的人工智能热潮，这股势头已经持续十年。在去年底 <strong>ChatGPT </strong>的出现，<strong>大模型的超能力</strong>完全展现在大众的视线中，将人工智能行业又推向了一个全新的发展阶段，许多研究者更是惊呼“<strong>ChatGPT 爆火后，NLP 技术不存在了</strong>” [1]。因为过去的自然语言专家，有着擅长于自己的领域，有人专门做文本分类、有人专门做信息抽取、有人做问答、有人做阅读理解，而在<strong>大模型范式下</strong>，一个大语言模型就能实现多种NLP任务的完美统一。</p> 
</blockquote> 
<p>计算机视觉（CV）领域，大家也都密切关注着“大一统”模型，所谓的“<strong>ImageGPT</strong>”以及“<strong>多模态 GPT</strong>”的发展。</p> 
<p>4 月 5 日 <strong>Meta</strong> 发布了<strong>Segment Anything Model（SAM）——第一个图像分割基础模型</strong>，可以称得上是当前最先进的一种图像分割模型，其将NLP领域的prompt范式引进CV，让模型通过prompt一键抠图，在照片或视频中对任意对象实现一键分割，并且能够零样本迁移到其他任务[2]，这意味着<strong>图片大模型时代</strong>已经来临。</p> 
<p>同时，让人不禁发问，学术界和商用落地场景使用的、耗时耗力的人工标注的标准答案（Ground truth）是否还有存在的必要？</p> 
<p>（文末有图像分割开源数据集推荐）</p> 
<h2>一、 什么是图像分割？</h2> 
<p><strong>图像分割（</strong><strong>Image </strong><strong>Segmentation</strong><strong>）</strong>是图像处理中的一种技术，也是计算机视觉领域核心任务之一。它是预测图像中每一个像素所属的类别或者物体，输出不同类别的像素级掩码。简单来说，就是将图像中的每个像素标注为属于哪一个对象，比如人、车、树等等，并精细地标注出每个物体的具体位置和形状。</p> 
<p>大体上，图像分割可以分为三个子任务: <strong>实例分割 (instance segmentation) </strong>、<strong>语义分割 (semantic segmentation)</strong> 、<strong>全景分割 (panoptic segmentation)</strong>，这三个子任务都有着大量的算法与模型。他们在计算机视觉、医学影像处理、数字艺术等领域都有广泛的应用。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4b/72/dtSbhqwU_o.gif"></p> 
<p>目标检测与语义分割标注</p> 
<h2>二、“Segment Anything”项目发布了什么？</h2> 
<p>Meta 发布了“<strong>Segment Anything Model（SAM）</strong>”和相应的数据集 <strong><a class="link-info" href="https://opendatalab.org.cn/SA-1B?source=Y3Nkbg" rel="nofollow" title="SA-1B（segment anything）">SA-1B（segment anything）</a></strong>，这是一项<strong>新的图像分割</strong><strong>任务</strong>、<strong>模型</strong>和<strong>数据集</strong>。</p> 
<p><strong>核心亮点：</strong></p> 
<p><strong>1. </strong>该模型被设计和训练为可提示性（promptable），支持文本、关键点、边界框等多模态提示。你可以用一个点、一个框、一句话等方式轻松分割出指定物体；甚至接受其他系统的输入提示，比如根据AR/VR头显传来的视觉焦点信息，来选择对应的物体；</p> 
<p><strong>2. </strong>可以非常灵活地泛化到新任务和新领域。积累了大量学习经验的SAM 已经能够理解对象的一般概念，不要额外训练，即可对不熟悉的物体和图像进行全自分割标注，可以为任意图像或视频中的任何对象生成掩码；</p> 
<p><strong>3. </strong>对于稠密的图片，仍然有非常好的分割效率和效果；</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2c/42/zq8VsTZs_o.png"></p> 
<p><strong>4. </strong>使用高效的SAM模型构建了迄今为止最大的分割数据集<a class="link-info" href="https://opendatalab.org.cn/SA-1B?source=Y3Nkbg" rel="nofollow" title="SA-1B（segment anything）">SA-1B（segment anything）</a>，包括超过 1 亿个 Mask 图和 1100 万张符合许可证的图片。为模型提供了充足的训练数据，有望成为未来计算机视觉分割模型训练和评测的经典数据集。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/65/51/JqfOpYOT_o.png"></p> 
<p>要知道，以往创建准确的分割模型“需要技术专家通过 AI 模型训练和大量人工精细标注数据进行高度专业化的工作”。而Meta 创建 SAM，旨在减少对专业培训和专业知识的需求，让这个过程更加“平等化”，以求推动计算机视觉研究的进一步发展。</p> 
<h2>三、 Segment Anything Model 的效果如何？</h2> 
<p>Meta 表示，SAM 已经掌握了对物体的一般概念，能为任何图像或视频中的任何物体生成 Mask，即使在训练过程中没有遇到过这些物体和图像类型。SAM 足够通用，覆盖了广泛的用例，并可在新的图像“领域”（例如水下照片或细胞显微镜图像）上直接使用，无需额外训练（这种能力通常称为零样本迁移）。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/1a/17/k9T6AGNL_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e4/42/S2ttHPO9_o.png"></p> 
<p> SAM与人工标注的对比</p> 
<p></p> 
<h2>四、我们所看到的挑战与机遇</h2> 
<h4><strong>市场上早有相似的技术</strong></h4> 
<p>在此之前我们见过很多不错的分割工具，例如 Photoshop 软件或 IOS 系统中自带的抠图功能，它们都能生成不错的效果，也用不错的交互体验提高了图像处理的效率，与SAM模型有着类似的功能。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c9/df/IoXaMOok_o.gif"></p> 
<p>在 iOS 16 及更高版本中，您可以将照片的主题抠图出来，然后复制或共享</p> 
<p></p> 
<h4><strong>开源数据集的“标准答案（Ground truth）错误百出</strong></h4> 
<p>图像算法工程师在商业项目中，经常会要求标注员重新标注开源数据集，这花费了不少的成本。这主要是原因是开源数据集的“标准答案（Ground truth）”并不标准，其中存在这大量的标注错误。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/b6/36/3YgGj0kK_o.png"></p> 
<p>对于一些细分场景的研究时，这些人工标注是不能达到数据质量的要求的。例如，一位工程师在做交通灯的场景相关研究时发现，<a class="link-info" href="https://opendatalab.org.cn/COCO_2017?source=Y3Nkbg" rel="nofollow" title="COCO">COCO</a> 等数据集的标注错误是非常明显的。这些问题也存在于其它的知名数据集，例如 <a class="link-info" href="https://opendatalab.org.cn/CIFAR-100?source=Y3Nkbg" rel="nofollow" title="CIFAR-100">CIFAR-100</a>和<a class="link-info" href="https://opendatalab.org.cn/ImageNet-21k?source=Y3Nkbg" rel="nofollow" title="ImageNet">ImageNet</a>。数据标注是一项容易出错的艰巨任务，其原因既有模糊的标注要求文档说明，也有人为主观判断不一致性等。SAM模型同样也存在部分漏标、误标问题。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5f/86/iNHKPmUH_o.png"></p> 
<p>COCO 数据集关于红绿灯的错误标注</p> 
<p></p> 
<h4><strong>无法胜任于专业领域</strong></h4> 
<p>因此，前文提到的 SAM 与人工标注的比较时，它已经非常接近或者在某些数据分割的表现超越了人工标注结果时，我们只是惊讶——为什么开源数据集的质量这样差？相信也没有人会 100% 使用 ChatGPT 回复的结果，放在自己的文章中，我们在其中不得不接受那些很有道理但是扭曲了事实的“胡说八道”。通用大模型所训练的专业数据不够时，也无法满足项目的需求。在一些严谨的学科，例如在医学诊断，自动驾驶，安防等领域，我们是无法容忍这样的错误的。对于专业领域、非通用型图像，SAM标注不够理想。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/59/8b/Vt4TTHUc_o.png"></p> 
<p>SAM 在医学方面的数据标注并不理想，这要求具有专业背景的医生来完成此项工作</p> 
<p></p> 
<h4><strong>其他挑战</strong></h4> 
<p>这里还有类似于 ChatGPT 已经遇到的算力问题和数据安全。在实际环境中，很多上线的小型模型也无法承受过大的运行成本等。</p> 
<p></p> 
<h4><strong>机遇</strong></h4> 
<p>确实，随着人工智能技术不断的发展，传统的 NLP、CV 技术未来可能会逐渐淘汰。未来的研究方向应该聚焦于更深层次、更抽象的框架下进行思考和探索。</p> 
<p><strong>● 接受并利用尖端技术</strong>：革命性的新技术所带来的不应是对行业的惶恐的绝望，我们要试图去理解并利用它，这些新范式将帮助我们提高已有的生产效率，从而为未来的技术创新奠定坚实的基础。在很多细分赛道，专业的人士仍需继续深耕。</p> 
<p><strong>● </strong><strong>软件开源</strong>：AI 技术的迅猛发展得益于开源的理念，它让每个人可以站在巨人的肩膀上，这也是我们打造 Xtreme1 的初衷：<em>https://github.com/xtreme1-io/xtreme1/</em></p> 
<p><em>Xtreme1 是全球首个开源多模态训练数据平台，通过提供 AI 赋能的软件工具、数万项目提炼的本体中心和丰富的数据治理特性，来加速多模态训练数据的处理效率，进而提高 AI 工程师的建模效率。特别是在 2D &amp; 3D 多模态融合数据方面，标注效率的提升可达 72%。自 2022 年 9 月 15 日正式开源以来，Xtreme1 平台已经在 2022 年 12 月 15 日成为了 LF AI &amp; DATA 托管项目。</em></p> 
<p><strong>● 数据开源</strong>：没有优秀的数据，AI 是无法正确运作的。我们最近也在研究全球首个多模态数据，涵盖了最新的传感器设备以及精准地人工标注数据，敬请期待~</p> 
<p> </p> 
<h2>五、SA-1B下载脚本与图像分割评测数据集</h2> 
<p>最后，分享SA-1B数据集快速下载脚本及其评测数据集资源：</p> 
<p><strong>● 下载代码：</strong></p> 
<pre><code>cat ~/fb-sam.txt | parallel -j 5  --colsep $'\t'  wget -nc  -c  {2}    -O {1}</code></pre> 
<p>注：fb-sam.txt是跳过标题行的<em><a class="link-info" href="https://scontent-hkg4-1.xx.fbcdn.net/m1/v/t6/An8MNcSV8eixKBYJ2kyw6sfPh-J9U4tH2BV7uPzibNa0pu4uHi6fyXdlbADVO4nfvsWpTwR8B0usCARHTz33cBQNrC0kWZsD1MbBWjw.txt?ccb=10-5&amp;oh=00_AfBLbYRBPWq8Ppbjgec6pego_Iz8Z4tMVlcnfKKM2Fk2Zg&amp;oe=64817B58&amp;_nc_sid=fb0754" rel="nofollow" title="数据集 txt ">数据集 txt </a></em>的文件副本。</p> 
<p></p> 
<p>● <a href="https://opendatalab.org.cn/SA-1B?source=Y3Nkbg" rel="nofollow" title="SA-1B（segment anything）">SA-1B（segment anything）</a>国内免费快速下载地址：</p> 
<p><a href="https://opendatalab.org.cn/SA-1B?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/SA-1B">https://opendatalab.com/SA-1B</a></p> 
<p></p> 
<p><strong>● 评测数据集资源（部分）：</strong></p> 
<p><strong>ADE20K：</strong><em><a class="link-info" href="https://opendatalab.org.cn/ADE20K_2016?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/ADE20K_2016">https://opendatalab.com/ADE20K_2016</a></em></p> 
<p><strong>NDD20 (Northumberland Dolphin Dataset 2020)：</strong><em><a class="link-info" href="https://opendatalab.org.cn/NDD20?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/NDD20">https://opendatalab.com/NDD20</a></em></p> 
<p><strong>LVIS：</strong><em><a class="link-info" href="https://opendatalab.org.cn/LVIS_v1.0?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/LVIS">https://opendatalab.com/LVIS</a></em></p> 
<p><strong>STREETS：</strong><em><a class="link-info" href="https://opendatalab.org.cn/STREETS?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/STREETS">https://opendatalab.com/STREETS</a></em></p> 
<p><strong>VISOR：</strong><em><a class="link-info" href="https://opendatalab.org.cn/VISOR?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/VISOR">https://opendatalab.com/VISOR</a></em></p> 
<p><strong>WoodScape：</strong><em><a class="link-info" href="https://opendatalab.com/WoodScape?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/WoodScape">https://opendatalab.com/WoodScape</a></em></p> 
<p><strong>TrashCan：</strong><em><a class="link-info" href="https://opendatalab.com/TrashCan?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/TrashCan">https://opendatalab.com/TrashCan</a></em></p> 
<p><strong>PIDray：</strong><em><a class="link-info" href="https://opendatalab.com/PIDray?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/PIDray">https://opendatalab.com/PIDray</a></em></p> 
<p><strong>GTEA (Georgia Tech Egocentric Activity)：</strong><em><a class="link-info" href="https://opendatalab.com/GTEA?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.com/GTEA">https://opendatalab.com/GTEA</a></em></p> 
<p></p> 
<p>本文作者｜张子千 Nico</p> 
<p>本期封面图来自 Segment Anything | Meta</p> 
<p></p> 
<p><strong>引用</strong></p> 
<p>[1] ChatGPT爆火后，NLP技术不存在了. </p> 
<p><em>https://mp.weixin.qq.com/s/FknHZ_FFdwdofp5vn9ot3g</em>；</p> 
<p>[2] Segment Anything. </p> 
<p><em>https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation</em>；</p> 
<p>[3] Create and share photo cutouts on your iPhone. </p> 
<p><em>https://support.apple.com/en-us/HT213459</em>；</p> 
<p>[4] The Mislabelled Objects in COCO. </p> 
<p><em>https://www.neuralception.com/mislabelled-traffic</em>；</p> 
<p>[5] How I found nearly 300,000 errors in MS COCO. </p> 
<p><em>https://medium.com/@jamie_34747/how-i-found-nearly-300-000-errors-in-ms-coco-79d382edf22b</em>；</p> 
<p>[6] 感谢OpenDataLab提供的数据集支持，更多数据集请访问：<a class="link-info" href="https://opendatalab.org.cn/?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.org.cn/">https://opendatalab.org.cn/</a><em>；</em></p> 
<p></p> 
<p>-END-</p> 
<p>更多公开数据集，欢迎访问OpenDataLab官网查看与下载：<strong><a class="link-info" href="https://opendatalab.org.cn/?source=Y3Nkbg" rel="nofollow" title="https://opendatalab.org.cn/">https://opendatalab.org.cn/</a></strong></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f4ffba25208e17ecf0600a421246272c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">数据库设计与优化</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/aaf28e9d523b76ef82130b0feb879e73/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue项目Tag导航</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>