<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习笔记（吴恩达） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习笔记（吴恩达）" />
<meta property="og:description" content="目录
第1周
1.引言(Introduction)
1.3 监督学习
1.4 无监督学习
2.单变量线性回归（Linear Regression with One Variable）
2.1模型表示
2.2 代价函数
2.5 梯度下降
2.7 梯度下降的线性回归
第2周
4.多变量线性回归(Linear Regression with Multiple Variables）
4.1 多维特征
4.2 多变量梯度下降
4.3 梯度下降法实践1-特征缩放
4.5 特征和多项式回归
4.6 正规方程
5.6 向量化
第3周
6.逻辑回归(Logistic Regression)
6.1 分类问题
6.2 假说表示
6.3 判定边界
6.4 代价函数
6.5 简化的成本函数和梯度下降
6.7 多类别分类：一对多
7.正则化(Regularization)
7.1 过拟合的问题
7.2 代价函数
7.3 正则化线性回归
7.4 正则化的逻辑回归模型
第4周
8. 神经网络的表述(Neural Networks: Representation)
8.3 模型表示1
8.4 模型表示2 8." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/02b05176db65fa5a1439999e4b4ef8ea/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-07T19:23:49+08:00" />
<meta property="article:modified_time" content="2023-05-07T19:23:49+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习笔记（吴恩达）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E7%AC%AC1%E5%91%A8-toc" style="margin-left:0px;"><a href="#%E7%AC%AC1%E5%91%A8" rel="nofollow">第1周</a></p> 
<p id="1.%E5%BC%95%E8%A8%80(Introduction)-toc" style="margin-left:40px;"><a href="#1.%E5%BC%95%E8%A8%80%28Introduction%29" rel="nofollow">1.引言(Introduction)</a></p> 
<p id="1.3%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:80px;"><a href="#1.3%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">1.3 监督学习</a></p> 
<p id="1.4%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-toc" style="margin-left:80px;"><a href="#1.4%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" rel="nofollow">1.4 无监督学习</a></p> 
<p id="2.%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear%20Regression%C2%A0with%20One%20Variable%EF%BC%89-toc" style="margin-left:40px;"><a href="#2.%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear%20Regression%C2%A0with%20One%20Variable%EF%BC%89" rel="nofollow">2.单变量线性回归（Linear Regression with One Variable）</a></p> 
<p id="2.1%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA-toc" style="margin-left:80px;"><a href="#2.1%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA" rel="nofollow">2.1模型表示</a></p> 
<p id="%C2%A02.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#%C2%A02.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" rel="nofollow">2.2 代价函数</a></p> 
<p id="2.5%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-toc" style="margin-left:80px;"><a href="#2.5%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" rel="nofollow">2.5 梯度下降</a></p> 
<p id="2.7%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-toc" style="margin-left:80px;"><a href="#2.7%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92" rel="nofollow">2.7 梯度下降的线性回归</a></p> 
<p id="%E7%AC%AC2%E5%91%A8-toc" style="margin-left:0px;"><a href="#%E7%AC%AC2%E5%91%A8" rel="nofollow">第2周</a></p> 
<p id="4.%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(Linear%20Regression%20with%20Multiple%20Variables%EF%BC%89-toc" style="margin-left:40px;"><a href="#4.%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%28Linear%20Regression%20with%20Multiple%20Variables%EF%BC%89" rel="nofollow">4.多变量线性回归(Linear Regression with Multiple Variables）</a></p> 
<p id="4.1%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81-toc" style="margin-left:80px;"><a href="#4.1%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81" rel="nofollow">4.1 多维特征</a></p> 
<p id="%C2%A04.2%20%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-toc" style="margin-left:80px;"><a href="#%C2%A04.2%20%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" rel="nofollow">4.2 多变量梯度下降</a></p> 
<p id="4.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B51-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-toc" style="margin-left:80px;"><a href="#4.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B51-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE" rel="nofollow">4.3 梯度下降法实践1-特征缩放</a></p> 
<p id="%C2%A04.5%20%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92-toc" style="margin-left:80px;"><a href="#%C2%A04.5%20%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92" rel="nofollow">4.5 特征和多项式回归</a></p> 
<p id="4.6%20%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B-toc" style="margin-left:80px;"><a href="#4.6%20%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B" rel="nofollow">4.6 正规方程</a></p> 
<p id="%C2%A05.6%20%E5%90%91%E9%87%8F%E5%8C%96-toc" style="margin-left:80px;"><a href="#%C2%A05.6%20%E5%90%91%E9%87%8F%E5%8C%96" rel="nofollow">5.6 向量化</a></p> 
<p id="%E7%AC%AC3%E5%91%A8-toc" style="margin-left:0px;"><a href="#%E7%AC%AC3%E5%91%A8" rel="nofollow">第3周</a></p> 
<p id="6.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(Logistic%20Regression)-toc" style="margin-left:40px;"><a href="#6.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%28Logistic%20Regression%29" rel="nofollow">6.逻辑回归(Logistic Regression)</a></p> 
<p id="6.1%20%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#6.1%20%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98" rel="nofollow">6.1 分类问题</a></p> 
<p id="%C2%A06.2%20%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA-toc" style="margin-left:80px;"><a href="#%C2%A06.2%20%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA" rel="nofollow">6.2 假说表示</a></p> 
<p id="6.3%20%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C-toc" style="margin-left:80px;"><a href="#6.3%20%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C" rel="nofollow">6.3 判定边界</a></p> 
<p id="%C2%A06.4%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#%C2%A06.4%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" rel="nofollow">6.4 代价函数</a></p> 
<p id="%C2%A06.5%20%E7%AE%80%E5%8C%96%E7%9A%84%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-toc" style="margin-left:80px;"><a href="#%C2%A06.5%20%E7%AE%80%E5%8C%96%E7%9A%84%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" rel="nofollow">6.5 简化的成本函数和梯度下降</a></p> 
<p id="6.7%20%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A-toc" style="margin-left:80px;"><a href="#6.7%20%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A" rel="nofollow">6.7 多类别分类：一对多</a></p> 
<p id="%C2%A07.%E6%AD%A3%E5%88%99%E5%8C%96(Regularization)-toc" style="margin-left:40px;"><a href="#%C2%A07.%E6%AD%A3%E5%88%99%E5%8C%96%28Regularization%29" rel="nofollow">7.正则化(Regularization)</a></p> 
<p id="7.1%20%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E9%97%AE%E9%A2%98-toc" style="margin-left:80px;"><a href="#7.1%20%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E9%97%AE%E9%A2%98" rel="nofollow">7.1 过拟合的问题</a></p> 
<p id="7.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#7.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" rel="nofollow">7.2 代价函数</a></p> 
<p id="%C2%A07.3%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-toc" style="margin-left:80px;"><a href="#%C2%A07.3%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92" rel="nofollow">7.3 正则化线性回归</a></p> 
<p id="%C2%A07.4%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px;"><a href="#%C2%A07.4%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B" rel="nofollow">7.4 正则化的逻辑回归模型</a></p> 
<p id="%C2%A0%E7%AC%AC4%E5%91%A8-toc" style="margin-left:0px;"><a href="#%C2%A0%E7%AC%AC4%E5%91%A8" rel="nofollow">第4周</a></p> 
<p id="8.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E8%BF%B0(Neural%20Networks%3A%20Representation)-toc" style="margin-left:40px;"><a href="#8.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E8%BF%B0%28Neural%20Networks%3A%20Representation%29" rel="nofollow">8. 神经网络的表述(Neural Networks: Representation)</a></p> 
<p id="8.3%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA1-toc" style="margin-left:80px;"><a href="#8.3%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA1" rel="nofollow">8.3 模型表示1</a></p> 
<p id="8.4%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA2%C2%A0-toc" style="margin-left:80px;"><a href="#8.4%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA2%C2%A0" rel="nofollow">8.4 模型表示2 </a></p> 
<p id="%C2%A08.7%20%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB-toc" style="margin-left:80px;"><a href="#%C2%A08.7%20%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB" rel="nofollow">8.7 多类分类</a></p> 
<p id="%C2%A0%E7%AC%AC5%E5%91%A8-toc" style="margin-left:0px;"><a href="#%C2%A0%E7%AC%AC5%E5%91%A8" rel="nofollow">第5周</a></p> 
<p id="9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0(Neural%20Networks%3A%20Learning)-toc" style="margin-left:40px;"><a href="#9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%28Neural%20Networks%3A%20Learning%29" rel="nofollow">9.神经网络的学习(Neural Networks: Learning)</a></p> 
<p id="9.1%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#9.1%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" rel="nofollow">9.1 代价函数</a></p> 
<p id="%C2%A09.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;"><a href="#%C2%A09.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" rel="nofollow">9.2 反向传播算法</a></p> 
<p id="%C2%A09.5%20%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C-toc" style="margin-left:80px;"><a href="#%C2%A09.5%20%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C" rel="nofollow">9.5 梯度检验</a></p> 
<p id="9.6%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96-toc" style="margin-left:80px;"><a href="#9.6%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96" rel="nofollow">9.6 随机初始化</a></p> 
<p id="9.7%20%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5-toc" style="margin-left:80px;"><a href="#9.7%20%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5" rel="nofollow">9.7 综合起来</a></p> 
<p id="%E7%AC%AC6%E5%91%A8-toc" style="margin-left:0px;"><a href="#%E7%AC%AC6%E5%91%A8" rel="nofollow">第6周</a></p> 
<p id="10.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE(Advice%20for%20Applying%20Machine%20Learning)-toc" style="margin-left:40px;"><a href="#10.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE%28Advice%20for%20Applying%20Machine%20Learning%29" rel="nofollow">10.应用机器学习的建议(Advice for Applying Machine Learning)</a></p> 
<p id="10.2%20%E8%AF%84%E4%BC%B0%E4%B8%80%E4%B8%AA%E5%81%87%E8%AE%BE-toc" style="margin-left:80px;"><a href="#10.2%20%E8%AF%84%E4%BC%B0%E4%B8%80%E4%B8%AA%E5%81%87%E8%AE%BE" rel="nofollow">10.2 评估一个假设</a></p> 
<p id="%C2%A010.3%20%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%88%C2%A0Model%20Selection%20and%20Train_Validation_Test%20Sets%EF%BC%89-toc" style="margin-left:80px;"><a href="#%C2%A010.3%20%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%88%C2%A0Model%20Selection%20and%20Train_Validation_Test%20Sets%EF%BC%89" rel="nofollow">10.3 模型选择和交叉验证集（ Model Selection and Train_Validation_Test Sets）</a></p> 
<p id="%C2%A010.4%20%E8%AF%8A%E6%96%AD%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88Diagnosing%20bias%20vs.%20variance%EF%BC%89-toc" style="margin-left:80px;"><a href="#%C2%A010.4%20%E8%AF%8A%E6%96%AD%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88Diagnosing%20bias%20vs.%20variance%EF%BC%89" rel="nofollow">10.4 诊断偏差和方差（Diagnosing bias vs. variance）</a></p> 
<p id="10.5%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E5%81%8F%E5%B7%AE%2F%E6%96%B9%E5%B7%AE%EF%BC%88Regularization%20and%20Bias_Variance%EF%BC%89-toc" style="margin-left:80px;"><a href="#10.5%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E5%81%8F%E5%B7%AE%2F%E6%96%B9%E5%B7%AE%EF%BC%88Regularization%20and%20Bias_Variance%EF%BC%89" rel="nofollow">10.5 正则化和偏差/方差（Regularization and Bias_Variance）</a></p> 
<p id="10.6%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF-toc" style="margin-left:80px;"><a href="#10.6%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF" rel="nofollow">10.6 学习曲线</a></p> 
<p id="%C2%A010.7%20%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88-toc" style="margin-left:80px;"><a href="#%C2%A010.7%20%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88" rel="nofollow">10.7 决定下一步做什么</a></p> 
<p id="%E7%AC%AC8%E5%91%A8-toc" style="margin-left:0px;"><a href="#%E7%AC%AC8%E5%91%A8" rel="nofollow">第8周</a></p> 
<p id="13.%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89-toc" style="margin-left:40px;"><a href="#13.%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89" rel="nofollow">13.聚类（Clustering）</a></p> 
<p id="13.1%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%AE%80%E4%BB%8B-toc" style="margin-left:80px;"><a href="#13.1%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%AE%80%E4%BB%8B" rel="nofollow">13.1 无监督学习：简介</a></p> 
<p id="%C2%A013.2%20K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;"><a href="#%C2%A013.2%20K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95" rel="nofollow">13.2 K-均值算法</a></p> 
<p id="%C2%A013.3%20%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87-toc" style="margin-left:80px;"><a href="#%C2%A013.3%20%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87" rel="nofollow">13.3 优化目标</a></p> 
<p id="13.4%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%C2%A0-toc" style="margin-left:80px;"><a href="#13.4%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%C2%A0" rel="nofollow">13.4 随机初始化 </a></p> 
<p id="13.5%20%E9%80%89%E6%8B%A9%E8%81%9A%E7%B1%BB%E6%95%B0-toc" style="margin-left:80px;"><a href="#13.5%20%E9%80%89%E6%8B%A9%E8%81%9A%E7%B1%BB%E6%95%B0" rel="nofollow">13.5 选择聚类数</a></p> 
<p id="%C2%A0%E9%99%8D%E7%BB%B4(Dimensionality%20Reduction)-toc" style="margin-left:40px;"><a href="#%C2%A0%E9%99%8D%E7%BB%B4%28Dimensionality%20Reduction%29" rel="nofollow">14.降维(Dimensionality Reduction)</a></p> 
<p id="14.1%20%E5%8A%A8%E6%9C%BA%E4%B8%80%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%88Motivation%20I_%20Data%20Compression%EF%BC%89-toc" style="margin-left:80px;"><a href="#14.1%20%E5%8A%A8%E6%9C%BA%E4%B8%80%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%88Motivation%20I_%20Data%20Compression%EF%BC%89" rel="nofollow">14.1 动机一：数据压缩（Motivation I_ Data Compression）</a></p> 
<p id="14.2%20%E5%8A%A8%E6%9C%BA%E4%BA%8C%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88%C2%A0Motivation%20II_%20Visualization%EF%BC%89-toc" style="margin-left:80px;"><a href="#14.2%20%E5%8A%A8%E6%9C%BA%E4%BA%8C%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88%C2%A0Motivation%20II_%20Visualization%EF%BC%89" rel="nofollow">14.2 动机二：数据可视化（ Motivation II_ Visualization）</a></p> 
<p id="14.3%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98%EF%BC%88Principal%20Component%20Analysis%20Problem%20Formulation%EF%BC%89-toc" style="margin-left:80px;"><a href="#14.3%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98%EF%BC%88Principal%20Component%20Analysis%20Problem%20Formulation%EF%BC%89" rel="nofollow">14.3 主成分分析问题（Principal Component Analysis Problem Formulation）</a></p> 
<p id="14.4%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;"><a href="#14.4%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95" rel="nofollow">14.4 主成分分析算法</a></p> 
<p id="%C2%A014.5%20%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86%E7%9A%84%E6%95%B0%E9%87%8F-toc" style="margin-left:80px;"><a href="#%C2%A014.5%20%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86%E7%9A%84%E6%95%B0%E9%87%8F" rel="nofollow">14.5 选择主成分的数量</a></p> 
<p id="14.6%20%E9%87%8D%E5%BB%BA%E5%8E%8B%E7%BC%A9%E8%A1%A8%E7%A4%BA-toc" style="margin-left:80px;"><a href="#14.6%20%E9%87%8D%E5%BB%BA%E5%8E%8B%E7%BC%A9%E8%A1%A8%E7%A4%BA" rel="nofollow">14.6 重建压缩表示</a></p> 
<p id="%C2%A014.7%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8%E5%BB%BA%E8%AE%AE-toc" style="margin-left:80px;"><a href="#%C2%A014.7%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8%E5%BB%BA%E8%AE%AE" rel="nofollow">14.7 主成分分析法的应用建议</a></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/2e/a2/ehlaznw1_o.png"></p> 
<h2 id="%E7%AC%AC1%E5%91%A8"><strong>第1周</strong></h2> 
<h3 id="1.%E5%BC%95%E8%A8%80(Introduction)"><strong>1.引言(Introduction)</strong></h3> 
<h4 id="1.3%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><strong>1.3 监督学习</strong></h4> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffd900;">监督学习</span>指的就是我们给学习算法一个数据集。</p> 
<p style="margin-left:.0001pt;text-align:justify;"><span style="background-color:#ffd900;">回归</span>这个词的意思是，我们在试着推测出这一系列连续值属性。</p> 
<p><span style="background-color:#ffd900;">分类</span>指的是，我们试着推测出离散的输出值：0或1良性或恶性，而事实上在分类问题中，输出可能不止两个值。</p> 
<h4 id="1.4%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><strong><strong><strong>1.4 无监督学习</strong></strong></strong></h4> 
<p><span style="background-color:#ffd900;">无监督学习</span>中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签。针对数据集，无监督学习就能判断出数据有两个不同的<span style="background-color:#ffd900;">聚集簇</span>。</p> 
<h3 id="2.%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88Linear%20Regression%C2%A0with%20One%20Variable%EF%BC%89"><strong>2.单变量线性回归（Linear Regression with One Variable）</strong></h3> 
<h4 id="2.1%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA"><strong>2.1模型表示</strong></h4> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffd900;"><img alt="" height="28" src="https://images2.imgbox.com/77/6a/iiIuZ61O_o.png" width="124"></span>，因为只含有一个特征/输入变量，因此这样的问题叫作<span style="background-color:#ffd900;">单变量线性回归问题</span>。</p> 
<h4 id="%C2%A02.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><strong><a name="_Toc38636782"><span style="color:#0d0016;">2.2 </span></a>代价函数</strong></h4> 
<p>目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得<span style="background-color:#ffd900;">代价函数</span> <img alt="" height="35" src="https://images2.imgbox.com/f1/64/I6wJQVDf_o.png" width="252">最小。代价函数也被称作平方误差函数，平方误差代价函数可能是解决回归问题最常用的手段了。</p> 
<h4 id="2.5%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><strong><a name="_Toc38636785"><span style="color:#0d0016;">2.5 </span></a><span style="color:#0d0016;">梯度下降</span></strong></h4> 
<p> <span style="background-color:#ffd900;">梯度下降</span>是一个用来求函数最小值的算法，<span style="background-color:#ffd900;">批量梯度下降</span>（<strong>batch gradient descent</strong>）算法的公式为：</p> 
<p><img alt="" height="60" src="https://images2.imgbox.com/88/f6/LAi0N7bL_o.png" width="287"></p> 
<p>其中<em>a</em> 是<span style="background-color:#ffd900;">学习率</span>（<strong>learning rate</strong>），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。 如果<em><span style="background-color:#ffd900;">a</span></em><span style="background-color:#ffd900;"> 太小</span>的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。如果<em><span style="background-color:#ffd900;">a</span></em><span style="background-color:#ffd900;"> 太大</span>，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点。</p> 
<p style="margin-left:0;text-align:justify;">结合梯度下降法，以及平方代价函数，我们会得出第一个机器学习算法，即<span style="background-color:#ffd900;">线性回归算法</span>。</p> 
<h4 id="2.7%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636787"><span style="color:#0d0016;">2.7 </span></a><span style="color:#0d0016;">梯度下降的线性回归</span></strong></h4> 
<p>将梯度下降和代价函数结合，应用于具体的拟合直线的线性回归算法里。</p> 
<p><img alt="" height="166" src="https://images2.imgbox.com/20/ce/gREHRcyC_o.png" width="512"></p> 
<p> <img alt="" height="198" src="https://images2.imgbox.com/6b/4d/TYmKW7dj_o.png" width="318"></p> 
<p>上述称为<span style="background-color:#ffd900;"> 批量梯度下降。</span></p> 
<h2 id="%E7%AC%AC2%E5%91%A8" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636796"><span style="color:#0d0016;">第2</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="4.%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92(Linear%20Regression%20with%20Multiple%20Variables%EF%BC%89" style="margin-left:0px;text-align:justify;"><strong><span style="color:#0d0016;">4.多</span><a name="_Toc38636797"><span style="color:#0d0016;">变量线性回归(Linear Regression with Multiple Variables）</span></a></strong></h3> 
<h4 id="4.1%20%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636798"><span style="color:#0d0016;">4.1 </span></a><span style="color:#0d0016;">多维特征</span></strong></h4> 
<p><img alt="" height="27" src="https://images2.imgbox.com/97/c5/UWZCwZQT_o.png" width="279"></p> 
<p>此时模型中的参数是一个<img alt="" height="16" src="https://images2.imgbox.com/26/99/aoE2AHiU_o.png" width="33"> 维的向量，任何一个训练实例也都是<img alt="" height="16" src="https://images2.imgbox.com/04/af/8Dx2Qn5J_o.png" width="33"> 维的向量，特征矩阵<img alt="" height="16" src="https://images2.imgbox.com/87/d4/G7dchh8Q_o.png" width="9"> 的维度是 <img alt="" height="16" src="https://images2.imgbox.com/02/74/h99b5bTG_o.png" width="69"> 。 因此公式可以简化为：<span style="background-color:#ffd900;"><img alt="" height="17" src="https://images2.imgbox.com/9c/a0/swrUt56e_o.png" width="78"></span></p> 
<h4 id="%C2%A04.2%20%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><strong><a name="_Toc38636799"><span style="color:#0d0016;">4.2 </span></a><span style="color:#0d0016;">多变量梯度下降</span></strong></h4> 
<p>找出使得代价函数最小的一系列参数。 多变量线性回归的<span style="background-color:#ffd900;">批量梯度下降</span>算法：</p> 
<p><img alt="" height="152" src="https://images2.imgbox.com/f4/55/tS24ORBX_o.png" width="318"></p> 
<p>开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p> 
<h4 id="4.3%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B51-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636800"><span style="color:#0d0016;">4.3 </span></a><span style="color:#0d0016;">梯度下降法实践1-特征缩放</span></strong></h4> 
<p>面对多维特征问题的时候，要保证这些<span style="background-color:#ffd900;">特征都具有相近的尺度</span>，这将帮助梯度下降算法更快地收敛。尝试<span style="background-color:#ffd900;">将所有特征的尺度都尽量缩放到-1到1</span>之间。</p> 
<p><img alt="" height="37" src="https://images2.imgbox.com/1d/a8/QkwjeoIn_o.png" width="311"></p> 
<h4 id="%C2%A04.5%20%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><strong><a name="_Toc38636802"><span style="color:#0d0016;">4.5 </span></a><span style="color:#0d0016;">特征和多项式回归</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：<img alt="" height="27" src="https://images2.imgbox.com/04/3a/j4J7x0Ik_o.png" width="189">或者三次方模型：<img alt="" height="28" src="https://images2.imgbox.com/36/c5/337I7nYw_o.png" width="240"></p> 
<p style="margin-left:0;text-align:justify;">通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：<span style="background-color:#ffd900;"><img alt="" height="29" src="https://images2.imgbox.com/19/e9/EHBSEiUA_o.png" width="120"></span></p> 
<p>从而将模型转化为线性回归模型。采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</p> 
<h4 id="4.6%20%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636803"><span style="color:#0d0016;">4.6 </span></a><span style="color:#0d0016;">正规方程</span></strong></h4> 
<p>求解方程来找出使得代价函数最小的参数，<span style="background-color:#ffd900;">正规方程</span>解出向量<img alt="" height="17" src="https://images2.imgbox.com/25/d2/CjjjV3vD_o.png" width="103">  </p> 
<p style="margin-left:0;text-align:justify;">梯度下降与正规方程的比较：</p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:bottom;width:207.4pt;"> <p style="margin-left:0;text-align:center;">梯度下降</p> </td><td style="border-color:#000000;vertical-align:bottom;width:207.4pt;"> <p style="margin-left:0;text-align:center;">正规方程</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">需要选择学习率<em>α</em></p> </td><td style="vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">不需要</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">需要多次迭代</p> </td><td style="vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">一次运算得出</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">当特征数量<em>n</em> 大时也能较好适用</p> </td><td style="vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">需要求逆  如果特征数量<em>n</em> 较大则<span style="background-color:#ffd900;">运算代价大</span>，因为矩阵逆的计算时间复杂度为<em>O</em><em>n</em><em>3</em> ，通常来说当<em>n</em> 小于10000 时还是可以接受的</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">适用于各种类型的模型</p> </td><td style="vertical-align:top;width:207.4pt;"> <p style="margin-left:0;text-align:justify;">只适用于线性模型，不适合逻辑回归模型等其他模型</p> </td></tr></tbody></table> 
<p>总结，只要特征变量的数目并不大，标准方程是一个很好的计算参数<em>θ</em> 的替代方法</p> 
<h4 id="%C2%A05.6%20%E5%90%91%E9%87%8F%E5%8C%96"><strong><a name="_Toc38636811"><span style="color:#0d0016;">5.6 </span></a><span style="color:#0d0016;">向量化</span></strong></h4> 
<p>写代码做矩阵乘法，不如用<span style="background-color:#ffd900;">合适的向量化方法</span>来实现。</p> 
<h2 id="%E7%AC%AC3%E5%91%A8" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636813"><span style="color:#0d0016;">第3</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="6.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(Logistic%20Regression)"><strong><span style="color:#0d0016;">6.</span><a name="_Toc38636814"><span style="color:#0d0016;">逻辑回归(Logistic Regression)</span></a></strong></h3> 
<h4 id="6.1%20%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636815"><span style="color:#0d0016;">6.1 </span></a><span style="color:#0d0016;">分类问题</span></strong></h4> 
<p>在分类问题中，你要预测的变量 y 是离散的值，我们将学习一种叫做<span style="background-color:#ffd900;">逻辑回归</span>。将因变量(<strong>dependent variable</strong>)可能属于的两个类分别称为负向类（<strong>negative class</strong>）和正向类（<strong>positive class</strong>），则因变量<img alt="" height="27" src="https://images2.imgbox.com/cc/c1/S7G42tDl_o.png" width="67"> ，其中 0 表示负向类，1 表示正向类。这个算法的性质是：<span style="background-color:#ffd900;">它的输出值永远在0到 1 之间</span>。这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它适用于标签 <em>y</em> 取值离散的情况。</p> 
<h4 id="%C2%A06.2%20%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA"><strong><a name="_Toc38636816"><span style="color:#0d0016;">6.2 </span></a><span style="color:#0d0016;">假说表示</span></strong></h4> 
<p>逻辑回归模型的假设是： <span style="background-color:#ffd900;"><img alt="" height="17" src="https://images2.imgbox.com/59/c1/rSloGUaX_o.png" width="98"></span>  其中： <img alt="" height="16" src="https://images2.imgbox.com/c4/95/do2EA1g6_o.png" width="9">  代表特征向量 <img alt="" height="16" src="https://images2.imgbox.com/53/05/HdQuivyL_o.png" width="9"> 代表逻辑函数（<strong>logistic function</strong>)，常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为：<img alt="" height="24" src="https://images2.imgbox.com/fe/17/enEURXYX_o.png" width="76"> 。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="20" src="https://images2.imgbox.com/b3/9d/HY8M5tXd_o.png" width="49">的作用是，对于给定的输入变量，<span style="background-color:#ffd900;">根据选择的参数计算输出变量=1的可能性</span>（<strong>estimated probablity</strong>）即<img alt="" height="28" src="https://images2.imgbox.com/23/f7/jQgNjIZK_o.png" width="158"> 例如，如果对于给定的<em>x</em> ，通过已经确定的参数计算得出<img alt="" height="20" src="https://images2.imgbox.com/9b/3e/R7oNSfQN_o.png" width="49"><em>=0.7</em> ，则表示有70%的几率<em>y</em> 为正向类，相应地<em>y</em> 为负向类的几率为1-0.7=0.3。</p> 
<h4 id="6.3%20%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636817"><span style="color:#0d0016;">6.3 </span></a><span style="color:#0d0016;">判定边界</span></strong></h4> 
<p><span style="background-color:#ffd900;">决策边界</span>(<strong>decision boundary</strong>)，我们可以绘制直线<img alt="" height="16" src="https://images2.imgbox.com/29/05/AVeypWe8_o.png" width="70"> ，这条线便是我们模型的分界线，将预测为1的区域和预测为 0的区域分隔开。</p> 
<p><img alt="" height="169" src="https://images2.imgbox.com/66/f7/ekRzeL1a_o.png" width="200"></p> 
<h4 id="%C2%A06.4%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><strong><a name="_Toc38636818"><span style="color:#0d0016;">6.4 </span></a><span style="color:#0d0016;">代价函数</span></strong></h4> 
<p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和<img alt="" height="34" src="https://images2.imgbox.com/08/94/zcZsrF6S_o.png" width="209">对逻辑回归模型沿用这个定义，得到的代价函数将是一个非凸函数（<strong>non-convexfunction</strong>），有许多局部最小值，影响梯度下降算法寻找全局最小值。</p> 
<p style="margin-left:0;text-align:justify;">重新定义<span style="background-color:#ffd900;">逻辑回归的代价函数</span>为：<img alt="" height="31" src="https://images2.imgbox.com/74/cb/U0GZdS8C_o.png" width="238">，其中<img alt="" height="59" src="https://images2.imgbox.com/d7/f0/w6678uC7_o.png" width="370"></p> 
<p> 构建的<img alt="" height="16" src="https://images2.imgbox.com/89/67/IxmpdVCj_o.png" width="88"> 函数的特点是：当实际的 <img alt="" height="16" src="https://images2.imgbox.com/4f/e7/taVneEIN_o.png" width="34">  且<img alt="" height="16" src="https://images2.imgbox.com/8d/28/oJQoBOXV_o.png" width="34"> 也为 1 时误差为 0，当<img alt="" height="16" src="https://images2.imgbox.com/0d/de/lHO8UUtp_o.png" width="34">  但<img alt="" height="16" src="https://images2.imgbox.com/f4/40/ZfEIScyr_o.png" width="34"> 不为1时误差随着<img alt="" height="16" src="https://images2.imgbox.com/12/d2/zvetVgzI_o.png" width="34"> 变小而变大；当实际的 <img alt="" height="16" src="https://images2.imgbox.com/22/55/C2xS9VPC_o.png" width="34">  且<img alt="" height="16" src="https://images2.imgbox.com/cc/a3/7N0rWBO5_o.png" width="34"> 也为 0 时代价为 0，当<img alt="" height="16" src="https://images2.imgbox.com/6d/13/NS3X39ki_o.png" width="34">  但<img alt="" height="16" src="https://images2.imgbox.com/05/44/gvwr1roc_o.png" width="34"> 不为 0时误差随着<img alt="" height="16" src="https://images2.imgbox.com/57/c5/iHwSgb23_o.png" width="34"> 的变大而变大。简化为<img alt="" height="42" src="https://images2.imgbox.com/84/a1/t8KyvTNa_o.png" width="464"></p> 
<p> 虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的<img alt="" height="17" src="https://images2.imgbox.com/d9/91/al1OApz8_o.png" width="98"><span style="background-color:#ffd900;">与线性回归中不同</span>，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行<span style="background-color:#ffd900;">特征缩放</span>依旧是非常必要的。</p> 
<h4 id="%C2%A06.5%20%E7%AE%80%E5%8C%96%E7%9A%84%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><strong><a name="_Toc38636819"><span style="color:#0d0016;">6.5 </span></a><span style="color:#0d0016;">简化的成本函数和梯度下降</span></strong></h4> 
<p>用 <span style="background-color:#ffd900;">梯度下降</span>最小化逻辑回归中代价函数<img alt="" height="16" src="https://images2.imgbox.com/e9/e7/kYLusrDn_o.png" width="26"> ：<img alt="" height="40" src="https://images2.imgbox.com/c8/3a/knztPZVo_o.png" width="163">，简化为</p> 
<p><img alt="" height="34" src="https://images2.imgbox.com/48/8d/p3dovdjF_o.png" width="273"></p> 
<h4 id="6.7%20%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A"><strong><a name="_Toc38636821"><span style="color:#0d0016;">6.7 </span></a><span style="color:#0d0016;">多类别分类：一对多</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">使用逻辑回归 (<strong>logistic regression</strong>)来解决<span style="background-color:#ffd900;">多类别分类问题</span>，具体来说，叫做"<span style="background-color:#ffd900;">一对多</span>" (<strong>one-vs-all</strong>) 的分类算法。</p> 
<p><img alt="" height="133" src="https://images2.imgbox.com/f9/0a/1iKynZnd_o.png" width="289"></p> 
<p>（上图右，将其分成3个二元分类问题。）创建一个<span style="background-color:#ffd900;">新的"伪"训练集</span>。将多个类中的一个类标记为正向类（<em>y=1</em> ），然后将其他所有类都标记为负向类，这个模型记作<img alt="" height="34" src="https://images2.imgbox.com/85/fa/lv1Kk2fP_o.png" width="50">，拟合出一个合适的分类器。接着，类似地第我们选择另一个类标记为正向类（<em>y=2</em> ），再将其它类都标记为负向类，将这个模型记作 <img alt="" height="32" src="https://images2.imgbox.com/a7/e8/ONoIYMqq_o.png" width="54"> ,依此类推。 最后我们得到一系列的模型简记为：<img alt="" height="33" src="https://images2.imgbox.com/d4/c4/bebBHjap_o.png" width="323"></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="161" src="https://images2.imgbox.com/30/5e/dcFclf5S_o.png" width="253"></p> 
<p><img alt="" height="104" src="https://images2.imgbox.com/84/79/PXwDhfOf_o.png" width="554"></p> 
<p>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，<span style="background-color:#ffd900;">三个分类器里面输入 </span><em><span style="background-color:#ffd900;">x</span></em> ，然后我们选择一个让<img alt="" height="31" src="https://images2.imgbox.com/68/6f/CnI5oj3e_o.png" width="48"> <span style="background-color:#ffd900;">最大的</span><em><span style="background-color:#ffd900;">i</span></em><span style="background-color:#ffd900;"> </span>，即<img alt="" height="36" src="https://images2.imgbox.com/5c/7e/VYRSr8Vi_o.png" width="81">。</p> 
<h3 id="%C2%A07.%E6%AD%A3%E5%88%99%E5%8C%96(Regularization)"><strong><span style="color:#0d0016;">7.</span><a name="_Toc38636822"><span style="color:#0d0016;">正则化(Regularization)</span></a></strong></h3> 
<h4 id="7.1%20%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E9%97%AE%E9%A2%98" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636823"><span style="color:#0d0016;">7.1 </span></a><span style="color:#0d0016;">过拟合的问题</span></strong></h4> 
<p> <img alt="" height="148" src="https://images2.imgbox.com/51/89/MyKSO76D_o.png" width="560"></p> 
<p>第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质即预测新数据，称为<span style="background-color:#ffd900;">过拟合</span>(<strong>over-fitting</strong>)。中间的模型似乎最合适。<span style="background-color:#ffd900;">拟合的越好，相应的预测的能力就可能变差</span>，应该如何处理？</p> 
<p style="margin-left:0;text-align:justify;">1.丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</p> 
<p style="margin-left:0;text-align:justify;">2.正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p> 
<h4 id="7.2%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636824"><span style="color:#0d0016;">7.2 </span></a><span style="color:#0d0016;">代价函数</span></strong></h4> 
<p>上面的回归问题中如果我们的模型是：<img alt="" height="35" src="https://images2.imgbox.com/cc/b3/mwCS9huw_o.png" width="284">，那些<span style="background-color:#ffd900;">高次项导致了过拟合</span>的产生，所以如果我们能让这些高次项的系数接近于0的话，就能很好的拟合了，所以在一定程度上减小这些参数<img alt="" height="16" src="https://images2.imgbox.com/d2/c7/Y5fEU1lt_o.png" width="8">  的值，这就是<span style="background-color:#ffd900;">正则化</span>的基本方法。减少<img alt="" height="16" src="https://images2.imgbox.com/75/ea/k3TyzSpA_o.png" width="14"> 和<img alt="" height="16" src="https://images2.imgbox.com/29/62/010iZV3c_o.png" width="13"> 的大小，我们要做的便是修改代价函数，在其中<img alt="" height="16" src="https://images2.imgbox.com/a7/58/tPJGk5Dm_o.png" width="14"> 和<img alt="" height="16" src="https://images2.imgbox.com/cd/df/vEMmelen_o.png" width="13">  设置一点<span style="background-color:#ffd900;">惩罚</span>，在尝试最小化代价时也需要将这个惩罚纳入考虑中，并最终导致选择较小一些的<em>θ</em><em>3</em> 和<em>θ</em><em>4</em> 。</p> 
<p>修改后的代价函数如下：<img alt="" height="34" src="https://images2.imgbox.com/df/f9/DJvENASx_o.png" width="376">，通过这样的代价函数选择出的<img alt="" height="16" src="https://images2.imgbox.com/18/73/kc2NAg32_o.png" width="14"> 和<img alt="" height="16" src="https://images2.imgbox.com/72/98/koJSwQZh_o.png" width="13">  对预测结果的影响就比之前要小许多。假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：<span style="background-color:#ffd900;"><img alt="" height="32" src="https://images2.imgbox.com/f0/20/LrQFbPkE_o.png" width="323"></span>，其中<em>λ</em> 又称为<span style="background-color:#ffd900;">正则化参数</span>（<strong>Regularization Parameter</strong>）。 注：根据惯例，我们不对<img alt="" height="27" src="https://images2.imgbox.com/60/21/FjcVsBtf_o.png" width="23">进行惩罚。经过正则化处理的模型与原模型的可能对比如下图所示：</p> 
<p></p> 
<p> <img alt="" height="157" src="https://images2.imgbox.com/05/89/rk1bXXqt_o.png" width="241"></p> 
<p> 如果选择的<span style="background-color:#ffd900;">正则化参数λ过大</span>，则会把所有的参数都最小化了，导致模型变成<img alt="" height="16" src="https://images2.imgbox.com/af/bf/m9QY2RGT_o.png" width="66"> ，也就是上图中红色直线所示的情况，造成<span style="background-color:#ffd900;">欠拟合</span>。要取一个合理的 <img alt="" height="16" src="https://images2.imgbox.com/28/de/c82o8xSb_o.png" width="7">  的值。把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p> 
<h4 id="%C2%A07.3%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><strong><a name="_Toc38636825"><span style="color:#0d0016;">7.3 </span></a><span style="color:#0d0016;">正则化线性回归</span></strong></h4> 
<p> <span style="background-color:#ffd900;">基于梯度下降</span>：每次都在原有算法更新规则的基础上令<em>θ</em> 值减少了一个额外的值。</p> 
<p><img alt="" height="171" src="https://images2.imgbox.com/cd/19/a120uFXz_o.png" width="326"></p> 
<p style="margin-left:0;text-align:justify;">对上面的算法中<em>j=1,2,...,n</em>  时的更新式子进行调整可得：<img alt="" height="28" src="https://images2.imgbox.com/b1/48/GbmEeqjA_o.png" width="281"></p> 
<p> <span style="background-color:#ffd900;">基于正规方程</span>：图中的矩阵尺寸为 <em>(n+1)*(n+1)</em> 。</p> 
<p></p> 
<p> <img alt="" height="80" src="https://images2.imgbox.com/41/2b/S66bsb10_o.png" width="275"></p> 
<h4 id="%C2%A07.4%20%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><strong><a name="_Toc38636826"><span style="color:#0d0016;">7.4 </span></a><span style="color:#0d0016;">正则化的逻辑回归模型</span></strong></h4> 
<p> 对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：<span style="color:#ffd900;"><span style="background-color:#ffd900;"><img alt="" height="62" src="https://images2.imgbox.com/ef/b6/VBVlael7_o.png" width="535"></span></span></p> 
<p style="margin-left:0;text-align:justify;">要最小化该代价函数，通过求导，得出梯度下降算法为：</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="160" src="https://images2.imgbox.com/17/8a/28R6PuI0_o.png" width="342"></p> 
<p> 注：1、看上去同线性回归一样，但是知道<img alt="" height="36" src="https://images2.imgbox.com/db/43/zBsf2gyS_o.png" width="116">  ，所以与线性回归不同。2、<img alt="" height="29" src="https://images2.imgbox.com/a9/ba/JNAnoYpa_o.png" width="18"> 不参与其中的任何一个正则化。</p> 
<h2 id="%C2%A0%E7%AC%AC4%E5%91%A8"><strong><a name="_Toc38636827"><span style="color:#0d0016;">第4</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="8.%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E8%BF%B0(Neural%20Networks%3A%20Representation)"><strong><span style="color:#0d0016;">8. 神</span><a name="_Toc38636828"><span style="color:#0d0016;">经网络的表述(Neural Networks: Representation)</span></a></strong></h3> 
<h4 id="8.3%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA1" style="text-align:justify;"><strong><a name="_Toc38636831"><span style="color:#0d0016;">8.3 </span></a><span style="color:#0d0016;">模型表示1</span></strong></h4> 
<p>在神经网络中，<span style="background-color:#ffd900;">参数</span>又可被称为<span style="background-color:#ffd900;">权重</span>（<strong>weight</strong>）。<span style="background-color:#ffd900;">神经网络模型</span>是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。下图为一个3层的神经网络，第一层成为输入层（<strong>Input Layer</strong>），最后一层称为输出层（<strong>Output Layer</strong>），中间一层成为隐藏层（<strong>Hidden Layers</strong>）。我们为<span style="background-color:#ffd900;">每一层都增加一个偏差单位</span>（<strong>bias unit</strong>）：</p> 
<p><img alt="" height="217" src="https://images2.imgbox.com/bf/81/gJ7TbEhW_o.png" width="406">       <span style="background-color:#ffd900;">激活单元</span></p> 
<p> <img alt="" height="528" src="https://images2.imgbox.com/0f/72/n07vOm1x_o.png" width="663"></p> 
<p style="margin-left:0;text-align:justify;">我们把这样从左到右的算法称为<span style="background-color:#ffd900;">前向传播算法</span>( <strong>FORWARD PROPAGATION</strong> )，把<em>x</em> , <em>θ</em> , <em>a</em>  分别用矩阵表示，我们可以得到<em>θ⋅X=a</em>  ：</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="120" src="https://images2.imgbox.com/3f/bb/HZxMW3sj_o.png" width="365"></p> 
<h4 id="8.4%20%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA2%C2%A0"><strong><a name="_Toc38636832"><span style="color:#0d0016;">8.4 </span></a><span style="color:#0d0016;">模型表示2 </span></strong></h4> 
<p> 我们可以把<em>a</em><em>0</em><em>,</em><em>a</em><em>1</em><em>,</em><em>a</em><em>2</em><em>,</em><em>a</em><em>3</em> 看成更为高级的特征值，也就是<em>x</em><em>0</em><em>,</em><em>x</em><em>1</em><em>,</em><em>x</em><em>2</em><em>,</em><em>x</em><em>3</em> 的进化体，并且它们是由 <em>x</em> 决定的，因为是梯度下降的，所以<em>a</em> 是变化的，并且变得越来越厉害，所以这些<span style="background-color:#ffd900;">更高级的特征值</span>远比 <em>x</em> 次方厉害，也能更好的预测新数据。这就是神经网络相比于逻辑回归和线性回归的优势。</p> 
<p>从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中，我们被限制为使用数据中的原始特征<em>x</em><em>1</em><em>,</em><em>x</em><em>2</em><em>,...,</em><em>x</em><em>n</em> ，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络<span style="background-color:#ffd900;">通过学习后自己得出的一系列用于预测输出变量的新特征。</span></p> 
<h4 id="%C2%A08.7%20%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><strong><a name="_Toc38636835"><span style="color:#0d0016;">8.7 </span></a><span style="color:#0d0016;">多类分类</span></strong></h4> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="88" src="https://images2.imgbox.com/f4/12/TwPo8XIH_o.png" width="406"></p> 
<h2 id="%C2%A0%E7%AC%AC5%E5%91%A8"><strong><a name="_Toc38636836"><span style="color:#0d0016;">第5</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0(Neural%20Networks%3A%20Learning)"><strong><span style="color:#0d0016;">9.</span><a name="_Toc38636837"><span style="color:#0d0016;">神经网络的学习(Neural Networks: Learning)</span></a></strong><strong> </strong></h3> 
<h4 id="9.1%20%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636838"><span style="color:#0d0016;">9.1 </span></a><span style="color:#0d0016;">代价函数</span></strong></h4> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="476" src="https://images2.imgbox.com/4c/24/9V3sEpNz_o.png" width="649"></p> 
<p> <span style="background-color:#ffd900;">代价函数</span>会比逻辑回归更加复杂一些，为：<img alt="" height="38" src="https://images2.imgbox.com/09/86/vHhOuhVe_o.png" width="267"></p> 
<p> <img alt="" height="134" src="https://images2.imgbox.com/39/d8/emDET9Z8_o.png" width="598"></p> 
<p>通过代价函数来观察算法预测的结果与真实情况的误差有多大。</p> 
<h4 id="%C2%A09.2%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><strong><a name="_Toc38636839"><span style="color:#0d0016;">9.2 </span></a><span style="color:#0d0016;">反向传播算法</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的<img alt="" height="32" src="https://images2.imgbox.com/6e/fc/axYvOOTd_o.png" width="41">。现在，为了计算代价函数的偏导数<img alt="" height="42" src="https://images2.imgbox.com/09/29/SNjr93pS_o.png" width="61"> ，我们需要采用一种<span style="background-color:#ffd900;">反向传播算法</span>，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。在求出了<img alt="" height="31" src="https://images2.imgbox.com/32/df/XfYUXdgo_o.png" width="29">之后，我们便可以计算代价函数的偏导数了</p> 
<h4 id="%C2%A09.5%20%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><strong><a name="_Toc38636842"><span style="color:#0d0016;">9.5 </span></a><span style="color:#0d0016;">梯度检验</span></strong></h4> 
<p>对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。为了避免这样的问题，我们采取一种叫做<span style="background-color:#ffd900;">梯度的数值检验</span>（<strong>Numerical Gradient Checking</strong>）方法：在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。</p> 
<h4 id="9.6%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636843"><span style="color:#0d0016;">9.6 </span></a><span style="color:#0d0016;">随机初始化</span></strong></h4> 
<p>任何优化算法都需要一些初始的参数。通常初始参数为正负<img alt="" height="16" src="https://images2.imgbox.com/59/72/qjAnBSSq_o.png" width="7"> 之间的随机值。</p> 
<h4 id="9.7%20%E7%BB%BC%E5%90%88%E8%B5%B7%E6%9D%A5" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636844"><span style="color:#0d0016;">9.7 </span></a><span style="color:#0d0016;">综合起来</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">小结一下使用神经网络时的步骤：</p> 
<p style="margin-left:0;text-align:justify;">1、网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。<span style="background-color:#ffd900;">第一层的单元数</span>即我们<span style="background-color:#ffd900;">训练集的特征数量</span>。<span style="background-color:#ffd900;">最后一层的单元数</span>是我们<span style="color:#0d0016;"><span style="background-color:#ffd900;">训练集的结果的类的数量</span></span>。如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p> 
<p style="margin-left:0;text-align:justify;">2、训练神经网络：</p> 
<ol><li style="text-align:left;">参数的随机初始化</li><li style="text-align:left;">利用正向传播方法计算所有的<em>h</em><em>θ</em><em>(x)</em></li><li style="text-align:left;">编写计算代价函数 <em>J</em>  的代码</li><li style="text-align:left;">利用反向传播方法计算所有偏导数</li><li style="text-align:left;">利用数值检验方法检验这些偏导数</li><li style="text-align:left;">使用优化算法来最小化代价函数</li></ol> 
<h2 id="%E7%AC%AC6%E5%91%A8" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636846"><span style="color:#0d0016;">第6</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="10.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE(Advice%20for%20Applying%20Machine%20Learning)"><strong><span style="color:#0d0016;">10.</span><a name="_Toc38636847"><span style="color:#0d0016;">应用机器学习的建议(Advice for Applying Machine Learning)</span></a></strong></h3> 
<h4 id="10.2%20%E8%AF%84%E4%BC%B0%E4%B8%80%E4%B8%AA%E5%81%87%E8%AE%BE" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636849"><span style="color:#0d0016;">10.2 </span></a><span style="color:#0d0016;">评估一个假设</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">为了<span style="background-color:#ffd900;">检验算法是否过拟合</span>，我们将<span style="background-color:#ffd900;">数据分成训练集和测试集</span>，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。</p> 
<p style="margin-left:0;text-align:justify;">测试集评估：在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p> 
<p style="margin-left:0;text-align:justify;">1.对于线性回归模型，我们利用测试集数据计算代价函数<em>J</em></p> 
<p style="margin-left:0;text-align:justify;">2.对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外，还可以计算误分类的比率，然后对计算结果求平均。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="50" src="https://images2.imgbox.com/f1/57/uMP7cVPN_o.png" width="579"></p> 
<h4 id="%C2%A010.3%20%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86%EF%BC%88%C2%A0Model%20Selection%20and%20Train_Validation_Test%20Sets%EF%BC%89"><strong><a name="_Toc38636850"><span style="color:#0d0016;">10.3 </span></a><span style="color:#0d0016;">模型选择和交叉验证集（</span> Model Selection and Train_Validation_Test Sets<span style="color:#0d0016;">）</span></strong></h4> 
<p>越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用<span style="background-color:#ffd900;">交叉验证集</span>来帮助选择模型，即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集</p> 
<p style="margin-left:0;text-align:justify;">模型选择的方法为：</p> 
<p style="margin-left:0;text-align:justify;">1. 使用训练集训练出10个模型</p> 
<p style="margin-left:0;text-align:justify;">2. 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p> 
<p style="margin-left:0;text-align:justify;">3. 选取代价函数值最小的模型</p> 
<p style="margin-left:0;text-align:justify;">4. 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</p> 
<p><img alt="" height="433" src="https://images2.imgbox.com/cc/ef/OTrbXVGI_o.png" width="382"></p> 
<h4 id="%C2%A010.4%20%E8%AF%8A%E6%96%AD%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88Diagnosing%20bias%20vs.%20variance%EF%BC%89"><strong><a name="_Toc38636851"><span style="color:#0d0016;">10.4 </span></a><span style="color:#0d0016;">诊断偏差和方差（Diagnosing bias vs. variance）</span></strong></h4> 
<p>运行一个学习算法时，如果算法的表现<span style="background-color:#ffd900;">不理想</span>，那么多半是出现两种情况：<span style="background-color:#ffd900;">要么是偏差比较大，要么是方差比较大</span>。换句话说，出现的情况<span style="background-color:#ffd900;">要么是欠拟合，要么是过拟合问题</span>。</p> 
<p style="margin-left:0;text-align:justify;">我们通常会通过将训练集和交叉验证集的代价函数误差与<span style="background-color:#ffd900;">多项式的次数</span>绘制在同一张图表上来帮助分析：</p> 
<p><img alt="" height="215" src="https://images2.imgbox.com/77/d3/XT3YBloD_o.png" width="323"></p> 
<p>对于训练集，当 <em>d</em>  较小时，模型拟合程度更低，误差较大；随着 <em>d</em>  的增长，拟合程度提高，误差减小。</p> 
<p style="margin-left:0;text-align:justify;">对于交叉验证集，当 <em>d</em>  较小时，模型拟合程度低，误差较大；但是随着 <em>d</em>  的增长，误差呈现先减小后增大的趋势，<span style="background-color:#ffd900;">转折点</span>是我们的模型开始<span style="background-color:#ffd900;">过拟合</span>训练数据集的时候。</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffd900;">训练集误差和交叉验证集误差近似时：偏差/欠拟合。 交叉验证集误差远大于训练集误差时：方差/过拟合。</span></p> 
<h4 id="10.5%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E5%81%8F%E5%B7%AE%2F%E6%96%B9%E5%B7%AE%EF%BC%88Regularization%20and%20Bias_Variance%EF%BC%89" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636852"><span style="color:#0d0016;">10.5 </span></a><span style="color:#0d0016;">正则化和偏差/方差（</span></strong>Regularization and Bias_Variance<strong><span style="color:#0d0016;">）</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p> 
<p><img alt="" height="154" src="https://images2.imgbox.com/02/88/pmFyuGhX_o.png" width="514"></p> 
<p>选择一系列的想要测试的 <img alt="" height="16" src="https://images2.imgbox.com/db/7e/RUCFz543_o.png" width="7">  值，通常是 0-10之间的呈现2倍关系的值。同样把数据分为训练集、交叉验证集和测试集。</p> 
<p> <img alt="" height="271" src="https://images2.imgbox.com/bb/0f/CtRHOHds_o.png" width="400"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffd900;">选择</span><em><span style="background-color:#ffd900;">λ</span></em><span style="background-color:#ffd900;"> 的方法</span>为：</p> 
<p style="margin-left:0;text-align:justify;">1.使用训练集训练出12个不同程度正则化的模型</p> 
<p style="margin-left:0;text-align:justify;">2.用12个模型分别对交叉验证集计算的出交叉验证误差</p> 
<p style="margin-left:0;text-align:justify;">3.选择得出交叉验证误差<strong>最小</strong>的模型</p> 
<p style="margin-left:0;text-align:justify;">4.运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</p> 
<p><img alt="" height="276" src="https://images2.imgbox.com/c2/48/2rG7SUm1_o.png" width="498"></p> 
<ul><li> <span style="background-color:#ffd900;">当 </span><em><span style="background-color:#ffd900;">λ</span></em><span style="background-color:#ffd900;">  较小时，训练集误差较小（过拟合）而交叉验证集误差较大</span></li><li> <p><span style="color:#0d0016;"><span style="background-color:#ffd900;">随着 </span><em><span style="background-color:#ffd900;">λ</span></em><span style="background-color:#ffd900;">  的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</span></span></p> </li></ul> 
<h4 id="10.6%20%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636853"><span style="color:#0d0016;">10.6 </span></a><span style="color:#0d0016;">学习曲线</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">学习曲线是学习算法的一个很好的<strong>合理检验</strong>（<strong>sanity check</strong>）。<span style="background-color:#ffd900;">学习曲线</span>是将<span style="background-color:#ffd900;">训练集误差和交叉验证集误差（因变量）</span>作为训练集<span style="background-color:#ffd900;">实例数量（自变量）</span>（<em>m</em> ）的函数绘制的图表。即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p> 
<p><img alt="" height="161" src="https://images2.imgbox.com/9e/82/TWYXIHjH_o.png" width="262"></p> 
<p>利用<span style="background-color:#ffd900;">学习曲线识别高偏差/欠拟合</span>：我们用一条直线来适应下面的数据，可以看出，无论训练集实例数量有多么大，误差都不会有太大改观。即在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。</p> 
<p><img alt="" height="248" src="https://images2.imgbox.com/3a/e0/dO3Th1KJ_o.png" width="515"></p> 
<p>利用<span style="background-color:#ffd900;">学习曲线识别高方差/过拟合</span>：我们用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。即在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。</p> 
<p><img alt="" height="269" src="https://images2.imgbox.com/ca/a3/t9uw6lup_o.png" width="491"></p> 
<h4 id="%C2%A010.7%20%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88"><strong><a name="_Toc38636854"><span style="color:#0d0016;">10.7 </span></a><span style="color:#0d0016;">决定下一步做什么</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">我们来看一看我们在什么情况下应该怎样选择：</p> 
<p style="margin-left:0;text-align:justify;">1. 获得更多的训练实例——解决高方差</p> 
<p style="margin-left:0;text-align:justify;">2. 尝试减少特征的数量——解决高方差</p> 
<p style="margin-left:0;text-align:justify;">3. 尝试获得更多的特征——解决高偏差</p> 
<p style="margin-left:0;text-align:justify;">4. 尝试增加多项式特征——解决高偏差</p> 
<p style="margin-left:0;text-align:justify;">5. 尝试减少正则化程度λ——解决高偏差</p> 
<p style="margin-left:0;text-align:justify;">6. 尝试增加正则化程度λ——解决高方差</p> 
<p style="margin-left:0;text-align:justify;">神经网络的方差和偏差：</p> 
<p><img alt="" height="267" src="https://images2.imgbox.com/ed/c3/GE2cpBLn_o.png" width="474"></p> 
<h2 id="%E7%AC%AC8%E5%91%A8" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636869"><span style="color:#0d0016;">第8</span></a><span style="color:#0d0016;">周</span></strong></h2> 
<h3 id="13.%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89" style="margin-left:0px;text-align:justify;"><strong><span style="color:#0d0016;">13.聚类（Clustering）</span></strong></h3> 
<h4 id="13.1%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E7%AE%80%E4%BB%8B" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636871"><span style="color:#0d0016;">13.1 </span></a><span style="color:#0d0016;">无监督学习：简介</span></strong></h4> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffd900;">无监督学习</span>中，我们的<span style="background-color:#ffd900;">数据没有附带任何标签</span>，我们拿到的数据就是这样的：</p> 
<p><img alt="" height="203" src="https://images2.imgbox.com/a0/33/Bgq5YSnP_o.png" width="284"></p> 
<p>我们有一系列点，却没有标签。因此，我们的训练集可以写成只有<img alt="" height="25" src="https://images2.imgbox.com/fb/6f/qJ5xMiYg_o.png" width="75">一直到<img alt="" height="24" src="https://images2.imgbox.com/50/b6/IndzK65M_o.png" width="35">，没有任何标签<img alt="" height="16" src="https://images2.imgbox.com/09/e9/no0Hya3f_o.png" width="8"> 。我们需要将一系列无标签的训练数据，输入到一个算法中，让这个算法为我们找找这个数据的内在结构。图上的数据看起来可以分成两个<span style="background-color:#ffd900;">分开的点集（称为簇</span>），一个能够找到我圈出的这些点集的算法，就被称为<span style="background-color:#ffd900;">聚类算法</span>，这将是我们介绍的<span style="background-color:#ffd900;">第一个非监督学习算法</span>。聚类算法可以用来：</p> 
<p> <img alt="" height="322" src="https://images2.imgbox.com/9e/dc/V8gY2H2J_o.png" width="518"></p> 
<h4 id="%C2%A013.2%20K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95"><strong><a name="_Toc38636872"><span style="color:#0d0016;">13.2 K-</span></a><span style="color:#0d0016;">均值算法</span></strong></h4> 
<p>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。<span style="background-color:#ffd900;">K-均值</span>是一个迭代算法，假设我们<span style="background-color:#ffd900;">想要将数据聚类成n个组</span>，其<span style="background-color:#ffd900;">方法</span>为:</p> 
<p style="margin-left:0;text-align:justify;">首先选择<em>K</em> 个随机的点，称为<span style="background-color:#ffd900;">聚类中心</span>（cluster centroids）；</p> 
<p style="margin-left:0;text-align:justify;">对于数据集中的每一个数据，按照<span style="background-color:#ffd900;">距离</span><em><span style="background-color:#ffd900;">K</span></em><span style="background-color:#ffd900;"> 个中心点的距离</span>，将其与距离最近的中心点<span style="background-color:#ffd900;">关联</span>起来，与同一个中心点关联的所有点<span style="background-color:#ffd900;">聚成一类</span>。</p> 
<p style="margin-left:0;text-align:justify;">计算每一个组的平均值，将该组所关联的<span style="background-color:#ffd900;">中心点移动到平均值</span>的位置。</p> 
<p style="margin-left:0;text-align:justify;">重复步骤<span style="background-color:#ffd900;">直至中心点不再变化</span>。</p> 
<p style="margin-left:0;text-align:justify;">下面是一个聚类示例：</p> 
<p> <img alt="" height="147" src="https://images2.imgbox.com/4d/e7/FhS8qfHF_o.png" width="224"><img alt="" height="151" src="https://images2.imgbox.com/e6/4e/9OODFJ2r_o.png" width="211"><img alt="" height="154" src="https://images2.imgbox.com/44/58/8UEo0w6p_o.png" width="205"></p> 
<h4 id="%C2%A013.3%20%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><strong><a name="_Toc38636873"><span style="color:#0d0016;">13.3 </span></a><span style="color:#0d0016;">优化目标</span></strong></h4> 
<p> <img alt="" height="31" src="https://images2.imgbox.com/5e/23/9NNn1Rd5_o.png" width="613">心的索引。算法分为两个步骤，<span style="background-color:#ffd900;">第一个for循环是赋值步骤</span>，即：对于每一个样例<em>i</em> ，计算其应该属于的类。<span style="background-color:#ffd900;">第二个for循环是聚类中心的移动</span>，即：对于每一个类<em>K</em> ，重新计算该类的质心。K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的<span style="background-color:#ffd900;">代价函数</span>（又称畸变函数 Distortion function）为：</p> 
<p><img alt="" height="207" src="https://images2.imgbox.com/cf/88/jS8TcVX2_o.png" width="645"></p> 
<p> 我们知道，第一个循环是用于减小<img alt="" height="17" src="https://images2.imgbox.com/7e/8c/cXncKAuy_o.png" width="20"> 引起的代价，而第二个循环则是用于减小<img alt="" height="16" src="https://images2.imgbox.com/de/fb/fCSaJKEM_o.png" width="12"> 引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p> 
<h4 id="13.4%20%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%C2%A0"><strong><a name="_Toc38636874"><span style="color:#0d0016;">13.4 </span></a></strong><span style="color:#0d0016;"><strong>随机初始化</strong> </span></h4> 
<p><span style="background-color:#ffd900;">随机初始化所有的聚类中心点</span>：</p> 
<ol><li>我们应该选择<em>K&lt;m</em> ，即聚类中心点的个数要小于所有训练集实例的数量</li><li>随机选择<em>K</em> 个训练实例，然后令<em>K</em> 个聚类中心分别与这<em>K</em> 个训练实例相等</li></ol> 
<p style="margin-left:0;text-align:justify;">K-均值的一个<span style="background-color:#ffd900;">问题</span>在于，它有<span style="background-color:#ffd900;">可能会停留在一个局部最小值处</span>，而这取决于初始化的情况。为了解决这个问题，我们通常需要多次运行算法，每一次都重新进行随机初始化，最后再比较多次运行的结果，选择代价函数最小的结果。这种方法在<em>K</em> 较小的时候（2--10）还是可行的，但是如果<em>K</em> 较大，这么做也可能不会有明显地改善。</p> 
<h4 id="13.5%20%E9%80%89%E6%8B%A9%E8%81%9A%E7%B1%BB%E6%95%B0" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636875"><span style="color:#0d0016;">13.5 </span></a><span style="color:#0d0016;">选择聚类数</span></strong></h4> 
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的，选择时思考运用算法聚类的动机是什么。<span style="background-color:#ffd900;">选择聚类数目</span>的方法时，有一个可能会谈及的方法叫作“<span style="background-color:#ffd900;">肘部法则</span>”，我们所需要做的是改变<img alt="" height="16" src="https://images2.imgbox.com/da/4a/cahTRewN_o.png" width="10"> 值，也就是聚类类别数目的总数。</p> 
<p><img alt="" height="229" src="https://images2.imgbox.com/c6/ab/p6cUnYnX_o.png" width="457"></p> 
<p> 曲线的肘点，畸变值下降得很快，那么使用3个聚类来进行聚类是正确的。</p> 
<h3 id="%C2%A0%E9%99%8D%E7%BB%B4(Dimensionality%20Reduction)"><strong><a name="_Toc38636876"><span style="color:#0d0016;">14.降维(Dimensionality Reduction)</span></a></strong></h3> 
<h4 id="14.1%20%E5%8A%A8%E6%9C%BA%E4%B8%80%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%EF%BC%88Motivation%20I_%20Data%20Compression%EF%BC%89" style="margin-left:0px;text-align:justify;"><strong><a name="_Toc38636877"><span style="color:#0d0016;">14.1 </span></a><span style="color:#0d0016;">动机一：数据压缩（Motivation I_ Data Compression）</span></strong></h4> 
<p><span style="background-color:#ffd900;">第二种类型的无监督学习问题</span>，称为<span style="background-color:#ffd900;">降维</span>。有几个不同的的原因使你可能想要做降维，一是<span style="background-color:#ffd900;">数据压缩</span>。数据压缩不仅允许我们压缩数据，因而使用较少的计算机内存或磁盘空间，也可以加快我们的学习算法。降维是什么？例子：我们收集的数据集有许多特征，我绘制两个在这里。</p> 
<p><img alt="" height="154" src="https://images2.imgbox.com/17/ca/gCTGxncz_o.png" width="322"><img alt="" height="188" src="https://images2.imgbox.com/f1/2f/37SFkRRV_o.png" width="356"></p> 
<p> 假设我们未知两个的特征（长度）：x1：用厘米表示；x2：用英寸表示同一物体的长度。这给了我们高度冗余表示，我们想要做的是减少数据到一维，只有一个数据测量这个长度。</p> 
<h4 id="14.2%20%E5%8A%A8%E6%9C%BA%E4%BA%8C%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88%C2%A0Motivation%20II_%20Visualization%EF%BC%89" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636878"><span style="color:#0d0016;">14.2 </span></a><span style="color:#0d0016;">动机二：数据可视化（</span> <span style="color:#0d0016;">Motivation II_ Visualization</span><span style="color:#0d0016;">）</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">在许多及其学习问题中，如果我们能将<span style="background-color:#ffd900;">数据可视化</span>，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p> 
<p><img alt="" height="271" src="https://images2.imgbox.com/57/37/OvdGrnKh_o.png" width="507"></p> 
<p>假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如<strong>GDP</strong>，人均<strong>GDP</strong>，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。</p> 
<p><img alt="" height="272" src="https://images2.imgbox.com/2b/dc/NWWRHGgp_o.png" width="527"></p> 
<p>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p> 
<h4 id="14.3%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98%EF%BC%88Principal%20Component%20Analysis%20Problem%20Formulation%EF%BC%89" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636879"><span style="color:#0d0016;">14.3 </span></a><span style="color:#0d0016;">主成分分析问题（Principal Component Analysis Problem Formulation</span><span style="color:#0d0016;">）</span></strong></h4> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#ffd900;">主成分分析(PCA)</span>是最常见的降维算法。在PCA中，我们要做的是找到一个<span style="background-color:#ffd900;">方向向量</span>（Vector direction），当我们<span style="background-color:#ffd900;">把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小</span>。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p> 
<p><img alt="" height="237" src="https://images2.imgbox.com/24/21/thWg5vEE_o.png" width="300"></p> 
<p>将<img alt="" height="16" src="https://images2.imgbox.com/50/dd/Eo4GGTR3_o.png" width="8"> 维数据降至<img alt="" height="16" src="https://images2.imgbox.com/32/33/tYBXMcex_o.png" width="8"> 维，进行数据压缩，目标是找到向量<img alt="" height="17" src="https://images2.imgbox.com/7f/42/qtddRKve_o.png" width="23"> ,<img alt="" height="17" src="https://images2.imgbox.com/ae/75/llI0oiKX_o.png" width="23"> ,...,<img alt="" height="17" src="https://images2.imgbox.com/b1/07/uM70rAd1_o.png" width="24"> 使得总的投射误差最小，如果100维的向量最后可以用10维来表示，那么压缩率为90%。<span style="background-color:#ffd900;">主成分分析与线性回归是两种不同的算法</span>。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p> 
<p><img alt="" height="184" src="https://images2.imgbox.com/bb/92/HV9C55dA_o.png" width="463"></p> 
<p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。<span style="background-color:#ffd900;">图像处理领域</span>的KL变换使用PCA做图像压缩。但PCA 要保证降维后，还要保证数据的特性损失最小。</p> 
<p style="margin-left:0;text-align:justify;">PCA技术的一大好处是对数据进行降维的处理。我们可以<span style="background-color:#ffd900;">对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分</span>，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果，同时<span style="background-color:#ffd900;">最大程度的保持了原有数据的信息</span>。</p> 
<p style="margin-left:0;text-align:justify;">PCA技术的一个很大的优点是，它是完全<span style="background-color:#ffd900;">无参数限制</span>的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，<span style="background-color:#ffd900;">最后的结果只与数据相关，与用户是独立的</span>。但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p> 
<h4 id="14.4%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95" style="margin-left:0;text-align:justify;"><strong><a name="_Toc38636880"><span style="color:#0d0016;">14.4 </span></a><span style="color:#0d0016;">主成分分析算法</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">减少<em>n</em> 维到<em>k</em> 维：</p> 
<p style="margin-left:0;text-align:justify;">（1）<span style="background-color:#ffd900;">均值归一化</span>。我们需要计算出所有特征的均值<em>μ</em><em>j</em> ，然后令 <em>x</em><em>j</em><em>=</em><em>x</em><em>j</em><em>-</em><em>μ</em><em>j</em> 。如果特征是在不同的数量级上，我们还需要将其除以标准差 <em>σ</em><em>2</em> 。</p> 
<p>（2）计算<span style="background-color:#ffd900;">协方差矩阵</span>（covariance matrix）： <img alt="" height="39" src="https://images2.imgbox.com/06/f6/STYgW2PK_o.png" width="174"></p> 
<p> （3）计算<span style="background-color:#ffd900;">协方差矩阵的特征向量</span>（eigenvectors）。利用<span style="background-color:#ffd900;">奇异值分解</span>（singular value decomposition）来求<span style="color:#0d0016;">解[U, S, V]。</span></p> 
<p>对于一个 <img alt="" height="16" src="https://images2.imgbox.com/d4/aa/WjrX1VpT_o.png" width="33"> 维度的矩阵，上式中的<span style="background-color:#ffd900;"><img alt="" height="16" src="https://images2.imgbox.com/b0/30/IWG6ouIC_o.png" width="10"> 是一个具有与数据之间最小投射误差的方向向量构成的矩阵</span>。如果我们希望将数据从<img alt="" height="16" src="https://images2.imgbox.com/1b/53/609EzB3q_o.png" width="8"> 维降至<img alt="" height="16" src="https://images2.imgbox.com/98/f8/36SjSNsC_o.png" width="8"> 维，我们只需要<span style="background-color:#ffd900;">从<img alt="" height="16" src="https://images2.imgbox.com/df/74/NMbsLoWq_o.png" width="10"> 中选取前<img alt="" height="16" src="https://images2.imgbox.com/9a/1b/P6bPGPKR_o.png" width="8"> 个向量</span>，获得一个<em>n×k</em>维度的矩阵<img alt="" height="16" src="https://images2.imgbox.com/0d/45/DEzMMHXD_o.png" width="44">，通过<span style="background-color:#ffd900;"><img alt="" height="18" src="https://images2.imgbox.com/33/62/xcjpreMQ_o.png" width="116"></span> 计算获得要求的新特征向量，其中<em>x</em> 是<em>n×1</em> 维的，因此结果为<em>k×1</em> 维度。注，我们不对方差特征进行处理。</p> 
<h4 id="%C2%A014.5%20%E9%80%89%E6%8B%A9%E4%B8%BB%E6%88%90%E5%88%86%E7%9A%84%E6%95%B0%E9%87%8F"><strong><a name="_Toc38636881"><span style="color:#0d0016;">14.5 </span></a><span style="color:#0d0016;">选择主成分的数量</span></strong></h4> 
<p>主要成分分析是减少投射的平均均方误差：<img alt="" height="39" src="https://images2.imgbox.com/88/25/W1WT2ZSM_o.png" width="183">，训练集的方差为：<img alt="" height="35" src="https://images2.imgbox.com/b4/d1/wVIrQLTu_o.png" width="98">平均均方误差与训练集方差的比例：<img alt="" height="71" src="https://images2.imgbox.com/bb/6f/71IIOjXD_o.png" width="188">，我们希望<span style="background-color:#ffd900;">在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的</span><em><span style="background-color:#ffd900;">k</span></em><span style="background-color:#ffd900;"> 值</span>。如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，也能显著地降低模型中特征的维度。可以先令<em>k=1</em> ，然后进行主要成分分析，获得<em>U</em><em>reduce</em> 和<em>z</em> ，然后计算比例是否小于1%。如果不是的话再令<em>k=2</em> ，如此类推，直到找到可以使得比例小于1%的最小<em>k</em>  值（原因是各个特征之间通常情况存在某种相关性）。</p> 
<h4 id="14.6%20%E9%87%8D%E5%BB%BA%E5%8E%8B%E7%BC%A9%E8%A1%A8%E7%A4%BA"><strong><a name="_Toc38636882"><span style="color:#0d0016;">14.6 </span></a><span style="color:#0d0016;">重建压缩表示</span></strong></h4> 
<p><span style="background-color:#ffd900;">压缩表示回到原有的近似高维数据</span>。给定的<img alt="" height="17" src="https://images2.imgbox.com/9e/99/EwU9rhxA_o.png" width="20"> ，可能100维，怎么回到原来的表示<em>x</em><em>(i)</em> ，可能是1000维的数组？</p> 
<p> <img alt="" height="290" src="https://images2.imgbox.com/02/34/a9KPzRDl_o.png" width="518"></p> 
<p style="margin-left:0;text-align:justify;">相反的方程为：<span style="background-color:#ffd900;"><img alt="" height="25" src="https://images2.imgbox.com/c2/d1/jDU46sjJ_o.png" width="220"></span>。所以，给定未标记的数据集，应用PCA，低维表示<img alt="" height="16" src="https://images2.imgbox.com/91/6a/8lKZufbb_o.png" width="7">可以映射到高维特征<img alt="" height="16" src="https://images2.imgbox.com/fb/72/dgoOh1Gv_o.png" width="8"> 。</p> 
<h4 id="%C2%A014.7%20%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8%E5%BB%BA%E8%AE%AE"><strong><a name="_Toc38636883"><span style="color:#0d0016;">14.7 </span></a><span style="color:#0d0016;">主成分分析法的应用建议</span></strong></h4> 
<p style="margin-left:0;text-align:justify;">假设我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。</p> 
<p style="margin-left:0;text-align:justify;">1. 第一步是运用主要成分分析将数据压缩至1000个特征</p> 
<p style="margin-left:0;text-align:justify;">2. 然后对训练集运行学习算法。</p> 
<p style="margin-left:0;text-align:justify;">3. 在预测时，采用之前学习而来的<img alt="" height="16" src="https://images2.imgbox.com/93/46/J4iuvSG1_o.png" width="44"> ，将输入的特征<em>x</em> 转换成特征向量<em>z</em> ，然后再进行预测 </p> 
<p> 注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的<img alt="" height="16" src="https://images2.imgbox.com/27/21/RJ2YlAcL_o.png" width="44"> 。</p> 
<p><span style="background-color:#ffd900;">错误的主要成分分析情况</span>：</p> 
<p style="margin-left:0;text-align:justify;">（1）将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p> 
<p style="margin-left:0;text-align:justify;">（2）默认地将主要成分分析作为学习过程中的一部分。虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/382aa0de8b853ca52fea73eab9559546/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Tre靶场通关过程（linpeas使用&#43;启动项编辑器提权）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/64933e3edd4527179e9986298be0a964/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">头歌机器学习---Pandas数值统计​ Pandas数据清洗​​</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>