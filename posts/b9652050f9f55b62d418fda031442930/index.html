<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>33张图解flink sql应用提交（建议收藏！） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="33张图解flink sql应用提交（建议收藏！）" />
<meta property="og:description" content="前言 大家好，我是土哥。
这已经是我为读者写的第21篇 Flink系列文章了。
上周有粉丝在群里问，在流计算平台编写完Flink sql后，为什么通过一键提交按钮，就可以将sql提交到yarn集群上面了？
由于现在各大厂对业务分层特别清晰，平台方向和底层技术开发会被单独划分，所以好多大数据同学编写完 Flink Sql 后，只需通过提交按钮将其提交到集群上，对背后的提交原理也许不太清楚。
下面土哥将为大家揭开这层神秘的面纱，挖掘 Flink Sql 背后的提交原理和源码设计。（硬核文章，建议收藏！）
熟悉平台 故事 小笨猪阿土刚入职某大数据公司担任实习生，然后主管交给阿土一个任务，让其熟悉公司的 Flink 流计算平台。
阿土登录流计算平台后，看到平台上面可以编写 Sql 语法，于是就写了一个简单的 sql。
他发现旁边有个效验功能、于是就点击了一下，这时平台弹出 SQL 语法效验正确。阿土心中暗暗自喜，看来我的 sql 功底还是不错嘛。
SQL 语法效验完成后，阿土点击提交按钮，流计算平台提示，SQl 语法效验正确，已成功提交集群。
Flink sql 代码居然提交到yarn集群上了？？？
小笨猪阿土感到很惊讶，sql 就这样直接提交到集群了哇，这时候小笨猪的导师猴哥过来了，看到小笨猪的操作后，表扬了几句。
阿土，完成的不错啊，已经可以提交 sql 代码啦。但是你可别小看这简单的提交，这背后的门路可不浅呦。
这样吧，你好好探索一下这个sql提交的原理，然后写一篇分析报告，在咱们组分享一下。
啊......啊......
小笨猪阿土听到猴哥的要求后，一下就蔫了。从此之后，阿土就和 Flink sql走在了一起。
刚开始阿土很懵，于是就开始搜查 Flink sql 相关文章，过了几天，终于理清了一些思路。小笨猪将其流程总结为以下几个点：
Flink Sql解析器
Flink Planner 和 Blink Planner
Blink Sql提交流程
1. Flink Sql解析器 1.1、了解Calcite
为方便用户使用 Flink 流计算组件，Flink 社区设计了四种抽象，在这些抽象中，Sql API 属于Flink的最上层抽象，是 Flink 的一等公民，这就方便用户或者开发者直接通过 Sql 编写来提交任务。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/b9652050f9f55b62d418fda031442930/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-14T09:00:00+08:00" />
<meta property="article:modified_time" content="2021-10-14T09:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">33张图解flink sql应用提交（建议收藏！）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <h2>前言</h2> 
 <p>大家好，我是土哥。</p> 
 <p>这已经是我为读者写的第<code>21</code>篇 <code>Flink</code>系列文章了。</p> 
 <p>上周有粉丝在群里问，在流计算平台编写完<code>Flink sql</code>后，为什么通过一键提交按钮，就可以将<code>sql</code>提交到<code>yarn</code>集群上面了？</p> 
 <img src="https://images2.imgbox.com/68/db/uQqoRhC4_o.png" alt="88344088cef1074625e5fd6593b2d1be.png"> 
 <p>由于现在各大厂对业务分层特别清晰，平台方向和底层技术开发会被单独划分，所以好多大数据同学编写完 <code>Flink Sql</code> 后，只需通过提交按钮将其提交到集群上，对背后的提交原理也许不太清楚。</p> 
 <img src="https://images2.imgbox.com/f4/e9/7AzN1VIL_o.png" alt="391d260d400d35f30c43d62da24d5883.png"> 
 <p>下面土哥将为大家揭开这层神秘的面纱，挖掘 <code>Flink Sql</code> 背后的提交原理和源码设计。（硬核文章，建议收藏！）</p> 
 <h2>熟悉平台</h2> 
 <h4>故事</h4> 
 <p>小笨猪<code>阿土</code>刚入职某大数据公司担任实习生，然后主管交给阿土一个任务，让其熟悉公司的 <code>Flink</code> 流计算平台。</p> 
 <img src="https://images2.imgbox.com/27/d0/ifVLWuLp_o.png" alt="57f84c22f39aeffb3cc0523b5e7cb728.png"> 
 <p>阿土登录流计算平台后，看到平台上面可以编写 <code>Sql</code> 语法，于是就写了一个简单的 <code>sql。</code></p> 
 <img src="https://images2.imgbox.com/de/c1/Dhqh1up8_o.png" alt="246564cba5cef3bc536f4b0328d22403.png"> 
 <p>他发现旁边有个<strong>效验功能</strong>、于是就点击了一下，这时平台弹出 <code>SQL</code> 语法效验正确。阿土心中暗暗自喜，看来我的 <code>sql</code> 功底还是不错嘛。</p> 
 <p><code>SQL</code> 语法效验完成后，阿土点击提交按钮，流计算平台提示，<code>SQl</code> 语法效验正确，已成功提交集群。</p> 
 <p><strong>Flink sql 代码居然提交到yarn集群上了？？？</strong></p> 
 <img src="https://images2.imgbox.com/53/fe/dPE4Jgcr_o.png" alt="b9bd8991e849bf271e8fa64ea1cca21a.png"> 
 <p>小笨猪阿土感到很惊讶，<code>sql</code> 就这样直接提交到集群了哇，这时候小笨猪的导师猴哥过来了，看到小笨猪的操作后，表扬了几句。</p> 
 <img src="https://images2.imgbox.com/9c/de/oQh9R01u_o.png" alt="84478b010948b2a342d417364fc95bb7.png"> 
 <p>阿土，完成的不错啊，已经可以提交 <code>sql</code> 代码啦。但是你可别小看这简单的提交，这背后的门路可不浅呦。</p> 
 <p>这样吧，你好好探索一下这个sql提交的原理，然后写一篇分析报告，在咱们组分享一下。</p> 
 <p>啊......啊......</p> 
 <p>小笨猪阿土听到猴哥的要求后，一下就蔫了。从此之后，阿土就和 <code>Flink sql</code>走在了一起。</p> 
 <p>刚开始阿土很懵，于是就开始搜查 <code>Flink sql</code> 相关文章，过了几天，终于理清了一些思路。小笨猪将其流程总结为以下几个点：</p> 
 <ol><li><p>Flink Sql解析器</p></li><li><p>Flink Planner 和 Blink Planner</p></li><li><p>Blink Sql提交流程</p></li></ol> 
 <h3>1. Flink Sql解析器</h3> 
 <p><strong>1.1、了解Calcite</strong></p> 
 <p>为方便用户使用 <code>Flink</code> 流计算组件，<code>Flink</code> 社区设计了四种抽象，在这些抽象中，<code>Sql API</code> 属于<code>Flink</code>的最上层抽象，是 <code>Flink</code> 的一等公民，这就方便用户或者开发者直接通过 <code>Sql</code> 编写来提交任务。</p> 
 <img src="https://images2.imgbox.com/53/0b/dK6Ouv2O_o.png" alt="8f83a50d50dc8d92be2bdaa39cee089f.png"> 
 <p>但经过阿土的调查后 发现，<code>Flink sql</code> 在提交任务时，并不是向 <code>DataStream API</code> 那样，直接被转为 <code>StreamGraph</code>，经过优化生成 <code>JobGraph</code> 提交到集群的,而是需要对编写的 <code>Sql</code> 进行解析、验证、优化等操作，在这中间，社区引入了一个强大的解析器，那就是<code>Calcite</code>。</p> 
 <p><strong>阿土好好调研了一番Calcite</strong></p> 
 <p><code>Calcite</code>属于<code>Apache</code>旗下的一个动态数据管理框架，具备很多数据库管理系统的功能，它可以对<code>SQL</code>进行 <strong>SQL 解析，SQL 校验，SQL 查询优化，SQL 生成</strong>以及数据连接查询等操作，它不存储元数据和基本数据，不包含处理数据的算法。而是作为一个中介的角色，<strong>将上层SQL和底层处理引擎打通</strong>，将其<code>SQL</code>转为底层处理引擎需要的数据格式。</p> 
 <p>它不受上层编程语言的限制，前端可以使用 SQL、Pig、Cascading 等语言，只要通过 Calcite 提供的 <code>SQL Api</code> <strong>将它们转化成关系代数的抽象语法树即可</strong>，并根据一定的规则和成本对抽象语法树进行优化，最后推给各个数据处理引擎来执行。</p> 
 <p>所以 Calcite 不涉及物理规划层，它通过扩展适配器来连接多种后端的数据源和数据处理引擎，如 Hive，Drill，Flink，Phoenix等。</p> 
 <p><strong>1.2、Calcite执行步骤</strong></p> 
 <p>小笨猪阿土简单画了一下<code>Calcite</code>的执行流程，主要涉及5个部分 <strong>SQL解析、SQL校验、SQL查询优化、SQL生成、执行</strong>等。</p> 
 <img src="https://images2.imgbox.com/9e/f3/KgN2EVu3_o.png" alt="f492106e135102729c0d2603dbd88e6c.png"> 
 <p><strong>在这个流程中，Calcite各阶段扮演的角色如下：</strong></p> 
 <ol><li><p>SQL解析。通过 <code>JavaCC</code> 实现，使用 JavaCC 编写 SQL 语法描述文件，将 SQL 解析成未经校验的 <code>AST 语法树</code>。</p></li><li><p>SQL校验。通过与元数据结合<code>验证</code> SQL 中的 Schema、Field、 Function 是否存在，输入输出类型是否匹配等。</p></li><li><p>SQL优化。<code>对上个步骤的输出( RelNode ，逻辑计划树)进行优化</code>，使用两种规则：基于规则优化 和 基于代价优化，得到优化后的物理执行计划。</p></li><li><p>SQL生成。<code>将物理执行计划生成为在特定平台/引擎的可执行程序</code>，如生成符合 MySQL 或 Oracle 等不同平台规则的 SQL 查询语句等。</p></li><li><p>执行。执行是通过各个执行平台执行查询，得到输出结果。</p></li></ol> 
 <p>其中，Calcite再与其他处理引擎结合时，到<strong>SQL优化</strong>阶段就已经结束。所以流程图简化为：</p> 
 <img src="https://images2.imgbox.com/a5/5f/RRvBKBff_o.png" alt="cf76ff8ca5de6b5643779fc5307dfcd5.png"> 
 <h3>2. Flink Planner 和 Blink Planner</h3> 
 <p><strong>阿土看完Calcite的原理后，开始想，那Calcite是怎么在Flink中扮演的角色呢？</strong></p> 
 <p>这时猴哥过来给阿土说，单纯的看一些理论文章，是搞不清楚底层设计实现的，阿土啊，你可以看看源码。</p> 
 <p>听了猴哥的一番话后，阿土开始啃起了<code>Flink1.13.2</code>的<code>Flink Sql</code>源码</p> 
 <p><strong>2.1 Flink Planner和Blink Planner</strong></p> 
 <p>在1.9.0版本以前，社区使用<code>Flink Planner</code>作为查询处理器，通过与<code>Calcite</code>进行连接，为<code>Table/SQL API</code>提供完整的解析、优化和执行环境，使其SQL被转为<code>DataStream API</code>的 <code>Transformation</code>，然后再经过<code>StreamJraph -&gt; JobGraph -&gt; ExecutionGraph</code>等一系列流程，最终被提交到集群。</p> 
 <p>在1.9.0版本，社区引入阿里巴巴的<code>Blink</code>，对<code>FIink TabIe &amp; SQL</code>模块做了重大的重构，保留了 <code>Flink Planner</code> 的同时，引入了 <code>Blink PIanner</code>，没引入以前，<code>Flink</code> 没考虑流批作业统一，针对流批作业，底层实现两套代码，引入后，基于流批一体理念，重新设计算子，以流为核心，流作业和批作业最终都会被转为<code>transformation</code>。</p> 
 <p><strong>2.2 Blink Planner与Calcite关系</strong></p> 
 <p>在之后的版本，为了实现Flink流批一体的愿景，通过Blink Planner与Calcite进行对接,对接流程如下：</p> 
 <ol><li><p>在Table/SQL 编写完成后，<strong>通过Calcite 中的parse、validate、rel阶段，以及Blink额外添加的convert阶段,将其先转为Operation</strong>；</p></li><li><p>通过Blink Planner 的<code>translateToRel</code>、<code>optimize</code>、<code>translateToExecNodeGraph</code>和<code>translateToPlan</code>四个阶段，<strong>将Operation转换成DataStream API的 Transformation</strong>；</p></li><li><p>再经过StreamJraph -&gt; JobGraph -&gt; ExecutionGraph等一系列流程，SQL最终被提交到集群。</p></li></ol> 
 <p>小笨猪根据查询后的资料以及查看<code>Flink 1.13.2</code>版本源码后，画出如下SQL执行流程图。</p> 
 <img src="https://images2.imgbox.com/e3/93/6QrKnkNl_o.png" alt="feff593054222946d106f0899196d931.png"> 
 <h3>3. Blink Sql提交流程（源码分析）</h3> 
 <p>阿土根据对源码的分析后，发现无论是Flink SQL执行DDL操作、还是DQL操作或者DML操作、最终都可以将其总结为两个阶段：</p> 
 <ol><li><p>SQL 语句到 Operation 过程，即Parse阶段；</p></li><li><p>Operation 到 Transformations 过程，即Translate阶段。</p></li></ol> 
 <h4>3.1、Parse阶段</h4> 
 <p>在Parse阶段一共包含parse、validate、rel、convert部分</p> 
 <p>ã€�</p> 
 <img src="https://images2.imgbox.com/0d/b0/s8Q91h4d_o.png" alt="57314f8f7b55efdf5914007d57a39728.png"> 
 <p><code>Calcite的 parse 解析模块是基于javacc实现的</code>。javacc是一个<strong>词法分析生成器</strong>和<strong>语法分析生成器</strong>。词法分析器于将输入字符流解析成一个一个的token，以下面这段SQL语句为例:</p> 
 <p><strong>示例1 ：</strong></p> 
 <img src="https://images2.imgbox.com/09/74/iWUwWr2K_o.png" alt="fab1b6ed6cb99f7146314930bc1f30d6.png"> 
 <p>在 parse 部分，上面的SQL语句最后会被解析为如下一组token：</p> 
 <img src="https://images2.imgbox.com/e4/5d/5n6LzvaX_o.png" alt="b72a78c0585f0564903fa52574847f40.png"> 
 <p>接下来<strong>语法分析器</strong>会以<strong>词法分析器</strong>解析出来的token序列作为输入来进行语法分析。分析过程使用<strong>递归下降语法</strong>解析，LL(k)。</p> 
 <p>其中，第一个<code>L</code>表示从左到右扫描输入；第二个<code>L</code>表示每次都进行最左推导(在推导语法树的过程中每次都替换句型中最左的非终结符为终结符。类似还有最右推导)；</p> 
 <p><code>k</code>表示的是每次向前探索(lookahead)<code>k</code> 个终结符。</p> 
 <p>分析所依赖的的词法法则定义在一个parser.jj文件中。</p> 
 <img src="https://images2.imgbox.com/70/3c/pNCHnhhj_o.png" alt="accbb3566aab4387efd5ede747986468.png"> 
 <p>在经过词法分析和语法分析后，一段 SQL 语句会被解析成一颗<code>抽象语法树</code>（Abstract Syntax Tree，AST），树的节点类型在 Calcite 中以 SqlNode 来表示，不同节点以不同子类型的SqlNode来表示。</p> 
 <p>同样以上面的SQL为例，在这段SQL中:</p> 
 <ol><li><p>id, score, T 等为 SqlIdentifier，表示一个字段名或表名的标识符;</p></li><li><p>select和cast()为<code>SqlCall</code>，表示一个行为或动作，其中cast()为一个<code>SqlBasicCall</code>，表示一个函数调用，具体调用的是什么函数，由其内部的SqlOperator决定，比如这里是一个二元操作符“&lt;”，对应SqlBinaryOperator，operator的名字是“&lt;”，类别是SqlKind.LESS_THAN;</p></li><li><p>int 为 <code>SqlDataTypeSpec</code>，表示一个类型定义;</p></li><li><p>'hello'和 10 为<code>SqlLiteral</code>，表示一个常量;</p></li></ol> 
 <p>在Calcite中，所有的操作都是一个<code>SqlCall</code>, 如查询是一个 <code>SqlSelect</code>, 删除是一个 <code>SqlDelete</code> 等，它们都是 <code>SqlCall</code> 的子类型。select的查询条件等为 <code>SqlCall</code> 中的参数。示例1 的 SQL 语句最终生成的语法树形式如下：</p> 
 <img src="https://images2.imgbox.com/1b/a2/VR0D0Vvm_o.png" alt="357185987a981ff89671ee8d3f160a64.png"> 
 <p>如果把示例1中的直接从一个表查询数据，改为从两张表的关联结果中查询数据，例如：</p> 
 <p><strong>示例2：</strong></p> 
 <img src="https://images2.imgbox.com/5d/0c/PeVpTB4E_o.png" alt="2bf23ebe5e4a452c858b7827d7e8c030.png"> 
 <p>则相应的AST形式如下：</p> 
 <img src="https://images2.imgbox.com/d8/12/cdKs6U3V_o.png" alt="87f1c1539f85ea54888612835ff8817f.png"> 
 <p>其中只有FROM子树部分由原来的<code>SqlIdentifier</code>节点变成了一棵<code>SqlJoin</code>子树，其他部分与示例1相同所以在图中省略了。</p> 
 <p>ã€�</p> 
 <p><strong>校验（validate）阶段</strong></p> 
 <img src="https://images2.imgbox.com/da/cb/jkilkdLZ_o.png" alt="f820676fcdb7406ac62d406a50777c47.png"> 
 <p>对经过parser解析出的AST进行有效性验证，验证的方面主要包括以下两方面：</p> 
 <ol><li><p>表名、字段名、函数名是否正确，<strong>如在某个查询的字段在当前<code>SQL</code>位置上是否存在或有歧义</strong>（当前可见的多个数据源中同时存在该名称的字段）</p></li><li><p>特定类型操作自身的合法性，如<code>group by</code>聚合中的聚合函数是否存在嵌套调用，使用<code>AS</code>重命名时，<strong>新名字是否是x.y的形式等</strong></p></li></ol> 
 <p>针对上面的第一种情况，在校验过程中首先需要明确两个最重要的概念：<strong>NameSpace</strong>和<strong>Scope</strong>。</p> 
 <p><code>NameSpace</code>代表一个逻辑上的数据源，可以是一张表，也可以是一个子查询，而<code>Scope</code>则代表了在 <code>SQL</code> 的某个位置，表和字段的可见范围。</p> 
 <p>从概念中可以看出，在某个 <code>SQL</code>位置上，某个字段所对应的 <code>scope</code> 可能包含多个 <code>namespace</code>。在 <code>validate</code> 阶段解析出来的 <code>scope</code> 和 <code>namespace</code> 信息会被保存下来，在后面转换成逻辑执行计划的时候还会用到。</p> 
 <p>通过一个示例来看什么是 <code>NameSpace</code> 和 <code>scope</code></p> 
 <p><strong>示例3</strong></p> 
 <img src="https://images2.imgbox.com/90/01/fu2wxTvm_o.png" alt="22270c3a5f37cb11f364325a1f9e2b92.png"> 
 <p>在上面这样一段SQL语句中包含四个namespace：</p> 
 <img src="https://images2.imgbox.com/e0/84/LZvqe3vK_o.png" alt="d90b0401b82edc524fbec4db614d4476.png"> 
 <p>对于SQL中的不同表达式，根据它们所在的位置，它们所对应的scope如下：</p> 
 <img src="https://images2.imgbox.com/9e/5b/dF3AebRY_o.png" alt="bd09b29940d58eaad194319c13579e2b.png"> 
 <p>那么在校验第一种情况的时候，整个校验过程的核心就在于<strong>为不同的SqlNode节点生成其对应的namespace和scope</strong>，然后对该<code>SqlNode</code>涉及的字段和<code>namespace</code>与<code>scope</code>的对应关系进行校验。</p> 
 <p>对于第二种情况的校验，则需要根据具体的节点类型分别实现了。</p> 
 <p>在Calcite中，validator的具体实现类是<code>SqlValidatorImpl</code>，namespace和scope分别由接口<code>SqlValidatorNamespace</code>和<code>SqlValidatorScope</code>表示，图中涉及到的xxxNamespace和xxxScope分别是这两个类的子类。</p> 
 <p>下图是从调用validator.validate(sqlNode)开始，对一段查询语句的表名和字段名进行校验的时序图。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/a4/1c/zQshWUM2_o.png" alt="16958154bb1b275403a94273e3fadd1b.png"></p> 
 <p>大体过程都已经在图中的注解里进行了说明，需要补充的一点是，在通过 emptyScope.resolve解析表名时，表信息是通过具体的catalogReader从catalog的schema中查找出来的。</p> 
 <p><strong>具体使用什么catalog和catalogReader，是在validator创建之初决定的</strong>。</p> 
 <p>在flink中，根据用户的配置，catalog可能是 <code>GenericInMemoryCatalog</code>（基于内存的catalog）或<code>HiveCatalog</code>（基于hive metastore的catalog）。</p> 
 <p>ã€�</p> 
 <p>如下图所示：</p> 
 <img src="https://images2.imgbox.com/0d/f3/O797tahR_o.png" alt="1f1da18b8306b7b33b62b9b34527dcbe.png"> 
 <p><code>rel</code>阶段是将<code>SqlNode</code>组成的一棵<code>抽象语法树</code>转化为一棵<code>由RelNode和RexNode</code>组成的<code>关系代数树</code>，或者称为执行计划。RelNode表示关系表达式，如投影（Project），即SELECT，和连接（JOIN）等；</p> 
 <p>RexNode表示行表达式，如示例中的 <strong>CAST(score AS INT)、T1.id &lt; 10</strong>。</p> 
 <p>以示例2的语法树为例，在经过rel阶段转换后会生成下图所示的执行计划：</p> 
 <img src="https://images2.imgbox.com/e3/5d/YNUy7ADG_o.png" alt="ab91be8d339ee70318cb55ee8285f078.png"> 
 <p><strong>rel阶段只处理DML和DQL</strong></p> 
 <p>因为<code>DDL</code>实际上可以认为是对元数据的修改，不涉及复杂关系查询，也就不用进行关系代数转换来优化执行，所以也无需转换为表示，根据对应的SqlNode中保存的信息已经可以直接执行了。</p> 
 <p>在calcite中，SqlToRelConverter用于对关系表达式进行转换。Flink中通过如下方式使用calcite将AST转换成逻辑执行计划，如下图源码所示。</p> 
 <img src="https://images2.imgbox.com/d8/7b/jFEpPl92_o.png" alt="5f7746e3cc3c79ab77ca02a381612d72.png"> 
 <p><strong>从Flink1.13.2源码</strong>中可以看到转换的入口是<code>convertQuery</code>方法。</p> 
 <p><code>SqlToRelConverter</code>中的简单的转换流程如下图所示:</p> 
 <img src="https://images2.imgbox.com/ae/17/I4aa3VxM_o.png" alt="8e47ba679de68c20c773bdd0288f970d.png"> 
 <p>针对每种可能的根节点类型都有对应的转换方法。其中<code>DELETE</code>、<code>UPDATE</code>、<code>MERGE</code>、<code>WITH</code>和<code>VALUES</code>这几种语法在flink流式SQL中还不支持，并且其转换过程也比较简单，后文不再详细分析。</p> 
 <p>对于一棵转换后得到的逻辑执行计划树中的节点，其实在<code>AST</code>中都是可以一一对应的找到对应的节点的，所以转换过程本身并不涉及很复杂的算法，大部分过程是提取已有<code>SqlNode</code>节点中记录的信息，然后生成对应的<code>RelNode</code>和<code>RexNode</code>，并设置<code>RelNode</code>间的父子关系。</p> 
 <p>从图中也可以看出在<code>calcite</code>里最终都会生成一个<code>LogicalModify</code>节点，通过节点内的<code>operation</code>属性来标识不同的含义。但是目前<code>flink</code>支持的<code>DML</code>只有insert语句，而且并不会生成<code>LogicalModify</code>节点，而是直接转换成了<code>ModifyOperation</code>，并在需要的时候转换成<code>flink</code>内部自己定义的节点类型<code>LogicalSink</code>。也因为这个原因，对于<code>DML</code>的转换流程图中是略有简化的，<code>insert</code>、<code>delete</code>、<code>update</code>和<code>merge</code>本身都可以带查询语句，因此实际转换的时候都会递归地先对查询部分进行转换。</p> 
 <p>上图所示流程中只展示了对关系表达式的转换，但是每个关系节点（RelNode）中的行表达式同样需要经过转换得来。</p> 
 <p><strong>Calcite中行表达式的转换依赖于两个对象：<code>BlackBoard</code>和<code>SqlNodeToRexConverter</code></strong>。</p> 
 <p><code>BlackBoard</code>是对<code>select</code>进行转换时的一个临时工作空间，它就像一块“黑板”一样，可以临时记录下转换过程中需要的信息，比如<code>select</code>依赖的<code>scope</code>、当前的<code>root</code>节点、当前节点是否是<code>top</code>节点等。</p> 
 <p><code>BlackBoard</code>本身还是一个<code>shuttle</code>，针对不同类型的<code>SqlNode</code>，其内部都有对应的<code>visit</code>方法。其中除<code>SqlCall</code>、<code>SqlLiteral</code>、<code>SqlIntervalQualifier</code>外，都可由<code>BlackBoard</code>和<code>SqlToRelConverter</code>中定义的各种<code>convertXXX</code>方法进行转换，这三种类型的<code>SqlNode</code>则需要借助<code>SqlNodeToRexConverter</code>来进行转换。</p> 
 <p><code>SqlLiteral</code>、<code>SqlIntervalQualifier</code>的转换比较简单，就是从原来的<code>SqlNode</code>中提取信息进行简单的处理和转换，然后生成对应的<code>RexNode</code>。</p> 
 <p>ã€�</p> 
 <p><strong>重点：这一步负责将RelNode tree转换成operation</strong></p> 
 <img src="https://images2.imgbox.com/d0/38/wkVMZU0H_o.png" alt="9ea7da8f763a4603a6bb72f8300ab5fc.png"> 
 <p>这里只有以及的查询子句会首先通过转换为</p> 
 <p>RelNode转换成Operation的过程很简单，针对四种类型的操作，其各自的转换过程如下：·</p> 
 <ol><li><p>CreateTable @convertCreateTable</p><p><strong>如果AST的根节点是SqlCreateTable</strong>，提取节点中记录的<code>schema</code>、<code>properties</code>、<code>comment</code>、<code>primary</code> <code>keys</code>、<code>if not exists</code>信息，创建CatalogTable对象，然后<code>创建CreateTableOperation</code></p></li><li><p>DropTable @convertDropTable</p><p><strong>如果AST的根节点是SqlDropTable</strong>，提取节点中记录的<code>full table name</code>、<code>if exists</code>信息，<code>创建DropTableOperation对象</code></p></li><li><p>Insert @convertInsert</p><p><strong>如果AST的根节点是RichSqlInsert</strong>，提取节点中记录的目标表的完整路径和查询表达式，先将查询表达式通过<code>convertSqlQuery</code>转换成<code>QueryOperation</code>，然后以转换后的<code>QueryOperation</code>为子节点创建<code>ModifyOperation</code>对象。</p></li></ol> 
 <p>这里分两种情况:</p> 
 <blockquote> 
  <p>(1)使用SQL API执行了 insert into 语句，将数据写入已经通过 TableEnvironment注册过的表中，此时创建的是CatalogSinkModifyOperation</p> 
 </blockquote> 
 <blockquote> 
  <p>(2)使用Table API的toXXXStream将table对象转换成了DataStream，创建的是OutputConversionModifyOperation</p> 
 </blockquote> 
 <ol><li><p>Query @convertSqlQuery</p><p><strong>如果根节点的SqlKind是SqlKind.Query</strong>，先通过FlinkPlannerImpl.rel将SqlNode转换成RelNode，然后创建PlannerQueryOperation对象</p></li></ol> 
 <h4>3.2、Translate阶段</h4> 
 <p>在Translate阶段，通过Blink Planner 的<code>translateToRel</code>、<code>optimize</code>、<code>translateToExecNodeGraph</code>和<code>translateToPlan</code>四个阶段:<strong>将Operation转换成 Transformations。</strong></p> 
 <p><strong>重点</strong>：</p> 
 <ol><li><p>从operation开始，先将ModifyOperation通过translateToRel方法转换成Calcite RelNode逻辑计划树，在对应转换成FlinkLogicalRel（RelNode逻辑计划树）；</p></li><li><p>然后经过 调用optimize方法将FlinkLogicalRel 优化成FlinkPhysicalRel。</p></li><li><p>再调用translateToExecNodeGraph方法将FlinkPhysicalRel转为execGraph</p></li><li><p>最后调用translateToPlan方法将execGraph转为transformations</p></li></ol> 
 <img src="https://images2.imgbox.com/7e/c7/0DHD0NpN_o.png" alt="e0c075b9cb79b110136a2bdf67682fad.png"> 
 <img src="https://images2.imgbox.com/90/a8/GQRd6N2p_o.png" alt="e3f91da5ceeb0c8d2b75ac786aff1d97.png"> 
 <p>从逻辑计划变成物理计划（RelNode），</p> 
 <img src="https://images2.imgbox.com/a3/f0/b1j1nfW1_o.png" alt="278b1b38355e2935a738ff2f40ce7154.png"> 
 <p>Flink1.13.2源码如下：</p> 
 <img src="https://images2.imgbox.com/bc/60/X8g12ZVl_o.png" alt="9ee4ca679d763fb338d4ef6e4d6bc794.png"> 
 <p><strong>、</strong></p> 
 <p>这个过程可以看成是<code>convert: RelNode =&gt; Operation</code>的逆过程。</p> 
 <p>逻辑也很简单，<strong>无论是使用SQL API还是Table API，最终生成的operation的根节点一定是ModifyOperation</strong>，因为只有insert语句或者将Table转换成DataStream后，在DataStream结果上面写入sink才能触发执行。</p> 
 <p>前文提到过<strong>ModifyOperation</strong>最终都会被转换成<code>flink</code>内自定义的<code>LogicalSink</code>节点，该节点主要记录数据输出信息，核心在于需要创建出表示数据输出的<code>sink</code>。所以针对三种<code>ModifyOperation</code>类型分别创建sink的过程如下：</p> 
 <ol><li><p><code>UnregisteredSinkModifyOperation</code>：</p><p>这个operation中直接记录了sink信息，因此直接提取出来创建LogicalSink即可。</p></li><li><p><code>CatalogSinkModifyOperation</code>：</p><p>根据operation中记录的table path找到对应的<code>table</code>，然后根据table创建出<code>table sink</code>，最后使用table sink创建出<code>LogicalSink</code>节点。</p><p>这个过程中涉及到了在catalog中解析table和使用ServiceLoader根据table信息在classpath中查找并用于创建table sink的TableSinkFactory的过程，具体如下图所示。</p></li></ol> 
 <p><img src="https://images2.imgbox.com/c5/3b/XZdnnKxA_o.png" alt="9f336f5493420f30b3a9a2edca8f3d07.png"></p> 
 <p><strong>、</strong></p> 
 <p>会使用两个优化器:<strong>RBO(基于规则的优化器) 和 CBO(基于代价的优化器)</strong></p> 
 <ol><li><p><code>RBO</code>(基于规则的优化器)会将原有表达式裁剪掉，遍历一系列规则<code>（Rule）</code>，只要满足条件就转换，生成最终的执行计划。一些常见的规则包括分区裁剪<code>（Partition Prune）</code>、列裁剪、谓词下推<code>（Predicate Pushdown）</code>、投影下推<code>（Projection Pushdown）</code>、聚合下推、limit下推、sort下推、常量折叠（Constant Folding）、子查询内联转join等。</p></li></ol> 
 <p>2.<code>CBO</code>(基于代价的优化器)会将原有表达式保留，基于统计信息和代价模型，尝试探索生成等价关系表达式，最终取代价最小的执行计划。<strong>CBO的实现有两种模型，Volcano模型，Cascades模型</strong>。这两种模型思想很是相似，不同点在于Cascades模型一边遍历SQL逻辑树，一边优化，从而进一步裁剪掉一些执行计划。</p> 
 <p>源码如下：</p> 
 <img src="https://images2.imgbox.com/ab/16/pu7tvQbI_o.png" alt="f52b416febef43c4e241e123b3163944.png"> 
 <p><strong>ã€�</strong></p> 
 <p>调用translateToExecNodeGraph方法将FlinkPhysicalRel转为execGraph</p> 
 <img src="https://images2.imgbox.com/85/fd/rmzVNITJ_o.png" alt="c2072b226aab45b32827a25d51fe5c92.png"> 
 <p><strong>ã€�</strong></p> 
 <p>调用translateToPlan方法将execGraph转为transformations</p> 
 <img src="https://images2.imgbox.com/e7/6f/wA2sDOOV_o.png" alt="fac977b31e7a858b64d4f8fe91c59320.png"> 
 <p>通过上述四个步骤，实现<strong>将Operation转换成 Transformations。</strong></p> 
 <p>小笨猪通过完整的流程分析后，终于搞懂了Flink sql的解析和转换过程，最终SQL被转为Transformations,后面的步骤就变成了Flink DataStream的提交流程，小笨猪还是比较了解的。</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7c9248ee2ae3c85d564827842e299e56/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何高效地做游戏测试？</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6bde1b99719744dc1e960a5509209f4c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">shellcode找块福地－ 通过VDSO绕过PXN</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>