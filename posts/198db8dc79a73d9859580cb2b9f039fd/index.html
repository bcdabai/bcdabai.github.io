<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>批次标准化（batch normalization） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="批次标准化（batch normalization）" />
<meta property="og:description" content="批次标准化（batch normalization） 当一个模型的多个输入差异比较大时（比如 x 1 x_1 x1​在0-1， x 2 x_2 x2​在100-200之间，那么 x 1 x_1 x1​如果发生改变对最后Loss的影响一定是远大于 x 2 x_2 x2​的，因此我们希望让不同维度的输入有一致的范围，让优化更简单
feature Normalization（特征归一化） 我们将每个维度的输入都转化为同一范围的量的过程称为标准化，如上图所示即为标准化的过程，但是图中的标准化mean和 σ i \sigma_i σi​的来源都是所有资料，但是我们知道，我们正常是要分成很多个batch，那么该怎么办呢？
batch Normalization 每个batch并不是同步进行的，为了完成feature Normalization的过程，我们可以将求所有资料的mean和 σ \sigma σ变为这个batch的。
多次标准化 在深度学习中，是有多层layer的，上一个layer的输出也是下一个layer的输入，第一层输入做了feature Normalization，但后面的输出却没有做，这显然是不合理的，因此我们可以对每层的输出都做一个feature Normalization。至于这个feature Normalization放在sigmoid这类激化函数前后都是可以的（不过sigmoid一般建议放在前面，因为sigmoid函数在0附近grandient较大)。
在某些情况下，我们希望最后的输出并不是以0为mean的，因此在输出精确到一定程度后可以对其进行变换：
testing时的batch Normalization 在实际使用的测试过程中，可能一次输入并没有一个batch，这时batch Normalization可能会出现问题，pytorch给了解决方案，即在训练时每一个batch，都会将其mean和 σ \sigma σ拿来做一个平均值，之后用来作为测试时的mean和标准差
做一个平均值，之后用来作为测试时的mean和标准差" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/198db8dc79a73d9859580cb2b9f039fd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-05T21:24:54+08:00" />
<meta property="article:modified_time" content="2023-05-05T21:24:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">批次标准化（batch normalization）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="batch_normalization_0"></a>批次标准化（batch normalization）</h3> 
<p>当一个模型的多个输入差异比较大时（比如<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          1 
         
        
       
      
        x_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>在0-1，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          2 
         
        
       
      
        x_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>在100-200之间，那么<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          1 
         
        
       
      
        x_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>如果发生改变对最后Loss的影响一定是远大于<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          2 
         
        
       
      
        x_2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的，因此我们希望<strong>让不同维度的输入有一致的范围，让优化更简单</strong></p> 
<h4><a id="feature_Normalization_4"></a>feature Normalization（特征归一化）</h4> 
<p><img src="https://images2.imgbox.com/63/88/Db0QivYI_o.png" alt="feature normalization"></p> 
<p>我们将每个维度的输入都转化为同一范围的量的过程称为标准化，如上图所示即为标准化的过程，但是图中的标准化mean和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          σ 
         
        
          i 
         
        
       
      
        \sigma_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的来源都是所有资料，但是我们知道，我们正常是要分成很多个batch，那么该怎么办呢？</p> 
<h4><a id="batch_Normalization_10"></a>batch Normalization</h4> 
<p>每个batch并不是同步进行的，为了完成feature Normalization的过程，我们可以将求所有资料的mean和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span></span></span></span></span>变为这个batch的。</p> 
<h4><a id="_14"></a>多次标准化</h4> 
<p><img src="https://images2.imgbox.com/98/1f/9Ars1rNZ_o.png" alt="deep-num"></p> 
<p>在深度学习中，是有多层layer的，上一个layer的输出也是下一个layer的输入，第一层输入做了feature Normalization，但后面的输出却没有做，这显然是不合理的，因此我们可以对每层的输出都做一个feature Normalization。至于这个feature Normalization放在sigmoid这类激化函数前后都是可以的（不过sigmoid一般建议放在前面，因为sigmoid函数在0附近grandient较大)。</p> 
<p>在某些情况下，我们希望最后的输出并不是以0为mean的，因此在输出精确到一定程度后可以对其进行变换：</p> 
<p><img src="https://images2.imgbox.com/34/e6/g1lxpeRW_o.png" alt="afbt"></p> 
<h4><a id="testingbatch_Normalization_24"></a>testing时的batch Normalization</h4> 
<p>在实际使用的测试过程中，可能一次输入并没有一个batch，这时batch Normalization可能会出现问题，pytorch给了解决方案，即在训练时每一个batch，都会将其mean和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         σ 
        
       
      
        \sigma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">σ</span></span></span></span></span>拿来做一个平均值，之后用来作为测试时的mean和标准差</p> 
<p>做一个平均值，之后用来作为测试时的mean和标准差</p> 
<p><img src="https://images2.imgbox.com/92/f3/TfxMVN9e_o.png" alt="meanmean"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/47c2b93f7d27026c4e6bb1c19cfa5742/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MySQL Server 连接工具</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/395135759bad126e71d07cc7588509ba/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">cmd指令无法从C盘切到D盘或E盘</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>