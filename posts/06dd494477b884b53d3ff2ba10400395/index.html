<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Caffe中卷基层和全连接层训练参数个数如何确定 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Caffe中卷基层和全连接层训练参数个数如何确定" />
<meta property="og:description" content="慢慢填坑中，今天来仔细讲一下卷基层和全连接层训练参数个数如何确定的问题。我们以Mnist为例，首先贴出网络配置文件：
name: &#34;LeNet&#34; layer { name: &#34;mnist&#34; type: &#34;Data&#34; top: &#34;data&#34; top: &#34;label&#34; data_param { source: &#34;examples/mnist/mnist-train-leveldb&#34; backend: LEVELDB batch_size: 64 } transform_param { scale: 0.00390625 } include: { phase: TRAIN } } layer { name: &#34;mnist&#34; type: &#34;Data&#34; top: &#34;data&#34; top: &#34;label&#34; data_param { source: &#34;examples/mnist/mnist-test-leveldb&#34; backend: LEVELDB batch_size: 100 } transform_param { scale: 0.00390625 } include: { phase: TEST } } layer { name: &#34;conv1&#34; type: &#34;Convolution&#34; bottom: &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/06dd494477b884b53d3ff2ba10400395/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-05-17T15:26:24+08:00" />
<meta property="article:modified_time" content="2016-05-17T15:26:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Caffe中卷基层和全连接层训练参数个数如何确定</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="white-space:pre"></span><span style="font-size:18px">慢慢填坑中，今天来仔细讲一下卷基层和全连接层训练参数个数如何确定的问题。我们以Mnist为例，首先贴出网络配置文件：</span></p> 
<p></p> 
<pre><code class="language-python">name: "LeNet"
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  data_param {
    source: "examples/mnist/mnist-train-leveldb"
    backend: LEVELDB
    batch_size: 64
  }
  transform_param {
    scale: 0.00390625
  }
  include: { phase: TRAIN }
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  data_param {
    source: "examples/mnist/mnist-test-leveldb"
    backend: LEVELDB
    batch_size: 100
  }
  transform_param {
    scale: 0.00390625
  }
  include: { phase: TEST }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  bottom: "conv1"
  top: "conv1"
  name: "bn_conv1"
  type: "BatchNorm"
  param {
    lr_mult: 0
    decay_mult: 0
  } 
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  bottom: "conv1"
  top: "conv1"
  name: "scale_conv1"
  type: "Scale"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu_pool1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  bottom: "conv2"
  top: "conv2"
  name: "bn_conv2"
  type: "BatchNorm"
  param {
    lr_mult: 0
    decay_mult: 0
  } 
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  bottom: "conv2"
  top: "conv2"
  name: "scale_conv2"
  type: "Scale"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "relu_pool2"
  type: "ReLU"
  bottom: "pool2"
  top: "pool2"
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
</code></pre> 
<span style="font-size:18px; color:#ff0000"><span style="white-space:pre"> </span>OK，在开始讲解之前我们先说明几个问题：</span> 
<p></p> 
<p><span style="font-size:18px; color:#ff0000"><span style="white-space:pre"></span>1、输入的图片大小是28*28,；</span></p> 
<p><span style="font-size:18px; color:#ff0000"><span style="white-space:pre"></span>2、我们将分三部分讲解，因为三部分计算方式不同；</span></p> 
<p><span style="font-size:18px; color:#ff0000"><span style="white-space:pre"></span>3、由于偏置量b的个数与卷积核的个数相同，因此我们讲解的主要是权重，偏置量个数加上就可以了。</span></p> 
<p><span style="font-size:18px"><span style="color:#ff0000"><br> </span><span style="white-space:pre"></span>1、第一个卷积conv1，之所把第一个卷积单独拿出来，是因为他和后面的卷积计算方式不同，他训练参数个数计算并不关心输入，这里的数据就是指data层中batch_size大小。<span style="color:#ff0000">也可以说第一个卷基层并不关心特征组合，只是提取特征。</span></span></p> 
<p><span style="font-size:18px; color:#333333"><span style="white-space:pre"></span>在每一个卷积层中都以一个参数</span><span style="font-size:18px; color:#ff0000">num_output，这个参数怎么理解呢？</span><span style="font-size:18px; color:#333333">两种理解方式1、卷积的种类个数；2、输出特征图的个数，我么可以认为一种卷积核提取一种特征，然后输出一张特征。</span></p> 
<p><span style="font-size:18px; color:#333333"><span style="white-space:pre"></span>由于第一个卷积层只是简单的提取特征，并没有进行特征组合，因此训练参数个数计算只是num_output*kernel_size^2.这里怎么理解呢？（由于我不会画图，需要大家一点想象力）假设我们的输入有5张图，num_output=3,kernel_size=5。没有进行特征组合，只是简单提取特征，指的是一种卷积核对5张图的同一区域使用相同的权重进行卷积计算，这样每幅图使用相同的卷积核就能提取到相同的特征，然后相同的特征组成一张特征图。</span></p> 
<p><span style="font-size:18px; color:#333333"><span style="white-space:pre"></span>2、第二个卷积至全连接层之间的卷积，这些卷积层的训练参数个数和输入特征图的数量有关，因为这些卷积层需要进行特征组合。举个例子：conv1的num_output=20，说明卷积1层输出了20个特征图，那么卷积2层的输入就是20。conv2的num_output=50，kernel_size=5,那么计算公式是20*50*5*5.</span></p> 
<p><span style="font-size:18px"><span style="color:rgb(51,51,51); white-space:pre"></span><span style="color:#ff0000">为什么这些卷积层的训练个数和输入的特征图的数量有关呢？重点还是在特征组合。输入的20个特征图，每个特征图代表一种特征，如果我们给每种特征不同的权重那是不是就进行了特征组合呢？conv2的卷积核是5*5，对20个特征图进行卷积，那就会有20组（5*5）个连接（每张特征图是一组），如果这20组卷积核的权重相同，那就回到了第一个卷积层的情况，没有对20个特征进行组合，因为权重相同嘛！只能看成简单的相加，如果20组权重不同，是不是就进行了线性相加了呢？所以对于一个卷积核（5*5）我们要学习的参数不是25个，而是25*20个。说到这里我相信你应该已经明白了吧！</span></span></p> 
<p><span style="font-size:18px"><span style="color:rgb(255,0,0); white-space:pre"></span><span style="color:#333333">3、全连接层，全连接层就是普通的神经网络，全连接层的num_output和卷积层中num_output的理解不同，全连接层的num_output应该看成神经元的个数。</span></span></p> 
<p><span style="font-size:18px"><span style="color:#333333"><span style="white-space:pre"></span>3.1、这里要细分一下，先说IP1也就是第一个全连接层。先讲一下ip1的输入，比如最后一个卷积层的num_output=50，那么IP1的输入是50吗？注意这里不是，要理解这个问题，我们只需将全连接层看成是一些列的普通神经网络就可以。比如IP1的num_output=500，也就是有500个神经元，每个神经元都和输入的每一个像素相连，最后一个卷积层输出了50个特征图，每个特征图大小是4*4（输入图像是28*28）那么每个神经元连接的个数就是50*16=800个，也就有800个参数需要学习。总共有500个神经元，因此对于IP1层共需要学习800*500=400,000个参数。</span></span></p> 
<p><span style="font-size:18px"><span style="color:#333333"><span style="white-space:pre"></span>3.2、对于iP2层，iP2的输入就是IP1的输出了，因为IP1输出的不是图像了（或矩阵）而是500个数字。比如ip2的num_output=10，也就是输出数据500维，输出10维的普通神经网络，那么需要学习的参数就是500*10=5000个。</span></span></p> 
<p><span style="font-size:18px"><span style="color:#333333"><br> </span></span></p> 
<p><span style="font-size:18px; color:#333333">以上，只是我的个人见解，如果有错误，欢迎大家指正！</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b3944dfbe3abc24282c0b7c83af3a97b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Android 自定义带进度显示的半圆形进度条ArcTextProgressBar</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/58453d2ab1008979f9769f227ec85202/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">可视化篇：Echarts个人轨迹可视化实现</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>