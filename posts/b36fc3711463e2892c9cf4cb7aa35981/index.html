<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DIFFUSIONCLIP: TEXT-GUIDED IMAGE MANIPULATION USING DIFFUSION MODELS - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DIFFUSIONCLIP: TEXT-GUIDED IMAGE MANIPULATION USING DIFFUSION MODELS" />
<meta property="og:description" content="1.DIFFUSIONCLIP: TEXT-GUIDED IMAGE MANIPULATION USING DIFFUSION MODELS(使用扩散模型的文本引导图像处理)
机构：韩国高等科学技术研究院
github:GitHub - gwang-kim/DiffusionCLIP: [CVPR 2022] Official PyTorch Implementation for DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models
2.介绍和摘要：
摘要：
扩散模型是最近的生成模型，在具有最新性能的图像生成中显示出巨大的成功。然而，对于使用扩散模型进行图像处理的研究很少。在这里，我们提出了一种新颖的DiffusionCLIP，它使用对比语言-图像预训练 (CLIP) 丢失，使用扩散模型执行文本驱动的图像处理。对于域内外图像处理任务，我们的方法具有与现代基于GAN的图像处理方法相当的性能，即使没有额外的编码器或优化，也具有几乎完美的反演优势。此外，我们的方法可以轻松地用于各种新颖的应用程序，从而可以将图像从看不见的域转换为另一个看不见的域，或者在看不见的域中生成笔画条件的图像，等等。最后，我们通过结合多个微调的扩散模型，提出了一种新颖的带有扩散剪辑的多属性控制。
3.diffusionClip
用于图像处理的 DiffusionCLIP 的整体流程如图 2 所示。这里，输入图像 x0 首先通过前向扩散转换为潜在 xl。然后，在 CLIP 损失的指导下，对扩散模型进行微调，并从微调扩散模型生成更新样本。在扩散模型微调方面，可以修改潜在模型或扩散模型。在这项工作中，我们发现直接模型微调更有效，这将在后面的实验中显示.
其中x0是原始图像，而这些参数为优化参数 的被操纵图像，tref是参考文本，ttar是要操纵的目标文本.
我们使用以下由 CLIP 损失和身份损失组成的目标
direction loss
id loss
其中 Lface 是人脸身份损失 (Deng et al., 2019)。 λL1 &gt; 0 和 λface &gt; 0 是每个损失的权重参数。身份丢失的必要性取决于控件的类型。对于某些控件，像素相似性和人类身份的保留很重要（例如表情、头发颜色），而其他控件则更喜欢剧烈的形状和颜色变化 4.EXPERIMENTAL RESULTS
对于 DiffusionCLIP 的所有操作结果，我们使用 256×256 大小的图像。我们使用在 CelebA-HQ (Karras et al." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/b36fc3711463e2892c9cf4cb7aa35981/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-27T15:28:06+08:00" />
<meta property="article:modified_time" content="2022-11-27T15:28:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DIFFUSIONCLIP: TEXT-GUIDED IMAGE MANIPULATION USING DIFFUSION MODELS</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><span style="color:#fe2c24;"><strong>1.DIFFUSIONCLIP: TEXT-GUIDED IMAGE MANIPULATION USING DIFFUSION MODELS(使用扩散模型的文本引导图像处理)</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>机构：韩国高等科学技术研究院</strong></span></p> 
<p><img alt="" height="310" src="https://images2.imgbox.com/2d/5b/Db6skxj7_o.png" width="1200"></p> 
<p> </p> 
<p><span style="color:#fe2c24;"><strong>github:</strong></span><a href="https://github.com/gwang-kim/DiffusionCLIP" title="GitHub - gwang-kim/DiffusionCLIP: [CVPR 2022] Official PyTorch Implementation for DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models">GitHub - gwang-kim/DiffusionCLIP: [CVPR 2022] Official PyTorch Implementation for DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models</a></p> 
<p>2.介绍和摘要：<img alt="" height="440" src="https://images2.imgbox.com/b5/29/DpEfUJ1B_o.png" width="1200"></p> 
<p> 摘要：</p> 
<p>        扩散模型是最近的生成模型，在具有最新性能的图像生成中显示出巨大的成功。然而，对于使用扩散模型进行图像处理的研究很少。在这里，我们提出了一种新颖的DiffusionCLIP，它使用对比语言-图像预训练 (CLIP) 丢失，使用扩散模型执行文本驱动的图像处理。对于域内外图像处理任务，我们的方法具有与现代基于GAN的图像处理方法相当的性能，即使没有额外的编码器或优化，也具有几乎完美的反演优势。此外，我们的方法可以轻松地用于各种新颖的应用程序，从而可以将图像从看不见的域转换为另一个看不见的域，或者在看不见的域中生成笔画条件的图像，等等。最后，我们通过结合多个微调的扩散模型，提出了一种新颖的带有扩散剪辑的多属性控制。</p> 
<p><span style="color:#fe2c24;"><strong>3.diffusionClip</strong></span></p> 
<p>        用于图像处理的 DiffusionCLIP 的整体流程如图 2 所示。这里，输入图像 x0 首先通过前向扩散转换为潜在 xl。然后，在 CLIP 损失的指导下，对扩散模型进行微调，并从微调扩散模型生成更新样本。在扩散模型微调方面，可以修改潜在模型或扩散模型。在这项工作中，我们发现直接模型微调更有效，这将在后面的实验中显示.</p> 
<p><img alt="" height="771" src="https://images2.imgbox.com/84/b0/Dkg030h3_o.png" width="1200"></p> 
<p> <img alt="" height="311" src="https://images2.imgbox.com/79/3c/wz8XIr8u_o.png" width="1200"></p> 
<p> 其中x0是原始图像，而这些参数为优化参数 <img alt="\hat{x}_{0}(\Theta )" class="mathcode" src="https://images2.imgbox.com/da/62/7esW6c5p_o.png">的被操纵图像，tref是参考文本，ttar是要操纵的目标文本.</p> 
<p></p> 
<p><strong>我们使用以下由 CLIP 损失和身份损失组成的目标</strong></p> 
<p><img alt="" height="142" src="https://images2.imgbox.com/6e/dc/ZE6iwvKL_o.png" width="1073"></p> 
<p><strong>direction loss</strong></p> 
<p><img alt="" height="307" src="https://images2.imgbox.com/1b/69/xUzvkFX8_o.png" width="1108"> </p> 
<p><strong> id loss</strong></p> 
<p><img alt="" height="151" src="https://images2.imgbox.com/b5/f8/ZVqaVkmY_o.png" width="990"></p> 
<p>        其中 Lface 是人脸身份损失 (Deng et al., 2019)。 λL1 &gt; 0 和 λface &gt; 0 是每个损失的权重参数。身份丢失的必要性取决于控件的类型。对于某些控件，像素相似性和人类身份的保留很重要（例如表情、头发颜色），而其他控件则更喜欢剧烈的形状和颜色变化 </p> 
<p><span style="color:#fe2c24;"><strong>4.EXPERIMENTAL RESULTS</strong></span></p> 
<p>        对于 DiffusionCLIP 的所有操作结果，我们使用 256×256 大小的图像。我们使用在 CelebA-HQ (Karras et al., 2017)、AFHQ-Dog (Choi et al., 2020)、LSUN-Bedroom、LSUN-Church (Yu et al., 2015) 数据集上预训练的模型进行操作分别是人脸、狗、教堂和卧室的图像。对于 Celeba-HQ、LSUN-Church 和 LSUN-bedroom 模型，我们使用了 (Meng et al., 2021) 中的预训练模型，对于 AFHQ-Dog.</p> 
<p><img alt="" height="762" src="https://images2.imgbox.com/0a/50/iViQXwj0_o.png" width="752"></p> 
<p><span style="color:#fe2c24;"><strong> 5 CONCLUSION</strong></span></p> 
<p>        在本文中，我们提出了DiffusionCLIP，这是一种使用预先训练的扩散模型和clip损失的文本引导图像处理方法。得益于近乎完美的反演特性，DiffusionCLIP通过微调扩散模型显示出出色的域内和域外操纵性能。我们还介绍了使用微调扩散模型的几种新颖应用。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b87c422b4b56d36a523cc6977cbaf4f4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">unity实现坦克对战</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8e301cef0fc699271085f0a991c0e6cc/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Vue2怎么实现响应式原理的？前端面试必知必会100题（八股文汇总）第1题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>