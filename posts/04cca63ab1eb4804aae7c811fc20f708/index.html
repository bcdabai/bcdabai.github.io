<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习入门（六）：深度学习 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习入门（六）：深度学习" />
<meta property="og:description" content="深度学习 进一步提高识别精度 集成学习，学习率衰减，数据扩充
Data Augmentation基于算法“人为地”扩充输入图像。
对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化。
通过裁剪图像的“crop处理”、将图像左右翻转的“flip处理”对于一般图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。
加深层的动机 可以减少网络的参数数量。加深了层的网络可以用更小的参数达到同等水平的表现力。叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大感受野（给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将ReLU等激活函数夹在卷积层的中间，进一步提高了网络的表现力，通过非线性函数的叠加，可以表现更加复杂的东西。
另一个好处就是使学习更加高效。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效学习。
VGG VGG是由卷积层和池化层构成的基础的CNN。特点在于将有权重的层叠加至16层，具备了深度。
VGG中需要注意的地方是，基于3×3的小型滤波器的卷积层的运算是连续进行的。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半“的处理，最后经由全连接层输出结果。
GoogLeNet GoogLeNet在横向上有“宽度”，这称为“Inception结构”。Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果。GoogLeNet的特征就是将这个Inception结构用作一个构件（构成元素）。在GoogLeNet中很多地方使用了1×1的滤波器的卷积层。这个1×1的卷积运算通过在通道方向上减小大小，有助于减少参数和实现高速化处理。
ResNet ResNet的特征在于具有比以前的网络更深的结构。
在深度学习中，过度加深层的话，很多情况下学习将不能顺利进行，导致最终性能不佳。为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个结构后，就可以随着层的加深而不断提高性能了。
在连续两层的卷积层中，将输入x跳着连接至2层后的输出。通过快捷结构，原来的2层卷积层的输出 F （ x ) F（x) F（x)变成了 F （ x ） &#43; x F（x）&#43;x F（x）&#43;x。
通过快捷结构，反向传播信号可以无衰退地传递。
因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将上游的梯度原封不动地传向下游。这里的重点表示对来自上游的梯度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。
迁移学习
实践中经常会灵活应用使用ImageNet这个巨大的数据集学习到的权重数据，这称为迁移学习，将学习完的权重（的一部分）复制到其他神经网络，进行再学习。
说明 此为本人学习《深度学习入门》的学习笔记，详情请阅读原书." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/04cca63ab1eb4804aae7c811fc20f708/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-02T22:23:18+08:00" />
<meta property="article:modified_time" content="2021-01-02T22:23:18+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习入门（六）：深度学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>深度学习</h2> 
<h4><a id="_1"></a>进一步提高识别精度</h4> 
<p>集成学习，学习率衰减，数据扩充<br> <strong>Data Augmentation</strong>基于算法“人为地”扩充输入图像。<br> 对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化。<br> 通过裁剪图像的“crop处理”、将图像左右翻转的“flip处理”对于一般图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。</p> 
<h4><a id="_7"></a>加深层的动机</h4> 
<p>可以减少网络的参数数量。加深了层的网络可以用更小的参数达到同等水平的表现力。叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大<strong>感受野</strong>（给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将ReLU等激活函数夹在卷积层的中间，进一步提高了网络的表现力，通过非线性函数的叠加，可以表现更加复杂的东西。<br> 另一个好处就是使学习更加高效。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效学习。</p> 
<h4><a id="VGG_11"></a>VGG</h4> 
<p>VGG是由卷积层和池化层构成的基础的CNN。特点在于将有权重的层叠加至16层，具备了深度。<br> VGG中需要注意的地方是，基于3×3的小型滤波器的卷积层的运算是连续进行的。重复进行“卷积层重叠2次到4次，再通过池化层将大小减半“的处理，最后经由全连接层输出结果。</p> 
<h4><a id="GoogLeNet_14"></a>GoogLeNet</h4> 
<p>GoogLeNet在横向上有“宽度”，这称为“Inception结构”。Inception结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果。GoogLeNet的特征就是将这个Inception结构用作一个构件（构成元素）。在GoogLeNet中很多地方使用了1×1的滤波器的卷积层。这个1×1的卷积运算通过在通道方向上减小大小，有助于减少参数和实现高速化处理。</p> 
<h4><a id="ResNet_16"></a>ResNet</h4> 
<p>ResNet的特征在于具有比以前的网络更深的结构。<br> 在深度学习中，过度加深层的话，很多情况下学习将不能顺利进行，导致最终性能不佳。为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个结构后，就可以随着层的加深而不断提高性能了。<br> <img src="https://images2.imgbox.com/c4/28/BarZ5AH4_o.png" alt="在这里插入图片描述"><br> 在连续两层的卷积层中，将输入x跳着连接至2层后的输出。通过快捷结构，原来的2层卷积层的输出<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
          （ 
         
        
          x 
         
        
          ) 
         
        
       
         F（x) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mord cjk_fallback">（</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span>变成了<span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
          （ 
         
        
          x 
         
        
          ） 
         
        
          + 
         
        
          x 
         
        
       
         F（x）+x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mord cjk_fallback">（</span><span class="mord mathdefault">x</span><span class="mord cjk_fallback">）</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></span>。<br> 通过快捷结构，反向传播信号可以无衰退地传递。<br> 因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将上游的梯度原封不动地传向下游。这里的重点表示对来自上游的梯度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。<br> <strong>迁移学习</strong><br> 实践中经常会灵活应用使用ImageNet这个巨大的数据集学习到的权重数据，这称为迁移学习，将学习完的权重（的一部分）复制到其他神经网络，进行再学习。</p> 
<h3><a id="_25"></a>说明</h3> 
<p>此为本人学习《深度学习入门》的学习笔记，详情请阅读原书.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4c30bfae6359ab7d82b327d7ac5993b6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">操作系统导论OSTEP 第七章作业答案 进程调度：介绍</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/78fc2463c19c8caaa1cefcc5f0b90a3c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">onu光功率多少是正常_什么是流氓ONU</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>