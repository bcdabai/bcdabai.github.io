<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Swin-Transformer（原理 &#43; 代码）详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Swin-Transformer（原理 &#43; 代码）详解" />
<meta property="og:description" content="参考博文 图解Swin Transformer
Swin-Transformer网络结构详解
【机器学习】详解 Swin Transformer (SwinT)
论文下载
（二）代码的下载与配置 2.1、需要的安装包 官方源码下载
学习的话，请下载Image Classification的代码，配置相对简单，其他的配置会很麻烦。如下图所示：
Install ：
pytorch安装：感觉pytorch &gt; 1.4版本都没问题的。
2、pip install timm==0.3.2(最新版本也行)
1、pip install Apex
win 10系统下安装NVIDIA apex 这个我认为windows安装可能会很啃。
1、首先在github下载源码https://github.com/NVIDIA/apex到本地文件夹
2、打开cmd命令窗口，切换到apex所在的文件夹
3、使用命令：python setup.py install 即可完成安装
注意事项： 可能会出现的问题：
setuptools有ModuleNotFoundError→更新setuptools
pip install --upgrade setuptools
linux系统下安装NVIDIA apex git clone https://github.com/NVIDIA/apex cd apex pip install -v --no-cache-dir --global-option=&#34;--cpp_ext&#34; --global-option=&#34;--cuda_ext&#34; ./ 2.2、代码运行配置 注意：不要用ImageNet数据集：显卡可能会受不了，就是为了学习swin代码对吧，可以自己找一个小的ImageNet的数据集。
2.2.1、代码配置 首先运行main.py文件，如下图：
再点击main.py配置：
最后在下图Parameters处填入：
--cfg configs/swin_tiny_patch4_window7_224.yaml --data-path imagenet --local_rank 0 --batch-size 2 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/524a7854ff40ed511a5ab5b5a1ad5408/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-05T09:43:43+08:00" />
<meta property="article:modified_time" content="2022-09-05T09:43:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Swin-Transformer（原理 &#43; 代码）详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>参考博文</h2> 
<p><a href="https://blog.csdn.net/qq_37541097/article/details/121119988?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165111452016782350992559%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165111452016782350992559&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-121119988.142%5Ev9%5Econtrol,157%5Ev4%5Enew_style&amp;utm_term=swin%20transformer&amp;spm=1018.2226.3001.4187">图解Swin Transformer</a><br> <a href="https://blog.csdn.net/qq_37541097/article/details/121119988?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165111452016782350992559%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165111452016782350992559&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-121119988.142%5Ev9%5Econtrol,157%5Ev4%5Enew_style&amp;utm_term=swin%20transformer&amp;spm=1018.2226.3001.4187">Swin-Transformer网络结构详解</a><br> <a href="https://blog.csdn.net/qq_39478403/article/details/120042232?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165111452016782350992559%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165111452016782350992559&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-120042232.142%5Ev9%5Econtrol,157%5Ev4%5Enew_style&amp;utm_term=swin%20transformer&amp;spm=1018.2226.3001.4187">【机器学习】详解 Swin Transformer (SwinT)</a><br> <a href="https://arxiv.org/abs/2103.14030" rel="nofollow">论文下载</a></p> 
<h2><a id="_5"></a>（二）代码的下载与配置</h2> 
<h3><a id="21_6"></a>2.1、需要的安装包</h3> 
<p><a href="https://github.com/microsoft/Swin-Transformer">官方源码下载</a><br> <font color="Blue">学习的话，请下载<font color="red">Image Classification</font>的代码，配置相对简单，其他的配置会很麻烦。如下图所示：<br> <img src="https://images2.imgbox.com/1f/c0/sXIT0vZ6_o.png" alt="在这里插入图片描述"></font></p> 
<blockquote> 
 <p>Install ：<br> pytorch安装：感觉pytorch &gt; 1.4版本都没问题的。<br> 2、pip install timm==0.3.2(最新版本也行)<br> 1、pip install Apex</p> 
</blockquote> 
<blockquote> 
 <ul><li>win 10系统下安装NVIDIA apex</li></ul> 
 <blockquote> 
  <p>这个我认为windows安装可能会很啃。<br> 1、首先在github下载源码<a href="https://gitcode.net/mirrors/nvidia/apex?utm_source=csdn_github_accelerator" rel="nofollow">https://github.com/NVIDIA/apex</a>到本地文件夹<br> 2、打开cmd命令窗口，切换到apex所在的文件夹<br> 3、使用命令：<font color="red"><strong>python setup.py install</strong> </font>即可完成安装</p> 
  <blockquote> 
   <p>注意事项： 可能会出现的问题：<br> setuptools有ModuleNotFoundError→更新setuptools<br> <font color="red"><strong>pip install --upgrade setuptools</strong></font></p> 
  </blockquote> 
 </blockquote> 
</blockquote> 
<blockquote> 
 <ul><li>linux系统下安装NVIDIA apex</li></ul> 
</blockquote> 
<pre><code>git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
</code></pre> 
<h3><a id="22font_colorredfont_30"></a>2.2、<font color="red">代码运行配置</font></h3> 
<p><font color="Blue">注意：不要用ImageNet数据集：显卡可能会受不了，就是为了学习swin代码对吧，可以自己找一个小的ImageNet的数据集。</font></p> 
<h4><a id="221_32"></a>2.2.1、代码配置</h4> 
<p>首先运行main.py文件，如下图：<br> <img src="https://images2.imgbox.com/83/84/5IViL7iC_o.jpg" alt="在这里插入图片描述"><br> 再点击main.py配置：<br> <img src="https://images2.imgbox.com/a9/4c/UgPiPONM_o.png" alt="在这里插入图片描述"><br> 最后在下图Parameters处填入：</p> 
<pre><code class="prism language-python"><span class="token operator">-</span><span class="token operator">-</span>cfg configs<span class="token operator">/</span>swin_tiny_patch4_window7_224<span class="token punctuation">.</span>yaml <span class="token operator">-</span><span class="token operator">-</span>data<span class="token operator">-</span>path imagenet <span class="token operator">-</span><span class="token operator">-</span>local_rank <span class="token number">0</span> <span class="token operator">-</span><span class="token operator">-</span>batch<span class="token operator">-</span>size <span class="token number">2</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/4a/5e/z0cDzfqR_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="222_42"></a>2.2.2、本人运行报错修改</h4> 
<table><tbody><tr><td bgcolor="bred">报错1如下：</td></tr></tbody></table> 
<pre><code>Swin transformer TypeError: __init__() got an unexpected keyword argument ‘t_mul‘
</code></pre> 
<table><tbody><tr><td bgcolor="bred">报错2如下：</td></tr></tbody></table> 
<pre><code>from timm.data.transforms import _pil_interp无法导入_pil_interp
</code></pre> 
<blockquote> 
 <p>pip install timm==0.3.2(最新版本也行)</p> 
 <blockquote> 
  <p>但是我安装最新版本后：from timm.data.transforms import _pil_interp无法导入_pil_interp，然后我查看了timm.data 中的transforms.py文件，完全就没有定义_pil_interp。<a href="https://github.com/rwightman/pytorch-image-models/commit/a41de1f666f9187e70845bbcf5b092f40acaf097">完整的timm.data 中的transforms.py文件</a>我在下面也把这个文件_pil_interp代码复制在下面，可以自行补充_pil_interp。</p> 
 </blockquote> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">_pil_interp</span><span class="token punctuation">(</span>method<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> method <span class="token operator">==</span> <span class="token string">'bicubic'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Image<span class="token punctuation">.</span>BICUBIC
    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">'lanczos'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Image<span class="token punctuation">.</span>LANCZOS
    <span class="token keyword">elif</span> method <span class="token operator">==</span> <span class="token string">'hamming'</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> Image<span class="token punctuation">.</span>HAMMING
<span class="token keyword">if</span> has_interpolation_mode<span class="token punctuation">:</span>
    _torch_interpolation_to_str <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        InterpolationMode<span class="token punctuation">.</span>NEAREST<span class="token punctuation">:</span> <span class="token string">'nearest'</span><span class="token punctuation">,</span>
        InterpolationMode<span class="token punctuation">.</span>BILINEAR<span class="token punctuation">:</span> <span class="token string">'bilinear'</span><span class="token punctuation">,</span>
        InterpolationMode<span class="token punctuation">.</span>BICUBIC<span class="token punctuation">:</span> <span class="token string">'bicubic'</span><span class="token punctuation">,</span>
        InterpolationMode<span class="token punctuation">.</span>BOX<span class="token punctuation">:</span> <span class="token string">'box'</span><span class="token punctuation">,</span>
        InterpolationMode<span class="token punctuation">.</span>HAMMING<span class="token punctuation">:</span> <span class="token string">'hamming'</span><span class="token punctuation">,</span>
        InterpolationMode<span class="token punctuation">.</span>LANCZOS<span class="token punctuation">:</span> <span class="token string">'lanczos'</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
    _str_to_torch_interpolation <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>b<span class="token punctuation">:</span> a <span class="token keyword">for</span> a<span class="token punctuation">,</span> b <span class="token keyword">in</span> _torch_interpolation_to_str<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    _pil_interpolation_to_torch <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    _torch_interpolation_to_str <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
</code></pre> 
<p><a href="https://github.com/rwightman/pytorch-image-models/commit/a41de1f666f9187e70845bbcf5b092f40acaf097">完整的timm.data 中的transforms.py文件：界面如下图</a><br> <img src="https://images2.imgbox.com/a7/b4/awAjoEkc_o.png" alt="在这里插入图片描述"></p> 
<table><tbody><tr><td bgcolor="bred">报错3：如下图所示：</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/f9/de/oQ4n9wSz_o.png" alt="在这里插入图片描述"><br> 解决办法如下：</p> 
<pre><code>解决办法，删除Swin-Transformer/lr_scheduler.py的第24行‘t_mul=1.,’
</code></pre> 
<p><img src="https://images2.imgbox.com/39/f4/ikQ7TjN3_o.jpg" alt="在这里插入图片描述"></p> 
<h2><a id="_85"></a>（三）原理概括</h2> 
<table><tbody><tr><td bgcolor="yellow">下面PPT是对Swin-Transformer做了一个大概的概括，具体细节可以参考第四节代码部分。</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/c4/b1/D4RXm2fK_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/e3/46/eh7Nw9RK_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/ac/c1/PTaDOcLi_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/e5/99/DTXsGJly_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/08/07/towblCVX_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/e8/27/rzYdGiIe_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/36/ee/IBahwA9i_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/b3/cd/8bc5D5MZ_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/43/0f/e5oN8deR_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/1e/f8/FDGo5p02_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/e4/2a/vluFOU8g_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/a4/51/4jJklc8A_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/65/25/Io2Fw5rd_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_92"></a>（四）代码详解</h2> 
<p><font color="Pink">注意：本人代码部分是按照Debug顺序进行编写的。并不是按照一个一个模块去分开讲解的，所以大家看起来可能会很按难受。这里推荐一篇博客<a href="https://zhuanlan.zhihu.com/p/367111046" rel="nofollow">图解Swin Transformer</a>，是按照每个结构分开单独编写，容易理解，思路清晰。</font></p> 
<table><tbody><tr><td bgcolor="bred">准备工作</td></tr></tbody></table> 
<blockquote> 
 <p>1、首先我们打开main.py和swin_transformer.py文件。</p> 
</blockquote> 
<blockquote> 
 <p>2、然后在swin_transformer.py中找到<font color="red">class SwinTransformer(nn.Module):类</font>，在其<font color="red">def forward_features(self, x):</font>下第一行插入断点，那么下面我们就开始一步一步debug吧。</p> 
</blockquote> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">forward_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># [2, 3, 224, 224], batch_size = 2</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 详解在3.1节</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ape<span class="token punctuation">:</span>    <span class="token comment"># self.ape = False不用考虑</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>absolute_pos_embed
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 就是一个Droupout层</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># [2, 3136, 96]</span>

        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># B L C</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># B C 1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<h3><a id="31PatchEmbed_123"></a>3.1、PatchEmbed</h3> 
<p>在输入开始的时候，做了一个Patch Embedding，将图片切成一个个图块，并嵌入到Embedding。<br> 在每个Stage里，由<font color="Blue">Patch Merging</font>和<font color="Blue">多个Block</font>组成。<br> <font color="Blue">其中Patch Merging模块主要在每个Stage一开始降低图片分辨率。</font><br> <img src="https://images2.imgbox.com/08/f4/xwgSjyvj_o.png" alt="在这里插入图片描述"></p> 
<p>而Block具体结构如右图所示，主要是LayerNorm，MLP，Window Attention 和 Shifted Window Attention组成 (为了方便讲解，我会省略掉一些参数)</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PatchEmbed</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span> patch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> in_chans<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> embed_dim<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        img_size <span class="token operator">=</span> to_2tuple<span class="token punctuation">(</span>img_size<span class="token punctuation">)</span> 
        patch_size <span class="token operator">=</span> to_2tuple<span class="token punctuation">(</span>patch_size<span class="token punctuation">)</span>
        patches_resolution <span class="token operator">=</span> <span class="token punctuation">[</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> patch_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> patch_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>img_size <span class="token operator">=</span> img_size
        self<span class="token punctuation">.</span>patch_size <span class="token operator">=</span> patch_size
        self<span class="token punctuation">.</span>patches_resolution <span class="token operator">=</span> patches_resolution
        self<span class="token punctuation">.</span>num_patches <span class="token operator">=</span> patches_resolution<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> patches_resolution<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>in_chans <span class="token operator">=</span> in_chans
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim

        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_chans<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>patch_size<span class="token punctuation">)</span>
        <span class="token keyword">if</span> norm_layer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape  <span class="token comment"># [2, 3, 224, 224]</span>
        <span class="token comment"># FIXME look at relaxing size constraints</span>
        <span class="token keyword">assert</span> H <span class="token operator">==</span> self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">and</span> W <span class="token operator">==</span> self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> \
            <span class="token string-interpolation"><span class="token string">f"Input image size (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>H<span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>W<span class="token punctuation">}</span></span><span class="token string">) doesn't match model (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">)."</span></span>
        
        <span class="token comment"># proj是先卷积，再flatten(2)把三四列变成一列（即56*56=3136）</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># B Ph*Pw C</span>
        <span class="token comment"># x = torch.Size([2, 3136, 96])  </span>
        <span class="token comment"># 56*56 = 3136个patch=tokens</span>
        <span class="token comment"># 每个patch或tokens的向量维度为96</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment">#4 3136 96 其中3136就是 224/4 * 224/4 相当于有这么长的序列，其中每个元素是96维向量</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<p><font color="Blue">其实只要看forward就行了。</font></p> 
<pre><code>x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
</code></pre> 
<p><font color="red">这一行就是把原图[2, 3, 224, 224]转化为3136个patch，每个patch的维度等于96。</font><br> <img src="https://images2.imgbox.com/bb/ba/9sOd6Occ_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="32class_SwinTransformerBlock_183"></a>3.2、class SwinTransformerBlock()</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">SwinTransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
        B<span class="token punctuation">,</span> L<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token keyword">assert</span> L <span class="token operator">==</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> <span class="token string">"input feature has wrong size"</span>

        shortcut <span class="token operator">=</span> x   <span class="token comment"># x = [2,3136,96]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># (2, 56, 56, 96)</span>

        <span class="token comment"># cyclic shift</span>
        <span class="token comment"># 在第一次我们是W-MSA，没有滑动窗口，所以self.shift_size &gt; 0 =False</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> x

        <span class="token comment"># partition windows</span>
        x_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>shifted_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size, window_size, C</span>
        x_windows <span class="token operator">=</span> x_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size*window_size, C</span>

        <span class="token comment"># W-MSA/SW-MSA</span>
        attn_windows <span class="token operator">=</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x_windows<span class="token punctuation">,</span> mask<span class="token operator">=</span>self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size*window_size, C</span>

        <span class="token comment"># merge windows （把attention后的数据还原成原来输入的shape）</span>
        attn_windows <span class="token operator">=</span> attn_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        shifted_x <span class="token operator">=</span> window_reverse<span class="token punctuation">(</span>attn_windows<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>  <span class="token comment"># B H' W' C</span>

        <span class="token comment"># reverse cyclic shift</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>shifted_x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> shifted_x
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        <span class="token comment"># FFN</span>
        x <span class="token operator">=</span> shortcut <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<table><tbody><tr><td bgcolor="yellow">第一部分：W-MSA部分和窗口的构建</td></tr></tbody></table> 
<pre><code class="prism language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># (2, 56, 56, 96)</span>
<span class="token comment"># cyclic shift</span>
        <span class="token comment"># 在第一次我们是W-MSA，没有滑动窗口，所以self.shift_size &gt; 0 =False</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> x

        <span class="token comment"># partition windows</span>
        x_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>shifted_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size, window_size, C</span>
        x_windows <span class="token operator">=</span> x_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size*window_size, C</span>
</code></pre> 
<table><tbody><tr><td bgcolor="yellow">第一部分中的window_partition(）类：（就是把序列转化为窗口）</td></tr></tbody></table> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">window_partition</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """</span>
    B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> W <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># (2,8,7,8,7,96):指把56*56的patch按照7*7的窗口划分</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># (2,8,7,8,7,96)</span>
    windows <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token comment"># window的数量 H/7 * W/7 *batch</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>windows<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  
    <span class="token comment"># windows=(128, 7, 7, 96)  </span>
    <span class="token comment"># 128 = batch_size * 8 * 8 = 128窗口的数量</span>
    <span class="token comment"># 7 = window_size 窗口的大小尺寸，说明每个窗口包含49个patch</span>
    <span class="token keyword">return</span> windows
</code></pre> 
<p><img src="https://images2.imgbox.com/b9/bc/DgNnzcj2_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/d6/83/zp4WFgqx_o.jpg" alt="在这里插入图片描述"></p> 
<table><tbody><tr><td bgcolor="yellow">详解self.attn(x_windows, mask=self.attn_mask)</td></tr></tbody></table> 
<br> 先定位到class WindowAttention(nn.Module):处，在其forward上打上断点，现在我们去看看吧。（下面我只复制了forward代码） 
<p></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">WindowAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """</span>
        B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> C <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>qkv<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([3, 128, 3, 49, 32])</span>
        q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># make torchscript happy (cannot use tensor as tuple)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>q<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([128, 3, 49, 32])</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>k<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([128, 3, 49, 32])</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([128, 3, 49, 32])</span>
        q <span class="token operator">=</span> q <span class="token operator">*</span> self<span class="token punctuation">.</span>scale  <span class="token comment"># q = [128, 3, 49, 32]</span>
        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([128, 3, 49, 49])</span>

        relative_position_bias <span class="token operator">=</span> self<span class="token punctuation">.</span>relative_position_bias_table<span class="token punctuation">[</span>self<span class="token punctuation">.</span>relative_position_index<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww,Wh*Ww,nH</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>relative_position_bias<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> 
        relative_position_bias <span class="token operator">=</span> relative_position_bias<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># nH, Wh*Ww, Wh*Ww</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>relative_position_bias<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        attn <span class="token operator">=</span> attn <span class="token operator">+</span> relative_position_bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span> 
            nW <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B_ <span class="token operator">//</span> nW<span class="token punctuation">,</span> nW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token operator">+</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>

        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_drop<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 全连接层，用于整合新信息的</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># 还原成输入的形式[2，3136，96]</span>
        <span class="token keyword">return</span> x
</code></pre> 
<blockquote> 
 <p>qkv.shape = [3, 128, 3, 49, 32]<br> （1）3：是指Q、K、V三个<br> （2）128：是指128个windows<br> （3）3：是指Multi–Head = 3（多头注意力机制）<br> （4）49：是指每个窗口含有49个patchs，每个窗口的49个patchs之间要相互做self–attention<br> （5）32：是指经过多头后，每个head分配32个维度。</p> 
</blockquote> 
<blockquote> 
 <p>attn = (q @ k.transpose(-2, -1))<br> print(attn.shape) # torch.Size([128, 3, 49, 49])<br> 就是正常的计算<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
       
         \alpha 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>相关性。</p> 
</blockquote> 
<blockquote> 
 <p>attn = attn + relative_position_bias.unsqueeze(0)<br> 这是把attention（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          α 
         
        
       
         \alpha 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0037em;">α</span></span></span></span></span>）和 位置编码进行相加，和ViT中在tokens加上位置编码。（具体如下图所示）<br> 后面的代码和VIT中基本一样，详细请看本人上一篇博客<a href="https://blog.csdn.net/weixin_54546190/article/details/124372799?spm=1001.2014.3001.5502">ViT（ Vision Transformer）详解</a>。<br> <img src="https://images2.imgbox.com/4e/ff/czYTU4ex_o.png" alt="在这里插入图片描述"><br> <font color="Blue">ViT的Attention公式如下：<br> <font color="red"><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            A 
           
          
            t 
           
          
            t 
           
          
            e 
           
          
            n 
           
          
            t 
           
          
            i 
           
          
            o 
           
          
            n 
           
          
            ( 
           
          
            Q 
           
          
            , 
           
          
            K 
           
          
            , 
           
          
            V 
           
          
            ) 
           
          
            = 
           
          
            s 
           
          
            o 
           
          
            f 
           
          
            t 
           
          
            m 
           
          
            a 
           
          
            x 
           
          
            ( 
           
           
            
            
              Q 
             
             
             
               K 
              
             
               T 
              
             
            
            
             
             
               d 
              
             
               k 
              
             
            
           
          
            ) 
           
          
            V 
           
          
            = 
           
          
            s 
           
          
            o 
           
          
            f 
           
          
            t 
           
          
            m 
           
          
            a 
           
          
            x 
           
          
            ( 
           
           
           
             α 
            
            
             
             
               d 
              
             
               k 
              
             
            
           
          
            ) 
           
          
            V 
           
          
         
           Attention(Q,K,V) = softmax(\frac {QK^T}{\sqrt{d_k}}) V= softmax(\frac {\boldsymbol{\alpha}}{\sqrt{d_k}}) V 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.6275em; vertical-align: -0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0895em;"><span class="" style="top: -2.5864em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3488em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                      <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                       <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                      </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4461em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9191em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.288em; vertical-align: -0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7051em;"><span class="" style="top: -2.5864em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3488em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                      <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                       <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                      </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight">α</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span>:</font><br> <font color="Blue">Swin–Transformer公式如下：<br> <font color="red"><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
         
          
           
           
             A 
            
           
             t 
            
           
             t 
            
           
             e 
            
           
             n 
            
           
             t 
            
           
             i 
            
           
             o 
            
           
             n 
            
           
             ( 
            
           
             Q 
            
           
             , 
            
           
             K 
            
           
             , 
            
           
             V 
            
           
             ) 
            
           
             = 
            
           
             s 
            
           
             o 
            
           
             f 
            
           
             t 
            
           
             m 
            
           
             a 
            
           
             x 
            
           
             ( 
            
            
             
             
               Q 
              
              
              
                K 
               
              
                T 
               
              
             
             
              
              
                d 
               
              
                k 
               
              
             
            
           
             + 
            
           
             B 
            
           
             ) 
            
           
             V 
            
           
             = 
            
           
             s 
            
           
             o 
            
           
             f 
            
           
             t 
            
           
             m 
            
           
             a 
            
           
             x 
            
           
             ( 
            
            
            
              α 
             
             
              
              
                d 
               
              
                k 
               
              
             
            
           
             + 
            
           
             B 
            
           
             ) 
            
           
             V 
            
           
          
            Attention(Q,K,V) = softmax(\frac {QK^T}{\sqrt{d_k}} + B) V= softmax(\frac {\boldsymbol{\alpha}}{\sqrt{d_k}} + B) V 
           
          
        </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.6275em; vertical-align: -0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0895em;"><span class="" style="top: -2.5864em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3488em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                       <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                        <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                       </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4461em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9191em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0502em;">B</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.288em; vertical-align: -0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7051em;"><span class="" style="top: -2.5864em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.3488em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.8222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                       <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                        <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path> 
                       </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord boldsymbol mtight">α</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0502em;">B</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span>:</font><br> 上面公式的主要区别是在原始计算Attention的公式中的Q,K时加入了相对位置编码B。后续实验有证明相对位置编码的加入提升了模型性能。</font></font></p> 
</blockquote> 
<table><tbody><tr><td bgcolor="yellow">相对位置编码</td></tr></tbody></table> 
<p>由于论文中并没有详解讲解这个相对位置偏执，所以我自己根据阅读源码做了简单的总结。(<a href="https://blog.csdn.net/qq_37541097/article/details/121119988?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165111452016782350992559%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165111452016782350992559&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-121119988.142%5Ev9%5Econtrol,157%5Ev4%5Enew_style&amp;utm_term=swin%20transformer&amp;spm=1018.2226.3001.4187">主要借鉴了Swin-Transformer网络结构详解这篇博客</a>)如下图，假设输入的feature map高宽都为2，那么首先我们可以构建出每个像素的绝对位置（左下方的矩阵），对于每个像素的绝对位置是使用行号和列号表示的。比如蓝色的像素对应的是第0行第0列所以绝对位置索引是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         0 
        
       
         ) 
        
       
      
        (0,0) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span>，接下来再看看相对位置索引。首先看下蓝色的像素，在蓝色像素使用q与所有像素k进行匹配过程中，是以蓝色像素为参考点。然后用蓝色像素的绝对位置索引与其他位置索引进行相减，就得到其他位置相对蓝色像素的<strong>相对位置索引</strong>。例如黄色像素的绝对位置索引是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ) 
        
       
      
        (0,1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>,则它相对蓝色像素的相对位置索引为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         0 
        
       
         ) 
        
       
         − 
        
       
         ( 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ) 
        
       
         = 
        
       
         ( 
        
       
         0 
        
       
         , 
        
       
         − 
        
       
         1 
        
       
         ) 
        
       
      
        (0,0) - (0,1) = (0,-1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>，这里是严格按照源码中来讲的，请不要杠。那么同理可以得到其他位置相对蓝色像素的相对位置索引矩阵。同样，也能得到相对黄色，红色以及绿色像素的相对位置索引矩阵。接下来将每个相对位置索引矩阵按行展平，并拼接在一起可以得到下面的4x4矩阵 。</p> 
<blockquote> 
 <p>个人理解：<br> 四个绝对位置编码分别为：(0,0)(0,1)1,0(1,0)(1,1)，每个位置对应的相对位置为(0,0),我们看一下<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          4 
         
        
          × 
         
        
          4 
         
        
       
         4\times4 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">4</span></span></span></span></span>矩阵第二行，蓝色对应黄色：【用真实位置编码坐标相减】(0,1)-(0,0) = (0,1), 红的对和黄色(0,1)-(1,0) = (-1,1)，绿色对黄色：(0,1)-(1,1)=(-1,0)，直接得到第二行所有元素。</p> 
 <blockquote> 
  <p><img src="https://images2.imgbox.com/56/2d/rinYnbaA_o.png" alt="在这里插入图片描述"></p> 
 </blockquote> 
</blockquote> 
<p><img src="https://images2.imgbox.com/64/48/TiQi6pcD_o.png" alt="在这里插入图片描述"><br> <strong>代码过程如下：</strong></p> 
<pre><code class="prism language-python">coords_h <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
coords_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span><span class="token punctuation">[</span>coords_h<span class="token punctuation">,</span> coords_w<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coords<span class="token punctuation">)</span>

coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>coords<span class="token punctuation">)</span>  <span class="token comment"># 2, Wh, Ww</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"1 1 1 "</span><span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coords<span class="token punctuation">)</span>
coords_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 2, Wh*Ww</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"2 2 2  "</span><span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coords_flatten<span class="token punctuation">)</span>

relative_coords_first <span class="token operator">=</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># 2, wh*ww, 1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"3 3 3 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords_first<span class="token punctuation">)</span>
relative_coords_second <span class="token operator">=</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 2, 1, wh*ww</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"4 4 4 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords_second<span class="token punctuation">)</span>
relative_coords <span class="token operator">=</span> relative_coords_first <span class="token operator">-</span> relative_coords_second <span class="token comment"># 最终得到 2, wh*ww, wh*ww 形状的张量</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"5 5 5 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">)</span>

relative_coords <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Wh*Ww, Wh*Ww, 2</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"6 6 6 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/cb/08/LtttxeCh_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/a1/fd/FPHSh4Kc_o.png" alt="在这里插入图片描述"><br> 请注意，我这里描述的一直是<strong>相对位置索引</strong>，并不是<strong>相对位置偏执参数</strong>。因为后面我们会根据相对位置索引去取对应的参数。比如说黄色像素是在蓝色像素的右边，所以相对蓝色像素的相对位置索引为( 0 , − 1 ) 。绿色像素是在红色像素的右边，所以相对红色像素的相对位置索引为( 0 , − 1 )。可以发现这两者的相对位置索引都是( 0 , − 1 ) ，所以他们使用的<strong>相对位置偏执参数</strong>都是一样的。其实讲到这基本已经讲完了，但在源码中作者为了方便把二维索引给转成了一维索引。具体这么转的呢，有人肯定想到，简单啊直接把行、列索引相加不就变一维了吗？比如上面的相对位置索引中有( 0 , − 1 ) 和( − 1 , 0 )在二维的相对位置索引中明显是代表不同的位置，但如果简单相加都等于-1那不就出问题了吗？接下来我们看看源码中是怎么做的。首先在原始的相对位置索引上加上M-1(M为窗口的大小，在本示例中M=2)，加上之后索引中就不会有负数了。<br> <img src="https://images2.imgbox.com/95/83/JcGBzX2m_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"7 7 7 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">)</span>
relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"8 8 8 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/6c/66/STwp0txK_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/e6/39/aeEHz26I_o.png" alt="在这里插入图片描述"><br> 接着将所有的行标都乘上2M-1。<br> <img src="https://images2.imgbox.com/1d/74/HKl94PAY_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">2</span> <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"9 9 9 "</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_coords<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ab/e9/18iVJgxI_o.png" alt="在这里插入图片描述"></p> 
<p>最后将行标和列标进行相加。这样即保证了相对位置关系，而且不会出现上述<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         0 
        
       
         + 
        
       
         ( 
        
       
         − 
        
       
         1 
        
       
         ) 
        
       
         = 
        
       
         ( 
        
       
         − 
        
       
         1 
        
       
         ) 
        
       
         + 
        
       
         0 
        
       
      
        0 + (-1) = (-1) + 0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">0</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span>的问题了，是不是很神奇。<br> <img src="https://images2.imgbox.com/be/bb/juBVNNRN_o.png" alt="在这里插入图片描述"><br> 代码过程如下：</p> 
<pre><code class="prism language-python">relative_position_index <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww, Wh*Ww</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"10 10 10 "</span><span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>relative_position_index<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/b8/06/7t6MAcPh_o.png" alt="在这里插入图片描述"></p> 
<p>刚刚上面也说了，之前计算的是相对位置索引，并不是相对位置偏执参数。真正使用到的可训练参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         B 
        
       
      
        B 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0502em;">B</span></span></span></span></span>是保存在relative position bias table表里的，这个表的长度是等于<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         2 
        
       
         M 
        
       
         − 
        
       
         1 
        
       
         ) 
        
       
         × 
        
       
         ( 
        
       
         2 
        
       
         M 
        
       
         − 
        
       
         1 
        
       
         ) 
        
       
      
        ( 2 M − 1 ) × ( 2 M − 1 ) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.109em;">M</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal" style="margin-right: 0.109em;">M</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>的。那么上述公式中的相对位置偏执参数B是根据上面的相对位置索引表根据查relative position bias table表得到的，如下图所示。<br> <img src="https://images2.imgbox.com/6d/13/UGkDl4yR_o.png" alt="在这里插入图片描述"><br> 以上过程结束，代表Swin–Transformer–Block中的第一部分（W–MSA）结束。返回的x = [2, 3136, 49]。如下图所示：<br> <img src="https://images2.imgbox.com/fe/d2/SgW9s6V6_o.png" alt="在这里插入图片描述"></p> 
<table><tbody><tr><td bgcolor="yellow">那么接下来我们要继续执行Swin--Transformer--Block中的第二部分（SW--MSA）。</td></tr></tbody></table> 
<p><img src="https://images2.imgbox.com/95/f4/napt50OM_o.png" alt="在这里插入图片描述"></p> 
<blockquote> 
 <p>与前一部份的Block的不同之处在于SW-MSA，有个滑动窗口、偏移量等新的东西加入。相同的部分我们就不再代码讲述，下面我们只看不同的部分。</p> 
</blockquote> 
<p><font color="red">首先我们看到这一部分：</font></p> 
<pre><code class="prism language-python"><span class="token comment"># cyclic shift</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>   <span class="token comment"># self.shift_size = 3,偏移量为3.</span>
            shifted_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> x
</code></pre> 
<p><font color="red">再看以下Mask部分：可以看到，一直到运行到attn部分，都和前面的W-MSA参数的size是一样的。</font></p> 
<pre><code class="prism language-python">		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  
		<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>       
        attn <span class="token operator">=</span> attn <span class="token operator">+</span> relative_position_bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([128, 3, 49, 49])</span>
		<span class="token keyword">print</span><span class="token punctuation">(</span>attn<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>  <span class="token comment"># torch.Size([128, 3, 49, 49])</span>
		<span class="token comment"># mask部分,mask = self.attn_mask</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            nW <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B_ <span class="token operator">//</span> nW<span class="token punctuation">,</span> nW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token operator">+</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
</code></pre> 
<p><font color="red">Shifted Window Attention，前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。下面看一下self.shift_size 的定义吧</font><br> <img src="https://images2.imgbox.com/45/71/XwcRbx5n_o.png" alt="在这里插入图片描述"><br> 左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即<strong>window的个数翻倍了</strong>，由原本四个窗口变成了9个窗口。<br> 在实际代码里，我们是通过对特征图移位，并给Attention设置mask来间接实现的。能在保持原有的window个数下，最后的计算结果等价。<br> <img src="https://images2.imgbox.com/f5/3a/e1P2o4WP_o.png" alt="在这里插入图片描述"><br> <strong>特征图移位操作</strong></p> 
<blockquote> 
 <p>代码里对特征图移位是通过<font color="Blue">torch.roll</font>来实现的，<br> <font color="red">shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))</font><br> 下面是示意图：<img src="https://images2.imgbox.com/ad/f0/ebkOrecH_o.png" alt="在这里插入图片描述"><br> 如果需要reverse cyclic shift(就是还原操作)的话只需把参数shifts设置为对应的正数值。</p> 
</blockquote> 
<p><strong>Attention Mask</strong><br> 我认为这是Swin Transformer的精华，通过设置合理的mask，让<font color="Green">Shifted Window Attention</font>在与<font color="Green">Window Attention</font><font color="red">相同的窗口个数下，达到等价的计算结果。</font><br> 首先我们对Shift Window后的每个窗口都给上index，并且做一个<font color="red">roll操作（window_size=2, shift_size=-1）</font><img src="https://images2.imgbox.com/0b/29/gfFRkLWT_o.png" alt="在这里插入图片描述"><br> 我们希望在计算Attention的时候，<font color="red"><strong>让具有相同index QK进行计算，而忽略不同index QK计算结果</strong>。</font><br> 最后正确的结果如下图所示:</p> 
<p><img src="https://images2.imgbox.com/fd/fd/9MYQeUD3_o.png" alt="在这里插入图片描述"><br> 而要想在原始四个窗口下得到正确的结果，我们就必须给<strong>Attention</strong>的结果加入一个<strong>mask</strong>（如上图最右边所示）相关代码如下：</p> 
<blockquote> 
 <p><font color="red">slice(start,end)函数：方法可从已有数组中返回选定的元素，返回一个新数组，包含从start到end（不包含该元素）的数组元素</font></p> 
 <ul><li>start参数：必须，规定从何处开始选取，如果为负数，规定从数组尾部算起的位置，-1是指最后一个元素。</li><li>end参数：可选（如果该参数没有指定，那么切分的数组包含从start倒数组结束的所有元素，如果这个参数为负数，那么规定是从数组尾部开始算起的元素）。<br> <img src="https://images2.imgbox.com/0b/aa/iDSUKeFJ_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/0f/20/3t7qLSGK_o.png" alt="在这里插入图片描述"></li></ul> 
</blockquote> 
<pre><code class="prism language-python">
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># calculate attention mask for SW-MSA</span>
            H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
            img_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 1 H W 1</span>
            h_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># h_slices = (slice(0, -7, None) ,slice(7, -3, None) ,slice(-3, None, None))</span>
            w_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            cnt <span class="token operator">=</span> <span class="token number">0</span>
            <span class="token keyword">for</span> h <span class="token keyword">in</span> h_slices<span class="token punctuation">:</span>
                <span class="token keyword">for</span> w <span class="token keyword">in</span> w_slices<span class="token punctuation">:</span>
                    img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> cnt
                    cnt <span class="token operator">+=</span> <span class="token number">1</span>

            mask_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>img_mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW, window_size, window_size, 1</span>
            mask_windows <span class="token operator">=</span> mask_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>
            attn_mask <span class="token operator">=</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
            attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn_mask <span class="token operator">=</span> <span class="token boolean">None</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/bb/2c/6CKBcJLC_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"> mask_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>img_mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW, window_size, window_size, 1</span>
</code></pre> 
<p>各项细节如下图所示：<br> <img src="https://images2.imgbox.com/a5/48/8c8DOY6H_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">attn_mask <span class="token operator">=</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<p>细节如下图所示：<br> <img src="https://images2.imgbox.com/d5/cf/gXYGWKAK_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python">attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>意思就是对于attn_mask不为0的部分填充为-100，有什么用呢？想一想softmax函数的计算公式，<img src="https://images2.imgbox.com/5f/d8/F87fhI3B_o.png" alt="在这里插入图片描述"><br> 当<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
         
         
           − 
          
         
           100 
          
         
        
       
         ≈ 
        
       
         0 
        
       
      
        e^{-100}\approx 0 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">100</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span> ， 那么这样是不是等于忽略不同index（指下图中的0，1，2，···，8） QK计算结果。<img src="https://images2.imgbox.com/6f/b2/WPSjRvX8_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/0c/ad/WFSAaruL_o.png" alt="在这里插入图片描述"></p> 
<table><tbody><tr><td bgcolor="yellow">Downsample(下采样操作):Patch Merging</td></tr></tbody></table> 
<p><font color="Blue">注意：这里的Patch Merging下采样操作用的可不是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          1 
         
        
          × 
         
        
          1 
         
        
       
         1 \times 1 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1</span></span></span></span></span>卷积进行的下采样。<br> 该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。</font></p> 
<blockquote> 
 <p>在CNN中，则是在每个Stage开始前用stride=2的卷积/池化层来降低分辨率。</p> 
</blockquote> 
<p><font color="Blue">每次降采样是两倍，因此在行方向和列方向上，间隔2选取元素。然后拼接在一起作为一整个张量，最后展开。此时通道维度会变成原先的4倍（因为H,W各缩小2倍），此时再通过一个全连接层再调整通道维度为原来的两倍</font></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PatchMerging</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_resolution<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x: B, H*W, C
        """</span>
        H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
        B<span class="token punctuation">,</span> L<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token keyword">assert</span> L <span class="token operator">==</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> <span class="token string">"input feature has wrong size"</span>
        <span class="token keyword">assert</span> H <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> W <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"x size (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>H<span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>W<span class="token punctuation">}</span></span><span class="token string">) are not even."</span></span>

        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>

        x0 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x3 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x3<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># B H/2 W/2 4*C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> C<span class="token punctuation">)</span>  <span class="token comment"># B H/2*W/2 4*C</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>reduction<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x
</code></pre> 
<p>下面是一个示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）<img src="https://images2.imgbox.com/af/29/zQza55HH_o.png" alt="在这里插入图片描述"></p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BasicLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">:</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_checkpoint<span class="token punctuation">:</span>
                x <span class="token operator">=</span> checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>blk<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<table><tbody><tr><td bgcolor="yellow">整体结构SwinTransformer()，最后分类输出层的概述</td></tr></tbody></table> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">SwinTransformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span> patch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> in_chans<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>
                 embed_dim<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> depths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                 window_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> mlp_ratio<span class="token operator">=</span><span class="token number">4.</span><span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 drop_rate<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> attn_drop_rate<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">,</span> drop_path_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                 norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">,</span> ape<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> patch_norm<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 use_checkpoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>depths<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
        self<span class="token punctuation">.</span>ape <span class="token operator">=</span> ape
        self<span class="token punctuation">.</span>patch_norm <span class="token operator">=</span> patch_norm
        self<span class="token punctuation">.</span>num_features <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>embed_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp_ratio <span class="token operator">=</span> mlp_ratio

        <span class="token comment"># split image into non-overlapping patches</span>
        self<span class="token punctuation">.</span>patch_embed <span class="token operator">=</span> PatchEmbed<span class="token punctuation">(</span>
            img_size<span class="token operator">=</span>img_size<span class="token punctuation">,</span> patch_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> in_chans<span class="token operator">=</span>in_chans<span class="token punctuation">,</span> embed_dim<span class="token operator">=</span>embed_dim<span class="token punctuation">,</span>
            norm_layer<span class="token operator">=</span>norm_layer <span class="token keyword">if</span> self<span class="token punctuation">.</span>patch_norm <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
        num_patches <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">.</span>num_patches
        patches_resolution <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">.</span>patches_resolution
        self<span class="token punctuation">.</span>patches_resolution <span class="token operator">=</span> patches_resolution

        <span class="token comment"># absolute position embedding</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ape<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>absolute_pos_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_patches<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            trunc_normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>absolute_pos_embed<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>pos_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>drop_rate<span class="token punctuation">)</span>

        <span class="token comment"># stochastic depth</span>
        dpr <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> drop_path_rate<span class="token punctuation">,</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment"># stochastic depth decay rule</span>

        <span class="token comment"># build layers</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i_layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer <span class="token operator">=</span> BasicLayer<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>embed_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               input_resolution<span class="token operator">=</span><span class="token punctuation">(</span>patches_resolution<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 patches_resolution<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               depth<span class="token operator">=</span>depths<span class="token punctuation">[</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">,</span>
                               num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">[</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">,</span>
                               window_size<span class="token operator">=</span>window_size<span class="token punctuation">,</span>
                               mlp_ratio<span class="token operator">=</span>self<span class="token punctuation">.</span>mlp_ratio<span class="token punctuation">,</span>
                               qkv_bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">,</span> qk_scale<span class="token operator">=</span>qk_scale<span class="token punctuation">,</span>
                               drop<span class="token operator">=</span>drop_rate<span class="token punctuation">,</span> attn_drop<span class="token operator">=</span>attn_drop_rate<span class="token punctuation">,</span>
                               drop_path<span class="token operator">=</span>dpr<span class="token punctuation">[</span><span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">[</span><span class="token punctuation">:</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">[</span><span class="token punctuation">:</span>i_layer <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">,</span>
                               downsample<span class="token operator">=</span>PatchMerging <span class="token keyword">if</span> <span class="token punctuation">(</span>i_layer <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                               use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_features<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_features<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span> <span class="token keyword">if</span> num_classes <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_init_weights<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_init_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
            trunc_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span> <span class="token keyword">and</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>ignore</span>
    <span class="token keyword">def</span> <span class="token function">no_weight_decay</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span><span class="token string">'absolute_pos_embed'</span><span class="token punctuation">}</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>ignore</span>
    <span class="token keyword">def</span> <span class="token function">no_weight_decay_keywords</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span><span class="token string">'relative_position_bias_table'</span><span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">forward_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># [2, 3, 224, 224], batch_size = 2</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ape<span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>absolute_pos_embed
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># B L C</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># [2, 49, 768]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># B C 1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># [2, 768, 1]</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>   <span class="token comment"># [2, 768]</span>
        <span class="token keyword">return</span> x
        
     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># [2,1000]做imagenet的1000分类</span>
        <span class="token keyword">return</span> x
</code></pre> 
<h2><a id="_704"></a>（五）总结流程</h2> 
<p>整体流程如下</p> 
<ul><li>先对特征图进行LayerNorm</li><li>通过self.shift_size决定是否需要对特征图进行shift</li><li>然后将特征图切成一个个窗口</li><li>计算Attention，通过self.attn_mask来区分Window Attention还是Shift Window Attention</li><li>将各个窗口合并回来</li><li>如果之前有做shift操作，此时进行reverse shift，把之前的shift操作恢复.</li></ul> 
<blockquote> 
 <p><strong>Window Partition/Reverse</strong><br> <font color="Blue">window partition</font>函数是用于对张量划分窗口，指定窗口大小。将原本的张量从<font color="Blue"> N H W C</font>, 划分成 <font color="Blue">num_windows<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           × 
          
         
        
          \times 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">×</span></span></span></span></span>B, window_size, window_size, C，</font>其中 <font color="Blue">num_windows = H<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           × 
          
         
        
          \times 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6667em; vertical-align: -0.0833em;"></span><span class="mord">×</span></span></span></span></span>W / window_size</font>，即窗口的个数。而<font color="Blue">window reverse</font>函数则是对应的逆过程。这两个函数会在后面的<font color="Blue">Window Attention</font>用到。</p> 
</blockquote> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">window_partition</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> W <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    windows <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    <span class="token keyword">return</span> windows


<span class="token keyword">def</span> <span class="token function">window_reverse</span><span class="token punctuation">(</span>windows<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    B <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>windows<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>H <span class="token operator">*</span> W <span class="token operator">/</span> window_size <span class="token operator">/</span> window_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> window_size<span class="token punctuation">,</span> W <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x
</code></pre> 
<ul><li>做dropout和残差连接</li><li>再通过一层LayerNorm+全连接层，以及dropout和残差连接</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d50dd543ab2f3fcca026720b0da0deb4/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">模拟电路设计（10）--- 运算放大器简介</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/882e12c320c01b2e866628b92bf89074/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java 数组字符串转数组，list集合(带泛型)，二维数组，或者二维数组集合（带泛型）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>