<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>k8s 集群部署问题整理 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="k8s 集群部署问题整理" />
<meta property="og:description" content="对kubernetes感兴趣的可以加群885763297，一起玩转kubernetes
1、hostname “master” could not be reached 在host中没有加解析
2、curl -sSL http://localhost:10248/healthz curl: (7) Failed connect to localhost:10248; 拒绝连接 在host中没有localhost的解析
3、Error starting daemon: SELinux is not supported with the overlay2 graph driver on this kernel. Either boot into a newer kernel or…abled=false) vim /etc/ssconfig/docker --selinux-enabled=False
4、bridge-nf-call-iptables 固化的问题： #下面的是关于bridge的配置： net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 1 #意味着二层的网络在转发包的时候会被iptables的forward规则过滤 net.bridge.bridge-nf-call-arptables = 0
5、The connection to the server localhost:8080 was refused - did you specify the right host or port?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a6e99335d192163bf9071961c000cc34/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-09-09T21:21:54+08:00" />
<meta property="article:modified_time" content="2018-09-09T21:21:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">k8s 集群部署问题整理</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night-eighties">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>对kubernetes感兴趣的可以加群885763297，一起玩转kubernetes</p> 
</blockquote> 
<h4><a id="1hostname_master_could_not_be_reached_1"></a>1、hostname “master” could not be reached</h4> 
<p><code>在host中没有加解析</code></p> 
<h4><a id="2curl_sSL_httplocalhost10248healthz_3"></a>2、curl -sSL <a href="http://localhost:10248/healthz" rel="nofollow">http://localhost:10248/healthz</a></h4> 
<p><code>curl: (7) Failed connect to localhost:10248; 拒绝连接 在host中没有localhost的解析</code></p> 
<h4><a id="3Error_starting_daemon_SELinux_is_not_supported_with_the_overlay2_graph_driver_on_this_kernel_Either_boot_into_a_newer_kernel_orabledfalse_6"></a>3、Error starting daemon: SELinux is not supported with the overlay2 graph driver on this kernel. Either boot into a newer kernel or…abled=false)</h4> 
<p><code>vim /etc/ssconfig/docker --selinux-enabled=False</code></p> 
<h4><a id="4bridgenfcalliptables__9"></a>4、bridge-nf-call-iptables 固化的问题：</h4> 
<p><code>#下面的是关于bridge的配置： net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 1 #意味着二层的网络在转发包的时候会被iptables的forward规则过滤 net.bridge.bridge-nf-call-arptables = 0</code></p> 
<h4><a id="5The_connection_to_the_server_localhost8080_was_refused__did_you_specify_the_right_host_or_port_14"></a>5、The connection to the server localhost:8080 was refused - did you specify the right host or port?</h4> 
<p><code>unable to recognize "kube-flannel.yml": Get http://localhost:8080/api?timeout=32s: dial tcp [::1]:8080: connect: connection refused 下面如果在root用户下执行的，就不会报错 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code><br> ###6、error: unable to recognize “mycronjob.yml”: no matches for kind “CronJob” in version “batch/v2alpha1”<br> <code>去kube-apiserver.yaml文件中添加： - --runtime-config=batch/v2alpha1=true，然后重启kubelet服务，就可以了</code></p> 
<h4><a id="7Container_runtime_network_not_ready_NetworkReadyfalse_reasonNetworkPluginNotReady_messagedocker_network_plugin_is_not_ready_cni_config_uninitialized_Unable_to_update_cni_config_No_networks_found_in_etccninetd_Failed_to_get_system_container_stats_for_systemslicekubeletservice_failed_to_get_cgroup_stats_for_systemslicekubeletservice_failed_to_get_container_info_for_systemslicekubeletservice_unknown_container_systemslicekubeletservice_24"></a>7、Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized Unable to update cni config: No networks found in /etc/cni/net.d Failed to get system container stats for “/system.slice/kubelet.service”: failed to get cgroup stats for “/system.slice/kubelet.service”: failed to get container info for “/system.slice/kubelet.service”: unknown container “/system.slice/kubelet.service”</h4> 
<pre><code>docker pull quay.io/coreos/flannel:v0.10.0-amd64 
mkdir -p /etc/cni/net.d/
cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf
{"name":"cbr0","type":"flannel","delegate": {"isDefaultGateway": true}}
EOF
mkdir /usr/share/oci-umount/oci-umount.d -p
mkdir /run/flannel/
cat &lt;&lt;EOF&gt; /run/flannel/subnet.env
FLANNEL_NETWORK=172.100.0.0/16
FLANNEL_SUBNET=172.100.1.0/24
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
</code></pre> 
<h4><a id="8Unable_to_connect_to_the_server_x509_certificate_signed_by_unknown_authority_possibly_because_of_cryptorsa_verification_error__while_trying_to_verify_candidate_authority_certificate_kubernetes_41"></a>8、Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of “crypto/rsa: verification error” while trying to verify candidate authority certificate “kubernetes”)</h4> 
<p><code>export KUBECONFIG=/etc/kubernetes/kubelet.conf</code></p> 
<h4><a id="9Failed_to_get_system_container_stats_for_systemslicedockerservice_failed_to_get_cgroup_stats_for_systemslicedockerservice_failed_to_get_container_info_for_systemslicedockerservice_unknown_container_systemslicedockerservice_44"></a>9、Failed to get system container stats for “/system.slice/docker.service”: failed to get cgroup stats for “/system.slice/docker.service”: failed to get container info for “/system.slice/docker.service”: unknown container “/system.slice/docker.service”</h4> 
<p><code>vim /etc/sysconfig/kubelet --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice systemctl restart kubelet</code></p> 
<p>大概意思是Flag --cgroup-driver --kubelet-cgroups 驱动已经被禁用，这个参数应该通过kubelet 的配置指定配置文件来配置</p> 
<h4><a id="10The_HTTP_call_equal_to_curl_sSL_httplocalhost10255healthz_failed_with_error_Get_httplocalhost10255healthz_dial_tcp_12700110255_getsockopt_connection_refused_51"></a>10、The HTTP call equal to ‘curl -sSL <a href="http://localhost:10255/healthz" rel="nofollow">http://localhost:10255/healthz</a>’ failed with error: Get <a href="http://localhost:10255/healthz:" rel="nofollow">http://localhost:10255/healthz:</a> dial tcp 127.0.0.1:10255: getsockopt: connection refused.</h4> 
<p><code>vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --fail-swap-on=false"</code></p> 
<p>###11、failed to run Kubelet: failed to create kubelet: miscon figuration: kubelet cgroup driver: “systemd” is different from docker cgroup driver: “cgroupfs”<br> <code>kubelet： Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd" docker：　 　vi /lib/systemd/system/docker.service -exec-opt native.cgroupdriver=systemd</code></p> 
<h4><a id="12ERROR_CRI_unable_to_check_if_the_container_runtime_at_varrundockershimsock_is_running_exit_status_1_61"></a>12、[ERROR CRI]: unable to check if the container runtime at “/var/run/dockershim.sock” is running: exit status 1</h4> 
<p><code>rm -f /usr/bin/crictl</code></p> 
<h4><a id="13_Warning__FailedScheduling__2s_x7_over_33s__defaultscheduler__04_nodes_are_available_4_nodes_didnt_match_node_selector_64"></a>13、 Warning FailedScheduling 2s (x7 over 33s) default-scheduler 0/4 nodes are available: 4 node(s) didn’t match node selector.</h4> 
<p><code>如果指定的label在所有node上都无法匹配，则创建Pod失败，会提示无法调度：</code></p> 
<h4><a id="14kubeadm_token_67"></a>14、kubeadm 生成的token过期后，集群增加节点</h4> 
<pre><code> kubeadm token create

openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | 
openssl dgst -sha256 -hex | sed 's/^.* //'

kubeadm join --token aa78f6.8b4cafc8ed26c34f --discovery-token-ca-cert-hash sha256:0fd95a9bc67a7bf0ef42da968a0d55d92e52898ec37c971bd77ee501d845b538  172.16.6.79:6443 --skip-preflight-checks
</code></pre> 
<h4><a id="15_systemctl_status_kubelet_76"></a>15、### systemctl status kubelet告警</h4> 
<p>cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d<br> May 29 06:30:28 fnode kubelet[4136]: E0529 06:30:28.935309 4136 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized<br> <code>删除 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 的 KUBELET_NETWORK_ARGS，然后重启kubelet服务 临时解决。没啥用</code><br> <code>根本原因是缺少： k8s.gcr.io/pause-amd64:3.1</code></p> 
<h4><a id="16_flannel_83"></a>16 删除flannel网络：</h4> 
<pre><code>ifconfig cni0 down
ifconfig flannel.1 down
ifconfig del flannel.1
ifconfig del cni0

ip link del flannel.1
ip link del cni0

yum install bridge-utils
brctl delbr  flannel.1
brctl delbr cni0
rm -rf /var/lib/cni/flannel/* &amp;&amp; rm -rf /var/lib/cni/networks/cbr0/* &amp;&amp; ip link delete cni0 &amp;&amp;  rm -rf /var/lib/cni/network/cni0/*
</code></pre> 
<h4><a id="17E0906_151055415662_______1_leaderelectiongo234_error_retrieving_resource_lock_defaultcephcomrbd_endpoints_cephcomrbd_is_forbidden_User_systemserviceaccountdefaultrbdprovisioner_cannot_get_endpoints_in_the_namespace_default_98"></a>17、E0906 15:10:55.415662 1 leaderelection.go:234] error retrieving resource lock default/ceph.com-rbd: endpoints “ceph.com-rbd” is forbidden: User “system:serviceaccount:default:rbd-provisioner” cannot get endpoints in the namespace “default”</h4> 
<p>`在 添加下面的这一段 （会重新申请资源） kubectl apply -f ceph/rbd/deploy/rbac/clusterrole.yaml</p> 
<ul><li>apiGroups: [""]<br> resources: [“endpoints”]<br> verbs: [“get”, “list”, “watch”, “create”, “update”, “patch”]`</li></ul> 
<h4><a id="18flannel_104"></a>18、flannel指定网卡设备：</h4> 
<p><code>- --iface=eth0</code></p> 
<h4><a id="21_Failed_create_pod_sandbox_rpc_error_code__Unknown_desc__failed_to_set_up_sandbox_container_957541888b8a0e5b9ad65da932f688eb02cc182808e10d1a89a6e8db2132c253_network_for_pod_coredns7655b945bc6hgj9_NetworkPlugin_cni_failed_to_set_up_pod_coredns7655b945bc6hgj9_kubesystem_network_failed_to_find_plugin_loopback_in_path_optcnibin_failed_to_clean_up_sandbox_container_957541888b8a0e5b9ad65da932f688eb02cc182808e10d1a89a6e8db2132c253_network_for_pod_coredns7655b945bc6hgj9_NetworkPlugin_cni_failed_to_teardown_pod_coredns7655b945bc6hgj9_kubesystem_network_failed_to_find_plugin_portmap_in_path_optcnibin_106"></a>21、 Failed create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container “957541888b8a0e5b9ad65da932f688eb02cc182808e10d1a89a6e8db2132c253” network for pod “coredns-7655b945bc-6hgj9”: NetworkPlugin cni failed to set up pod “coredns-7655b945bc-6hgj9_kube-system” network: failed to find plugin “loopback” in path [/opt/cni/bin], failed to clean up sandbox container “957541888b8a0e5b9ad65da932f688eb02cc182808e10d1a89a6e8db2132c253” network for pod “coredns-7655b945bc-6hgj9”: NetworkPlugin cni failed to teardown pod “coredns-7655b945bc-6hgj9_kube-system” network: failed to find plugin “portmap” in path [/opt/cni/bin]]</h4> 
<p><a href="https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/#coredns-pods-have-crashloopbackoff-or-error-state" rel="nofollow">https://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/#coredns-pods-have-crashloopbackoff-or-error-state</a><br> 如果您的网络提供商不支持portmap CNI插件，您可能需要使用服务的NodePort功能或使用HostNetwork=true。</p> 
<h4><a id="22kubeletsystemreserved800mkubereserved500mevictionhard800800m800m500m__kill_111"></a>22、问题：kubelet设置了system-reserved（800m）、kube-reserved(500m)、eviction-hard(800)，其实集群实际可用的内存是总内存-800m-800m-500m ，但是发现还 是会触发系统级别kill进程，</h4> 
<p>排查：使用top查看前几名的内存使用情况，发现etcd服务使用了内存达到500M以上，kubelet使用内存200m，ceph使用内存总和是200多m，加起来就已经900m了，这些都是k8s之外的系统开销，已经完全超出了系统预留内存，因此可能会触发系统级别的kill，</p> 
<h4><a id="23apiserver_114"></a>23、如何访问api-server？</h4> 
<p>使用kubectl proxy功能</p> 
<h4><a id="24svcendpointendpoint_117"></a>24、使用svc的endpoint代理集群外部服务，经常出现endpoint丢失的问题</h4> 
<p>解决：去掉service.spec.selecter 标签就好了。</p> 
<h4><a id="25nodenoreading_120"></a>25、集群雪崩的一次问题处理，node节点偶尔出现noreading状态，</h4> 
<p>排查：此node节点上cpu使用率过高。</p> 
<pre><code>1、没有触发node节点上的cpuPressure的状态，判断出来不是k8s所管理的cpu占用过高的问题，应该是system、kube组件预留的cpu高导致的。
2、查看cpu和mem的cgroup分组，发现kubelet，都在system.sliec下面，因此判断kube预留资源没有生效导致的。
3、
--enforce-node-allocatable=pods,kube-reserved,system-reserved  #采用硬限制，超出限制就oom
--system-reserved-cgroup=/system.slice  #指定系统reserved-cgroup对那些cgroup限制。
--kube-reserved-cgroup=/system.slice/kubelet.service #指定kube-reserved-cgroup对那些服务的cgroup进行限制
--system-reserved=memory=1Gi,cpu=500m  
--kube-reserved=memory=500Mi,cpu=500m,ephemeral-storage=10Gi 
</code></pre> 
<h4><a id="26etcd_Checking_Etcd_cluster_health_132"></a>26、[etcd] Checking Etcd cluster health</h4> 
<p>etcd cluster is not healthy: context deadline exceeded</p> 
<blockquote> 
 <p>对kubernetes感兴趣的可以加群885763297，一起玩转kubernetes</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b941ec4644d41ddff4af739f87866194/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VIORB-SLAM编译</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a9b2b00067bc20c81022d5a4890bcdd9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SSM 动态SQL文件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>