<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>爬虫爬服务器文件夹,我整来了几台服务器，就是为了给你演示一下分布式爬虫的整个过程... - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="爬虫爬服务器文件夹,我整来了几台服务器，就是为了给你演示一下分布式爬虫的整个过程..." />
<meta property="og:description" content="一般情况下
scrapy是这样的
可以看到
1、调度器 Scheduler 会调度 Requests 队列中的请求
2、然后将每个请求交给下载器 Downloader 下载
3、这时候就会得到相应的 item 数据交给 item Pipeline 处理
如果我们希望有多个爬虫来爬取一个网站的数据那么我们的请求队列就要共享而不能说都是 Request 队列要不然爬到的数据就都乱套了所以我们需要一个共享的队列就比如说你希望一块蛋糕能被快速吃掉
你叫来了小明可是小明只有一张嘴啊于是你又叫来了他的 py 们小红和小紫
但是你总不能说给他们每人各一块蛋糕吧你是希望一块蛋糕被快速吃掉你要他们3 张嘴一起吃掉同一块蛋糕你把他们的嘴比作爬虫把蛋糕比作要爬取的网站是不是就比较容易理解了那么多个爬虫如何共享一个请求队列以及如何搭建分布式部署爬虫呢接下来就是学习 Python 的正确姿势
我们需要用到 redis 队列来共享巧的是scrapy-redis这个库就可以使用 redis 队列在此之前我们先搞几台服务器一台用来搭建 redis一台用来搭建数据库再搞三台来运行爬虫服务器买起
别问我哪来这么多服务器有钱任性(良心帅b喊起来)
服务器有了搭建起来呗先把 redis 装上
连接到 redis 服务器
下载 redis
解压到 /usr/local 下
解压完之后进入
安装一波
安装完之后可以顺便复制一下 redis 所需的配置文件
接着进入 bin 目录
启动 redis
没毛病
使用本地连接一下
恩～连上了
redis 搭建完
接着我们来搭建数据库的服务器
把 MongoDB 给装上
先连接到数据库的服务器上
下载 mongoDB
解压一波
把解压下来的文件夹" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/fbe3e237965f8f5c4999f4a2fae80559/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-11T10:41:36+08:00" />
<meta property="article:modified_time" content="2021-08-11T10:41:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">爬虫爬服务器文件夹,我整来了几台服务器，就是为了给你演示一下分布式爬虫的整个过程...</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>一般情况下</p> 
 <p>scrapy是这样的</p> 
 <p align="center"><img src="https://images2.imgbox.com/b0/d5/1qyYDGYL_o.png" alt="227d525428cdeaff9ee38b3a3d6949cb.png"></p> 
 <p>可以看到</p> 
 <p>1、调度器 Scheduler  会调度 Requests 队列中的请求</p> 
 <p>2、然后将每个请求交给下载器 Downloader 下载</p> 
 <p>3、这时候就会得到相应的 item 数据交给 item Pipeline 处理</p> 
 <p align="center">如果我们希望有多个爬虫来爬取一个网站的数据那么我们的请求队列就要共享而不能说都是 Request 队列要不然爬到的数据就都乱套了<img src="https://images2.imgbox.com/e6/86/2e48wDRs_o.png" alt="78e800e6e94fc8958e8cb0a30895a275.png">所以我们需要一个共享的队列就比如说你希望一块蛋糕能被快速吃掉</p> 
 <p>你叫来了小明可是小明只有一张嘴啊于是你又叫来了他的 py 们小红和小紫</p> 
 <p>但是你总不能说给他们每人各一块蛋糕吧你是希望一块蛋糕被快速吃掉你要他们3 张嘴一起吃掉同一块蛋糕你把他们的嘴比作爬虫把蛋糕比作要爬取的网站是不是就比较容易理解了那么多个爬虫如何共享一个请求队列以及如何搭建分布式部署爬虫呢接下来就是学习 Python 的正确姿势</p> 
 <p>我们需要用到 redis 队列来共享巧的是scrapy-redis这个库就可以使用 redis 队列在此之前我们先搞几台服务器一台用来搭建 redis一台用来搭建数据库再搞三台来运行爬虫服务器买起</p> 
 <p>别问我哪来这么多服务器有钱任性(良心帅b喊起来)</p> 
 <p>服务器有了搭建起来呗先把 redis 装上</p> 
 <p>连接到 redis 服务器</p> 
 <p>下载 redis</p> 
 <p>解压到 /usr/local 下</p> 
 <p>解压完之后进入</p> 
 <p>安装一波</p> 
 <p>安装完之后可以顺便复制一下 redis 所需的配置文件</p> 
 <p>接着进入 bin 目录</p> 
 <p>启动 redis</p> 
 <p>没毛病</p> 
 <p>使用本地连接一下</p> 
 <p>恩～连上了</p> 
 <p>redis 搭建完</p> 
 <p>接着我们来搭建数据库的服务器</p> 
 <p>把 MongoDB 给装上</p> 
 <p>先连接到数据库的服务器上</p> 
 <p>下载 mongoDB</p> 
 <p>解压一波</p> 
 <p>把解压下来的文件夹</p> 
 <p>轻轻的移动一下位置</p> 
 <p>创建个 db 文件夹</p> 
 <p>开启 mongodb</p> 
 <p>这里顺便把 bind_ip 设置一下</p> 
 <p>这样才能被远程访问</p> 
 <p>往下拉可以看到</p> 
 <p>mongoDB 监听的是 27017 端口</p> 
 <p>说明安装和启动成功</p> 
 <p>使用本地连接试试</p> 
 <p>可以可以</p> 
 <p>接着我们需要在</p> 
 <p>爬虫服务器安装 Python3 环境</p> 
 <p>slave-02 和 slave-03 服务器</p> 
 <p>同上安装 python3</p> 
 <p>ok</p> 
 <p>环境搭起来了</p> 
 <p>回到我们之前写的代码</p> 
 <p>我们把它改成适用分布式的</p> 
 <p>将 Pipeline 中的数据库地址配置成</p> 
 <p>我们创建的 mongodb 数据库地址</p> 
 <p>接着在 setting 中配置</p> 
 <p>redis 调度和去重</p> 
 <p align="center"><img src="https://images2.imgbox.com/c6/23/Wagv2Eps_o.png" alt="05b2cb5c72c146823ad0074912c4f710.png"></p> 
 <p>再设置一下延迟访问</p> 
 <p align="center"><img src="https://images2.imgbox.com/f1/46/qAyYjZiD_o.png" alt="9a2265aaf66a9d3ec8ae34a6938a99f8.png"></p> 
 <p>搞完了之后</p> 
 <p>将虚拟环境中的库打包一下</p> 
 <p align="center"><img src="https://images2.imgbox.com/71/84/RuJFPgk1_o.png" alt="2d43f9a80bd936d07e65ec9afee3431e.png"></p> 
 <p align="center"><img src="https://images2.imgbox.com/f5/31/FvfnE9F0_o.png" alt="d65eef03bb0dbd60db58681df5bb26c3.png"></p> 
 <p>接着把项目</p> 
 <p>都扔到爬虫服务器上去</p> 
 <p align="center"><img src="https://images2.imgbox.com/26/dc/0y4tFhf0_o.png" alt="6e657a0d0636f119fd11d07aa79a3681.png"></p> 
 <p>连接到爬虫服务器</p> 
 <p>可以看到刚刚传来的文件</p> 
 <p align="center"><img src="https://images2.imgbox.com/23/5d/QfIZXnzN_o.png" alt="0a04c1b3100f18730b6b12862a6a222f.png"></p> 
 <p>把刚刚在虚拟环境中</p> 
 <p>生成的第三方库列表</p> 
 <p>在服务器上一顿安装</p> 
 <p align="center"><img src="https://images2.imgbox.com/89/a6/Go7qL4E4_o.png" alt="da9a18f17dd1e2aba9ab765be82413c5.png"></p> 
 <p>其它两台爬虫服务器</p> 
 <p>和上面一样安装所需要的库</p> 
 <p>都安装完之后</p> 
 <p>就终于可以都 TM 跑起来了</p> 
 <p>所有的爬虫</p> 
 <p>都特么给我跑起来</p> 
 <p>哈哈哈哈哈</p> 
 <p>4台机器开始一顿爬取</p> 
 <p>可以看到 mongodb 都监听到了</p> 
 <p>这几台服务器的连接了</p> 
 <p>我们连到 redis 看看</p> 
 <p>可以看到</p> 
 <p>redis 在调度着请求的消息队列</p> 
 <p>以及过滤重复的请求</p> 
 <p>再连接到 mongodb 看看</p> 
 <p align="center"><img src="https://images2.imgbox.com/3f/e6/991rjz5q_o.png" alt="b74c8855ff23b79ecea4ac40a6ad4fce.png"></p> 
 <p align="center"><img src="https://images2.imgbox.com/f8/a1/2uPByUtT_o.png" alt="a2354467de9a781a3714b33a49a78908.png"></p> 
 <p>小帅b每隔一小会就查询一下</p> 
 <p>爬取下来的数量</p> 
 <p>可以看到</p> 
 <p>速度还是可以的</p> 
 <p>ok</p> 
 <p>以上就是分布式爬虫的</p> 
 <p>搭建及部署的过程了</p> 
 <p>当然了</p> 
 <p>数据库还可以搭建一下集群</p> 
 <p>数据库之间的连接最好设置账户</p> 
 <p>进行安全访问等</p> 
 <p>主要还是让你了解这个过程中</p> 
 <p>分布式爬虫的搭建和使用</p> 
 <p>以及体会它的可扩展性和高效性</p> 
 <p>那么咱们就到这里了</p> 
 <p>我们下回见，peace</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8fa6a86ee43e3bcfdb2c54b65b62c0af/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">优化OpenCV视频的读取速度</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e99672e8f53ad5db8e7c1dbd0930eccf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Ranorex问题第四章 ：错误 MSB3644 未找到框架“.NETFramework,Version=v4.6.2”的引用程序集 的解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>