<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>torch.cuda.OutOfMemoryError: CUDA out of memory. - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="torch.cuda.OutOfMemoryError: CUDA out of memory." />
<meta property="og:description" content="训练清华ChatGLM-6B时报错, 原因是显存不够
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 4.37 GiB already allocated; 64.81 MiB free; 4.37 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
尝试将
model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).half().cuda() 改为
model = AutoModel.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True).half().quantize(4).cuda() 仍然报错
RuntimeError: CUBLAS error: CUBLAS_STATUS_NOT_INITIALIZED
排错流程如下
查看服务器显存占用情况" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d5d9b5b48ffdb710611c09442a948bfc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-04T19:39:17+08:00" />
<meta property="article:modified_time" content="2023-05-04T19:39:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">torch.cuda.OutOfMemoryError: CUDA out of memory.</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>训练清华ChatGLM-6B时报错, 原因是显存不够</strong></p> 
<blockquote> 
 <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 23.70 GiB total capacity; 4.37 GiB already allocated; 64.81 MiB free; 4.37 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p> 
</blockquote> 
<p>尝试将</p> 
<pre><code class="language-python">model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()</code></pre> 
<p>改为</p> 
<pre><code class="language-python">model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()</code></pre> 
<p>仍然报错</p> 
<blockquote> 
 <p>RuntimeError: CUBLAS error: CUBLAS_STATUS_NOT_INITIALIZED</p> 
</blockquote> 
<p><strong>排错流程如下</strong></p> 
<p>查看服务器显存占用情况</p> 
<pre><code class="language-bash">watch -n 0.1 nvidia-smi</code></pre> 
<p><img alt="" height="687" src="https://images2.imgbox.com/c8/de/wCHD1ti0_o.png" width="793"></p> 
<p> 发现gpu:0显存被PID:19409程序大量占用, 报错应该是默认在gpu:0训练导致显存不足, 接着查看gpu:0上程序所属用户(如果不是师兄的我就kill了)</p> 
<pre><code class="language-bash">top</code></pre> 
<p><img alt="" height="173" src="https://images2.imgbox.com/f6/00/wmHNUCMP_o.png" width="863"></p> 
<p>一看是root的, 惹不起还躲不起嘛, 换张卡跑, 顺嘴一提, 权限内的程序可以kill -9 {pid}掉释放显存</p> 
<pre><code class="language-bash">kill -9 19409</code></pre> 
<p> 发现gpu:1空闲, 指定gpu:1上训练模型, 有多种方法,</p> 
<p>(1) 可以在py代码开头（一定要在开头）加</p> 
<pre><code class="language-python">import os
os.environ['CUDA_VISIBLE_DEVICES']='1'</code></pre> 
<p>这样即可指定在gpu:1上训练, 实际上是只设置gpu:1可见, 而屏蔽其他gpu卡</p> 
<p>(2) 可以在代码运行前shell或bash脚本中加</p> 
<pre><code class="language-bash">CUDA_VISIBLE_DEVICES=1 python xxx.py</code></pre> 
<p>这样即可指定在gpu:1上训练, 实际上是只设置gpu:1可见, 而屏蔽其他gpu卡</p> 
<p>(3)在程序中使用set_device()</p> 
<pre><code class="language-python">import torch
torch.cuda.set_device(id)
</code></pre> 
<p>设置完成后查看显存占用情况可以看到, gpu:1显存占用马上上升了, 不影响其他gpu卡的显存</p> 
<p> 可以看到清华的ChatGLM-6B约占12G显存(其他卡显存增加是写文章的时候其他小伙伴在跑)</p> 
<p><img alt="" height="793" src="https://images2.imgbox.com/24/ea/eChivO9l_o.png" width="812"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8e4f38ea8f97f33905eccb5dafaa99e1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【机器学习】最经典案例：手写数字识别（完整流程：DNN/CNN结构设计、模型参数保存、断点续训、acc/loss可视化）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/293d9f63f7a457557311cdb29dbf57a0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java面试题总结（附答案）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>