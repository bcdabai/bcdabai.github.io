<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>爬虫初学习 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="爬虫初学习" />
<meta property="og:description" content="目录 认识爬虫爬虫的过程urllib 认识爬虫 以下的学习都基于一个博主的练习，我就跟着他一起学习练习，这篇博客只是依然记录一下自己的学习历程（😀）加油💪
这是这位博主的原博客，可以学习看看
这是另外一位博主，更入门更详细一些
首先先认识爬虫这个概念，首先学会基本的Python语法知识、学习Python爬虫常用到的几个重要内置库urllib, http等，用于下载网页、学习正则表达式re、BeautifulSoup（bs4）、Xpath（lxml）等网页解析工具、开始一些简单的网站爬取，了解爬取数据过程、了解爬虫的一些反爬机制，header，robot，时间间隔，代理ip，隐含字段等、学习一些特殊网站的爬取，解决登录、Cookie、动态网页等问题、了解爬虫与数据库的结合，如何将爬取数据进行储存、学习应用Python的多线程、多进程进行爬取，提高爬虫效率、学习爬虫的框架，Scrapy、PySpider等、学习分布式爬虫（数据量庞大的需求）
爬虫的过程 在键盘上输入网址点击搜索之后，通过网络经过DNS服务器分析域名，找到真正的服务器。然后通过HTTP协议对服务器发出GET或POST请求，若请求成功就得到了网页，一般都是用HTML, CSS, JS等前端技术来构建的，若请求不成功，服务器返回请求失败的状态码，常见到的503，403等。爬虫通过对服务器发出请求得到HTML网页，然后对下载的网页进行解析，得到我们想要的内容。
urllib 这个库是python内置的，在Python的urllib库中doc开头是这样简短描述的：
Error：“Exception classesraised by urllib.”----就是由urllib举出的exception类Parse：“Parse (absolute andrelative) URLs.”----解析绝对和相对的URLsRequest：“An extensiblelibrary for opening URLs using a variety of protocols”用各种协议打开URLs的一个扩展库Response：“Response classesused by urllib.”----被urllib使用的response类 刚开始先试试request的使用，request请求最简单的操作是用urlopen方法，代码如下：
import urllib.request response = urllib.request.urlopen(&#39;http://python.org/&#39;) result = response.read() print(result) 运行结果编码出现问题：
一般这种情况把编码改成常见的UTF-8就可以了。
import urllib.request response = urllib.request.urlopen(&#39;http://python.org/&#39;) result = response.read().decode(&#39;utf-8&#39;) print(result) 结果很长：
现在来学一下urlopen，urlopen是request的其中一个方法，功能是打开一个网址，URL参数可以是一串字符串（如上例子中一样），也可以是Request对象。
url：即输入的url网址，（如：http://www.xxxx.com/）；data：发给服务器请求的额外信息（比如登录网页需要主动填写的用户信息）。如果需要添加data参数，那么是POST请求，默认无data参数时，就是GET请求；一般来讲，data参数只有在http协议下请求才有意义data参数被规定为byte object，也就是字节对象。data参数应该使用标准的结构，这个需要使用urllib.parse.urlencode()将data进行转换，一般把data设置成字典格式再进行转换即可timeout：是选填的内容，定义超时时间，单位是秒，防止请求时间过长，不填就是默认的时间；cafile：是指向单独文件的，包含了一系列的CA认证 （很少使用，默认即可）;capath：是指向文档目标，也是用于CA认证（很少使用，默认即可）；cafile：可以忽略context：设置SSL加密传输（很少使用，默认即可）geturl(): 返回URL，用于看是否有重定向。info()：返回元信息，例如HTTP的headers。getcode()：返回回复的HTTP状态码，成功是200，失败可能是503等，可以用来检查代理IP的可使用性。 def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TI MEOUT,*, cafile=None, capath=None, cadefault=False, context=None): url，data和上面urlopen中的提到的一样。headers是HTTP请求的报文信息，如User_Agent参数等，它可以让爬虫伪装成浏览器而不被服务器发现你正在使用爬虫。origin_reg_host, unverifiable, method等不太常用。origin_reg_host指的是原产地注册主机。 2021/6/28" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6ec668f822607b0775074dd7e51737af/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-28T16:43:04+08:00" />
<meta property="article:modified_time" content="2021-06-28T16:43:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">爬虫初学习</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">认识爬虫</a></li><li><ul><li><a href="#_6" rel="nofollow">爬虫的过程</a></li><li><a href="#urllib_8" rel="nofollow">urllib</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>认识爬虫</h2> 
<p>以下的学习都基于一个博主的练习，我就跟着他一起学习练习，这篇博客只是依然记录一下自己的学习历程（😀）加油💪<br> <a href="https://blog.csdn.net/zhaohancsdn/article/details/88577705">这是这位博主的原博客，可以学习看看</a><br> <a href="https://segmentfault.com/a/1190000012681700" rel="nofollow">这是另外一位博主，更入门更详细一些</a><br> 首先先认识爬虫这个概念，首先学会基本的Python语法知识、学习Python爬虫常用到的几个重要内置库urllib, http等，用于下载网页、学习正则表达式re、BeautifulSoup（bs4）、Xpath（lxml）等网页解析工具、开始一些简单的网站爬取，了解爬取数据过程、了解爬虫的一些反爬机制，header，robot，时间间隔，代理ip，隐含字段等、学习一些特殊网站的爬取，解决登录、Cookie、动态网页等问题、了解爬虫与数据库的结合，如何将爬取数据进行储存、学习应用Python的多线程、多进程进行爬取，提高爬虫效率、学习爬虫的框架，Scrapy、PySpider等、学习分布式爬虫（数据量庞大的需求）</p> 
<h3><a id="_6"></a>爬虫的过程</h3> 
<p>在键盘上输入网址点击搜索之后，通过网络经过DNS服务器分析域名，找到真正的服务器。然后通过HTTP协议对服务器发出GET或POST请求，若请求成功就得到了网页，一般都是用HTML, CSS, JS等前端技术来构建的，若请求不成功，服务器返回请求失败的状态码，常见到的503，403等。爬虫通过对服务器发出请求得到HTML网页，然后对下载的网页进行解析，得到我们想要的内容。</p> 
<h3><a id="urllib_8"></a>urllib</h3> 
<p>这个库是python内置的，在Python的urllib库中doc开头是这样简短描述的：</p> 
<ul><li>Error：“Exception classesraised by urllib.”----就是由urllib举出的exception类</li><li>Parse：“Parse (absolute andrelative) URLs.”----解析绝对和相对的URLs</li><li>Request：“An extensiblelibrary for opening URLs using a variety of protocols”用各种协议打开URLs的一个扩展库</li><li>Response：“Response classesused by urllib.”----被urllib使用的response类</li></ul> 
<p>刚开始先试试request的使用，request请求最简单的操作是用urlopen方法，代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
response <span class="token operator">=</span> urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">'http://python.org/'</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> response<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span>
</code></pre> 
<p>运行结果编码出现问题：<br> <img src="https://images2.imgbox.com/8d/10/9HyraAxv_o.png" alt="在这里插入图片描述"><br> 一般这种情况把编码改成常见的UTF-8就可以了。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> urllib<span class="token punctuation">.</span>request
response <span class="token operator">=</span> urllib<span class="token punctuation">.</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span><span class="token string">'http://python.org/'</span><span class="token punctuation">)</span>
result <span class="token operator">=</span> response<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span> 
</code></pre> 
<p>结果很长：<br> <img src="https://images2.imgbox.com/21/fa/KvGUv2F8_o.png" alt="在这里插入图片描述"><br> 现在来学一下urlopen，urlopen是request的其中一个方法，功能是打开一个网址，URL参数可以是一串字符串（如上例子中一样），也可以是Request对象。</p> 
<ul><li>url：即输入的url网址，（如：http://www.xxxx.com/）；</li><li>data：发给服务器请求的额外信息（比如登录网页需要主动填写的用户信息）。如果需要添加data参数，那么是POST请求，默认无data参数时，就是GET请求；一般来讲，data参数只有在http协议下请求才有意义data参数被规定为byte object，也就是字节对象。data参数应该使用标准的结构，这个需要使用urllib.parse.urlencode()将data进行转换，一般把data设置成字典格式再进行转换即可</li><li>timeout：是选填的内容，定义超时时间，单位是秒，防止请求时间过长，不填就是默认的时间；</li><li>cafile：是指向单独文件的，包含了一系列的CA认证 （很少使用，默认即可）;</li><li>capath：是指向文档目标，也是用于CA认证（很少使用，默认即可）；</li><li>cafile：可以忽略</li><li>context：设置SSL加密传输（很少使用，默认即可）</li><li>geturl(): 返回URL，用于看是否有重定向。</li><li>info()：返回元信息，例如HTTP的headers。</li><li>getcode()：返回回复的HTTP状态码，成功是200，失败可能是503等，可以用来检查代理IP的可使用性。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">urlopen</span><span class="token punctuation">(</span>url<span class="token punctuation">,</span> data<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> timeout<span class="token operator">=</span>socket<span class="token punctuation">.</span>_GLOBAL_DEFAULT_TI
            MEOUT<span class="token punctuation">,</span><span class="token operator">*</span><span class="token punctuation">,</span> cafile<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> capath<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> 
            cadefault<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> context<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
<ul><li>url，data和上面urlopen中的提到的一样。</li><li>headers是HTTP请求的报文信息，如User_Agent参数等，它可以让爬虫伪装成浏览器而不被服务器发现你正在使用爬虫。</li><li>origin_reg_host, unverifiable, method等不太常用。origin_reg_host指的是原产地注册主机。</li></ul> 
<p>2021/6/28</p> 
<hr>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c398b4db2ea3e78d5b0816d91dba1f6e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">机器人正运动学（9）—— 修改DH参数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dc5aad66f152a9298b1cb177bf957927/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">怎么看计算机远程桌面的端口号,远程桌面端口-windows如何设置远程桌面登录的端口号...</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>