<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>批量下载指定公司专利信息 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="批量下载指定公司专利信息" />
<meta property="og:description" content="领导安排了一个任务，下载竞争对手公司申请的专利信息，主要对手包括以下几个：Vestas Wind System A/S 集团，歌美飒技术集团股份有限公司，GE，西门子。
google专利被墙了，不好弄，发现美国专利及商标局这个网站还健在，打开https://www.uspto.gov/，发现美帝还是比较良心的，网站简约，搜索人性，检索迅速，比较一下国内的这个网站http://cpquery.sipo.gov.cn/真是高下立判。
检索界面如下：
检索结果如下： 任务就是把这些都批量下载下来，形成一个记录信息的txt文档，同时下载全文pdf，两个文件放进一个文件夹。
talk is cheap，show the code
#爬取美国专利网指定公司的专利 #by---jzx #2016-11-8 import re from bs4 import BeautifulSoup import requests import os #存储数据 def store_data(dictionary, search_string): dictionary[&#39;patent_name&#39;] = dictionary[&#39;patent_name&#39;].replace(&#39;\n&#39;, &#39; &#39;).replace(&#39;&#34;&#39;, &#39; &#39;)[:100] folder_name = &#39;./&#39; &#43; search_string &#43; &#39;/&#39; &#43; dictionary[&#39;patent_name&#39;] &#43; &#39;(&#39; &#43; dictionary[&#39;patent_code&#39;] &#43; &#39;)&#39; if not os.path.exists(folder_name): os.makedirs(folder_name) filename = folder_name &#43; &#39;/data.txt&#39; pdfname = folder_name &#43; &#39;/full_pdf.pdf&#39; data_list = [&#39;patent_code&#39;, &#39;patent_name&#39;, &#39;year&#39;, &#39;inventor_and_country_data&#39;, &#39;description&#39;, &#39;full_pdf_file_link&#39;] with open(filename, &#39;w&#39;) as f: for data in data_list: f." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/61dc906e85cc67c923d2e7960af2678b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-11-08T14:36:12+08:00" />
<meta property="article:modified_time" content="2016-11-08T14:36:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">批量下载指定公司专利信息</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>领导安排了一个任务，下载竞争对手公司申请的专利信息，主要对手包括以下几个：Vestas Wind System A/S 集团，歌美飒技术集团股份有限公司，GE，西门子。</p> 
<p>google专利被墙了，不好弄，发现美国专利及商标局这个网站还健在，打开<a href="https://www.uspto.gov/" rel="nofollow">https://www.uspto.gov/</a>，发现美帝还是比较良心的，网站简约，搜索人性，检索迅速，比较一下国内的这个网站<a href="http://cpquery.sipo.gov.cn/" rel="nofollow">http://cpquery.sipo.gov.cn/</a>真是高下立判。</p> 
<p>检索界面如下：</p> 
<p><img src="https://images2.imgbox.com/e9/df/FhR5Tq6E_o.png" alt="这里可以实现各种搜索，指定申请人，指定申请日期，指定范围" title=""></p> 
<p>检索结果如下： <br> <img src="https://images2.imgbox.com/5e/3d/s4n5W35J_o.png" alt="检索信息如图" title=""> <br> 任务就是把这些都批量下载下来，形成一个记录信息的txt文档，同时下载全文pdf，两个文件放进一个文件夹。</p> 
<p>talk is cheap，show the code</p> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#爬取美国专利网指定公司的专利</span>
<span class="hljs-comment">#by---jzx</span>
<span class="hljs-comment">#2016-11-8</span>
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> os
<span class="hljs-comment">#存储数据</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">store_data</span><span class="hljs-params">(dictionary, search_string)</span>:</span>
    dictionary[<span class="hljs-string">'patent_name'</span>] = dictionary[<span class="hljs-string">'patent_name'</span>].replace(<span class="hljs-string">'\n'</span>, <span class="hljs-string">' '</span>).replace(<span class="hljs-string">'"'</span>, <span class="hljs-string">' '</span>)[:<span class="hljs-number">100</span>]
    folder_name = <span class="hljs-string">'./'</span> + search_string + <span class="hljs-string">'/'</span> + dictionary[<span class="hljs-string">'patent_name'</span>] + <span class="hljs-string">'('</span> + dictionary[<span class="hljs-string">'patent_code'</span>] + <span class="hljs-string">')'</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(folder_name):
        os.makedirs(folder_name)
    filename = folder_name + <span class="hljs-string">'/data.txt'</span>
    pdfname = folder_name + <span class="hljs-string">'/full_pdf.pdf'</span>
    data_list = [<span class="hljs-string">'patent_code'</span>, <span class="hljs-string">'patent_name'</span>, <span class="hljs-string">'year'</span>, <span class="hljs-string">'inventor_and_country_data'</span>, <span class="hljs-string">'description'</span>, <span class="hljs-string">'full_pdf_file_link'</span>]
    <span class="hljs-keyword">with</span> open(filename, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> data_list:
            f.write(data + <span class="hljs-string">': \n'</span> + dictionary[data] + <span class="hljs-string">'\n'</span>)
    f.close()
    <span class="hljs-keyword">try</span>:
        r = requests.get(dictionary[<span class="hljs-string">'full_pdf_file_link'</span>], stream=<span class="hljs-keyword">True</span>)
        <span class="hljs-keyword">with</span> open(pdfname, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> fd:
            <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> r.iter_content(chunk_size=<span class="hljs-number">1024</span>):
                fd.write(chunk)
        fd.close()
    <span class="hljs-keyword">except</span>:
        print(<span class="hljs-string">'there is something wrong with this patent'</span>)
<span class="hljs-comment">#获取数据</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_patent_data</span><span class="hljs-params">(url, search_string)</span>:</span>
    tmp_s = requests.session()
    <span class="hljs-keyword">try</span>:
        r2 = tmp_s.get(url)
    <span class="hljs-keyword">except</span>:
        print(<span class="hljs-string">'There is something wrong with this patent'</span>)
        <span class="hljs-keyword">return</span>
    text2 = r2.text
    tmp_soup = BeautifulSoup(text2, <span class="hljs-string">"html.parser"</span>)
    patent_data = dict()
    <span class="hljs-comment"># print(text2)</span>
    patent_data[<span class="hljs-string">'patent_code'</span>] = tmp_soup.find(<span class="hljs-string">'title'</span>).next[<span class="hljs-number">22</span>:]
    patent_data[<span class="hljs-string">'patent_name'</span>] = tmp_soup.find(<span class="hljs-string">'font'</span>, size=<span class="hljs-string">"+1"</span>).text[:-<span class="hljs-number">1</span>]
    tmp1 = text2[re.search(<span class="hljs-string">'BUF7='</span>, text2).span()[<span class="hljs-number">1</span>]:]
    patent_data[<span class="hljs-string">'year'</span>] = tmp1[:re.search(<span class="hljs-string">'\n'</span>, tmp1).span()[<span class="hljs-number">0</span>]]
    patent_data[<span class="hljs-string">'inventor_and_country_data'</span>] = tmp_soup.find_all(<span class="hljs-string">'table'</span>, width=<span class="hljs-string">"100%"</span>)[<span class="hljs-number">2</span>].contents[<span class="hljs-number">1</span>].text
    tmp1 = text2[re.search(<span class="hljs-string">'Description'</span>, text2).span()[<span class="hljs-number">1</span>]:]
    tmp2 = tmp1[re.search(<span class="hljs-string">'&lt;HR&gt;'</span>, tmp1).span()[<span class="hljs-number">1</span>]:]
    patent_data[<span class="hljs-string">'description'</span>] = tmp2[re.search(<span class="hljs-string">'&lt;BR&gt;'</span>, tmp2).span()[<span class="hljs-number">0</span>]:(re.search(<span class="hljs-string">'&lt;CENTER&gt;'</span>, tmp2).span()[<span class="hljs-number">0</span>] - <span class="hljs-number">9</span>)]. \
        replace(<span class="hljs-string">'&lt;BR&gt;&lt;BR&gt; '</span>, <span class="hljs-string">''</span>)
    tmp3 = tmp2[:re.search(<span class="hljs-string">'&lt;TABLE&gt;'</span>, tmp2).span()[<span class="hljs-number">0</span>]]
    tmp_soup = BeautifulSoup(tmp3, <span class="hljs-string">"html.parser"</span>)
    pdf_link = tmp_soup.find(<span class="hljs-string">'a'</span>).get(<span class="hljs-string">'href'</span>)
    r3 = tmp_s.get(pdf_link)
    text3 = r3.text
    tmp_soup = BeautifulSoup(text3, <span class="hljs-string">"html.parser"</span>)
    pdf_file_link = tmp_soup.find(<span class="hljs-string">'embed'</span>).get(<span class="hljs-string">'src'</span>)
    patent_data[<span class="hljs-string">'pdf_file_link'</span>] = pdf_file_link
    patent_data[<span class="hljs-string">'full_pdf_file_link'</span>] = pdf_file_link.replace(<span class="hljs-string">'pdfpiw.uspto.gov/'</span>, <span class="hljs-string">'pimg-fpiw.uspto.gov/fdd/'</span>). \
        replace(<span class="hljs-string">'1.pdf'</span>, <span class="hljs-string">'0.pdf'</span>)
    store_data(patent_data, search_string)
<span class="hljs-comment">#主函数</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
    s = requests.session()
    keywords = list()
    <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:
        print(<span class="hljs-string">'what do you want to do?(a: add a key word for searching, q:quit adding words and start)'</span>)
        command = input(<span class="hljs-string">'command:'</span>)
        <span class="hljs-keyword">if</span> command == <span class="hljs-string">'a'</span>:
            word = input(<span class="hljs-string">'keyword: '</span>)
            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> keywords:
                keywords.append(word)
        <span class="hljs-keyword">elif</span> command == <span class="hljs-string">'q'</span>:
            <span class="hljs-keyword">break</span>
        <span class="hljs-keyword">else</span>:
            print(<span class="hljs-string">'please input a valid command'</span>)
    <span class="hljs-keyword">if</span> len(keywords) == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">return</span>
    search_string = <span class="hljs-string">''</span>
    <span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> keywords:
        search_string += keyword
        search_string += <span class="hljs-string">'+'</span>
    search_string = search_string[:-<span class="hljs-number">1</span>]
    main_folder_name = <span class="hljs-string">'./'</span> + search_string
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(main_folder_name):
        os.makedirs(main_folder_name)
    <span class="hljs-comment"># search_url = 'http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-' \</span>
    <span class="hljs-comment">#              'bool.html&amp;r=0&amp;f=S&amp;l=50&amp;TERM1=' + search_string + '&amp;FIELD1=&amp;co1=AND&amp;TERM2=&amp;FIELD2=&amp;d=PTXT'</span>
    search_url =<span class="hljs-string">'http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=0&amp;f=S&amp;l=50&amp;TERM1=vestas&amp;FIELD1=&amp;co1=AND&amp;TERM2=wind&amp;FIELD2=&amp;d=PTXT'</span>
    r = s.get(search_url)
    text = r.text
    print(<span class="hljs-string">'finish collecting html...'</span>)
    soup = BeautifulSoup(text, <span class="hljs-string">"html.parser"</span>)
    number_of_patents = int(soup.find(<span class="hljs-string">'b'</span>).nextSibling[<span class="hljs-number">2</span>:-<span class="hljs-number">10</span>])
    print(<span class="hljs-string">'The total of patents under your key words is: '</span> + str(number_of_patents))
    <span class="hljs-keyword">for</span> number <span class="hljs-keyword">in</span> range(<span class="hljs-number">156</span>, number_of_patents + <span class="hljs-number">1</span>):
        print(<span class="hljs-string">'collecting patent data'</span> + <span class="hljs-string">'('</span> + str(number) + <span class="hljs-string">'/'</span> + str(number_of_patents) + <span class="hljs-string">')'</span>)
        <span class="hljs-comment"># patent_url = 'http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2F' \</span>
        <span class="hljs-comment">#              'search-bool.html&amp;r=' + str(number) + '&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22led+lamp%22&amp;OS=%22led+' \</span>
        <span class="hljs-comment">#                                                    'lamp%22&amp;RS=%22led+lamp%22'</span>
        patent_url = <span class="hljs-string">'http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r='</span> + str(number) + <span class="hljs-string">'&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=vestas&amp;OS=vestas&amp;RS=vestas'</span>
        get_patent_data(patent_url, search_string)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    main()</code></pre> 
<p>主要研究的就是几个跳转链接的格式，自己改一下就能实现不同的自定义搜索方式。 <br> 最后结果： <br> <img src="https://images2.imgbox.com/cb/6a/OkFGR98j_o.png" alt="这里写图片描述" title=""> <br> enjoy it！</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/363498a74ea3871f3771726c76898bd7/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">IEnumerable中的 Any方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/faae6054170f18b461b166a3c4cc3475/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Linux上Redis加入服务（开机自启）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>