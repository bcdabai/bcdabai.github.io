<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>开源模型应用落地-qwen模型小试-入门篇（三） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="开源模型应用落地-qwen模型小试-入门篇（三）" />
<meta property="og:description" content="一、前言
相信您已经学会了如何在Windows环境下以最低成本、无需GPU的情况下运行qwen大模型。现在，让我们进一步探索如何在Linux环境下，并且拥有GPU的情况下运行qwen大模型，以提升性能和效率。
二、术语
2.1. CentOS
CentOS是一种基于Linux的自由开源操作系统。它是从Red Hat Enterprise Linux（RHEL）衍生出来的，因此与RHEL具有高度的兼容性。CentOS的目标是提供一个稳定、可靠且免费的企业级操作系统，适用于服务器和桌面环境。
2.2. GPU
是Graphics Processing Unit（图形处理单元）的缩写。它是一种专门设计用于处理图形和图像计算的处理器。与传统的中央处理器（CPU）相比，GPU具有更高的并行计算能力，适用于处理大规模数据并进行复杂的计算任务。
三、技术实现
3.1. 创建虚拟环境
conda create --name ai python=3.10 3.2. 切换虚拟环境
conda activate ai 3.3. 安装第三方软件包
pip install -r requirements.txt requirements.txt文件：https://github.com/QwenLM/Qwen/blob/main/requirements.txt
具体内容如下：
transformers==4.32.0 accelerate tiktoken einops transformers_stream_generator==0.0.4 scipy 3.4. 代码实现
# -*- coding = utf-8 -*- import traceback from transformers import AutoTokenizer, AutoModelForCausalLM import time modelPath = &#34;/data/model/qwen-1-8b-chat&#34; def chat(model,tokenizer,message,history): position = 0 result = [] try: for response in model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/c6be063c46869d7220ab8cdc96184c7c/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-17T09:30:00+08:00" />
<meta property="article:modified_time" content="2024-01-17T09:30:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">开源模型应用落地-qwen模型小试-入门篇（三）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>一、前言</strong></p> 
<p>    相信您已经学会了如何在Windows环境下以最低成本、无需GPU的情况下运行qwen大模型。现在，让我们进一步探索如何在Linux环境下，并且拥有GPU的情况下运行qwen大模型，以提升性能和效率。</p> 
<hr> 
<p><strong>二、术语</strong></p> 
<p>  <strong>  2.1. CentOS</strong></p> 
<p>        CentOS是一种基于Linux的自由开源操作系统。它是从Red Hat Enterprise Linux（RHEL）衍生出来的，因此与RHEL具有高度的兼容性。CentOS的目标是提供一个稳定、可靠且免费的企业级操作系统，适用于服务器和桌面环境。</p> 
<p>  <strong>  2.2. GPU</strong></p> 
<p>        是Graphics Processing Unit（图形处理单元）的缩写。它是一种专门设计用于处理图形和图像计算的处理器。与传统的中央处理器（CPU）相比，GPU具有更高的并行计算能力，适用于处理大规模数据并进行复杂的计算任务。</p> 
<hr> 
<p><strong>三、技术实现</strong></p> 
<p><strong>3.1. 创建虚拟环境</strong></p> 
<pre><code class="language-python">conda create --name ai python=3.10</code></pre> 
<p><strong>3.2. 切换虚拟环境</strong></p> 
<pre><code class="language-python">conda activate ai</code></pre> 
<p><strong>3.3. 安装第三方软件包</strong></p> 
<pre><code class="language-python">pip install -r requirements.txt</code></pre> 
<p>requirements.txt文件：<a href="https://github.com/QwenLM/Qwen/blob/main/requirements.txt" title="https://github.com/QwenLM/Qwen/blob/main/requirements.txt">https://github.com/QwenLM/Qwen/blob/main/requirements.txt</a></p> 
<p>具体内容如下：</p> 
<pre><code class="language-python">transformers==4.32.0
accelerate
tiktoken
einops
transformers_stream_generator==0.0.4
scipy</code></pre> 
<p><strong>3.4. 代码实现</strong></p> 
<pre><code class="language-python"># -*-  coding = utf-8 -*-
import traceback

from transformers import AutoTokenizer, AutoModelForCausalLM
import time
modelPath = "/data/model/qwen-1-8b-chat"

def chat(model,tokenizer,message,history):
    position = 0
    result = []
    try:
        for response in model.chat_stream(tokenizer, message, history=history):
            result.append(response[position:])

            position = len(response)

            yield "".join(result)
    except Exception:
        traceback.print_exc()

def loadTokenizer():
    tokenizer = AutoTokenizer.from_pretrained(modelPath, trust_remote_code=True)
    return tokenizer

def loadModel():
    model = AutoModelForCausalLM.from_pretrained(modelPath, device_map="auto", trust_remote_code=True, fp16=True).eval()
    return model

if __name__ == '__main__':
    model = loadModel()
    tokenizer = loadTokenizer()
    start_time = time.time()

    message = "我家有什么特产？"
    history = [('hi，你好', '你好！有什么我可以帮助你的吗？'), ('我家在广州，很好玩哦', '广州是一个美丽的城市，有很多有趣的地方可以去。'), ]

    response = chat(model,tokenizer,message,history)
    result = []
    for r in response:
        result.append(r)

    print(result[-1])
    
    end_time = time.time()
    print("执行耗时: {:.2f}秒".format(end_time-start_time))
</code></pre> 
<p><strong>3.4. 代码执行</strong></p> 
<pre><code class="language-python">python -u qwen1_8b_chat_test.py</code></pre> 
<p><strong>3.5. 结果输出</strong></p> 
<p><img alt="" height="71" src="https://images2.imgbox.com/22/9a/8tm5FdFy_o.png" width="1200"></p> 
<p>PS：</p> 
<p>上述代码实现了流式输出，但它在内容完全生成后才进行一次性打印。</p> 
<hr> 
<p><strong>四、附带说明</strong></p> 
<p><strong>4.1. 在Python中，使用<code>-u</code>选项可以启用无缓冲的标准输出，这意味着输出将立即显示在终端上，而不会等待缓冲区填满或遇到换行符。</strong></p> 
<p><strong>4.2.可以安装flash-attention包来提高运行效率以及降低显存占用，但会受到cuda版本等限制导致安装失败。</strong></p> 
<p><strong>4.3.qwen大模型运行在CPU/GPU环境下的配置差异主要体现在：</strong></p> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td style="width:86px;"></td><td style="width:413px;">device_map</td><td style="width:413px;">dtype</td></tr><tr><td style="width:86px;">CPU</td><td style="width:413px;">cpu</td><td style="width:413px;"></td></tr><tr><td style="width:86px;">GPU</td><td style="width:413px;">auto/cuda/cuda:0</td><td style="width:413px;">fp16/bf16/fp32/bf32</td></tr></tbody></table> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6de3a491106759e088f8c44910caf5cb/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">服务器带宽</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bce5b033564c766cf33f3607c3e14e6f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">比亚迪发布璇玑AI大模型；微软推出Copilot Pro；国内首个MoE模型上线</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>