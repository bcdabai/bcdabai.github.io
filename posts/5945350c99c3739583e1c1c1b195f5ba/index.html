<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习--TensorFlow（4）BP神经网络（损失函数、梯度下降、常用激活函数、梯度消失&amp;&amp;梯度爆炸） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习--TensorFlow（4）BP神经网络（损失函数、梯度下降、常用激活函数、梯度消失&amp;&amp;梯度爆炸）" />
<meta property="og:description" content="目录
一、概念与定义
二、损失函数/代价函数（loss）
三、梯度下降法
二维w与loss：
三维w与loss： 四、常用激活函数
1、softmax激活函数 2、sigmoid激活函数
3、tanh函数
4、softsign函数
sigmoid、tanh、softsign函数对比
5、relu函数
对比tanh和relu激活函数
4个隐藏层tanh激活函数
4个隐藏层relu激活函数
2个隐藏层relu激活函数
五、梯度消失与梯度爆炸
1、梯度消失
2、梯度爆炸
3、解决梯度消失与梯度爆炸
一、概念与定义 BP神经网络：是一种按照误差逆向传播算法训练的多层前馈神经网络。 BP 算法的基本思想：学习过程由 信号的正向传播 和 误差的反向传播 两个过程组成。 正向传播：把样本的特征从输入层进行输入，信号经过各个隐藏层逐层处理后，最后从输出层传出。
反向传播：对于网络的实际输出与期望输出之间的误差，把误差信号从最后一层逐层反传，从而获得各个层的误差学习信号，再根据误差学习信号来修正各个层神经元的权值。
周而复始地进行，权值不断调整的过程，就是神经网学习训练的过程。
二、损失函数/代价函数（loss） 损失函数的值越小，说明模型的预测值越接近真实值。
我们可以利用这个函数来优化模型参数。
最常见的损失函数是均方差损失函数（二次损失函数）： 矩阵可以用大写字母来表示，这里的 T 表示真实标签，Y 表示网络输出，i 表示第 i 个数据。N 表示训练样本的个数(注意这里的 N 是一个大于 0 的整数，不是矩阵)。T-Y 可以到每个训练样本与真实标签的误差。误差的值有正有负，我们可以求平方，把所有的误差值都变成正的，然后除以 2N。这里 2 没有特别的含义，主要是我们对均方差代价函数求导的时候，公式中的 2 次方的 2 可以跟分母中的 2 约掉，使得公式推导看起来更加整齐 简洁。除以 N 表示求每个样本误差平均的平均值。 三、梯度下降法 梯度下降法是最常用的方法之一。
既然在变量空间的某一点处，函数沿梯度方向具有最大的变化率，那么在优化代价函数的时候，就可以沿着负梯度方向去减小代价函数的值。
梯度下降法优化公式：
二维w与loss： 首先w移动到了w=3的位置： 当 w 为-3 时，w 所处位置的梯度应该是一个负数，梯度下降法在优化代价函数的时候，是沿着负梯度方向去减小代价函数的值，所以负梯度是一个正数，w 的值应该变大。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5945350c99c3739583e1c1c1b195f5ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-09T16:51:17+08:00" />
<meta property="article:modified_time" content="2021-10-09T16:51:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习--TensorFlow（4）BP神经网络（损失函数、梯度下降、常用激活函数、梯度消失&amp;&amp;梯度爆炸）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%AE%9A%E4%B9%89-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%AE%9A%E4%B9%89" rel="nofollow" title="一、概念与定义">一、概念与定义</a></p> 
<p id="%E4%BA%8C%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88loss%EF%BC%89-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88loss%EF%BC%89" rel="nofollow" title="二、损失函数/代价函数（loss）">二、损失函数/代价函数（loss）</a></p> 
<p id="%E4%B8%89%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" rel="nofollow" title="三、梯度下降法">三、梯度下降法</a></p> 
<p id="%E4%BA%8C%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A-toc" style="margin-left:40px;"><a href="#%E4%BA%8C%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A" rel="nofollow" title="二维w与loss：">二维w与loss：</a></p> 
<p id="%E4%B8%89%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A%C2%A0-toc" style="margin-left:40px;"><a href="#%E4%B8%89%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A%C2%A0" rel="nofollow" title="三维w与loss： ">三维w与loss： </a></p> 
<p id="%E5%9B%9B%E3%80%81%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="四、常用激活函数">四、常用激活函数</a></p> 
<p id="1%E3%80%81softmax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%C2%A0-toc" style="margin-left:40px;"><a href="#1%E3%80%81softmax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%C2%A0" rel="nofollow" title="1、softmax激活函数 ">1、softmax激活函数 </a></p> 
<p id="1%E3%80%81sigmoid%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#1%E3%80%81sigmoid%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="2、sigmoid激活函数">2、sigmoid激活函数</a></p> 
<p id="2%E3%80%81tanh%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#2%E3%80%81tanh%E5%87%BD%E6%95%B0" rel="nofollow" title="3、tanh函数">3、tanh函数</a></p> 
<p id="3%E3%80%81softsign%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#3%E3%80%81softsign%E5%87%BD%E6%95%B0" rel="nofollow" title="4、softsign函数">4、softsign函数</a></p> 
<p id="sigmoid%E3%80%81tanh%E3%80%81softsign%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94-toc" style="margin-left:40px;"><a href="#sigmoid%E3%80%81tanh%E3%80%81softsign%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94" rel="nofollow" title="sigmoid、tanh、softsign函数对比">sigmoid、tanh、softsign函数对比</a></p> 
<p id="4%E3%80%81relu%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#4%E3%80%81relu%E5%87%BD%E6%95%B0" rel="nofollow" title="5、relu函数">5、relu函数</a></p> 
<p id="%E5%AF%B9%E6%AF%94tanh%E5%92%8Crelu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;"><a href="#%E5%AF%B9%E6%AF%94tanh%E5%92%8Crelu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="对比tanh和relu激活函数">对比tanh和relu激活函数</a></p> 
<p id="4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82tanh%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82tanh%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="4个隐藏层tanh激活函数">4个隐藏层tanh激活函数</a></p> 
<p id="4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="4个隐藏层relu激活函数">4个隐藏层relu激活函数</a></p> 
<p id="2%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-toc" style="margin-left:80px;"><a href="#2%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" rel="nofollow" title="2个隐藏层relu激活函数">2个隐藏层relu激活函数</a></p> 
<p id="%E4%BA%94%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-toc" style="margin-left:0px;"><a href="#%E4%BA%94%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8" rel="nofollow" title="五、梯度消失与梯度爆炸">五、梯度消失与梯度爆炸</a></p> 
<p id="1%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1" rel="nofollow" title="1、梯度消失">1、梯度消失</a></p> 
<p id="2%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8" rel="nofollow" title="2、梯度爆炸">2、梯度爆炸</a></p> 
<p id="3%E3%80%81%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8" rel="nofollow" title="3、解决梯度消失与梯度爆炸">3、解决梯度消失与梯度爆炸</a></p> 
<hr id="hr-toc"> 
<p> </p> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%AE%9A%E4%B9%89">一、概念与定义</h2> 
<p><img alt="" height="542" src="https://images2.imgbox.com/b2/f1/1qFiS8P6_o.png" width="607"></p> 
<p></p> 
<blockquote> 
 <div> 
  <strong><span style="color:#000000;">BP神经网络：是一种按照误差逆向传播算法训练的多层前馈神经网络。</span></strong> 
 </div> 
 <div> 
  <span style="color:#000000;"><strong>BP 算法的基本思想：学习过程由</strong></span> 
  <span style="color:#fe2c24;"><strong>信号的正向传播</strong></span> 
  <span style="color:#000000;"><strong>和</strong></span> 
  <span style="color:#fe2c24;"><strong>误差的反向传播</strong></span> 
  <span style="color:#000000;"><strong>两个过程组成</strong>。</span>  
 </div> 
</blockquote> 
<blockquote> 
 <p><strong><span style="color:#000000;">正向传播：把样本的特征从输入层进行输入，信号经过各个隐藏层逐层处理后，最后从</span></strong><strong><span style="color:#000000;">输出层传出。</span></strong></p> 
 <p><strong><span style="color:#000000;">反向传播：对于网络的实际输出与期望输出之间的误差，把误差信号从最后一层逐层反传，</span></strong><strong><span style="color:#000000;">从而获得各个层的误差学习信号，再根据误差学习信号来修正各个层神经元的权值。</span></strong></p> 
</blockquote> 
<p></p> 
<p><span style="color:#fe2c24;"><strong>周而复始地进行，权值不断调整的过程，就是神经网学习训练的过程</strong></span>。</p> 
<p></p> 
<p></p> 
<h2 id="%E4%BA%8C%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88loss%EF%BC%89">二、损失函数/代价函数（loss）</h2> 
<p><strong>损失函数的值越小，说明模型的预测值越接近真实值</strong>。</p> 
<p>我们可以<strong>利用这个函数来优化模型参数</strong>。</p> 
<p></p> 
<blockquote> 
 <p>最常见的损失函数是<strong>均方差损失函数（二次损失函数）：</strong> </p> 
 <p><img alt="" height="95" src="https://images2.imgbox.com/96/9e/g3YUxPX1_o.png" width="442"></p> 
 <div> 
  <span style="color:#000000;">矩阵可以用大写字母来表示，这里的 T 表示真实标签，Y 表示网络输出，i 表示第 i 个数据。N 表示训练样本的个数(注意这里的 N 是一个大于 0 的整数，不是矩阵)。T-Y 可以到每个训练样本与真实标签的误差。误差的值有正有负，我们可以求平方，把所有的误差值都变成正的，然后除以 2N。这里 2 没有特别的含义，主要是我们对均方差代价函数求导的时候，公式中的 2 次方的 2 可以跟分母中的 2 约掉，使得公式推导看起来更加整齐 简洁。除以 N 表示求每个样本误差平均的平均值。</span> 
 </div> 
</blockquote> 
<p></p> 
<p></p> 
<h2 id="%E4%B8%89%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">三、梯度下降法</h2> 
<p><strong>梯度下降法是最常用的方法之一</strong>。</p> 
<p><span style="color:#000000;">既然在变量空间的某一点处，函数<strong>沿梯度方向具有最大的变化率</strong>，那么在优化代价函数的时候，就可以<strong>沿着负梯度方向</strong></span><span style="color:#000000;">去<strong>减小代价函数的值</strong>。</span></p> 
<p><span style="color:#000000;">梯度下降法优化公式：</span><img alt="" height="79" src="https://images2.imgbox.com/e6/28/YdXwK6gp_o.png" width="194"></p> 
<p></p> 
<h3 id="%E4%BA%8C%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A">二维w与loss：</h3> 
<p><img alt="" height="416" src="https://images2.imgbox.com/f7/b0/d8ROZycX_o.png" width="528"></p> 
<p>首先w移动到了w=3的位置： </p> 
<p><img alt="" height="434" src="https://images2.imgbox.com/9b/e9/QzFhF3Ck_o.png" width="552"></p> 
<p><span style="color:#000000;">当 w 为-3 时，w 所处位置的梯度应该是一个负数，梯度下降法在优化代价函数的时候，是沿着负梯度方向去减小代价函数的值，所以负梯度是一个正数，w 的值应该变大。</span></p> 
<p>w再次移动，到w=2的位置： </p> 
<p><img alt="" height="437" src="https://images2.imgbox.com/01/8a/XBnIiD3D_o.png" width="563"></p> 
<p><span style="color:#000000;">当 w 为 2 时，w 所处位置的梯度应该是一个正数，梯度下降法在优化代价函数的时候，是沿着负梯度方向去减小代价函数的值，所以负梯度是一个负数， w 的值应该变小。</span></p> 
<div></div> 
<blockquote> 
 <div> 
  <span style="color:#000000;">        我们可以发现<strong>不管 w 处于那一个位置，当 w 向着负梯度的方向进行移动时，实际上就是向着可以使 loss 值减小的方向进行移动</strong>。</span>只不过 
  <strong>它每一次是移动一步，这个步子的大小会受到学习率和所处位置梯度的大小所影响</strong>。 
 </div> 
</blockquote> 
<p></p> 
<h3 id="%E4%B8%89%E7%BB%B4w%E4%B8%8Eloss%EF%BC%9A%C2%A0">三维w与loss： </h3> 
<p><img alt="" height="503" src="https://images2.imgbox.com/77/c5/Ye9paHee_o.png" width="699"></p> 
<p><span style="color:#000000;">在图中随机选取两个 w1 和 w2 的初始值 p1 和 p2，然后从 p1、p2 这两个初始位置，开始使用梯度下降法优化网络参数：</span></p> 
<div> 
 <img alt="" height="511" src="https://images2.imgbox.com/60/f8/gZAO18ip_o.png" width="702"> 
</div> 
<p></p> 
<p></p> 
<p></p> 
<h2 id="%E5%9B%9B%E3%80%81%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">四、常用激活函数</h2> 
<p>        前面介绍的sign函数（符号函数）、purelin函数（线性函数）都不能很好地解决非线性问题。下面介绍神经网络中常用的非线性激活函数：softmax函数、sigmoid函数、tanh函数、softsign函数、relu函数。这些非线性激活函数有助于帮忙解决非线性问题。</p> 
<p></p> 
<h3 id="1%E3%80%81softmax%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%C2%A0">1、softmax激活函数 </h3> 
<p><strong>将多个神经元的输出，映射到（0,1）区间内</strong>。（可以看成<strong>概率</strong>来理解，从而来进行<strong>多分类</strong>）</p> 
<p>公式：<img alt="" height="72" src="https://images2.imgbox.com/80/39/vIQWzIyQ_o.png" width="118"></p> 
<p><img alt="" height="284" src="https://images2.imgbox.com/7b/fd/wCx560Vq_o.png" width="377"></p> 
<p></p> 
<p><img alt="" height="396" src="https://images2.imgbox.com/4a/2c/m1i36i7s_o.jpg" width="670"></p> 
<p></p> 
<p></p> 
<h3 id="1%E3%80%81sigmoid%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">2、sigmoid激活函数</h3> 
<p><strong>sigmoid函数</strong>也称为<strong>逻辑函数</strong>，函数公式为：<img alt="" height="69" src="https://images2.imgbox.com/de/b7/4toLvDL7_o.png" width="191"></p> 
<p><img alt="" height="462" src="https://images2.imgbox.com/02/da/KC8WgyD6_o.png" width="600"></p> 
<p><span style="color:#000000;">        函数的取值范围是 0-1 之间，当 x 趋向于-∞的时候函数值趋向于 0； 当 x 趋向于+∞的时候函数值趋向于 1。</span></p> 
<p></p> 
<p></p> 
<h3 id="2%E3%80%81tanh%E5%87%BD%E6%95%B0">3、tanh函数</h3> 
<p>tanh函数也称为双曲正切函数。函数公式：<img alt="" height="77" src="https://images2.imgbox.com/bf/9f/jotLLn6L_o.png" width="206"></p> 
<p><img alt="" height="386" src="https://images2.imgbox.com/01/91/Ltr7ZmKU_o.png" width="599"></p> 
<div> 
 <span style="color:#000000;">        函数的取值范围是-1-1 之间，当 x 趋向于-∞的时候函数值趋向于-1； 当 x 趋向于+∞的时候函数值趋向于 1。</span> 
</div> 
<p></p> 
<p></p> 
<h3 id="3%E3%80%81softsign%E5%87%BD%E6%95%B0">4、softsign函数</h3> 
<p>softsign函数公式：<img alt="" height="79" src="https://images2.imgbox.com/1f/17/OPQxxHhE_o.png" width="192"></p> 
<p><img alt="" height="459" src="https://images2.imgbox.com/de/d4/3dh6N38i_o.png" width="602"></p> 
<p>        <span style="color:#000000;">函数的取值范围是-1-1 之间，当 x 趋向于-∞的时候函数值趋向于-1； 当 x 趋向于+∞的时候函数值趋向于 1。</span></p> 
<p></p> 
<h3 id="sigmoid%E3%80%81tanh%E3%80%81softsign%E5%87%BD%E6%95%B0%E5%AF%B9%E6%AF%94">sigmoid、tanh、softsign函数对比</h3> 
<p><img alt="" height="418" src="https://images2.imgbox.com/7b/1c/E7eA1dJE_o.png" width="559"></p> 
<div> 
 <span style="color:#000000;">        这三个激活函数都是 S 形函数，形状相似，只不过 sigmoid 函数取值范围是 0-1 之间，tanh 函数和 softsign 函数取值范围是-1-1 之间。我们还可以观察到 softsign 函数相对于 tanh 函数而言过渡更加平滑，在 x 等于 0 附近函数的数值改变更缓慢。</span> 
</div> 
<p></p> 
<p></p> 
<h3 id="4%E3%80%81relu%E5%87%BD%E6%95%B0">5、relu函数</h3> 
<p>relu函数公式：<img alt="" height="50" src="https://images2.imgbox.com/4c/2e/MhOBhOe1_o.png" width="219"></p> 
<p><img alt="" height="438" src="https://images2.imgbox.com/f3/b4/RaBgmCUa_o.png" width="548"></p> 
<blockquote> 
 <div> 
  <span style="color:#000000;">        当 x 小于 0 时，y 等于 0。当 x 大于 0 时，y 等于 x。ReLU 的中文名称是校正线性单元，虽然在 x 小于 0 时函数是线性的，x 大于 0 时函数也是线性的，但是组合起来之后，函数就具有了非线性的特征。（大于0部分导数也始终为1，</span> 
  <span style="color:#fe2c24;"><strong>relu函数能抗梯度消失与梯度爆炸</strong></span> 
  <span style="color:#000000;">）（在这些激活函数里面，relu激活函数最好）</span> 
 </div> 
</blockquote> 
<p></p> 
<h3 id="%E5%AF%B9%E6%AF%94tanh%E5%92%8Crelu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">对比tanh和relu激活函数</h3> 
<p><img alt="" height="483" src="https://images2.imgbox.com/ec/02/Z07tTV7O_o.png" width="641"></p> 
<p> <img alt="" height="485" src="https://images2.imgbox.com/0a/ef/zkifHWz9_o.png" width="650"></p> 
<p></p> 
<h4 id="4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82tanh%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">4个隐藏层tanh激活函数</h4> 
<p><img alt="" height="444" src="https://images2.imgbox.com/88/aa/kYlQnHSn_o.png" width="555"></p> 
<h4 id="4%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">4个隐藏层relu激活函数</h4> 
<p><img alt="" height="393" src="https://images2.imgbox.com/bb/04/pcNB7F5q_o.png" width="568"></p> 
<h4 id="2%E4%B8%AA%E9%9A%90%E8%97%8F%E5%B1%82relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">2个隐藏层relu激活函数</h4> 
<p><img alt="" height="452" src="https://images2.imgbox.com/4a/00/lfdJmkZr_o.png" width="558"></p> 
<blockquote> 
 <p>可以发现：</p> 
 <div> 
  <span style="color:#000000;">ReLU 激活函数所描绘出来的边界其实是一条一条的直线构成的，不存在曲线。</span> 
 </div> 
 <div> 
  <span style="color:#000000;">模型的拟合效果其实还跟其他一些因素相关，比如说</span> 
  <span style="color:#fe2c24;"><strong>每一层隐藏层的神经元越多，那么模型的拟合能力也就越强</strong></span> 
  <span style="color:#000000;">。</span> 
  <span style="color:#fe2c24;"><strong>模型训练的周期越多，模型的拟合能力就越强</strong></span> 
  <span style="color:#000000;">。</span> 
 </div> 
</blockquote> 
<p></p> 
<p></p> 
<h2 id="%E4%BA%94%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">五、梯度消失与梯度爆炸</h2> 
<h3 id="1%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1">1、梯度消失</h3> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>梯度消失概念：学习信号随着网络传播逐渐减小</strong></span>。</p> 
 <div> 
  <span style="color:#000000;">        学习信号从输出层一层一层向前反向传播的时候，每传播一层学习信号就会变小一点，经过多层传播后，学习信号就会接近于 0，从而使得权值∆w调整接近于 0。</span> 
  <span style="color:#000000;">∆w</span> 
  <span style="color:#000000;">接近于 0 那就意味着该层的参数不会发生改变，不能进行优化。参数不能优化，那整个网络就不能再进行学习了。</span> 
 </div> 
</blockquote> 
<p></p> 
<h3 id="2%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">2、梯度爆炸</h3> 
<blockquote> 
 <p><span style="color:#fe2c24;"><strong>梯度爆炸概念：学习信号随着网络传播逐渐增大</strong></span>。</p> 
 <div> 
  <span style="color:#000000;">        如果学习信号<img alt="\delta" class="mathcode" src="https://images2.imgbox.com/90/cb/F9TxiHEM_o.png">乘以一个大于 1 的数，那么</span> 
  <span style="color:#000000;">δ</span> 
  <span style="color:#000000;">就会变大。学习信号从输出层一层一层向前 </span> 
 </div> 
 <div> 
  <span style="color:#000000;">反向传播的时候，每传播一层学习信号就会变大一点，经过多层传播后，学习信号就会接近于无穷大，从而使得权值∆w</span> 
  <span style="color:#000000;">调整接近于无穷大。</span> 
  <span style="color:#000000;">∆w</span> 
  <span style="color:#000000;">接近于无穷大那就意味着该层的参数，处于一种极不稳定的状态，那么网络就不能正常工作了。</span> 
 </div> 
</blockquote> 
<p></p> 
<h3 id="3%E3%80%81%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8">3、解决梯度消失与梯度爆炸</h3> 
<p>ReLu表达式：f(x)=max(0,x)，当小于0时，f(x)的取值为0；当x&gt;0时，f(x)的取值等于x。</p> 
<p><img alt="" height="456" src="https://images2.imgbox.com/05/e0/ViVahuj7_o.png" width="587"></p> 
<blockquote> 
 <div> 
  <span style="color:#000000;">        当激活函数的导数小于 1 时，网络会产生梯度消失，激活函数的导数大于 1 时，网络会产生梯度爆炸。</span> 
 </div> 
 <div> 
  <div> 
   <span style="color:#000000;">        导数为 1 是一个很好的特性，不会使得学习信号越来越小，也不会让学习信号越来越大，可以让学习信号比较稳定地从后向前传播，解决了梯度消失和梯度下降的问题。</span> 
  </div> 
  <div></div> 
  <div>
    疑问： 
  </div> 
  <div> 
   <div> 
    <span style="color:#000000;">        ReLU 函数看起来是挺好的，既是非线性函数，导数又为 1，但是它好像也存在一些问题，当 x 小于 0 时，ReLU 函数输出为 0，导数也为0，有些信号不就丢失掉了吗？ 如果你是这么想的，那你就想对了，确实是丢失了一些信号，但是没关系。在神经网络中，信号是冗余的，也就是说其实网络最后在做预测的时候并不需要从前面传过来的所有的信号，实际上只需要一部分的信号，网络就可以进行预测。并且使用部分信号来进行预测与使用全部信号来进行预测得到的结果相差不大。</span> 
   </div> 
  </div> 
 </div> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/27bbd630c8a088bf07a829a2ce69c4d0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《庄子》中说到，“一尺之棰，日取其半，万世不竭”。第一天有一根长度为 a的木棍，从第二天开始，每天都要将这根木棍锯掉一半</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e201d820802e38c3b1b56b6240b9a1e2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Jetson nx或其他aarch64 使用anaconda虚拟环境构建GPU版本的Pytorch cuda可用的前提下完整教程</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>