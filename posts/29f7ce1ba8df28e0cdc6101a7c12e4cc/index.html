<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>FileSystem的append方法文件内容追加坑记 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="FileSystem的append方法文件内容追加坑记" />
<meta property="og:description" content="首先声明HDFS并不擅长append操作。 本文以循环追加内容到文件为例，文件大小达到1KB后，重新创建新文件继续写入，写满5个文件后程序停止……，代码如下：
package com.leboop.www; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; /** * Created by leb on 2018/4/16. */ public class HDFSAppend { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); // conf.set(&#34;fs.defaultFS&#34;, &#34;hdfs://192.168.128.11:9000&#34;); FileSystem fs = FileSystem.get(conf); System.out.println(&#34;fs = &#34; &#43; fs); String content = &#34;Hello Word!\r\n&#34;; Path filePath = new Path(&#34;/append/file&#34;); FSDataOutputStream fsDos = null; int fileCount = 0; while (true) { // 循环向文件中写入数据 if (!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/29f7ce1ba8df28e0cdc6101a7c12e4cc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-20T22:51:27+08:00" />
<meta property="article:modified_time" content="2020-12-20T22:51:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">FileSystem的append方法文件内容追加坑记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>首先声明HDFS并不擅长append操作。 </p> 
<p>本文以循环追加内容到文件为例，文件大小达到1KB后，重新创建新文件继续写入，写满5个文件后程序停止……，代码如下：</p> 
<pre><code class="language-java">package com.leboop.www;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

/**
 * Created by leb on 2018/4/16.
 */
public class HDFSAppend {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
//        conf.set("fs.defaultFS", "hdfs://192.168.128.11:9000");
        FileSystem fs = FileSystem.get(conf);
        System.out.println("fs = " + fs);
        String content = "Hello Word!\r\n";
        Path filePath = new Path("/append/file");
        FSDataOutputStream fsDos = null;
        int fileCount = 0;
        while (true) { // 循环向文件中写入数据
            if (!fs.exists(filePath)) {
                fsDos = fs.create(filePath, false);
                fileCount++;
            } else {
                if (fs.getFileStatus(filePath).getLen() &gt; 1024) { // 文件大小超过1KB
                    fs.rename(filePath, new Path("/append/file_" + fileCount));
                    fsDos = fs.create(filePath, false);
                    fileCount++;
                } else {
                    fsDos = fs.append(filePath);
                }
            }
            fsDos.writeBytes(content);
            if (fileCount &gt; 5) {
                break;
            }
        }
    }
}
</code></pre> 
<h4>本地方式</h4> 
<p>注释掉</p> 
<pre><code class="language-java">conf.set("fs.defaultFS", "hdfs://192.168.128.11:9000");</code></pre> 
<p>本地运行结果如下：</p> 
<p><img alt="" height="156" src="https://images2.imgbox.com/0b/bf/7CTXJdY1_o.png" width="656"></p> 
<pre><code class="language-java">fs = org.apache.hadoop.fs.LocalFileSystem@c267ef4
Exception in thread "main" java.io.IOException: Not supported
	at org.apache.hadoop.fs.ChecksumFileSystem.append(ChecksumFileSystem.java:357)
	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1166)
	at com.leboop.www.HDFSAppend.main(HDFSAppend.java:32)

Process finished with exit code 1</code></pre> 
<p>本地不支持FileSystem的append方法。</p> 
<p> </p> 
<h4>集群方式</h4> 
<p>将程序打包，提交到Hadoop集群测试（也可以把代码注释去掉conf.set("fs.defaultFS", "hdfs://192.168.128.11:9000");），如图：</p> 
<p><img alt="" height="301" src="https://images2.imgbox.com/7d/d0/qDlwUC9J_o.png" width="800"></p> 
<p>关键日志如下：</p> 
<p>Exception in thread "main" org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /append/file for DFSClient_NONMAPREDUCE_-<br> 136843200_1 on 192.168.128.11 because DFSClient_NONMAPREDUCE_-136843200_1 is already the current lease holder.</p> 
<p>集群测试FileSytem支持append方法，</p> 
<p> </p> 
<h4>本地方式Not Support和集群AlreadyBeingCreatedException问题解决</h4> 
<p>丢弃append方法，使用BufferedWriter，如下：</p> 
<pre><code class="language-java">package com.leboop.www;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

import java.io.*;

/**
 * Created by leb on 2018/4/16.
 */
public class HDFSAppend {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
//        conf.set("fs.defaultFS", "hdfs://192.168.128.11:9000");
        FileSystem fs = FileSystem.get(conf);
        System.out.println("fs = " + fs);
        String content = "Hello Word!Hello Word!Hello Word!Hello Word!Hello Word!\r\n";
        Path filePath = new Path("/append/file");

        int fileCount = 0;
        if (fs.exists(filePath)) {
            fs.delete(filePath, false);
        }
        OutputStream os = fs.create(filePath, false);
        fileCount++;
        BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(os));
        while (true) { // 循环向文件中写入数据
            long size = fs.getFileStatus(filePath).getLen();
            if (size &gt; 0) {
                System.out.println("size=" + size);
            }
            if (size &gt; 1024) { // 文件大小超过1KB
                bw.flush();
                bw.close(); // 重命名前需要关闭流
                Path newFile = new Path("/append/file_" + fileCount);
                fs.rename(filePath, newFile);
                os = fs.create(filePath, false);
                bw = new BufferedWriter(new OutputStreamWriter(os));
                fileCount++;
            }
            bw.write(content);
            if (fileCount &gt; 5) {
                break;
            }
        }
    }
}
</code></pre> 
<p>程序可以正常执行，如图：</p> 
<p><img alt="" height="168" src="https://images2.imgbox.com/fe/61/sxKXFqHD_o.png" width="628"></p> 
<p>并生成5个文件。但是文件并没有按照1KB分割，与BufferWriter缓存有关。</p> 
<p>将代码重新打包提交Hadoop集群测试，如图：</p> 
<p><img alt="" height="196" src="https://images2.imgbox.com/1d/99/NsN7nzSq_o.png" width="725"><br> 程序运行一段时间后，如图：</p> 
<p><img alt="" height="134" src="https://images2.imgbox.com/09/59/TzAJPLJm_o.png" width="694"></p> 
<p>HDFS上生产一个和数据块大小相同的文件，如图：</p> 
<p><img alt="" height="245" src="https://images2.imgbox.com/ba/84/TvtIa2sO_o.png" width="800"></p> 
<p>无论是本地还是集群，</p> 
<pre>fs.getFileStatus(filePath).getLen()</pre> 
<p>方法都无法获取到文件缓存在内存中的大小，而是达到溢出值写入磁盘后，才可以获取。所以 这种方式并不能按照指定的大小分割文件。</p> 
<p> </p> 
<h4>不能按指定文件大小切分解决</h4> 
<p>（1）方法1</p> 
<p>创建BufferWriter时，指定默认buffersize大小为分割大小</p> 
<p>（2）方法2</p> 
<p>每次调用获取文件大小时，关闭FileSystem，重新创建即可，核心代码如下：</p> 
<pre>fs.close();
fs=FileSystem.get(conf);</pre> 
<p>整体代码如下：</p> 
<pre><code class="language-java">package com.leboop.www;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

/**
 * Created by leb on 2018/4/16.
 */
public class HDFSAppend {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        //conf.set("fs.defaultFS", "hdfs://192.168.128.11:9000");
        FileSystem fs = FileSystem.get(conf);
        System.out.println("fs = " + fs);
        String content = "Hello Word!\r\n";
        Path filePath = new Path("/append/file");
        FSDataOutputStream fsDos = null;
        int fileCount = 0;
        while (true) { // 循环向文件中写入数据
            fs.close();
            fs=FileSystem.get(conf);
            if (!fs.exists(filePath)) {
                fsDos = fs.create(filePath, false);
                fileCount++;
            } else {
                if (fs.getFileStatus(filePath).getLen() &gt; 1024) { // 文件大小超过1KB
                    fs.rename(filePath, new Path("/append/file_" + fileCount));
                    fsDos = fs.create(filePath, false);
                    fileCount++;
                } else {
                    fsDos = fs.append(filePath);
                }
            }
            fsDos.writeBytes(content);
            if (fileCount &gt; 5) {
                break;
            }
        }
    }
}</code></pre> 
<p>原理是写一次，就刷到磁盘，问题是效率极低。效率问题解决：先写入一个文件，使用IOUtils进行copy切分，或者将整个文件作为mr输入（只包含map操作）按分片切分。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/582f49f66aaa042f3596efb477edcc97/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">深度学习入门（二）：神经网络的学习</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d47d2d472e0128c339736a5fcaf4b6ca/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">清理gradle缓存_android  - 如何清除gradle缓存？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>