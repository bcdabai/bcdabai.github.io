<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>R语言实现常用的5种分析方法（主成分&#43;因子&#43;多维标度&#43;判别&#43;聚类 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="R语言实现常用的5种分析方法（主成分&#43;因子&#43;多维标度&#43;判别&#43;聚类" />
<meta property="og:description" content="R语言多元分析系列之一：主成分分析
主成分分析（principal components analysis， PCA）是一种分析、简化数据集的技术。它把原始数据变换到一个新的坐标系统中，使得任何数据投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推。主成分分析经常用减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是在处理观测数目小于变量数目时无法发挥作用，例如基因数据。
R语言中进行主成分分析可以采用基本的princomp函数，将结果输入到summary和plot函数中可分别得到分析结果和碎石图。但psych扩展包更具灵活性。
一 、选择主成分个数
选择主成分个数通常有如下几种评判标准：
根据经验与理论进行选择
根据累积方差贡献率 ，例如选择使累积方差贡献率达到80%的主成分个数。
根据相关系数矩阵的特征值，选择特征值大于1的主成分。
另一种较为先进的方法是平行分析（parallel analysis）。该方法首先生成若干组与原始数据结构相同的随机矩阵，求出其特征值并进行平均，然后和真实数据的特征值进行比对，根据交叉点的位置来选择主成分个数。我们选择USJudgeRatings数据集举例，首先加载psych包，然后使用fa.parallel函数绘制下图，从图中可见第一主成分位于红线上方，第二主成分位于红线下方，因此主成分数目选择1。
二 、提取主成分
从上面的结果观察到，PC1即观测变量与主成分之间的相关系数，h2是变量能被主成分解释的比例，u2则是不能解释的比例。主成分解释了92%的总方差。注意此结果与princomp函数结果不同，princomp函数返回的是主成分的线性组合系数，而principal函数返回原始变量与主成分之间的相关系数，这样就和因子分析的结果意义相一致。
三 、旋转主成分
旋转是在保持累积方差贡献率不变条件下，将主成分负荷进行变换，以方便解释。成分旋转这后各成分的方差贡献率将重新分配，此时就不可再称之为“主成分”而仅仅是“成分”。旋转又可分为正交旋转和斜交旋转。正交旋转的流行方法是方差最大化，需要在principal中增加rotate=&#39;varimax&#39;参数加以实现。也有观点认为主成分分析一般不需要进行旋转。
四、计算主成分得分
主成分得分是各变量的线性组合，在计算出主成分得分之后，还可以将其进行回归等做进一步分析处理。但注意如果输入数据不是原始数据时，则无法计算主成分得分。我们需要在principal中增加score=T的参数设置，结果将存放在结果的score元素中。
R语言多元分析系列之二：探索性因子分析
探索性因子分析（Exploratory Factor Analysis，EFA）是一项用来找出多元观测变量的本质结构、并进行处理降维的技术。 因而EFA能够将具有错综复杂关系的变量综合为少数几个核心因子。EFA和PCA的区别在于：PCA中的主成分是原始变量的线性组合，而EFA中的原始变量是公共因子的线性组合，因子是影响变量的潜在变量，变量中不能被因子所解释的部分称为误差，因子和误差均不能直接观察到。进行EFA需要大量的样本，一般经验认为如何估计因子的数目为N，则需要有5N到10N的样本数目。
虽然EFA和PCA有本质上的区别，但在分析流程上有相似之处。下面我们用ability.cov这个心理测量数据举例，其变量是对人的六种能力，例如阅读和拼写能力进行了测验，其数据是一个协方差矩阵而非原始数据。R语言中stats包中的factanal函数可以完成这项工作，但这里我们使用更为灵活的psych包。
一、选择因子个数
一般选择因子个数可以根据相关系数矩阵的特征值，特征值大于0则可选择做为因子。我们仍使用平行分析法（parallel analysis）。该方法首先生成若干组与原始数据结构相同的随机矩阵，求出其特征值并进行平均，然后和真实数据的特征值进行比对，根据交叉点的位置来选择因子个数。根据下图我们可以观察到特征值与红线的关系，有两个因子都位于红线上方，显然应该选择两个因子。
二、提取因子
psych包中是使用fa函数来提取因子，将nfactors参数设定因子数为2，rotate参数设定了最大化方差的因子旋转方法，最后的fm表示分析方法，由于极大似然方法有时不能收敛，所以此处设为迭代主轴方法。从下面的结果中可以观察到两个因子解释了60%的总方差。Reading和vocabulary这两个变量于第一项因子有关，而picture、blocks和maze变量与第二项因子有关，general变量于两个因子都有关系。
如果采用基本函数factanal进行因子分析，那么函数形式应该是factanal(covmat=correlations，factors=2，rottion=&#39;varimax&#39;)，这会得到相同的结果。此外，我们还可以用图形来表示因子和变量之间的关系
三、因子得分
得到公共因子后，我们可以象主成分分析那样反过来考察每个样本的因子得分。如果输入的是原始数据，则可以在fa函数中设置score=T参数来获得因子得分。如果象上面例子那样输入的是相关矩阵，则需要根据因子得分系数来回归估计。
参考资料：R in Action
R语言多元分析系列之三：多维标度分析
多维标度分析(MDS)是一种将多维空间的研究对象简化到低维空间进行定位、分析和归类，同时又保留对象间原始关系的数据分析方法。
设想一下如果我们在欧氏空间中已知一些点的座标，由此可以求出欧氏距离。那么反过来，已知距离应该也能得到这些点之间的关系。这种距离可以是古典的欧氏距离，也可以是广义上的“距离”。MDS就是在尽量保持这种高维度“距离”的同时，将数据在低维度上展现出来。从这种意义上来讲，主成分分析也是多维标度分析的一个特例。
一、距离的度量
多元分析中常用有以下几种距离，即绝对值距离、欧氏距离(euclidean)、马氏距离(manhattan)、 两项距离(binary)、明氏距离(minkowski)。在R中通常使用disk函数得到样本之间的距离。MDS就是对距离矩阵进行分析，以展现并解释数据的内在结构。
在经典MDS中，距离是数值数据表示，将其看作是欧氏距离。在R中stats包的cmdscale函数实现了经典MDS。它是根据各点的欧氏距离，在低维空间中寻找各点座标，而尽量保持距离不变。
非度量MDS方法中，“距离&#34;不再看作数值数据，而只是顺序数据。例如在心理学实验中，受试者只能回答非常同意、同意、不同意、非常不同意这几种答案。在这种情况下，经典MDS不再有效。Kruskal在1964年提出了一种算法来解决这个问题。在R中MASS包的isoMDS函数可以实现这种算法，另一种流行的算法是由sammon函数实现的。
二、经典MDS
下面我们以HSAUR2包中的watervoles数据来举例。该数据是一个相似矩阵，表示了不同地区水田鼠的相似程度。首先加载数据然后用cmdscales进行分析。
下面计算前两个特征值在所有特征值中的比例，这是为了检测能否用两个维度的距离来表示高维空间中距离，如果达到了0.8左右则表示是合适的。
然后从结果中提取前两个维度的座标，用ggplot包进行绘图。
三、非度量MDS
第二例子中的数据是关于新泽西州议员投票行为的相似矩阵，这里我们用MASS包中的isoMDS函数进行分析
参考资料：
A Handbook of Statistical Analyses Using R
多元统计分析及R语言建模
R语言多元分析系列之四：判别分析
判别分析（discriminant analysis）是一种分类技术。它通过一个已知类别的“训练样本”来建立判别准则，并通过预测变量来为未知类别的数据进行分类。
判别分析的方法大体上有三类，即Fisher判别、Bayes判别和距离判别。Fisher判别思想是投影降维，使多维问题简化为一维问题来处理。选择一个适当的投影轴，使所有的样品点都投影到这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一组内的投影值所形成的组内离差尽可能小，而不同组间的投影值所形成的类间离差尽可能大。Bayes判别思想是根据先验概率求出后验概率，并依据后验概率分布作出统计推断。距离判别思想是根据已知分类的数据计算各类别的重心，对未知分类的数据，计算它与各类重心的距离，与某个重心距离最近则归于该类。
一.线性判别
当不同类样本的协方差矩阵相同时，我们可以在R中使用MASS包的lda函数实现线性判别。lda函数以Bayes判别思想为基础。当分类只有两种且总体服从多元正态分布条件下，Bayes判别与Fisher判别、距离判别是等价的。本例使用iris数据集来对花的品种进行分类。首先载入MASS包，建立判别模型，其中的prior参数表示先验概率。然后利用table函数建立混淆矩阵，比对真实类别和预测类别。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/39a41d269b7a1dbb70fff9019c93f8cc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-02-16T10:01:58+08:00" />
<meta property="article:modified_time" content="2023-02-16T10:01:58+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">R语言实现常用的5种分析方法（主成分&#43;因子&#43;多维标度&#43;判别&#43;聚类</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>R</strong><strong>语言多元分析系列之一：主成分分析</strong></p> 
<p></p> 
<p>主成分分析（principal components analysis， PCA）是一种分析、简化数据集的技术。它把原始数据变换到一个新的坐标系统中，使得任何数据投影的第一大方差在第一个坐标（称为第一主成分）上，第二大方差在第二个坐标（第二主成分）上，依次类推。主成分分析经常用减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是在处理观测数目小于变量数目时无法发挥作用，例如基因数据。</p> 
<p></p> 
<p>R语言中进行主成分分析可以采用基本的princomp函数，将结果输入到summary和plot函数中可分别得到分析结果和碎石图。但psych扩展包更具灵活性。</p> 
<p></p> 
<p><strong>一 、选择主成分个数</strong></p> 
<p></p> 
<p>选择主成分个数通常有如下几种评判标准：</p> 
<p></p> 
<p>根据经验与理论进行选择</p> 
<p></p> 
<p>根据累积方差贡献率 ，例如选择使累积方差贡献率达到80%的主成分个数。</p> 
<p></p> 
<p>根据相关系数矩阵的特征值，选择特征值大于1的主成分。</p> 
<p></p> 
<p>另一种较为先进的方法是平行分析（parallel analysis）。该方法首先生成若干组与原始数据结构相同的随机矩阵，求出其特征值并进行平均，然后和真实数据的特征值进行比对，根据交叉点的位置来选择主成分个数。我们选择USJudgeRatings数据集举例，首先加载psych包，然后使用fa.parallel函数绘制下图，从图中可见第一主成分位于红线上方，第二主成分位于红线下方，因此主成分数目选择1。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/85/c5/2GNswZQ2_o.jpg"></p> 
<p></p> 
<p><strong>二 、提取主成分</strong></p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/bc/1c/bw3HKV6n_o.jpg"></p> 
<p>从上面的结果观察到，PC1即观测变量与主成分之间的相关系数，h2是变量能被主成分解释的比例，u2则是不能解释的比例。主成分解释了92%的总方差。注意此结果与princomp函数结果不同，princomp函数返回的是主成分的线性组合系数，而principal函数返回原始变量与主成分之间的相关系数，这样就和因子分析的结果意义相一致。</p> 
<p></p> 
<p><strong>三 、旋转主成分</strong></p> 
<p></p> 
<p>旋转是在保持累积方差贡献率不变条件下，将主成分负荷进行变换，以方便解释。成分旋转这后各成分的方差贡献率将重新分配，此时就不可再称之为“主成分”而仅仅是“成分”。旋转又可分为正交旋转和斜交旋转。正交旋转的流行方法是方差最大化，需要在principal中增加rotate='varimax'参数加以实现。也有观点认为主成分分析一般不需要进行旋转。</p> 
<p></p> 
<p><strong>四、计算主成分得分</strong></p> 
<p></p> 
<p>主成分得分是各变量的线性组合，在计算出主成分得分之后，还可以将其进行回归等做进一步分析处理。但注意如果输入数据不是原始数据时，则无法计算主成分得分。我们需要在principal中增加score=T的参数设置，结果将存放在结果的score元素中。</p> 
<p></p> 
<p><strong>R</strong><strong>语言多元分析系列之二：探索性因子分析</strong></p> 
<p></p> 
<p>探索性因子分析（Exploratory Factor Analysis，EFA）是一项用来找出多元观测变量的本质结构、并进行处理降维的技术。 因而EFA能够将具有错综复杂关系的变量综合为少数几个核心因子。EFA和PCA的区别在于：PCA中的主成分是原始变量的线性组合，而EFA中的原始变量是公共因子的线性组合，因子是影响变量的潜在变量，变量中不能被因子所解释的部分称为误差，因子和误差均不能直接观察到。进行EFA需要大量的样本，一般经验认为如何估计因子的数目为N，则需要有5N到10N的样本数目。</p> 
<p></p> 
<p>虽然EFA和PCA有本质上的区别，但在分析流程上有相似之处。下面我们用ability.cov这个心理测量数据举例，其变量是对人的六种能力，例如阅读和拼写能力进行了测验，其数据是一个协方差矩阵而非原始数据。R语言中stats包中的factanal函数可以完成这项工作，但这里我们使用更为灵活的psych包。</p> 
<p></p> 
<p><strong>一、选择因子个数</strong></p> 
<p></p> 
<p>一般选择因子个数可以根据相关系数矩阵的特征值，特征值大于0则可选择做为因子。我们仍使用平行分析法（parallel analysis）。该方法首先生成若干组与原始数据结构相同的随机矩阵，求出其特征值并进行平均，然后和真实数据的特征值进行比对，根据交叉点的位置来选择因子个数。根据下图我们可以观察到特征值与红线的关系，有两个因子都位于红线上方，显然应该选择两个因子。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/81/1a/jjpa5WRB_o.jpg"></p> 
<p></p> 
<p><strong>二、提取因子</strong></p> 
<p></p> 
<p>psych包中是使用fa函数来提取因子，将nfactors参数设定因子数为2，rotate参数设定了最大化方差的因子旋转方法，最后的fm表示分析方法，由于极大似然方法有时不能收敛，所以此处设为迭代主轴方法。从下面的结果中可以观察到两个因子解释了60%的总方差。Reading和vocabulary这两个变量于第一项因子有关，而picture、blocks和maze变量与第二项因子有关，general变量于两个因子都有关系。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f0/75/jp4Blno4_o.jpg"></p> 
<p>如果采用基本函数factanal进行因子分析，那么函数形式应该是factanal(covmat=correlations，factors=2，rottion='varimax')，这会得到相同的结果。此外，我们还可以用图形来表示因子和变量之间的关系</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f1/c5/m3qTNDi1_o.jpg"></p> 
<p><strong>三、因子得分</strong></p> 
<p></p> 
<p>得到公共因子后，我们可以象主成分分析那样反过来考察每个样本的因子得分。如果输入的是原始数据，则可以在fa函数中设置score=T参数来获得因子得分。如果象上面例子那样输入的是相关矩阵，则需要根据因子得分系数来回归估计。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d6/a6/PbbwTKb9_o.jpg"></p> 
<p><strong>参考资料</strong>：R in Action</p> 
<p></p> 
<p><strong>R</strong><strong>语言多元分析系列之三：多维标度分析</strong></p> 
<p></p> 
<p></p> 
<p>多维标度分析(MDS)是一种将多维空间的研究对象简化到低维空间进行定位、分析和归类，同时又保留对象间原始关系的数据分析方法。</p> 
<p></p> 
<p>设想一下如果我们在欧氏空间中已知一些点的座标，由此可以求出欧氏距离。那么反过来，已知距离应该也能得到这些点之间的关系。这种距离可以是古典的欧氏距离，也可以是广义上的“距离”。MDS就是在尽量保持这种高维度“距离”的同时，将数据在低维度上展现出来。从这种意义上来讲，主成分分析也是多维标度分析的一个特例。</p> 
<p></p> 
<p><strong>一、距离的度量</strong></p> 
<p></p> 
<p>多元分析中常用有以下几种距离，即绝对值距离、欧氏距离(euclidean)、马氏距离(manhattan)、 两项距离(binary)、明氏距离(minkowski)。在R中通常使用disk函数得到样本之间的距离。MDS就是对距离矩阵进行分析，以展现并解释数据的内在结构。</p> 
<p></p> 
<p>在经典MDS中，距离是数值数据表示，将其看作是欧氏距离。在R中stats包的cmdscale函数实现了经典MDS。它是根据各点的欧氏距离，在低维空间中寻找各点座标，而尽量保持距离不变。</p> 
<p></p> 
<p>非度量MDS方法中，“距离"不再看作数值数据，而只是顺序数据。例如在心理学实验中，受试者只能回答非常同意、同意、不同意、非常不同意这几种答案。在这种情况下，经典MDS不再有效。Kruskal在1964年提出了一种算法来解决这个问题。在R中MASS包的isoMDS函数可以实现这种算法，另一种流行的算法是由sammon函数实现的。</p> 
<p></p> 
<p><strong>二、经典MDS</strong></p> 
<p></p> 
<p>下面我们以HSAUR2包中的watervoles数据来举例。该数据是一个相似矩阵，表示了不同地区水田鼠的相似程度。首先加载数据然后用cmdscales进行分析。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3e/4a/zITEvJJ0_o.jpg"></p> 
<p>下面计算前两个特征值在所有特征值中的比例，这是为了检测能否用两个维度的距离来表示高维空间中距离，如果达到了0.8左右则表示是合适的。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/04/f1/Z7WOfnD6_o.jpg"></p> 
<p>然后从结果中提取前两个维度的座标，用ggplot包进行绘图。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/9c/c7/S2X6zBIS_o.jpg"></p> 
<p></p> 
<p><strong>三、非度量MDS</strong></p> 
<p></p> 
<p>第二例子中的数据是关于新泽西州议员投票行为的相似矩阵，这里我们用MASS包中的isoMDS函数进行分析</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/72/4e/Tmx0Z6GP_o.jpg"></p> 
<p><strong>参考资料：</strong></p> 
<p></p> 
<p>A Handbook of Statistical Analyses Using R</p> 
<p></p> 
<p>多元统计分析及R语言建模</p> 
<p></p> 
<p><strong>R</strong><strong>语言多元分析系列之四：判别分析</strong></p> 
<p></p> 
<p>判别分析（discriminant analysis）是一种分类技术。它通过一个已知类别的“训练样本”来建立判别准则，并通过预测变量来为未知类别的数据进行分类。</p> 
<p></p> 
<p>判别分析的方法大体上有三类，即Fisher判别、Bayes判别和距离判别。Fisher判别思想是投影降维，使多维问题简化为一维问题来处理。选择一个适当的投影轴，使所有的样品点都投影到这个轴上得到一个投影值。对这个投影轴的方向的要求是：使每一组内的投影值所形成的组内离差尽可能小，而不同组间的投影值所形成的类间离差尽可能大。Bayes判别思想是根据先验概率求出后验概率，并依据后验概率分布作出统计推断。距离判别思想是根据已知分类的数据计算各类别的重心，对未知分类的数据，计算它与各类重心的距离，与某个重心距离最近则归于该类。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/52/92/Eavtevmh_o.jpg"></p> 
<p></p> 
<p><strong>一.线性判别</strong></p> 
<p></p> 
<p>当不同类样本的协方差矩阵相同时，我们可以在R中使用MASS包的lda函数实现线性判别。lda函数以Bayes判别思想为基础。当分类只有两种且总体服从多元正态分布条件下，Bayes判别与Fisher判别、距离判别是等价的。本例使用iris数据集来对花的品种进行分类。首先载入MASS包，建立判别模型，其中的prior参数表示先验概率。然后利用table函数建立混淆矩阵，比对真实类别和预测类别。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/0e/5e/d9QIFZK2_o.jpg"></p> 
<p></p> 
<p><strong>二.二次判别</strong></p> 
<p></p> 
<p>当不同类样本的协方差矩阵不同时，则应该使用二次判别。</p> 
<p></p> 
<p>model2=qda(Species~.，data=iris，cv=T)</p> 
<p></p> 
<p>这里将CV参数设置为T，是使用留一交叉检验（leave-one-out cross-validation），并自动生成预测值。这种条件下生成的混淆矩阵较为可靠。此外还可以使用predict(model)$posterior提取后验概率。</p> 
<p></p> 
<p>在使用lda和qda函数时注意：其假设是总体服从多元正态分布，若不满足的话则谨慎使用。</p> 
<p></p> 
<p><strong>参考资料：</strong></p> 
<p></p> 
<p>Modern Applied Statistics With S</p> 
<p></p> 
<p>Data_Analysis_and_Graphics_Using_R__An_Example_Based_Approach</p> 
<p></p> 
<p><strong>R</strong><strong>语言多元分析系列之五：聚类分析</strong></p> 
<p></p> 
<p>聚类分析（Cluster Analysis）是根据“物以类聚”的道理，对样品或指标进行分类的一种多元统计分析方法，它是在没有先验知识的情况下，对样本按各自的特性来进行合理的分类。</p> 
<p></p> 
<p>聚类分析被应用于很多方面，在商业上，聚类分析被用来发现不同的客户群，并且通过购买模式刻画不同的客户群的特征；在生物上，聚类分析被用来动植物分类和对基因进行分类，获取对种群固有结构的认识；在因特网应用上，聚类分析被用来在网上进行文档归类来修复信息。</p> 
<p></p> 
<p>聚类分析有两种主要计算方法，分别是凝聚层次聚类（Agglomerative hierarchical method）和K均值聚类（K-Means）。</p> 
<p></p> 
<p><strong>一、层次聚类</strong></p> 
<p></p> 
<p>层次聚类又称为系统聚类，首先要定义样本之间的距离关系，距离较近的归为一类，较远的则属于不同的类。可用于定义“距离”的统计量包括了欧氏距离(euclidean)、马氏距离(manhattan)、 两项距离(binary)、明氏距离(minkowski)。还包括相关系数和夹角余弦。</p> 
<p></p> 
<p>层次聚类首先将每个样本单独作为一类，然后将不同类之间距离最近的进行合并，合并后重新计算类间距离。这个过程一直持续到将所有样本归为一类为止。在计算类间距离时则有六种不同的方法，分别是最短距离法、最长距离法、类平均法、重心法、中间距离法、离差平方和法。</p> 
<p></p> 
<p>下面我们用iris数据集来进行聚类分析，在R语言中所用到的函数为hclust。首先提取iris数据中的4个数值变量，然后计算其欧氏距离矩阵。然后将矩阵绘制热图，从图中可以看到颜色越深表示样本间距离越近，大致上可以区分出三到四个区块，其样本之间比较接近。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a9/81/KaL6oXT3_o.jpg"></p> 
<p>然后使用hclust函数建立聚类模型，结果存在model1变量中，其中ward参数是将类间距离计算方法设置为离差平方和法。使用plot(model1)可以绘制出聚类树图。如果我们希望将类别设为3类，可以使用cutree函数提取每个样本所属的类别。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/be/3f/4fLEo9ll_o.jpg"></p> 
<p>为了显示聚类的效果，我们可以结合多维标度和聚类的结果。先将数据用MDS进行降维，然后以不同的的形状表示原本的分类，用不同的颜色来表示聚类的结果。可以看到setose品种聚类很成功，但有一些virginica品种的花被错误和virginica品种聚类到一起。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f3/da/t1IapN7j_o.jpg"></p> 
<p><strong>二、K均值聚类</strong></p> 
<p></p> 
<p>K均值聚类又称为动态聚类，它的计算方法较为简单，也不需要输入距离矩阵。首先要指定聚类的分类个数N，随机取N个样本作为初始类的中心，计算各样本与类中心的距离并进行归类，所有样本划分完成后重新计算类中心，重复这个过程直到类中心不再变化。</p> 
<p></p> 
<p>在R中使用kmeans函数进行K均值聚类，centers参数用来设置分类个数，nstart参数用来设置取随机初始中心的次数，其默认值为1，但取较多的次数可以改善聚类效果。model2$cluster可以用来提取每个样本所属的类别。</p> 
<p></p> 
<p>model2=kmeans(data，centers=3，nstart=10)</p> 
<p></p> 
<p>使用K均值聚类时需要注意，只有在类的平均值被定义的情况下才能使用，还要求事先给出分类个数。一种方法是先用层次聚类以决定个数，再用K均值聚类加以改进。或者以轮廓系数来判断分类个数。改善聚类的方法还包括对原始数据进行变换，如对数据进行降维后再实施聚类。</p> 
<p></p> 
<p>cluster扩展包中也有许多函数可用于聚类分析，如agnes函数可用于凝聚层次聚类，diana可用于划分层次聚类，pam可用于K均值聚类，fanny用于模糊聚类。</p> 
<p></p> 
<p><a href="https://mp.weixin.qq.com/s/qDVnBkfhGoo_1P4Ojtqx5A" rel="nofollow" title="树谷资料库资源大全（2月9日更新）">树谷资料库资源大全（2月9日更新）</a></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/79538144e42ea0986fdf719e4fd74ebc/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Redis实战—黑马点评（二）缓存篇</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7892f19bc762341a0688fee82f2cae9e/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">网页中 文件上传 input 标签 type=“file“ 设置 中间按钮 button的样式</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>