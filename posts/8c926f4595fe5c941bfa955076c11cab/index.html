<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Stable Diffusionæ¶æ„çš„3Dåˆ†å­ç”Ÿæˆæ¨¡å‹ GeoLDM - æµ‹è¯„ä¸ä»£ç è§£æ - ç¼–ç¨‹å¤§ç™½çš„åšå®¢</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Stable Diffusionæ¶æ„çš„3Dåˆ†å­ç”Ÿæˆæ¨¡å‹ GeoLDM - æµ‹è¯„ä¸ä»£ç è§£æ" />
<meta property="og:description" content="ä¹‹å‰ï¼Œå‘å¤§å®¶ä»‹ç»è¿‡3Dåˆ†å­ç”Ÿæˆæ¨¡å‹Â GeoLDMã€‚
GeoLDMæŒ‰ç…§Stable Diffusionæ¶æ„ï¼Œå°†3Dåˆ†å­ç”Ÿæˆçš„æ‰©æ•£è¿‡ç¨‹è¿è¡Œåœ¨éšç©ºé—´å†…ï¼Œä¼˜åŒ–äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å­ç”Ÿæˆã€‚å¯èƒ½æ˜¯æ‰“å¼€Drug-AIGCçš„å…³é”®ä¹‹ä½œã€‚è®©ç²¾ç¡®æ§åˆ¶åˆ†å­ç”Ÿæˆæœ‰äº†å¸Œæœ›ã€‚
è¯¦è§ï¼šåˆ†å­ç”Ÿæˆé¢†åŸŸçš„stable diffusion - GEOLDM-CSDNåšå®¢ï¼‰
ä½œè€…æä¾›äº†GitHubä»£ç ï¼šhttps://github.com/MinkaiXu/GeoLDMã€‚
å› æ­¤ï¼Œæˆ‘ç‰¹æ„æµ‹è¯•äº†ä¸€ä¸‹ä»£ç è´¨é‡ã€‚
ä¸€ã€ä»£ç æµ‹è¯• é¦–å…ˆ git clone é¡¹ç›®ä»£ç ï¼š
git clone https://github.com/MinkaiXu/GeoLDM.git é¡¹ç›®ç›®å½•ä¸ºï¼š
. â”œâ”€â”€ LICENSE â”œâ”€â”€ README.md â”œâ”€â”€ build_geom_dataset.py â”œâ”€â”€ configs â”œâ”€â”€ data â”œâ”€â”€ egnn â”œâ”€â”€ equivariant_diffusion â”œâ”€â”€ eval_analyze.py â”œâ”€â”€ eval_conditional_qm9.py â”œâ”€â”€ eval_sample.py â”œâ”€â”€ main_geom_drugs.py â”œâ”€â”€ main_qm9.py â”œâ”€â”€ qm9 â”œâ”€â”€ requirements.txt â”œâ”€â”€ train_test.py â””â”€â”€ utils.py 6 directories, 11 files å…¶ä¸­ï¼Œqm9æ–‡ä»¶å¤¹åŒ…å«äº†qm9æ•°æ®é›†é¢„å¤„ç†åˆ°dataloaderçš„æ–¹æ³•ï¼Œqm9æ•°æ®é›†ä¼šè‡ªåŠ¨ä¸‹è½½train, valid, teståˆ†å‰²å¥½çš„æ•°æ®é›†;drugæ•°æ®åˆ™éœ€è¦è‡ªå·±ä¸‹è½½ï¼Œç„¶åæ‰§è¡Œbuild_geom_dataset.pyï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚
1.1 ç¯å¢ƒå®‰è£… å®‰è£…torchåŠå…¶ç»„ä»¶ï¼Œ rdkit, numpy,tqdmç­‰ï¼ˆmacOSç³»ç»Ÿï¼‰ï¼š
conda install pytorch::pytorch torchvision torchaudio -c pytorch conda install -c conda-forge rdkit conda install numpy pandas scipy tqdm conda install imageio pip install msgpack # æ—¶åºæ•°æ®åº“ å®‰è£…è¿‡ç¨‹æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰é‡åˆ°ä»»ä½•é—®é¢˜ã€‚ 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8c926f4595fe5c941bfa955076c11cab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-06T11:03:38+08:00" />
<meta property="article:modified_time" content="2024-01-06T11:03:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="ç¼–ç¨‹å¤§ç™½çš„åšå®¢" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">ç¼–ç¨‹å¤§ç™½çš„åšå®¢</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Stable Diffusionæ¶æ„çš„3Dåˆ†å­ç”Ÿæˆæ¨¡å‹ GeoLDM - æµ‹è¯„ä¸ä»£ç è§£æ</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>ä¹‹å‰ï¼Œå‘å¤§å®¶ä»‹ç»è¿‡3Dåˆ†å­ç”Ÿæˆæ¨¡å‹Â GeoLDMã€‚</p> 
<p class="img-center"><img alt="" height="148" src="https://images2.imgbox.com/dc/8d/rhEY9bz4_o.png" width="612"></p> 
<p class="img-center"><img alt="" height="163" src="https://images2.imgbox.com/d9/34/wrnTFfN1_o.png" width="612"></p> 
<p>GeoLDMæŒ‰ç…§Stable Diffusionæ¶æ„ï¼Œå°†3Dåˆ†å­ç”Ÿæˆçš„æ‰©æ•£è¿‡ç¨‹è¿è¡Œåœ¨éšç©ºé—´å†…ï¼Œä¼˜åŒ–äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å­ç”Ÿæˆã€‚å¯èƒ½æ˜¯æ‰“å¼€Drug-AIGCçš„å…³é”®ä¹‹ä½œã€‚è®©ç²¾ç¡®æ§åˆ¶åˆ†å­ç”Ÿæˆæœ‰äº†å¸Œæœ›ã€‚</p> 
<p>è¯¦è§ï¼š<a href="https://blog.csdn.net/wufeil7/article/details/134891122?spm=1001.2014.3001.5502" title="åˆ†å­ç”Ÿæˆé¢†åŸŸçš„stable diffusion - GEOLDM-CSDNåšå®¢">åˆ†å­ç”Ÿæˆé¢†åŸŸçš„stable diffusion - GEOLDM-CSDNåšå®¢</a>ï¼‰</p> 
<p>ä½œè€…æä¾›äº†GitHubä»£ç ï¼š<a href="https://github.com/MinkaiXu/GeoLDM" title="https://github.com/MinkaiXu/GeoLDM">https://github.com/MinkaiXu/GeoLDM</a>ã€‚</p> 
<p>å› æ­¤ï¼Œæˆ‘ç‰¹æ„æµ‹è¯•äº†ä¸€ä¸‹ä»£ç è´¨é‡ã€‚</p> 
<p></p> 
<h2 style="background-color:transparent;">ä¸€ã€ä»£ç æµ‹è¯•</h2> 
<p></p> 
<p>é¦–å…ˆ git clone é¡¹ç›®ä»£ç ï¼š</p> 
<pre><code>git clone https://github.com/MinkaiXu/GeoLDM.git</code></pre> 
<p></p> 
<p>é¡¹ç›®ç›®å½•ä¸ºï¼š</p> 
<pre><code>.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ build_geom_dataset.py
â”œâ”€â”€ configs
â”œâ”€â”€ data
â”œâ”€â”€ egnn
â”œâ”€â”€ equivariant_diffusion
â”œâ”€â”€ eval_analyze.py
â”œâ”€â”€ eval_conditional_qm9.py
â”œâ”€â”€ eval_sample.py
â”œâ”€â”€ main_geom_drugs.py
â”œâ”€â”€ main_qm9.py
â”œâ”€â”€ qm9
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train_test.py
â””â”€â”€ utils.py


6 directories, 11 files</code></pre> 
<p></p> 
<p>å…¶ä¸­ï¼Œqm9æ–‡ä»¶å¤¹åŒ…å«äº†qm9æ•°æ®é›†é¢„å¤„ç†åˆ°dataloaderçš„æ–¹æ³•ï¼Œqm9æ•°æ®é›†ä¼šè‡ªåŠ¨ä¸‹è½½train, valid, teståˆ†å‰²å¥½çš„æ•°æ®é›†;drugæ•°æ®åˆ™éœ€è¦è‡ªå·±ä¸‹è½½ï¼Œç„¶åæ‰§è¡Œbuild_geom_dataset.pyï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚</p> 
<h5></h5> 
<h3>1.1 ç¯å¢ƒå®‰è£…</h3> 
<p></p> 
<p>å®‰è£…torchåŠå…¶ç»„ä»¶ï¼Œ rdkit, numpy,tqdmç­‰ï¼ˆmacOSç³»ç»Ÿï¼‰ï¼š</p> 
<pre><code>conda install pytorch::pytorch torchvision torchaudio -c pytorch
conda install -c conda-forge rdkit  
conda install numpy pandas scipy tqdm
conda install imageio

pip install msgpack # æ—¶åºæ•°æ®åº“</code></pre> 
<pre>å®‰è£…è¿‡ç¨‹æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰é‡åˆ°ä»»ä½•é—®é¢˜ã€‚
</pre> 
<p></p> 
<h3>1.2.1 ä¸‹è½½æ•°æ®é›†åŠé¢„å¤„ç†</h3> 
<p></p> 
<p>åœ¨git cloneä¸‹æ¥çš„ä»£ç ä¸­ï¼Œå¹¶æ²¡æœ‰åŒ…å«æ•°æ®é›†ï¼Œæ— æ³•ç›´æ¥è¿›è¡Œæ¨¡å‹çš„é¢„æµ‹æˆ–è€…è®­ç»ƒã€‚</p> 
<p></p> 
<p>qm9æ•°æ®çš„ä¸‹è½½é“¾æ¥è¦å‚è€ƒä¹‹å‰edmæ–‡ç« ã€‚</p> 
<p></p> 
<p>drugsæ•°æ®é›†çš„æ•°æ®ä¸‹è½½é“¾æ¥ï¼š<a href="https://dataverse.harvard.edu/file.xhtml?fileId=4360331&amp;version=2.0" rel="nofollow" title="https://dataverse.harvard.edu/file.xhtml?fileId=4360331&amp;version=2.0">https://dataverse.harvard.edu/file.xhtml?fileId=4360331&amp;version=2.0</a></p> 
<p class="img-center"><img alt="" height="356" src="https://images2.imgbox.com/62/34/zQmWeKgy_o.png" width="612"></p> 
<p></p> 
<p>ä¸‹è½½æ–‡ä»¶å¾ˆå¤§ï¼Œå‹ç¼©æ–‡ä»¶39.8Gã€‚</p> 
<p></p> 
<p>æ•°æ®é›†æ–‡ä»¶ï¼Œè¦ä¸‹è½½åˆ°/data/geomç›®å½•ã€‚</p> 
<pre><code>cd ./data/geom
# -t 0 -c æ–­ç‚¹æ¥ç»­ä¸‹è½½
wget -t 0 -c https://dataverse.harvard.edu/api/access/datafile/4360331</code></pre> 
<p>è¦ä¸‹è½½è¿™ä¸ªæ•°æ®é›†ï¼Œè¯·åŠ¡å¿…ç§‘å­¦ä¸Šç½‘ï¼Œåœ¨6MB/sçš„é€Ÿåº¦ä¸‹ï¼Œæˆ‘ä¸‹è½½äº†2hã€‚</p> 
<p></p> 
<p>ä¸‹è½½å®Œæˆåï¼Œè§£å‹ï¼š</p> 
<pre><code>tar -xzvf 4360331</code></pre> 
<p></p> 
<h3>1.2.2 é¢„è®­ç»ƒcheckpoint</h3> 
<p></p> 
<p>ä½œè€…æä¾›äº†è®­ç»ƒå¥½çš„æ¨¡å‹checkpiontï¼ŒåŒ…æ‹¬QM9å’ŒDrugæ•°æ®é›†çš„ä¸¤ä¸ªcheckpointï¼Œä¸‹è½½é“¾æ¥ä¸ºï¼š</p> 
<p><a href="https://drive.google.com/drive/folders/1EQ9koVx-GA98kaKBS8MZ_jJ8g4YhdKsL" rel="nofollow" title="https://drive.google.com/drive/folders/1EQ9koVx-GA98kaKBS8MZ_jJ8g4YhdKsL">https://drive.google.com/drive/folders/1EQ9koVx-GA98kaKBS8MZ_jJ8g4YhdKsL</a></p> 
<p></p> 
<p class="img-center"><img alt="" height="82" src="https://images2.imgbox.com/bb/d6/WLmbFGZH_o.png" width="612"></p> 
<p></p> 
<p>å…¶ä¸­ï¼Œ Drugæ•°æ®é›†æ˜¯Â GEOM-DRUG (Geometric Ensemble Of Molecules) dataseï¼Œç”±æ›´å¤§çš„æœ‰æœºåŒ–åˆç‰©ç»„æˆï¼Œæœ€å¤šæœ‰ 181 ä¸ªåŸå­ï¼Œå¹³å‡æœ‰ 44.2 ä¸ªåŸå­ï¼Œæœ‰ 5 ç§ä¸åŒçš„åŸå­ç±»å‹ã€‚ å®ƒæ¶µç›–äº†å¤§çº¦ 450,000 ä¸ªåˆ†å­çš„ 3700 ä¸‡ä¸ªåˆ†å­æ„è±¡ï¼Œå¹¶æ ‡æœ‰èƒ½é‡å’Œç»Ÿè®¡åˆ†å­è´¨é‡ã€‚Â </p> 
<p></p> 
<p>è¿™ä¸¤ä¸ªcheckpointçš„è®­ç»ƒè¶…å‚æ•°ä¸ä½œè€…æä¾›çš„è®­ç»ƒæ¨¡å‹çš„è¶…å‚æ•°å®Œå…¨ä¸€è‡´ï¼Œé™¤äº†--latent_nfå‚æ•°å€¼ä¸º2ï¼Œä½†æ˜¯ç»“æœåº”è¯¥ä¸å€¼ä¸º1ç›¸è¿‘ï¼Œæ²¡æœ‰å·®åˆ«ã€‚</p> 
<p></p> 
<p>æ³¨æ„ï¼šä¸‹è½½å®Œæˆçš„checkpointè¦æ”¾ç½®åœ¨./outputs/pretrainedæ–‡ä»¶å¤¹å†…ï¼Œè¯¥æ–‡ä»¶å¤¹æ˜¯æ–°å»ºçš„ã€‚å½“ä½¿ç”¨æ¨¡å‹æ—¶ï¼Œåº”è¯¥å°†--model_pathå‚æ•°è®¾ç½®ä¸ºï¼š./outputs/pretrainedã€‚</p> 
<h5></h5> 
<h3>1.3 ä½¿ç”¨checkpointæ¨¡å‹æµ‹è¯•</h3> 
<p></p> 
<p>æˆ‘ä»¬å°è¯•ç›´æ¥ä½¿ç”¨ä½œè€…æä¾›çš„checkpointè¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ã€‚</p> 
<p></p> 
<h4 style="background-color:transparent;">1.3.1 è¯„ä¼°æ¨¡å‹ç”Ÿæˆåˆ†å­çš„æ­£ç¡®ç‡ä¸æ–°é¢–æ€§</h4> 
<pre><code>python eval_analyze.py \
  --model_path outputs/pretrained \
  --n_samples 10</code></pre> 
<p>ä½†æ˜¯ä¼šæŠ¥é”™ï¼š</p> 
<p><em>FileNotFoundError: [Errno 2] No such file or directory: 'outputs/pretrained/args.pickle'</em></p> 
<p></p> 
<p>è¿™ä¸ªæ˜¯å› ä¸ºä¸‹è½½çš„æ¨¡å‹checkpiontæ–‡ä»¶æ˜¯å‹ç¼©taræ–‡ä»¶ï¼Œéœ€è¦è§£å‹ï¼š</p> 
<pre><code>tar -zxvf drugs_latent2.tar 
tar -zxvf qm9_latent2.tar </code></pre> 
<p></p> 
<p>ç„¶åå†æ¬¡æ‰§è¡Œï¼Œéœ€è¦æŒ‡å®šè¯„ä¼°çš„æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œè¿™é‡ŒæŒ‡æ˜æ˜¯drugs_latent2ï¼š</p> 
<pre><code> python eval_analyze.py \
  --model_path outputs/pretrained/drugs_latent2 \
  --n_samples 10</code></pre> 
<p>ä¼šè¾“å‡ºæŠ¥é”™ï¼š</p> 
<p>./data/geom/geom_drugs_30.npy</p> 
<p>æŸ¥çœ‹äº†ä¸€ä¸‹ï¼Œç¡®å®æ²¡æœ‰è¿™ä¸ªç›®å½•ã€‚è¿™ä¸ªå¯èƒ½æ˜¯Drugæ•°æ®é›†æ•°æ®å¤„ç†ä»¥åæ‰èƒ½æœ‰çš„ã€‚</p> 
<p></p> 
<p>åœ¨æ‰§è¡Œå®Œ2.2.1ä¸‹è½½æ•°æ®é›†åŠé¢„å¤„ç†éƒ¨åˆ†drugsæ•°æ®é›†çš„é¢„å¤„ç†ä»¥åï¼Œå³å¯è¿è¡Œã€‚åœ¨ç­‰å¾…æ•°æ®åˆ’åˆ†å’ŒåŠ è½½çš„2åˆ†é’Ÿå·¦å³æ—¶é—´ä»¥åï¼Œè¾“å‡ºä¸ºï¼š</p> 
<pre><code>Namespace(exp_name='rld_fixsig_enc1_latent2_geom_drugs', train_diffusion=True, ae_path=None, trainable_ae=True, latent_nf=2, kl_weight=0.01, model='egnn_dynamics', probabilistic_model='diffusion', diffusion_steps=1000, diffusion_noise_schedule='polynomial_2', diffusion_loss_type='l2', diffusion_noise_precision=1e-05, n_epochs=3000, batch_size=32, lr=0.0001, break_train_epoch=False, dp=True, condition_time=True, clip_grad=True, trace='hutch', n_layers=4, inv_sublayers=1, nf=256, tanh=True, attention=True, norm_constant=1, sin_embedding=False, ode_regularization=0.001, dataset='geom', filter_n_atoms=None, dequantization='argmax_variational', n_report_steps=50, wandb_usr=None, no_wandb=False, online=True, no_cuda=False, save_model=True, generate_epochs=1, num_workers=0, test_epochs=1, data_augmentation=False, conditioning=[], resume=None, start_epoch=0, ema_decay=0.9999, augment_noise=0, n_stability_samples=500, normalize_factors=[1, 4, 10], remove_h=False, include_charges=False, visualize_every_batch=10000, normalization_factor=1.0, aggregation_method='sum', filter_molecule_size=None, sequential=False, cuda=True, context_node_nf=0, current_epoch=13, device=device(type='mps'))
Entropy of n_nodes: H[N] -3.718651056289673
Autoencoder models are _not_ conditioned on time.
alphas2 [9.99990000e-01 9.99988000e-01 9.99982000e-01 ... 2.59676966e-05
 1.39959211e-05 1.00039959e-05]
gamma [-11.51291546 -11.33059532 -10.92513058 ...  10.55863126  11.17673063
  11.51251595]


# ä»¥ä¸‹ä¸ºè¾“å‡ºçš„è¯„ä¼°ç»“æœ
10/10 Molecules generated at 70.12 secs/sample
Validity over 10 molecules: 100.00%
Uniqueness over 10 valid molecules: 100.00%
{'mol_stable': 0.0, 'atm_stable': 0.8647540983606558}


#è¾“å‡º
Validity 1.0000, Uniqueness: 1.0000, Novelty: 0.0000
 Val NLL        , iter: 0/21633, NLL: -188.43
 Val NLL        , iter: 50/21633, NLL: -313.57
 Val NLL        , iter: 100/21633, NLL: -354.71
 Val NLL        , iter: 150/21633, NLL: -376.12
 Val NLL        , iter: 200/21633, NLL: -379.55
 Val NLL        , iter: 250/21633, NLL: -385.54
 Val NLL        , iter: 300/21633, NLL: -391.25
... ...

inal test nll -765.8581615602465
Overview: val nll -653.3101636028102 test nll -765.8581615602465 {'mol_stable': 0.0, 'atm_stable': 0.8647540983606558}</code></pre> 
<pre>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œdrugsæ¨¡å‹ç”Ÿæˆåˆ†å­çš„ç¨³å®šæ€§ä¸º100%ï¼Œ åŸå­çš„ç¨³å®šæ€§ä¸º0.86ã€‚åœ¨10ä¸ªåˆ†å­çš„æµ‹è¯•ä¸­ï¼Œåˆ†å­æœ‰æ•ˆç‡ä¸º100%ï¼Œç‹¬ç‰¹ç‡ä¸º100%ï¼Œ åœ¨æœ‰æ•ˆçš„åˆ†å­ä¸­ï¼Œæ–°é¢–åˆ†å­çš„æ¯”ä¾‹æ˜¯0.0% ï¼Ÿï¼ˆè¿™æœ‰ç‚¹å¥‡æ€ªï¼‰ã€‚
</pre> 
<p></p> 
<p>qm9æ¨¡å‹å¯ä»¥ç›´æ¥è¿è¡Œï¼Œæ— éœ€æ•°æ®é¢„å¤„ç†ï¼Œå› ä¸ºæµ‹è¯•æ‰€éœ€çš„å¤„ç†å¥½çš„æ•°æ®æ–‡ä»¶geom_permutation.npy, åœ¨./data/geom/ç›®å½•ä¸‹æœ‰ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥æµ‹è¯•ã€‚å› æ­¤ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç›´æ¥è¯„ä¼°qm9æ¨¡å‹ï¼š</p> 
<pre><code>python eval_analyze.py \
  --model_path outputs/pretrained/qm9_latent2 \
  --n_samples 10</code></pre> 
<p></p> 
<p>åœ¨ç»å†ä¸€é•¿ä¸²çš„obabelè¾“å‡ºä»¥åï¼Œä¼šå‡ºç°è¯„ä¼°çš„ç»“æœï¼Œå¦‚ä¸‹ï¼š</p> 
<pre><code># obabelè¾“å‡ºç¤ºä¾‹
[09:39:51] Explicit valence for atom # 4 N, 4, is greater than permitted
[09:39:52] Explicit valence for atom # 5 N, 4, is greater than permitted
[09:39:52] Explicit valence for atom # 3 N, 4, is greater than permitted
[09:39:52] Explicit valence for atom # 3 N, 4, is greater than permitted
[09:39:53] Explicit valence for atom # 2 C, 5, is greater than permitted


## ä»¥ä¸‹ä¸ºè¯„ä¼°è¾“å‡ºç»“æœ
Validity over 10 molecules: 90.00%
Uniqueness over 9 valid molecules: 100.00%
Novelty over 9 unique valid molecules: 66.67%
{'mol_stable': 0.9, 'atm_stable': 0.9890710382513661}
Validity 0.9000, Uniqueness: 1.0000, Novelty: 0.6667


## å†ç„¶åï¼Œä¼šæœ‰ä¸€ç³»åˆ—æ¼«é•¿çš„è¾“å‡ºï¼š
Test NLL       , iter: 17/205, NLL: -329.98
Test NLL       , iter: 18/205, NLL: -330.06
Test NLL       , iter: 19/205, NLL: -330.09
Test NLL       , iter: 20/205, NLL: -329.89
Test NLL       , iter: 21/205, NLL: -329.95
Test NLL       , iter: 22/205, NLL: -330.03
Test NLL       , iter: 23/205, NLL: -329.87


## æœ€ç»ˆè¾“å‡º
Test NLL       , iter: 204/205, NLL: -331.61
Final test nll -331.6104668697505
Overview: val nll -332.37032418705627 test nll -331.6104668697505 {'mol_stable': 0.9, 'atm_stable': 0.9890710382513661}</code></pre> 
<pre></pre> 
<p>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œqm9æ¨¡å‹ç”Ÿæˆåˆ†å­çš„ç¨³å®šæ€§ä¸º0.9ï¼Œ åŸå­çš„ç¨³å®šæ€§ä¸º0.98ã€‚åœ¨10ä¸ªåˆ†å­çš„æµ‹è¯•ä¸­ï¼Œåˆ†å­æœ‰æ•ˆç‡ä¸º90%ï¼Œç‹¬ç‰¹ç‡ä¸º100%ï¼Œ åœ¨æœ‰æ•ˆçš„åˆ†å­ä¸­ï¼Œæ–°é¢–åˆ†å­çš„æ¯”ä¾‹æ˜¯66.7%ã€‚</p> 
<p></p> 
<p>åœ¨æµ‹è¯•è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œä¼šåœ¨qm9æ–‡ä»¶å¤¹ä¸‹ï¼Œæ–°å»ºä¸€ä¸ªtmpçš„æ–‡ä»¶å¤¹ï¼Œç”¨äºä¿å­˜ç”Ÿæˆçš„æ–‡ä»¶ï¼Œè¿è¡Œç»“æŸåï¼Œtmpçš„å†…å®¹å¦‚ä¸‹ï¼š</p> 
<pre><code>â”œâ”€â”€ qm9
â”‚   â”œâ”€â”€ dsgdb9nsd.xyz.tar.bz2
â”‚   â”œâ”€â”€ test.npz
â”‚   â”œâ”€â”€ train.npz
â”‚   â””â”€â”€ valid.npz
â””â”€â”€ qm9_smiles.pickle</code></pre> 
<p></p> 
<p>æ¥ä¸‹æ¥å¯¹åˆšæ‰çš„è¯„ä¼°ç»“æœå¯è§†åŒ–ï¼š</p> 
<pre><code>python eval_sample.py --model_path outputs/pretrained/qm9_latent2 --n_samples 10</code></pre> 
<p>è¾“å‡ºç»“æœç¤ºä¾‹ï¼š</p> 
<pre><code>Average distance between atoms 2.941699266433716
Average distance between atoms 3.102877378463745
Average distance between atoms 3.127577066421509
Average distance between atoms 3.1570422649383545
... ...
Sampling visualization chain.
Found stable molecule to visualize :)
Creating gif with 108 images
Found stable molecule to visualize :)
Creating gif with 108 images
Found stable molecule to visualize :)
... ... è¾“å‡ºéå¸¸æ…¢ã€‚ã€‚ã€‚å½“ç„¶ä¹Ÿå¯èƒ½æ˜¯æˆ‘å¾…æœºçš„åŸå› </code></pre> 
<p></p> 
<p>å…¶ä¸­å‡ºå›¾æ˜¯ä¸€ä¸ªå¾ˆæ…¢çš„è¿‡ç¨‹ã€‚è¾“å‡ºçš„å¯è§†åŒ–ç»“æœï¼Œä¿å­˜åœ¨/outputs/pretrained/qm9_latent2/eval/moleculesè·¯å¾„ã€‚å…¶æ–‡ä»¶ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š</p> 
<pre><code>.
â”œâ”€â”€ chain_0
â”œâ”€â”€ chain_1
â”œâ”€â”€ chain_2
â”œâ”€â”€ chain_3
â”œâ”€â”€ chain_4
â”œâ”€â”€ chain_5
â”œâ”€â”€ chain_6
â”œâ”€â”€ chain_7
â”œâ”€â”€ chain_8
â”œâ”€â”€ chain_9
....
â””â”€â”€ molecules</code></pre> 
<p>å…¶ä¸­ï¼Œ moleculesä¸ºæ‰€æœ‰çš„åˆ†å­ã€‚chain_xä¸ºå»å™ªè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆåˆ†å­çš„å›¾ç‰‡è®°å½•ï¼Œæ¯ä¸ªæ–‡ä»¶å¤¹ä¸‹å‡æœ‰ä¸€ä¸ªgifæ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªåˆ†å­çš„åŠ¨æ€ç”Ÿæˆè¿‡ç¨‹ï¼Œå³æ¯ä¸€ä¸ªchain_xå¯¹åº”ä¸€ä¸ªåˆ†å­çš„ç”Ÿæˆè¿‡ç¨‹ã€‚æ¯ä¸ªgiféƒ½æœ‰108å¼ å›¾ç‰‡ç”Ÿæˆã€‚è¿›è¡Œåˆ°ä¸€åŠï¼Œæˆ‘å°±killäº†ï¼Œå¤ªè€—æ—¶é—´äº†ã€‚</p> 
<p></p> 
<p>æ³¨æ„ï¼Œç”Ÿæˆçš„åˆ†å­æ˜¯txtxæ ¼å¼ï¼Œå³xyzæ ¼å¼ã€‚ä½œè€…é€šè¿‡matplotlibä½œå›¾å¯è§†åŒ–ï¼ˆè¿™ä¸€è¿‡ç¨‹å¾ˆæ…¢ï¼‰ï¼Œå¹¶æ²¡æœ‰è½¬åŒ–æˆæˆ‘ä»¬ç†Ÿæ‚‰çš„sdfæ ¼å¼ã€‚ä»¥ä¸‹æ˜¯ç”Ÿæˆåˆ†å­çš„ä¾‹å­ã€‚</p> 
<p style="text-align:center;"><img alt="" height="259" src="https://images2.imgbox.com/19/6d/4g5cx61O_o.png" width="259"><img alt="" height="259" src="https://images2.imgbox.com/34/a8/44uhi2Xt_o.png" width="259"><img alt="" height="260" src="https://images2.imgbox.com/6a/1e/3Shft2Sk_o.png" width="260"></p> 
<p style="text-align:center;"><img alt="" height="258" src="https://images2.imgbox.com/a0/f6/2MCCyuWC_o.png" width="258"><img alt="" height="257" src="https://images2.imgbox.com/84/ee/nY5sBj9B_o.png" width="257"><img alt="" height="257" src="https://images2.imgbox.com/9f/c4/m2fQK9wg_o.png" width="257"></p> 
<p></p> 
<p>ç”Ÿæˆçš„xyz.txtæ ¼å¼ä¹Ÿæœ‰ä¸€äº›é—®é¢˜ï¼Œä¸èƒ½ç›´æ¥è½¬åŒ–ä¸ºsdfæ–‡ä»¶ã€‚</p> 
<p></p> 
<h5></h5> 
<h4>1.3.2Â è®­ç»ƒæ¡ä»¶ GeoLDM</h4> 
<p></p> 
<p>ç”±äºä½œè€…æ²¡æœ‰æä¾›è®­ç»ƒå¥½çš„æ¡ä»¶æ¨¡å‹ï¼Œå› æ­¤æ¡ä»¶æ¨¡å‹éœ€è¦æˆ‘ä»¬è‡ªå·±è®­ç»ƒï¼š</p> 
<p>ä»¥qm9ä¸ºä¾‹ï¼Œ</p> 
<pre><code>python main_qm9.py \
  --exp_name exp_cond_alpha  \
  --model egnn_dynamics \
  --lr 1e-4  \
  --nf 192 \
  --n_layers 9 \
  --save_model True \
  --diffusion_steps 1000 \
  --sin_embedding False \
  --n_epochs 3000 \
  --n_stability_samples 500 \
  --diffusion_noise_schedule polynomial_2 \
  --diffusion_noise_precision 1e-5 \
  --dequantization deterministic \
  --include_charges False \
  --diffusion_loss_type l2 \
  --batch_size 64 \
  --normalize_factors [1,8,1] \
  --conditioning alpha \
  --dataset qm9_second_half \
  --train_diffusion \
  --trainable_ae \
  --latent_nf 1</code></pre> 
<p></p> 
<p>å…¶ä¸­ï¼ŒÂ --conditioning alphaä¸­çš„alphaï¼Œ å¯ä»¥æ›¿æ¢æˆä¸ºï¼šalpha,Â gap,Â homo,Â lumo,Â muÂ Cvä¸­çš„ä»»æ„ä¸€ä¸ªã€‚è¿™é‡Œæˆ‘ä»¬ä»ä¿æŒä¸å˜ã€‚</p> 
<p></p> 
<h4>1.3.3 è®­ç»ƒ qm9æ¨¡å‹</h4> 
<p>åœ¨GeoLDMä¸»ç›®å½•ä¸‹æ‰§è¡Œï¼š</p> 
<pre><code>python main_qm9.py --n_epochs 10 \
  --n_stability_samples 10 \
  --diffusion_noise_schedule polynomial_2 \
  --diffusion_noise_precision 1e-5 \
  --diffusion_steps 1000 \
  --diffusion_loss_type l2 \
  --batch_size 64 --nf 256 \
  --n_layers 9 \
  --lr 1e-4 \
  --normalize_factors '[1,4,10]' \
  --test_epochs 20 \
  --ema_decay 0.9999 \
  --train_diffusion \
  --trainable_ae \
  --latent_nf 1 \
  --exp_name geoldm_qm9 \
  --wandb_usr geoldm</code></pre> 
<p></p> 
<p>å…¶ä¸­ï¼Œexp_nameæ˜¯å®éªŒåç§°ã€‚--wandb_usr geoldmæ˜¯æˆ‘æ·»åŠ çš„ï¼Œæˆ‘åœ¨wandbé‡Œé¢æ–°å»ºä¸€ä¸ªgeoldmçš„teamï¼Œåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œä¼šå°†è®­ç»ƒè¿‡ç¨‹ä¸Šä¼ åˆ°wandbä¸­ã€‚</p> 
<p></p> 
<p>è¿è¡Œåï¼Œä¼šç”Ÿæˆwandb ç›®å½•ï¼Œä¿å­˜è®­ç»ƒè¿‡ç¨‹ã€‚åŒæ—¶åœ¨outputç›®å½•ä¸‹ï¼Œä¼šç”Ÿæˆæˆ‘ä»¬æ­¤æ¬¡å®éªŒåç§°çš„æ–‡ä»¶å¤¹ï¼Œå³geoldm_qm9ï¼Œç”¨äºä¿å­˜æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œä¸”æˆ‘ä»¬ä»…ä»…æ˜¯å°è¯•ä»£ç æ˜¯å¦å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œæˆ‘ä»¬epochè®¾ç½®ä¸º10ã€‚</p> 
<p></p> 
<p>è¾“å‡ºç¤ºä¾‹ï¼š</p> 
<pre><code>... # è®­ç»ƒ
Epoch: 0, iter: 1127/1563, Loss 2.67, NLL: 2.67, RegTerm: 0.0, GradNorm: 6.6
Epoch: 0, iter: 1128/1563, Loss 2.63, NLL: 2.63, RegTerm: 0.0, GradNorm: 6.3
Epoch: 0, iter: 1129/1563, Loss 2.68, NLL: 2.68, RegTerm: 0.0, GradNorm: 3.2
... # è®­ç»ƒç»“æœ
Epoch took 27095.8 seconds.
{'log_SNR_max': 11.51291561126709, 'log_SNR_min': -11.512516021728516}
Analyzing molecule stability at epoch 0...
Validity over 100 molecules: 100.00%
Uniqueness over 100 valid molecules: 1.00%
Novelty over 1 unique valid molecules: 100.00%
... # éªŒè¯
 Val NLL         epoch: 0, iter: 24/278, NLL: 621.18
 Val NLL         epoch: 0, iter: 25/278, NLL: 615.10
 Val NLL         epoch: 0, iter: 26/278, NLL: 619.27
 Val NLL         epoch: 0, iter: 27/278, NLL: 622.72
 Val NLL         epoch: 0, iter: 28/278, NLL: 614.12
... # æµ‹è¯•
Test NLL        epoch: 0, iter: 100/205, NLL: 513.01
 Test NLL        epoch: 0, iter: 101/205, NLL: 511.20
 Test NLL        epoch: 0, iter: 102/205, NLL: 510.97
 Test NLL        epoch: 0, iter: 103/205, NLL: 512.93
... # epoch éªŒè¯å’Œæµ‹è¯•ç»“æœ
Val loss: 562.9518       Test loss:  543.4572
Best val loss: 562.9518          Best test loss:  543.4572
</code></pre> 
<p></p> 
<p>è®­ç»ƒç»“æŸä»¥åï¼Œæ¨¡å‹å‚æ•°ä¼šåœ¨./outputç›®å½•ä¸‹ç”Ÿæˆgeoldm_qm9æ–‡ä»¶å¤¹ã€‚æ–‡ä»¶å¤¹ä¸‹çš„å†…å®¹ä¸ºï¼š</p> 
<pre><code>.
â”œâ”€â”€ args.pickle
â”œâ”€â”€ args_0.pickle
â”œâ”€â”€ generative_model.npy
â”œâ”€â”€ generative_model_0.npy
â”œâ”€â”€ generative_model_ema.npy
â”œâ”€â”€ generative_model_ema_0.npy
â”œâ”€â”€ optim.npy
â””â”€â”€ optim_0.npy


1 directory, 8 files</code></pre> 
<p></p> 
<h2>äºŒã€ä»£ç ç®€æ</h2> 
<p></p> 
<h3>2.1 æ•°æ®å‡†å¤‡</h3> 
<p>æ•°æ®å‡†å¤‡ï¼Œdrugæ•°æ®é›†ï¼Œä¸‹è½½å®Œæˆåéƒ½éœ€è¦å…ˆè¿›è¡Œæ•°æ®å‡†å¤‡ã€‚ä¸‹è½½ä¸‹æ¥çš„æ•°æ®é›†æ˜¯xyzæ ¼å¼çš„ï¼Œä¸”æ•°æ®ä¿å­˜åœ¨å­—å…¸é‡Œé¢ã€‚æ•°æ®å‡†å¤‡çš„ç›®çš„æ˜¯å°†åˆ†å­è¡¨ç¤ºæˆxå’Œhï¼Œä»¥ä¾¿è¾“å…¥æ¨¡å‹ã€‚(qm9æ•°æ®é›†ä¼¼ä¹æ²¡æœ‰æä¾›ä¸‹è½½æ–¹å¼ï¼Œä½†æ˜¯æœ‰å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚drugæ•°æ®é›†å¯¹åº”çš„æ¨¡å‹ä½œè€…ä¹Ÿæä¾›äº†è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½†æ˜¯æˆ‘ä»¬ä¹Ÿå¯ä»¥è‡ªå·±è®­ç»ƒ)</p> 
<p></p> 
<p>æ¥ä¸‹æ¥æ˜¯drugsæ•°æ®é›†çš„å‡†å¤‡ï¼Œåœ¨é¡¹ç›®ä¸»ç›®å½•ä¸‹è¿è¡Œï¼š</p> 
<pre><code>python build_geom_dataset.py</code></pre> 
<p>è¿è¡Œè¾“å‡ºï¼š</p> 
<pre><code>Unpacking file 0...
Unpacking file 1...
Unpacking file 2...
Unpacking file 3...
Unpacking file 4..
... å¾ˆé•¿å¾ˆé•¿
... æœ€åè¾“å‡º
Total number of conformers saved 6922516
Total number of atoms in the dataset 322877623
Average number of atoms per molecule 46.64165788854804
Dataset processed.</code></pre> 
<p>è¿è¡Œæ—¶é—´å¤§çº¦æ˜¯1ä¸ªå°æ—¶ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†692W å¤šçš„åˆ†å­æ„è±¡ï¼Œå¹³å‡æ¯ä¸ªåˆ†å­å«æœ‰46ä¸ªåŸå­ã€‚è¿è¡Œè¾“å‡ºçš„æ–‡ä»¶ä»ç„¶ä¿å­˜åœ¨æ•°æ®ç›®å½•ï¼š/data/geomï¼Œä¸€å…±ç”Ÿæˆäº†geom_drugs_30.npyï¼Œ geom_drugs_n_30.npyï¼Œ geom_drugs_smiles.txtä¸‰ä¸ªæ–‡ä»¶ç›®å½•ã€‚</p> 
<p></p> 
<p>ä¸‹é¢æ˜¯ä»£ç è§£æã€‚é¦–å…ˆæ˜¯å‚æ•°ï¼Œåœ¨__mian__ä¸­ï¼š</p> 
<pre><code>if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # æ¯ä¸ªåˆ†å­ç”Ÿæˆçš„æ„è±¡æ•°ï¼Œé»˜è®¤ 30
    parser.add_argument("--conformations", type=int, default=30,
                        help="Max number of conformations kept for each molecule.")
    # æ˜¯å¦ä¿ç•™Hï¼Œé»˜è®¤ä¿ç•™
    parser.add_argument("--remove_h", action='store_true', help="Remove hydrogens from the dataset.")
    # åŸå§‹æ•°æ®ä¿å­˜è·¯å¾„ï¼Œå³ä¸‹è½½æ¥çš„æ•°æ®è·¯å¾„ï¼Œé»˜è®¤ä¿®æ”¹ä¸ºï¼š./data/geom/
    parser.add_argument("--data_dir", type=str, default='./data/geom/') # æ³¨æ„è¾“å…¥è·¯å¾„
    # æ•°æ®æ–‡ä»¶åï¼Œé»˜è®¤ä¸ºï¼šdrugs_crude.msgpack
    parser.add_argument("--data_file", type=str, default="drugs_crude.msgpack")
    args = parser.parse_args()
    extract_conformers(args)</code></pre> 
<p>extract_conformerså‡½æ•°ï¼š</p> 
<p>å®ç°ä»drugsæ•°æ®é›†çš„å‹ç¼©æ–‡ä»¶ä¸­ï¼Œæå–åˆ†å­æ„è±¡ï¼ˆæ¯ä¸ªåˆ†å­èƒ½é‡æœ€ä½çš„30ä¸ªæ„è±¡ï¼‰ï¼Œå³xyzæ–‡ä»¶ï¼ˆåæ ‡ï¼Œå…ƒç´ ï¼‰ï¼Œå¹¶è®¾ç½®åˆ†å­idã€‚</p> 
<p>åˆ†å­çš„smilesä¿å­˜åœ¨geom_drugs_smiles.txtæ–‡ä»¶ï¼Œæ¯ä¸ªåˆ†å­å¯¹åº”çš„åŸå­æ•°ä¿å­˜åœ¨geom_drugs_n_30.npyæ–‡ä»¶ï¼Œåˆ†å­çš„æ„è±¡ä¿¡æ¯ä¿å­˜åœ¨geom_drugs_30.npyæ–‡ä»¶ï¼ˆè¯¥æ–‡ä»¶æ˜¯python eval_analyze.pyæ–¹æ³•åˆ†ædrug_latent_2æ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ï¼Œä¹Ÿæ˜¯è®­ç»ƒdrugæ¨¡å‹æ‰€éœ€çš„æ•°æ®é›†ï¼‰ã€‚</p> 
<p></p> 
<pre><code>def extract_conformers(args):
    drugs_file = os.path.join(args.data_dir, args.data_file)
    save_file = f"geom_drugs_{'no_h_' if args.remove_h else ''}{args.conformations}"
    smiles_list_file = 'geom_drugs_smiles.txt'
    number_atoms_file = f"geom_drugs_n_{'no_h_' if args.remove_h else ''}{args.conformations}"


    # è§£å‹æ•°æ®æ–‡ä»¶
    unpacker = msgpack.Unpacker(open(drugs_file, "rb"))

    # ä¿å­˜smileså’Œæ„è±¡
    all_smiles = []
    all_number_atoms = []
    dataset_conformers = []
    mol_id = 0
    for i, drugs_1k in enumerate(unpacker):
        print(f"Unpacking file {i}...")
        for smiles, all_info in drugs_1k.items():
            all_smiles.append(smiles) # smiles
            conformers = all_info['conformers'] # æ„è±¡
            # Get the energy of each conformer. Keep only the lowest values
            all_energies = []
            for conformer in conformers:
                all_energies.append(conformer['totalenergy'])
            # æŒ‰ç…§èƒ½é‡æ’åºï¼Œæå–æœ€ä½èƒ½é‡top 30çš„åˆ†å­æ„è±¡
            all_energies = np.array(all_energies)

            argsort = np.argsort(all_energies)
            lowest_energies = argsort[:args.conformations]
            for id in lowest_energies:
                conformer = conformers[id]
                coords = np.array(conformer['xyz']).astype(float)        # n x 4ï¼Œ xyzæ ¼å¼
                if args.remove_h:
                    mask = coords[:, 0] != 1.0
                    coords = coords[mask]
                n = coords.shape[0]
                all_number_atoms.append(n)
                mol_id_arr = mol_id * np.ones((n, 1), dtype=float) # åˆ†å­id
                id_coords = np.hstack((mol_id_arr, coords)) # åˆ†å­ id, åŠå…¶åæ ‡
                
                dataset_conformers.append(id_coords)
                mol_id += 1


    print("Total number of conformers saved", mol_id)
    all_number_atoms = np.array(all_number_atoms)
    dataset = np.vstack(dataset_conformers) 


    print("Total number of atoms in the dataset", dataset.shape[0])
    print("Average number of atoms per molecule", dataset.shape[0] / mol_id)

    # Save conformations
    # æ„è±¡ä¿¡æ¯ä¿å­˜åˆ°npyæ–‡ä»¶ä¸­ï¼Œæ–‡ä»¶ï¼šgeom_drugs_30.npy
    np.save(os.path.join(args.data_dir, save_file), dataset)
    # Save SMILESï¼Œ ä¿å­˜åˆ°txtæ–‡ä»¶ä¸­ geom_drugs_smiles.txt
    with open(os.path.join(args.data_dir, smiles_list_file), 'w') as f:
        for s in all_smiles:
            f.write(s)
            f.write('\n')


    # Save number of atoms per conformation, æ–‡ä»¶ï¼šgeom_drugs_n_30.npy
    np.save(os.path.join(args.data_dir, number_atoms_file), all_number_atoms)
    print("Dataset processed.")
    </code></pre> 
<h3>2.2 GeoLDM æ¨¡å‹ä»£ç ï¼ˆè®­ç»ƒå’Œé‡‡æ ·ï¼‰</h3> 
<p></p> 
<p>å½“æˆ‘ä»¬è¦è®­ç»ƒGeoLDMæ¨¡å‹æ—¶ï¼Œä»¥QM9æ•°æ®é›†ä¸ºä¾‹ï¼Œ åœ¨ä¸»ç›®å½•ä¸‹è¿è¡Œï¼š</p> 
<pre><code>python main_qm9.py --n_epochs 10 \
  --n_stability_samples 10 \
  --diffusion_noise_schedule polynomial_2 \
  --diffusion_noise_precision 1e-5 \
  --diffusion_steps 1000 \
  --diffusion_loss_type l2 \
  --batch_size 64 --nf 256 \
  --n_layers 9 \
  --lr 1e-4 \
  --normalize_factors '[1,4,10]' \
  --test_epochs 20 \
  --ema_decay 0.9999 \
  --train_diffusion \ # é»˜è®¤ä¸ºTrue?
  --trainable_ae \
  --latent_nf 1 \
  --exp_name geoldm_qm9 \
  --wandb_usr geoldm</code></pre> 
<p>æ³¨æ„ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯ä¸ºäº†æµ‹è¯•ä»£ç ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†n_epochsè®¾ç½®ä¸º10ï¼ŒåŸä½œè€…è®¾ç½®ä¸º3000ï¼ŒåŒæ—¶ä½¿ç”¨äº†è‡ªå·±çš„geoldm çš„wandb çš„teamï¼Œè¿™ä¸¤ä¸ªå‚æ•°å¤§å®¶æŒ‰éœ€è®¾ç½®ã€‚</p> 
<p></p> 
<p>ä»¥ä¸‹æ˜¯ä»£ç è§£æã€‚</p> 
<p>é¦–å…ˆå€’å…¥ç›¸å…³æ¨¡å—ï¼š</p> 
<pre><code># Rdkit import should be first, do not move it
try:
    from rdkit import Chem
except ModuleNotFoundError:
    pass
import copy
import utils
import argparse
import wandb
from configs.datasets_config import get_dataset_info
from os.path import join
from qm9 import dataset
from qm9.models import get_optim, get_model, get_autoencoder, get_latent_diffusion
from equivariant_diffusion import en_diffusion
from equivariant_diffusion.utils import assert_correctly_masked
from equivariant_diffusion import utils as flow_utils
import torch
import time
import pickle
from qm9.utils import prepare_context, compute_mean_mad
from train_test import train_epoch, test, analyze_and_save</code></pre> 
<h5></h5> 
<h4>2.2.1 è®­ç»ƒè¶…å‚æ•°é…ç½®</h4> 
<p>è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼Œéå¸¸å¤šï¼Œ</p> 
<pre><code>python main_qm9.py --n_epochs 10 \
  --n_stability_samples 10 \
  --diffusion_noise_schedule polynomial_2 \
  --diffusion_noise_precision 1e-5 \
  --diffusion_steps 1000 \
  --diffusion_loss_type l2 \
  --batch_size 64 --nf 256 \
  --n_layers 9 \
  --lr 1e-4 \
  --normalize_factors '[1,4,10]' \
  --test_epochs 20 \
  --ema_decay 0.9999 \
  --train_diffusion \
  --trainable_ae \
  --latent_nf 1 \
  --exp_name geoldm_qm9 \
  --wandb_usr geoldm</code></pre> 
<p></p> 
<h4 style="background-color:transparent;">2.2.2Â è®­ç»ƒGeoLDMæ¨¡å‹ä»£ç </h4> 
<p>é¦–å…ˆï¼Œè·å–æ•°æ®é›†ï¼Œwandbï¼ŒGPUç­‰ä¿¡æ¯åŠé¢„è®¾ï¼š</p> 
<pre><code># è·å–æ•°æ®é›†çš„é¢„è®¾
dataset_info = get_dataset_info(args.dataset, args.remove_h)


# å°†å…ƒç´ ç¬¦å·è½¬ä¸ºæ•°å­—çš„ dict
atom_encoder = dataset_info['atom_encoder']
# å°†æ•°å­—è½¬ä¸ºå…ƒç´ ç¬¦å·çš„ list
atom_decoder = dataset_info['atom_decoder']


# args, unparsed_args = parser.parse_known_args()


# æå– wandb çš„é¡¹ç›®åï¼Œå¡«å…¥å‚æ•°å³å¯è¦†ç›–ï¼Œæ¯æ¬¡å®éªŒğŸ‰‘è®¾ç½®ä¸åŒåå­—ï¼Œåˆ†å¼€ä¿å­˜
# ç›¸åŒåå­—ï¼Œåˆ™è®°å½•ç¬¬å‡ æ¬¡run
args.wandb_usr = utils.get_wandb_username(args.wandb_usr)


# è®¾ç½® GPU çš„ä½¿ç”¨ï¼Œè¿™é‡Œä½¿ç”¨mps
# args.cuda = not args.no_cuda and torch.cuda.is_available()
# device = torch.device("cuda" if args.cuda else "cpu")
args.cuda = not args.no_cuda and torch.backends.mps.is_available()  # wufeil mpsè®­ç»ƒ
device = torch.device("mps" if args.cuda else "cpu")
    
dtype = torch.float32</code></pre> 
<pre>ç„¶åï¼Œè®¾ç½®æ¥ç»­è®­ç»ƒï¼š</pre> 
<pre><code># å¦‚æœæ¥ç»­è®­ç»ƒ
if args.resume is not None:
    exp_name = args.exp_name + '_resume' # å®éªŒåç§°æ·»åŠ _resume
    start_epoch = args.start_epoch # æ¥ç»­ epoch
    resume = args.resume
    wandb_usr = args.wandb_usr
    normalization_factor = args.normalization_factor
    aggregation_method = args.aggregation_method

    with open(join(args.resume, 'args.pickle'), 'rb') as f:
        # åŠ è½½å‚æ•°
        args = pickle.load(f)

    args.resume = resume
    args.break_train_epoch = False

    args.exp_name = exp_name
    args.start_epoch = start_epoch
    args.wandb_usr = wandb_usr

    # Careful with this --&gt;
    if not hasattr(args, 'normalization_factor'):
        args.normalization_factor = normalization_factor
    if not hasattr(args, 'aggregation_method'):
        args.aggregation_method = aggregation_method

    print(args)
</code></pre> 
<p>åˆ›å»ºä¿å­˜è®­ç»ƒæ¨¡å‹ç»“æœç›®å½•ï¼ŒåŠwandbåˆå§‹åŒ–é…ç½®ï¼š</p> 
<pre><code class="language-python"># åˆ›å»ºoutputåŠå…¶å®éªŒåç§°çš„æ–‡ä»¶å¤¹ï¼Œç”¨äºä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
# é»˜è®¤æ”¾åœ¨./outputä¸­
utils.create_folders(args)
# print(args)

# Wandb config
if args.no_wandb:
    mode = 'disabled' # æ˜¯å¦ä½¿ç”¨wandb
else:
    mode = 'online' if args.online else 'offline' # wandbæ˜¯offlineè¿˜æ˜¯online
# wandbçš„å‚æ•°
kwargs = {'entity': args.wandb_usr, 'name': args.exp_name, 'project': 'e3_diffusion_qm9', 'config': args,
          'settings': wandb.Settings(_disable_stats=True), 'reinit': True, 'mode': mode}
# wandb åˆå§‹åŒ–ï¼Œå…ˆåœ¨condaç¯å¢ƒä¸­è®¾ç½®ç§é’¥
wandb.init(**kwargs)
wandb.save('*.txt')</code></pre> 
<p></p> 
<p>æ•°æ®dataloaderå‡†å¤‡ï¼š</p> 
<pre><code class="language-python"># Retrieve QM9 dataloadersï¼Œ å–å›qm9çš„dataloaderï¼Œæš‚æ—¶ç•¥è¿‡
# æ•°æ®è·¯å¾„ ./qm9/temp/qm9
dataloaders, charge_scale = dataset.retrieve_dataloaders(args)

data_dummy = next(iter(dataloaders['train']))</code></pre> 
<p></p> 
<p>contextç»´åº¦è®¾ç½®åŠè®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„å‡å€¼ï¼Œä½¿ç”¨compute_mean_madå‡½æ•°ã€‚</p> 
<pre><code class="language-python">#å¦‚æœæœ‰æ¡ä»¶å±æ€§
if len(args.conditioning) &gt; 0:
    print(f'Conditioning on {args.conditioning}')
    # è®¡ç®—å±æ€§çš„æ•°æ®é›†å‡å€¼ï¼ŒåŠæ¯ä¸€ä¸ªæ‰¹æ¬¡ä¸­å‡å€¼ä¸æ•°æ®é›†å‡å€¼åå·®çš„ç»å¯¹å€¼
    property_norms = compute_mean_mad(dataloaders, \
                              args.conditioning, args.dataset)
    # å°† æ¡ä»¶ï¼Œä¾‹å¦‚ï¼š evç­‰ï¼Œå†™å…¥contextä¸­ï¼Œ 
    # 1ç»´ï¼ˆbs, ï¼‰ä¸ºåˆ†å­å±‚é¢å±æ€§ï¼Œ
    # 2ç»´æˆ–è€…3ç»´ï¼ˆbs, node, propetiesï¼‰ä¸ºåŸå­å±‚é¢å±æ€§
    context_dummy = prepare_context(args.conditioning, 
                                      data_dummy, property_norms)
    context_node_nf = context_dummy.size(2) # contextçš„ç»´åº¦
else:
    context_node_nf = 0
    property_norms = None
# contextçš„ç»´åº¦ï¼Œæœ€åä½“ç°åœ¨è¾“å…¥æ¨¡å‹ä¸­çš„futureä¸­    
args.context_node_nf = context_node_nf </code></pre> 
<p></p> 
<p>æ ¹æ®æ˜¯å¦è®­ç»ƒdiffusionæ¨¡å‹ï¼Œåˆ›å»ºVAEæ¨¡å‹æˆ–è€…GeoLDMæ¨¡å‹</p> 
<pre><code class="language-python"># Create Latent Diffusion Model or Audoencoder
if args.train_diffusion: # é»˜è®¤æ˜¯è®­ç»ƒtrain_diffusionæ¨¡å‹ï¼Œ 
    # å¦‚æœargsä¸­æœ‰aeæ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆargs.ae_pathï¼Œ'args.pickle'ï¼‰ï¼Œ
    # å¦åˆ™åŠ è½½aeæ¨¡å‹, å¦åˆ™ä¸ºç¬¬ä¸€æ¬¡è¿è¡Œ (é»˜è®¤æ˜¯æ²¡æœ‰ä»¥è®­ç»ƒè¿‡çš„VAEæ¨¡å‹çš„)
    model, nodes_dist, prop_dist = get_latent_diffusion(args, device, dataset_info, dataloaders['train'])
else:
    # åªè®­ç»ƒVAEæ¨¡å‹
    model, nodes_dist, prop_dist = get_autoencoder(args, device, dataset_info, dataloaders['train'])</code></pre> 
<p></p> 
<p>å¦‚æœæœ‰å±æ€§åˆ†å¸ƒï¼Œå³æœ‰å±æ€§ä½œä¸ºcondition, å³contextï¼Œéœ€è¦è¿›è¡Œå½’ä¸€åŒ–</p> 
<pre><code class="language-python">if prop_dist is not None:
    # å±æ€§æ¯ä¸€ä¸ªæ‰¹æ¬¡å½’ä¸€åŒ–
    prop_dist.set_normalizer(property_norms)</code></pre> 
<p></p> 
<p>æ¨¡å‹åŠ è½½åˆ°GPUï¼Œè®¾ç½®ä¼˜åŒ–å™¨ï¼Œæ¢¯åº¦å‰ªè£è®¾ç½®</p> 
<pre><code class="language-python">model = model.to(device)
optim = get_optim(args, model)
# print(model)

# æ¢¯åº¦å‰ªè£è®¾ç½®
gradnorm_queue = utils.Queue()
gradnorm_queue.add(3000)  # Add large value that will be flushed.</code></pre> 
<p></p> 
<p>ç„¶åï¼Œå°±ä¼šæ‰§è¡Œåˆ°Â main()å‡½æ•°ã€‚Â </p> 
<p>é¦–å…ˆæ˜¯ï¼Œæ¥ç»­è®­ç»ƒè®¾ç½®ã€‚</p> 
<pre><code class="language-python">    # å¦‚æœæ¥ç»­è®­ç»ƒ
    if args.resume is not None:
        flow_state_dict = torch.load(join(args.resume, 'flow.npy'))
        optim_state_dict = torch.load(join(args.resume, 'optim.npy'))
        model.load_state_dict(flow_state_dict)
        optim.load_state_dict(optim_state_dict)</code></pre> 
<p></p> 
<p>å¤šå¡GPUæ•°æ®å¹³è¡Œè®­ç»ƒè®¾ç½®</p> 
<pre><code class="language-python">    # å¤šGPUå¡ï¼Œæ•°æ®å¹³è¡Œè®­ç»ƒdpï¼Œå…¶å®å¯ä»¥ä½¿ç”¨pytorch_lightningè®¾ç½®
    # Initialize dataparallel if enabled and possible.
    if args.dp and torch.cuda.device_count() &gt; 1:
        print(f'Training using {torch.cuda.device_count()} GPUs')
        model_dp = torch.nn.DataParallel(model.cpu())
        model_dp = model_dp.cuda()
    else:
        model_dp = model</code></pre> 
<p></p> 
<p>æ¨¡å‹æƒé‡æŒ‡æ•°å¹³ç§»å¹³å‡ï¼ˆå³ï¼Œemaï¼‰æ¨¡å‹å‰¯æœ¬ï¼Œ</p> 
<pre><code class="language-python">    if args.ema_decay &gt; 0:
        # å‚æ•°æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼çš„æ¨¡å‹å‰¯æœ¬ï¼Œ å³ ema æ¨¡å‹
        model_ema = copy.deepcopy(model)
        # å‚æ•°æŒ‡æ•°å¹³ç§»å¯¹è±¡ï¼Œç”¨äºå¯¹æ¨¡å‹å‚æ•°çš„æŒ‡æ•°å¹³ç§»å¹³å‡æ“ä½œ
        ema = flow_utils.EMA(args.ema_decay)

        if args.dp and torch.cuda.device_count() &gt; 1:
            model_ema_dp = torch.nn.DataParallel(model_ema)
        else:
            model_ema_dp = model_ema
    else:
        ema = None
        model_ema = model
        model_ema_dp = model_dp</code></pre> 
<p></p> 
<p>æ¨¡å‹è®­ç»ƒï¼Œæµ‹è¯•ï¼ŒéªŒè¯æ­¥ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹å’Œè®­ç»ƒè¿‡ç¨‹æ¨¡å‹ï¼Œæ‰“å°lossï¼Œ wandbè®°å½•ã€‚</p> 
<pre><code class="language-python">    best_nll_val = 1e8
    best_nll_test = 1e8
    for epoch in range(args.start_epoch, args.n_epochs):
        start_epoch = time.time()
        # è®­ç»ƒæ­¥
        train_epoch(args=args, loader=dataloaders['train'], epoch=epoch, model=model, model_dp=model_dp,
                    model_ema=model_ema, ema=ema, device=device, dtype=dtype, property_norms=property_norms,
                    nodes_dist=nodes_dist, dataset_info=dataset_info,
                    gradnorm_queue=gradnorm_queue, optim=optim, prop_dist=prop_dist)
        print(f"Epoch took {time.time() - start_epoch:.1f} seconds.")

        # æµ‹è¯•æ­¥
        if epoch % args.test_epochs == 0:
            if isinstance(model, en_diffusion.EnVariationalDiffusion):
                wandb.log(model.log_info(), commit=True)

            # å¦‚æœè®­ç»ƒçš„diffusionæ¨¡å‹
            if not args.break_train_epoch and args.train_diffusion:
                analyze_and_save(args=args, epoch=epoch, model_sample=model_ema, nodes_dist=nodes_dist,
                                 dataset_info=dataset_info, device=device,
                                 prop_dist=prop_dist, n_samples=args.n_stability_samples)
            # éªŒè¯æ­¥ valid
            nll_val = test(args=args, loader=dataloaders['valid'], epoch=epoch, eval_model=model_ema_dp,
                           partition='Val', device=device, dtype=dtype, nodes_dist=nodes_dist,
                           property_norms=property_norms)
            # æµ‹è¯•æ­¥ test
            nll_test = test(args=args, loader=dataloaders['test'], epoch=epoch, eval_model=model_ema_dp,
                            partition='Test', device=device, dtype=dtype,
                            nodes_dist=nodes_dist, property_norms=property_norms)
            # valid æŸå¤±æ›´å°ï¼Œåˆ™ä¿å­˜æ¨¡å‹
            if nll_val &lt; best_nll_val:
                best_nll_val = nll_val
                best_nll_test = nll_test
                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                if args.save_model:
                    args.current_epoch = epoch + 1
                    utils.save_model(optim, 'outputs/%s/optim.npy' % args.exp_name)
                    utils.save_model(model, 'outputs/%s/generative_model.npy' % args.exp_name)
                    if args.ema_decay &gt; 0:
                        utils.save_model(model_ema, 'outputs/%s/generative_model_ema.npy' % args.exp_name)
                    with open('outputs/%s/args.pickle' % args.exp_name, 'wb') as f:
                        pickle.dump(args, f)
                # ä¿å­˜ epoch æ¨¡å‹ï¼Œè®°å½•
                if args.save_model:
                    utils.save_model(optim, 'outputs/%s/optim_%d.npy' % (args.exp_name, epoch))
                    utils.save_model(model, 'outputs/%s/generative_model_%d.npy' % (args.exp_name, epoch))
                    if args.ema_decay &gt; 0:
                        utils.save_model(model_ema, 'outputs/%s/generative_model_ema_%d.npy' % (args.exp_name, epoch))
                    with open('outputs/%s/args_%d.pickle' % (args.exp_name, epoch), 'wb') as f:
                        pickle.dump(args, f)
            print('Val loss: %.4f \t Test loss:  %.4f' % (nll_val, nll_test))
            print('Best val loss: %.4f \t Best test loss:  %.4f' % (best_nll_val, best_nll_test))
            wandb.log({"Val loss ": nll_val}, commit=True)
            wandb.log({"Test loss ": nll_test}, commit=True)
            wandb.log({"Best cross-validated test loss ": best_nll_test}, commit=True)</code></pre> 
<p></p> 
<p>train_epochå‡½æ•°æ‰§è¡Œäº†GeoLDMæ¨¡å‹è®­ç»ƒçš„ä»»åŠ¡ï¼Œå¹¶ä¿å­˜æ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">def train_epoch(args, loader, epoch, model, model_dp, model_ema, ema, device, dtype, property_norms, optim,
                nodes_dist, gradnorm_queue, dataset_info, prop_dist):
    '''
    è®­ç»ƒGEOLDMæ¨¡å‹ï¼Œä¸€ä¸ªepoch
    '''
    model_dp.train()
    model.train()
    nll_epoch = []
    n_iterations = len(loader)
    for i, data in enumerate(loader):
        x = data['positions'].to(device, dtype)
        node_mask = data['atom_mask'].to(device, dtype).unsqueeze(2)
        edge_mask = data['edge_mask'].to(device, dtype)
        one_hot = data['one_hot'].to(device, dtype)
        charges = (data['charges'] if args.include_charges else torch.zeros(0)).to(device, dtype)

        x = remove_mean_with_mask(x, node_mask)

        # args.augment_noise å¢å¼ºæ·»åŠ å™ªéŸ³
        if args.augment_noise &gt; 0:
            # Add noise eps ~ N(0, augment_noise) around points.
            eps = sample_center_gravity_zero_gaussian_with_mask(x.size(), x.device, node_mask)
            x = x + eps * args.augment_noise
        
        # éšæœºæ—‹è½¬
        x = remove_mean_with_mask(x, node_mask)
        if args.data_augmentation:
            x = utils.random_rotation(x).detach()

        # mask ç»´åº¦æ£€æŸ¥
        check_mask_correct([x, one_hot, charges], node_mask)
        # è´¨å¿ƒå½’é›¶æ£€æŸ¥
        assert_mean_zero_with_mask(x, node_mask)
        # èŠ‚ç‚¹ç‰¹å¾ h
        h = {'categorical': one_hot, 'integer': charges}

        # context æ¡ä»¶ï¼ˆåˆ†å­å±‚é¢ï¼‰
        if len(args.conditioning) &gt; 0:
            context = qm9utils.prepare_context(args.conditioning, data, property_norms).to(device, dtype)
            assert_correctly_masked(context, node_mask)
        else:
            context = None

        optim.zero_grad()

        # transform batch through flow è®¡ç®—æŸå¤±
        nll, reg_term, mean_abs_z = losses.compute_loss_and_nll(args, model_dp, nodes_dist,
                                                                x, h, node_mask, edge_mask, context)
        # standard nll from forward KL
        # æŸå¤±ï¼Œnll æŸå¤± + reg_term å›å½’é¡¹
        loss = nll + args.ode_regularization * reg_term
        loss.backward()

        # æ¢¯åº¦å‰ªè£
        if args.clip_grad:
            grad_norm = utils.gradient_clipping(model, gradnorm_queue)
        else:
            grad_norm = 0.

        optim.step()

        # Update EMA if enabled. å¦‚æœæŒ‡æ•°å¹³ç§»å¹³å‡ æƒé‡
        if args.ema_decay &gt; 0:
            ema.update_model_average(model_ema, model)

        if i % args.n_report_steps == 0:
            print(f"\rEpoch: {epoch}, iter: {i}/{n_iterations}, "
                  f"Loss {loss.item():.2f}, NLL: {nll.item():.2f}, "
                  f"RegTerm: {reg_term.item():.1f}, "
                  f"GradNorm: {grad_norm:.1f}")
        nll_epoch.append(nll.item())
        # ä¿å­˜æ¨¡å‹
        if (epoch % args.test_epochs == 0) and (i % args.visualize_every_batch == 0) and not (epoch == 0 and i == 0) and args.train_diffusion:
            start = time.time()
            if len(args.conditioning) &gt; 0:
                save_and_sample_conditional(args, device, model_ema, prop_dist, dataset_info, epoch=epoch)
            save_and_sample_chain(model_ema, args, device, dataset_info, prop_dist, epoch=epoch,
                                  batch_id=str(i))
            sample_different_sizes_and_save(model_ema, nodes_dist, args, device, dataset_info,
                                            prop_dist, epoch=epoch)
            print(f'Sampling took {time.time() - start:.2f} seconds')

            vis.visualize(f"outputs/{args.exp_name}/epoch_{epoch}_{i}", dataset_info=dataset_info, wandb=wandb)
            vis.visualize_chain(f"outputs/{args.exp_name}/epoch_{epoch}_{i}/chain/", dataset_info, wandb=wandb)
            if len(args.conditioning) &gt; 0:
                vis.visualize_chain("outputs/%s/epoch_%d/conditional/" % (args.exp_name, epoch), dataset_info,
                                    wandb=wandb, mode='conditional')
        wandb.log({"Batch NLL": nll.item()}, commit=True)
        if args.break_train_epoch:
            break
    wandb.log({"Train Epoch NLL": np.mean(nll_epoch)}, commit=False)</code></pre> 
<p></p> 
<p>å…¶ä¸­ï¼Œcompute_loss_and_nllä¸ºè®¡ç®—GeoLDMçš„æŸå¤±ï¼Œè°ƒç”¨GeoLDMçš„forwardå‡½æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŸå¤±ä»…ä¸ºnllé¡¹ï¼Œreg_termçš„å€¼ä¸º0ã€‚compute_loss_and_nllä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">def compute_loss_and_nll(args, generative_model, nodes_dist, x, h, node_mask, edge_mask, context):

    # è®¡ç®—GeoLDMæ¨¡å‹çš„æŸå¤±
    bs, n_nodes, n_dims = x.size()

    if args.probabilistic_model == 'diffusion':
        edge_mask = edge_mask.view(bs, n_nodes * n_nodes)

        assert_correctly_masked(x, node_mask)

        # Here x is a position tensor, and h is a dictionary with keys
        # 'categorical' and 'integer'.
        nll = generative_model(x, h, node_mask, edge_mask, context)

        N = node_mask.squeeze(2).sum(1).long()

        log_pN = nodes_dist.log_prob(N)

        assert nll.size() == log_pN.size()
        nll = nll - log_pN

        # Average over batch.
        nll = nll.mean(0)

        reg_term = torch.tensor([0.]).to(nll.device)
        mean_abs_z = 0.
    else:
        raise ValueError(args.probabilistic_model)

    return nll, reg_term, mean_abs_z
</code></pre> 
<p></p> 
<h5></h5> 
<h5 style="background-color:transparent;">2.2.2.1 GeoLDM æ¨¡å‹ (EnLatentDiffusion)</h5> 
<p>åœ¨qm9_main.py ä¸­ï¼Œå…³äºæ¨¡å‹åŠ è½½çš„ä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">if args.train_diffusion: # é»˜è®¤æ˜¯è®­ç»ƒtrain_diffusionæ¨¡å‹
    model, nodes_dist, prop_dist = get_latent_diffusion(args, device, dataset_info, dataloaders['train'])
else:
    model, nodes_dist, prop_dist = get_autoencoder(args, device, dataset_info, dataloaders['train'])
    </code></pre> 
<p></p> 
<p>args.train_diffusioné»˜è®¤ä¸ºtrue, è®­ç»ƒ VAE å’Œ Diffusion æ¨¡å‹ä¸€èµ·è®­ç»ƒï¼Œå³é‡å¤´å®Œæ•´è®­ç»ƒ GeoLDM æ¨¡å‹ã€‚</p> 
<p></p> 
<p>æ¥ä¸‹æ¥ï¼Œå°±åˆ†å¼€ä»‹ç»get_latent_diffusionè·å–GeoLDMæ¨¡å‹å’Œget_autoencoderè·å–VAEæ¨¡å‹ï¼Œ åˆ†åˆ«çœ‹çœ‹ä»–ä»¬çš„æ¨¡å‹ç»“æ„ã€‚</p> 
<p></p> 
<p>ä¸€ä¸ªå®Œæ•´çš„GeoLDMæ¨¡å‹ç”±VAEæ¨¡å‹å’ŒSE3ç­‰å˜ç½‘ç»œç»„æˆã€‚</p> 
<p></p> 
<p>ç°åœ¨æ¥çœ‹ä¸€ä¸‹ï¼Œè·å–GeoLDMæ¨¡å‹çš„get_latent_diffusionéƒ¨åˆ†çš„ä»£ç ï¼š</p> 
<ol><li>åŠ è½½VAEæ¨¡å‹çš„å‚æ•°pickleæ–‡ä»¶ï¼›</li><li>get_autoencoderï¼Œæ ¹æ®VAEæ¨¡å‹å‚æ•°ï¼Œå®ä¾‹åŒ–VAEæ¨¡å‹ï¼›</li><li>first_stage_model.load_state_dict(flow_state_dict)ï¼‰ï¼Œæ ¹æ®VAEæ¨¡å‹çš„æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ŒåŠ è½½/å®ä¾‹åŒ–VAEæ¨¡å‹ï¼›</li><li>EGNN_dynamics_QM9ï¼Œå®ä¾‹åŒ–ä¸€ä¸ªSE3ç­‰å˜ç½‘ç»œæ¨¡å‹ï¼›</li><li>EnLatentDiffusionï¼ŒåŸºäºVAEæ¨¡å‹å’Œç­‰å˜ç½‘ç»œæ¨¡å‹ï¼Œåˆ›å»ºå‡ ä½•éšå¼æ‰©æ•£æ¨¡å‹ GeoLDMã€‚</li></ol> 
<p></p> 
<pre><code class="language-python">def get_latent_diffusion(args, device, dataset_info, dataloader_train):
    # Create (and load) the first stage model (Autoencoder).
    # å¦‚æœæœ‰AEæ¨¡å‹å‚æ•°pickleæ–‡ä»¶ï¼ˆè®­ç»ƒè¿‡ï¼‰ï¼Œåˆ™åŠ è½½å‚æ•°
    # args.ae_path é»˜è®¤æ˜¯ Noneï¼Œå³AEæ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if args.ae_path is not None: 
        with open(join(args.ae_path, 'args.pickle'), 'rb') as f:
            first_stage_args = pickle.load(f)
    else:
        first_stage_args = args
    
    # CAREFUL with this --&gt;
    # åˆ¤æ–­first_stage_args å‚æ•°ä¸­ï¼Œæ˜¯å¦åŒ…å« normalization_factor å’Œ aggregation_methodå±æ€§
    if not hasattr(first_stage_args, 'normalization_factor'):
        first_stage_args.normalization_factor = 1
    if not hasattr(first_stage_args, 'aggregation_method'):
        first_stage_args.aggregation_method = 'sum'

    # device å·²ç»æ˜¯ä¼ å‚è¿›æ¥äº†ï¼Œæ— éœ€è‡ªå®šä¹‰
    # device = torch.device("cuda" if first_stage_args.cuda else "cpu")

    # å®ä¾‹åŒ–AEæ¨¡å‹ï¼Œlatentå‘é‡ä¸ºç»´åº¦ä¸ºlatent_nf
    first_stage_model, nodes_dist, prop_dist = get_autoencoder(
        first_stage_args, device, dataset_info, dataloader_train)
    first_stage_model.to(device)

    # å¦‚æœAEæ¨¡å‹å·²ç»è®­ç»ƒè¿‡ï¼ˆargs.ae_pathï¼‰ï¼Œåˆ™åŠ è½½AEæ¨¡å‹å‚æ•°
    if args.ae_path is not None:
        fn = 'generative_model_ema.npy' if first_stage_args.ema_decay &gt; 0 else 'generative_model.npy'
        flow_state_dict = torch.load(join(args.ae_path, fn),
                                        map_location=device)
        first_stage_model.load_state_dict(flow_state_dict)

    # Create the second stage model (Latent Diffusions).
    # AEæ¨¡å‹è¾“å‡ºçš„å‘é‡ç»´åº¦æ˜¯ï¼Œlatent_nfï¼ŒLatent Diffusionsä»¥æ­¤ä½œä¸ºè¾“å…¥
    args.latent_nf = first_stage_args.latent_nf
    in_node_nf = args.latent_nf

    if args.condition_time:
        # æ—¶é—´æ¡ä»¶
        dynamics_in_node_nf = in_node_nf + 1
    else:
        print('Warning: dynamics model is _not_ conditioned on time.')
        dynamics_in_node_nf = in_node_nf

    # å®ä¾‹åŒ–ä¸€ä¸ªSE3ç­‰å˜ç½‘ç»œæ¨¡å‹ï¼ŒEGNN_dynamics_QM9
    net_dynamics = EGNN_dynamics_QM9(
        in_node_nf=dynamics_in_node_nf, 
        context_node_nf=args.context_node_nf,
        n_dims=3, device=device, 
        hidden_nf=args.nf,
        act_fn=torch.nn.SiLU(), 
        n_layers=args.n_layers,
        attention=args.attention, 
        tanh=args.tanh, 
        mode=args.model, 
        norm_constant=args.norm_constant,
        inv_sublayers=args.inv_sublayers, 
        sin_embedding=args.sin_embedding,
        normalization_factor=args.normalization_factor, 
        aggregation_method=args.aggregation_method)

    # åŸºäºä¹‹å‰å®ä¾‹å¥½çš„SE3ç­‰å˜ç½‘ç»œæ¨¡å‹ï¼ŒEGNN_dynamics_QM9ï¼Œ å’Œ VAE ç½‘ç»œï¼Œ
    # åˆ›å»ºä¸€ä¸ª GeoLDM ç½‘ç»œï¼Œ å³ï¼Œ å‡ ä½•éšå¼æ‰©æ•£æ¨¡å‹ï¼Œ vdm
    if args.probabilistic_model == 'diffusion':
        vdm = EnLatentDiffusion(
            vae=first_stage_model, # VAE æ¨¡å‹
            trainable_ae=args.trainable_ae,
            dynamics=net_dynamics, # SE3ç­‰å˜ç½‘ç»œæ¨¡å‹
            in_node_nf=in_node_nf,
            n_dims=3,
            timesteps=args.diffusion_steps,
            noise_schedule=args.diffusion_noise_schedule,
            noise_precision=args.diffusion_noise_precision,
            loss_type=args.diffusion_loss_type,
            norm_values=args.normalize_factors,
            include_charges=args.include_charges
            )

        return vdm, nodes_dist, prop_dist

    else:
        raise ValueError(args.probabilistic_model)</code></pre> 
<p></p> 
<p>EnLatentDiffusionç±»æ˜¯GeoLDMæ¨¡å‹ï¼Œç»§æ‰¿äºEnVariationalDiffusionç±»ã€‚å› ä¸ºGeoLDMæ¨¡å‹æ˜¯åœ¨SE3ç­‰å˜æ‰©æ•£ç½‘ç»œä¸Šï¼Œå†æ·»åŠ äº†ä¸€ä¸ªVAEæ¨¡å—ï¼ˆç”±å‚æ•°vaeä¼ å…¥ï¼‰ï¼Œæ‰€ä»¥åœ¨EnLatentDiffusionç±»ä¸­ï¼Œéœ€è¦æ·»åŠ äº†å‡ ä¸ªæ–¹æ³•ï¼Œä»¥è¦†ç›–EnVariationalDiffusionçš„æ–¹æ³•ï¼Œæ¯”å¦‚forwadè®¡ç®—æŸå¤±ï¼Œsampleï¼Œ sample_chainé‡‡æ ·/ç”Ÿæˆåˆ†å­ã€‚EnVariationalDiffusionç±»å’ŒEnHierarchicalVAEç±»æ„æˆäº†GeoLDMæ¨¡å‹ï¼ˆEnLatentDiffusionï¼‰ã€‚</p> 
<p></p> 
<p>EnLatentDiffusionå®Œæ•´ä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">class EnLatentDiffusion(EnVariationalDiffusion):
    '''
    å‡ ä½•éšå¼æ‰©æ•£ç½‘ç»œ GeoLDM
    ç»§æ‰¿ç­‰å˜æ‰©æ•£ç½‘ç»œ EnVariationalDiffusion
    '''
    """
    The E(n) Latent Diffusion Module.
    """
    def __init__(self, **kwargs):
        # å‚æ•°ä¸­å¦‚æœæœ‰vaeæ¨¡å‹ï¼Œåˆ™è¿”å›ç»™vaeå˜é‡ï¼Œä»å‚æ•°ä¸­åˆ é™¤
        vae = kwargs.pop('vae') 
        # å‚æ•°ä¸­å¦‚æœæœ‰trainable_aeåˆ™è¿”å›ç»™trainable_aeå˜é‡ï¼Œä»å‚æ•°ä¸­åˆ é™¤ã€‚å¦‚æœæ²¡æœ‰ï¼Œåˆ™trainable_aeä¸ºFalse
        trainable_ae = kwargs.pop('trainable_ae', False)
        super().__init__(**kwargs)

        # Create self.vae as the first stage model.
        self.trainable_ae = trainable_ae
        self.instantiate_first_stage(vae) # VAE æ¨¡å‹åˆå§‹åŒ–è®¾ç½®ï¼Œ æ¢¯åº¦è®¾ç½®ï¼ˆéœ€è¦æ¢¯åº¦/æ— éœ€æ¢¯åº¦ï¼‰
    
    def unnormalize_z(self, z, node_mask):
        # å•¥éƒ½ä¸åšï¼Œåªæ˜¯ä¸ºäº†è¦†ç›–ä¹‹å‰çš„unnormalize_zï¼Œ ç”¨åœ¨sample_chainï¼›
        # Overwrite the unnormalize_z function to do nothing (for sample_chain). 
        # Parse from z
        x, h_cat = z[:, :, 0:self.n_dims], z[:, :, self.n_dims:self.n_dims+self.num_classes]
        h_int = z[:, :, self.n_dims+self.num_classes:self.n_dims+self.num_classes+1]
        assert h_int.size(2) == self.include_charges

        # Unnormalize ï¼Ÿï¼Ÿï¼Ÿä¸ºä»€ä¹ˆæ³¨é‡Šæ‰äº†ï¼Ÿï¼Ÿ
        # x, h_cat, h_int = self.unnormalize(x, h_cat, h_int, node_mask)
        output = torch.cat([x, h_cat, h_int], dim=2)
        return output
    
    def log_constants_p_h_given_z0(self, h, node_mask):
        """Computes p(h|z0)."""
        batch_size = h.size(0)

        n_nodes = node_mask.squeeze(2).sum(1)  # N has shape [B] Bä¸ºæ¯ä¸ªåˆ†å­çš„åŸå­æ•° ï¼ˆNï¼ŒBï¼‰
        assert n_nodes.size() == (batch_size,)
        degrees_of_freedom_h = n_nodes * self.n_dims

        zeros = torch.zeros((h.size(0), 1), device=h.device)
        gamma_0 = self.gamma(zeros)

        # Recall that sigma_x = sqrt(sigma_0^2 / alpha_0^2) = SNR(-0.5 gamma_0).
        log_sigma_x = 0.5 * gamma_0.view(batch_size)

        return degrees_of_freedom_h * (- log_sigma_x - 0.5 * np.log(2 * np.pi))

    def sample_p_xh_given_z0(self, z0, node_mask, edge_mask, context, fix_noise=False):
        """Samples x ~ p(x|z0)."""
        zeros = torch.zeros(size=(z0.size(0), 1), device=z0.device)
        gamma_0 = self.gamma(zeros)
        # Computes sqrt(sigma_0^2 / alpha_0^2)
        sigma_x = self.SNR(-0.5 * gamma_0).unsqueeze(1)
        net_out = self.phi(z0, zeros, node_mask, edge_mask, context)

        # Compute mu for p(zs | zt).
        mu_x = self.compute_x_pred(net_out, z0, gamma_0)
        xh = self.sample_normal(mu=mu_x, sigma=sigma_x, node_mask=node_mask, fix_noise=fix_noise)

        x = xh[:, :, :self.n_dims]

        # h_int = z0[:, :, -1:] if self.include_charges else torch.zeros(0).to(z0.device)
        # x, h_cat, h_int = self.unnormalize(x, z0[:, :, self.n_dims:-1], h_int, node_mask)

        # h_cat = F.one_hot(torch.argmax(h_cat, dim=2), self.num_classes) * node_mask
        # h_int = torch.round(h_int).long() * node_mask

        # Make the data structure compatible with the EnVariationalDiffusion sample() and sample_chain().
        h = {'integer': xh[:, :, self.n_dims:], 'categorical': torch.zeros(0).to(xh)}
        
        return x, h
    
    def log_pxh_given_z0_without_constants(
            self, x, h, z_t, gamma_0, eps, net_out, node_mask, epsilon=1e-10):

        # Computes the error for the distribution N(latent | 1 / alpha_0 z_0 + sigma_0/alpha_0 eps_0, sigma_0 / alpha_0),
        # the weighting in the epsilon parametrization is exactly '1'.
        log_pxh_given_z_without_constants = -0.5 * self.compute_error(net_out, gamma_0, eps)

        # Combine log probabilities for x and h.
        log_p_xh_given_z = log_pxh_given_z_without_constants

        return log_p_xh_given_z
    
    def forward(self, x, h, node_mask=None, edge_mask=None, context=None):
        """
        train æ—¶è®¡ç®—çš„æŸå¤±ï¼ˆç±»å‹ l2 æˆ– NLLï¼‰ã€‚ å¦‚æœ eval åˆ™å§‹ç»ˆè®¡ç®— NLLã€‚
        Computes the loss (type l2 or NLL) if training. And if eval then always computes NLL.
        """

        # Encode data to latent space. å°†x,h ç¼–ç åˆ°éšç©ºé—´ï¼Œåˆ†åˆ«å¾—åˆ°xå’Œhçš„å‡å€¼å’Œæ ‡å·®
        z_x_mu, z_x_sigma, z_h_mu, z_h_sigma = self.vae.encode(x, h, node_mask, edge_mask, context)
        # Compute fixed sigma values.
        t_zeros = torch.zeros(size=(x.size(0), 1), device=x.device)
        gamma_0 = self.inflate_batch_array(self.gamma(t_zeros), x)
        sigma_0 = self.sigma(gamma_0, x)

        # Infer latent z.
        z_xh_mean = torch.cat([z_x_mu, z_h_mu], dim=2)
        diffusion_utils.assert_correctly_masked(z_xh_mean, node_mask)
        z_xh_sigma = sigma_0
        # z_xh_sigma = torch.cat([z_x_sigma.expand(-1, -1, 3), z_h_sigma], dim=2)
        z_xh = self.vae.sample_normal(z_xh_mean, z_xh_sigma, node_mask)
        # z_xh = z_xh_mean
        z_xh = z_xh.detach()  # Always keep the encoder fixed.
        diffusion_utils.assert_correctly_masked(z_xh, node_mask)

        # Compute reconstruction loss.
        if self.trainable_ae:
            xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
            # Decoder output (reconstruction).
            x_recon, h_recon = self.vae.decoder._forward(z_xh, node_mask, edge_mask, context)
            xh_rec = torch.cat([x_recon, h_recon], dim=2)
            loss_recon = self.vae.compute_reconstruction_error(xh_rec, xh)
        else:
            loss_recon = 0

        z_x = z_xh[:, :, :self.n_dims]
        z_h = z_xh[:, :, self.n_dims:]
        diffusion_utils.assert_mean_zero_with_mask(z_x, node_mask)
        # Make the data structure compatible with the EnVariationalDiffusion compute_loss().
        z_h = {'categorical': torch.zeros(0).to(z_h), 'integer': z_h}

        if self.training:
            # Only 1 forward pass when t0_always is False.
            loss_ld, loss_dict = self.compute_loss(z_x, z_h, node_mask, edge_mask, context, t0_always=False)
        else:
            # Less variance in the estimator, costs two forward passes.
            loss_ld, loss_dict = self.compute_loss(z_x, z_h, node_mask, edge_mask, context, t0_always=True)
        
        # The _constants_ depending on sigma_0 from the
        # cross entropy term E_q(z0 | x) [log p(x | z0)].
        neg_log_constants = -self.log_constants_p_h_given_z0(
            torch.cat([h['categorical'], h['integer']], dim=2), node_mask)
        # Reset constants during training with l2 loss.
        if self.training and self.loss_type == 'l2':
            neg_log_constants = torch.zeros_like(neg_log_constants)

        neg_log_pxh = loss_ld + loss_recon + neg_log_constants

        return neg_log_pxh
    
    @torch.no_grad()
    def sample(self, n_samples, n_nodes, node_mask, edge_mask, context, fix_noise=False):
        """
        Draw samples from the generative model.
        """
        z_x, z_h = super().sample(n_samples, n_nodes, node_mask, edge_mask, context, fix_noise)

        z_xh = torch.cat([z_x, z_h['categorical'], z_h['integer']], dim=2)
        diffusion_utils.assert_correctly_masked(z_xh, node_mask)
        x, h = self.vae.decode(z_xh, node_mask, edge_mask, context)

        return x, h
    
    @torch.no_grad()
    def sample_chain(self, n_samples, n_nodes, node_mask, edge_mask, context, keep_frames=None):
        """
        Draw samples from the generative model, keep the intermediate states for visualization purposes.
        """
        chain_flat = super().sample_chain(n_samples, n_nodes, node_mask, edge_mask, context, keep_frames)

        # xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
        # chain[0] = xh  # Overwrite last frame with the resulting x and h.

        # chain_flat = chain.view(n_samples * keep_frames, *z.size()[1:])

        chain = chain_flat.view(keep_frames, n_samples, *chain_flat.size()[1:])
        chain_decoded = torch.zeros(
            size=(*chain.size()[:-1], self.vae.in_node_nf + self.vae.n_dims), device=chain.device)

        for i in range(keep_frames):
            z_xh = chain[i]
            diffusion_utils.assert_mean_zero_with_mask(z_xh[:, :, :self.n_dims], node_mask)

            x, h = self.vae.decode(z_xh, node_mask, edge_mask, context)
            xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
            chain_decoded[i] = xh
        
        chain_decoded_flat = chain_decoded.view(n_samples * keep_frames, *chain_decoded.size()[2:])

        return chain_decoded_flat

    def instantiate_first_stage(self, vae: EnHierarchicalVAE):
        '''
        VAE æ¨¡å‹åˆå§‹åŒ–è®¾ç½®ï¼Œ æ¢¯åº¦è®¾ç½®
        '''
        if not self.trainable_ae:
            # VAE æ¨¡å‹ä¸å¯è®­ç»ƒï¼Œ æ— éœ€æ¢¯åº¦
            self.vae = vae.eval()
            self.vae.train = disabled_train
            for param in self.vae.parameters():
                param.requires_grad = False
        else:
            # VAE æ¨¡å‹å¯è®­ç»ƒï¼Œ éœ€æ¢¯åº¦
            self.vae = vae.train()
            for param in self.vae.parameters():
                param.requires_grad = True
</code></pre> 
<p></p> 
<p>ä½œä¸ºä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬éå¸¸å…³å¿ƒï¼Œæ¨¡å‹çš„æŸå¤±æ˜¯ä»€ä¹ˆï¼Ÿæ€ä¹ˆè®¡ç®—çš„ï¼Ÿåˆ†å­æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Ÿ</p> 
<p></p> 
<p>EnLatentDiffusionä¸­ï¼Œæ¨¡å‹æŸå¤±ç”±forwadå‡½æ•°å®Œæˆï¼Œä¸‹é¢è¯¦ç»†ä»‹ç»ä¸€ä¸‹ã€‚åœ¨forwadå‡½æ•°ä¸­ï¼š</p> 
<p>ï¼ˆ1ï¼‰å°†xhè¾“å…¥åˆ°ç¼–ç å™¨ï¼Œè¾“å‡ºxå’Œhçš„å‡å€¼å’Œæ ‡å‡†å·®z_x_mu, z_x_sigma, z_h_mu, z_h_sigmaï¼›</p> 
<p>ï¼ˆ2ï¼‰åŸºäºz_x_muå’Œz_h_muï¼ŒVAEæ¨¡å‹sample_normalé‡‡æ ·æ–°çš„z_xhï¼›</p> 
<p>ï¼ˆ3ï¼‰å¦‚æœè®­ç»ƒVAEæ¨¡å‹ï¼Œé‚£ä¹ˆè®¡ç®—VAEæ¨¡å‹é‡æ„æŸå¤±ï¼Œç”±VAEæ¨¡å‹çš„compute_reconstruction_errorå®Œæˆï¼›</p> 
<p>ï¼ˆ4ï¼‰æ‹†åˆ†z_xhä¸ºz_xå’Œz_hï¼Œç„¶åè¾“å…¥åˆ°SE3ç­‰å˜æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç”±SE3ç­‰å˜æ‰©æ•£æ¨¡å‹çš„compute_lossè®¡ç®—SE3ç­‰å˜æ‰©æ•£æ¨¡å‹çš„æŸå¤±ï¼›</p> 
<p>ï¼ˆ5ï¼‰è®¡ç®—z_0æ—¶åˆ»çš„å¸¸æ•°é¡¹è´Ÿå¯¹æ•°æŸå¤±ï¼›</p> 
<p>ï¼ˆ6ï¼‰åˆå¹¶æ‰€æœ‰æŸå¤±ï¼›</p> 
<p></p> 
<p>é‡‡æ ·ç”±sampleå‡½æ•°å®Œæˆï¼Œè°ƒç”¨äº†SE3ç­‰å˜æ‰©æ•£æ¨¡å‹çš„sampleå‡½æ•°ï¼Œé‡‡æ ·éšå¼å‘é‡z_x, z_hã€‚ç„¶åç»è¿‡VAEçš„è§£ç å™¨ï¼Œè·å¾—ç”Ÿæˆçš„xå’Œhã€‚åŸºäºxå’Œhå°±å¯ä»¥åˆ©ç”¨obabelè½¬åŒ–æˆåˆ†å­ï¼ˆè¿™ä¸€éƒ¨åˆ†åŒ…å«åœ¨åˆ†å­ç”Ÿæˆéƒ¨åˆ†çš„ä»£ç ä¸­ï¼‰ã€‚</p> 
<p></p> 
<h5></h5> 
<h5 style="background-color:transparent;">2.2.2.2 SE3ç­‰å˜æ‰©æ•£æ¨¡å‹ï¼ˆEnVariationalDiffusionï¼‰</h5> 
<p>ä¸‹é¢å¼€å§‹è¯¦ç»†ä»‹ç»EnVariationalDiffusionã€‚</p> 
<p></p> 
<p>EnVariationalDiffusionæ˜¯z_x,z_héšå‘é‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå’Œä¹‹å‰ä»‹ç»çš„åˆ†å­ç”Ÿæˆæ¨¡å‹çš„æ‰©æ•£æ¨¡å‹å¾ˆåƒï¼ŒåŒ…å«äº†phiï¼Œunnormalizeï¼Œ normalizeç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬å¿«é€Ÿç®€å•ä»‹ç»ä¸€ä¸‹ï¼Œè¿™ä¸€éƒ¨åˆ†ï¼Œå¤§å®¶å…¶å®ç›´æ¥ä½¿ç”¨å³å¯ï¼Œåœ¨è°ƒæ•´æ¨¡å‹æ—¶ï¼ŒåŸºæœ¬ä¸Šä¸ä¼šåŠ¨ã€‚å€Ÿæ­¤æœºä¼šï¼Œæˆ‘ä»¬ä¹Ÿç®€å•ä»‹ç»ä¸€ä¸‹åŸºäºSE3ç­‰å˜ç½‘ç»œçš„ä»£ç æ¶æ„ï¼Œç‰¹åˆ«æ˜¯è®¡ç®—æŸå¤±å’Œé‡‡æ ·ã€‚</p> 
<p></p> 
<p>é¦–å…ˆï¼Œæ¨¡å‹çš„å®šä¹‰ï¼ŒEnVariationalDiffusion __init__():å®šä¹‰äº†åˆ†å­ç”ŸæˆSE3æ‰©æ•£æ¨¡å‹ï¼ŒSE3ç½‘ç»œæ¥è‡ªäºEGNN_dynamics_QM9ã€‚è¾“å…¥å‚æ•°æœ‰ï¼š</p> 
<p>in_node_nf: è¡¨ç¤ºè¾“å…¥èŠ‚ç‚¹çš„ç‰¹å¾æ•°ï¼›</p> 
<p>n_dims: è¡¨ç¤ºåæ ‡çš„ç»´åº¦ï¼Œé€šå¸¸ä¸º 3ï¼›</p> 
<p>timesteps: æ‰©æ•£æ­¥æ•°ï¼Œé»˜è®¤ä¸º 1000ï¼›</p> 
<p>parametrization: å‚æ•°åŒ–çš„æ–¹å¼ï¼Œæ­¤å¤„ä¼¼ä¹åªæ”¯æŒ 'eps'ï¼Œå³SE3é¢„æµ‹çš„æ˜¯å™ªéŸ³è¿˜æ˜¯x,hï¼›</p> 
<p>noise_schedule: å™ªéŸ³è°ƒåº¦ç­–ç•¥ï¼Œå¯ä»¥æ˜¯ 'learned' æˆ–å…¶ä»–é¢„å®šä¹‰çš„ç­–ç•¥ï¼›</p> 
<p>noise_precision: å™ªéŸ³ç²¾åº¦ï¼Œé»˜è®¤ä¸º 1e-4ï¼›</p> 
<p>norm_values: æ­£åˆ™åŒ–å€¼çš„å…ƒç»„ï¼Œç”¨äºè§„èŒƒåŒ–è¾“å…¥æ•°æ®ï¼›</p> 
<p>norm_biases: æ­£åˆ™åŒ–åå·®çš„å…ƒç»„ï¼Œä¹Ÿç”¨äºè§„èŒƒåŒ–è¾“å…¥æ•°æ®ï¼›</p> 
<p>include_charges: ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºèŠ‚ç‚¹ç‰¹å¾ä¸­æ˜¯å¦åŒ…å«ç”µè·ä¿¡æ¯ï¼›</p> 
<pre><code class="language-python">class EnVariationalDiffusion(torch.nn.Module):
    """
    ç­‰å˜æ‰©æ•£ç±»
    The E(n) Diffusion Module.
    """
    def __init__(
            self,
            dynamics: models.EGNN_dynamics_QM9, in_node_nf: int, n_dims: int,
            timesteps: int = 1000, parametrization='eps', noise_schedule='learned',
            noise_precision=1e-4, loss_type='vlb', norm_values=(1., 1., 1.),
            norm_biases=(None, 0., 0.), include_charges=True):
        super().__init__()

        # æŸå¤±ç±»å‹
        assert loss_type in {'vlb', 'l2'}
        self.loss_type = loss_type
        # èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦åŒ…å«ç”µè·charge
        self.include_charges = include_charges
        # å™ªéŸ³è°ƒåº¦å™¨noise_scheduleæ˜¯å¦å¯è®­ç»ƒï¼›
        # å¯è®­ç»ƒä¸º GammaNetwork()
        # ä¸å¯è®­ç»ƒä¸º PredefinedNoiseSchedule()
        if noise_schedule == 'learned':
            assert loss_type == 'vlb', 'A noise schedule can only be learned' \
                                       ' with a vlb objective.'

        # Only supported parametrization.
        assert parametrization == 'eps'

        if noise_schedule == 'learned':
            self.gamma = GammaNetwork()
        else:
            self.gamma = PredefinedNoiseSchedule(noise_schedule, timesteps=timesteps,
                                                 precision=noise_precision)

        # The network that will predict the denoising.
        # é¢„æµ‹å™ªéŸ³çš„æ¨¡å‹ï¼Œå³SE3æ¨¡å‹
        self.dynamics = dynamics

        # èŠ‚ç‚¹åŸå­ç±»å‹æ•°ï¼Œå«ç”µè·
        self.in_node_nf = in_node_nf
        # åæ ‡ç»´åº¦ï¼Œ3
        self.n_dims = n_dims
        # åŸå­ç§ç±»æ•°
        self.num_classes = self.in_node_nf - self.include_charges

        # æ‰©æ•£æ­¥æ•°
        self.T = timesteps
        self.parametrization = parametrization

        # æ­£åˆ™åŒ–å‚æ•°ï¼Œ norm_valuesï¼Œ norm_biases
        self.norm_values = norm_values
        self.norm_biases = norm_biases
        self.register_buffer('buffer', torch.zeros(1))

        # æ£€æŸ¥æ­£åˆ™åŒ–norm_valuesæ˜¯å¦åˆé€‚
        if noise_schedule != 'learned':
            self.check_issues_norm_values()
</code></pre> 
<p></p> 
<p>åœ¨çœ‹æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éå¸¸å…³å¿ƒæŸå¤±ï¼Œæ¨¡å‹çš„æŸå¤±æ˜¯ä»€ä¹ˆï¼Ÿæ€ä¹ˆè®¡ç®—çš„ï¼ŸEnVariationalDiffusionæ¨¡å‹çš„æŸå¤±æ˜¯åœ¨forward å‡½æ•°ä¸­ã€‚å…³äºï¼ŒEnVariationalDiffusion.forwardå‡½æ•°ï¼š</p> 
<p>ï¼ˆ1ï¼‰æ­£åˆ™åŒ–è¾“å…¥çš„x,hï¼›</p> 
<p>ï¼ˆ2ï¼‰è°ƒç”¨compute_lossè®¡ç®—è´Ÿå¯¹æ•°æŸå¤±æˆ–è€…l2æŸå¤±ã€‚å¦‚æœæ˜¯è®­ç»ƒæ—¶ï¼Œt0_always=Trueï¼Œå¦åˆ™t0_alwaysä¸ºFalseï¼›</p> 
<pre><code class="language-python"> def forward(self, x, h, node_mask=None, edge_mask=None, context=None):
        """
        è®­ç»ƒæ—¶ï¼Œè®¡ç®—l2æŸå¤±æˆ–è€…æ˜¯è´Ÿå¯¹æ•°æŸå¤±NLLï¼Œå¦åˆ™è®¡ç®—è´Ÿå¯¹æ•°æŸå¤±
        Computes the loss (type l2 or NLL) if training. And if eval then always computes NLL.
        """
        # Normalize data, take into account volume change in x.
        # æ­£åˆ™åŒ–è¾“å…¥æ•°æ®ï¼Œx,hï¼Œä¸ºäº†å‡å°‘ä½“ç§¯çš„å½±å“
        x, h, delta_log_px = self.normalize(x, h, node_mask)

        # Reset delta_log_px if not vlb objective.
        if self.training and self.loss_type == 'l2':
            delta_log_px = torch.zeros_like(delta_log_px)

        # è®¡ç®—æŸå¤±
        if self.training:
            # Only 1 forward pass when t0_always is False.
            loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context, t0_always=False)
        else:
            # Less variance in the estimator, costs two forward passes.
            loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context, t0_always=True)

        neg_log_pxh = loss

        # Correct for normalization on x.
        assert neg_log_pxh.size() == delta_log_px.size()
        neg_log_pxh = neg_log_pxh - delta_log_px

        return neg_log_pxh</code></pre> 
<p></p> 
<p>EnVariationalDiffusion.compute_loss()å…·ä½“è®¡ç®—æŸå¤±ï¼Œè®¡ç®—æŸå¤±çš„æµç¨‹ã€‚</p> 
<p>ï¼ˆ1ï¼‰åˆå§‹åŒ–ï¼Œéšæœºæ—¶é—´tï¼Œé€šè¿‡self.alphaï¼Œ self.sigmaç”Ÿæˆå™ªéŸ³epsï¼›</p> 
<p>ï¼ˆ2ï¼‰å°†å™ªéŸ³æ•´åˆåˆ°z_x,z_hä¸­ï¼Œå³ä»£ç ï¼šz_t = alpha_t * xh + sigma_t * epsï¼Œç”Ÿæˆç‰¹å®šæ—¶é—´æ­¥æ­¥å«æœ‰å™ªéŸ³çŠ¶æ€çš„z_tï¼›</p> 
<p>ï¼ˆ3ï¼‰SE3ç½‘ç»œé¢„æµ‹å™ªéŸ³ï¼Œå³net_outï¼›</p> 
<p>ï¼ˆ4ï¼‰è°ƒç”¨compute_errorè®¡ç®—net_outå’Œepsä¹‹é—´çš„l2æˆ–è€…mseæŸå¤±ï¼›l2æŸå¤±ä¸ºmseæŸå¤±æ­£åˆ™åŒ–æƒ©ç½šçš„ç»“æœï¼›</p> 
<p>ï¼ˆ5ï¼‰è®¡ç®—è´Ÿå¯¹æ•°å¸¸æ•°æŸå¤±ï¼ˆz_0ï¼‰ï¼›</p> 
<p>ï¼ˆ6ï¼‰è®¡ç®—xhçš„KLæŸå¤±ï¼›</p> 
<p>ï¼ˆ7ï¼‰estimator_loss_termsæŸå¤±ï¼›</p> 
<p>åˆå¹¶æ‰€æœ‰æŸå¤±è¿”å›lossï¼Œå…¶ä¸­ï¼Œerrorä¸ºæ¨¡å‹é¢„æµ‹å™ªéŸ³éƒ¨åˆ†çš„æŸå¤±ã€‚å› æ­¤ï¼Œ<strong>SE3ç­‰å˜æ‰©æ•£ç½‘ç»œçš„æŸå¤±åŒ…å«äº†ï¼šå™ªéŸ³çš„é¢„æµ‹çš„mse/l2æŸå¤±ï¼Œæ¨¡å‹çš„å¸¸æ•°é¡¹è´Ÿå¯¹æ•°æŸå¤±ï¼Œxhçš„KLæ•£åº¦æŸå¤±ï¼›</strong></p> 
<pre><code class="language-python">def compute_loss(self, x, h, node_mask, edge_mask, context, t0_always):
        # æ‰©æ•£æ¨¡å‹çš„æŸå¤±ï¼Œè¾“å…¥x, h
        """Computes an estimator for the variational lower bound, or the simple loss (MSE)."""

        # This part is about whether to include loss term 0 always.
        if t0_always:
            # loss_term_0 will be computed separately.
            # estimator = loss_0 + loss_t,  where t ~ U({1, ..., T})
            lowest_t = 1
        else:
            # estimator = loss_t,           where t ~ U({0, ..., T})
            lowest_t = 0
        # 1. éšæœºåˆå§‹åŒ–tï¼Œç”Ÿæˆå™ªéŸ³
        # Sample a timestep t.
        t_int = torch.randint(
            lowest_t, self.T + 1, size=(x.size(0), 1), device=x.device).float()
        s_int = t_int - 1
        t_is_zero = (t_int == 0).float()  # Important to compute log p(x | z0).

        # Normalize t to [0, 1]. Note that the negative
        # step of s will never be used, since then p(x | z0) is computed.
        s = s_int / self.T
        t = t_int / self.T

        # Compute gamma_s and gamma_t via the network.
        gamma_s = self.inflate_batch_array(self.gamma(s), x)
        gamma_t = self.inflate_batch_array(self.gamma(t), x)

        # Compute alpha_t and sigma_t from gamma.
        alpha_t = self.alpha(gamma_t, x)
        sigma_t = self.sigma(gamma_t, x)

        # Sample zt ~ Normal(alpha_t x, sigma_t) å™ªéŸ³
        eps = self.sample_combined_position_feature_noise(
            n_samples=x.size(0), n_nodes=x.size(1), node_mask=node_mask)

        # Concatenate x, h[integer] and h[categorical].
        xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
        # 2. å°†å™ªéŸ³æ·»åŠ åˆ°xhä¸­ï¼Œç”Ÿæˆz_xh
        # Sample z_t given x, h for timestep t, from q(z_t | x, h)
        z_t = alpha_t * xh + sigma_t * eps

        diffusion_utils.assert_mean_zero_with_mask(z_t[:, :, :self.n_dims], node_mask)

        # 3. se3ç½‘ç»œé¢„æµ‹å™ªéŸ³
        # Neural net prediction.
        net_out = self.phi(z_t, t, node_mask, edge_mask, context)

        # 4. se3ç½‘ç»œè®¡ç®—æŸå¤± l2 æˆ–è€… mseæŸå¤±ï¼›
        # 4.1 L2 æˆ–è€… mse æŸå¤±
        # Compute the error.
        error = self.compute_error(net_out, gamma_t, eps)

        if self.training and self.loss_type == 'l2':
            SNR_weight = torch.ones_like(error)
        else:
            # Compute weighting with SNR: (SNR(s-t) - 1) for epsilon parametrization.
            SNR_weight = (self.SNR(gamma_s - gamma_t) - 1).squeeze(1).squeeze(1)
        assert error.size() == SNR_weight.size()
        loss_t_larger_than_zero = 0.5 * SNR_weight * error

        # 4.2 è´Ÿå¯¹æ•°å¸¸æ•°æŸå¤±
        # The _constants_ depending on sigma_0 from the
        # cross entropy term E_q(z0 | x) [log p(x | z0)].
        neg_log_constants = -self.log_constants_p_x_given_z0(x, node_mask)

        # Reset constants during training with l2 loss.
        if self.training and self.loss_type == 'l2':
            neg_log_constants = torch.zeros_like(neg_log_constants)

        # 4.3 xhçš„KLæ•£åº¦æŸå¤±
        # The KL between q(z1 | x) and p(z1) = Normal(0, 1). Should be close to zero. 
        kl_prior = self.kl_prior(xh, node_mask)

        # Combining the terms
        if t0_always:
            loss_t = loss_t_larger_than_zero
            num_terms = self.T  # Since t=0 is not included here.
            estimator_loss_terms = num_terms * loss_t

            # Compute noise values for t = 0.
            t_zeros = torch.zeros_like(s)
            gamma_0 = self.inflate_batch_array(self.gamma(t_zeros), x)
            alpha_0 = self.alpha(gamma_0, x)
            sigma_0 = self.sigma(gamma_0, x)

            # Sample z_0 given x, h for timestep t, from q(z_t | x, h)
            eps_0 = self.sample_combined_position_feature_noise(
                n_samples=x.size(0), n_nodes=x.size(1), node_mask=node_mask)
            z_0 = alpha_0 * xh + sigma_0 * eps_0

            net_out = self.phi(z_0, t_zeros, node_mask, edge_mask, context)

            # z_0æ—¶åˆ»çš„è´Ÿå¯¹æ•°å¸¸æ•°æŸå¤±
            loss_term_0 = -self.log_pxh_given_z0_without_constants(
                x, h, z_0, gamma_0, eps_0, net_out, node_mask)

            assert kl_prior.size() == estimator_loss_terms.size()
            assert kl_prior.size() == neg_log_constants.size()
            assert kl_prior.size() == loss_term_0.size()

            loss = kl_prior + estimator_loss_terms + neg_log_constants + loss_term_0

        else:
            # Computes the L_0 term (even if gamma_t is not actually gamma_0)
            # and this will later be selected via masking.
            loss_term_0 = -self.log_pxh_given_z0_without_constants(
                x, h, z_t, gamma_t, eps, net_out, node_mask)

            t_is_not_zero = 1 - t_is_zero

            loss_t = loss_term_0 * t_is_zero.squeeze() + t_is_not_zero.squeeze() * loss_t_larger_than_zero

            # Only upweigh estimator if using the vlb objective.
            if self.training and self.loss_type == 'l2':
                estimator_loss_terms = loss_t
            else:
                num_terms = self.T + 1  # Includes t = 0.
                estimator_loss_terms = num_terms * loss_t

            assert kl_prior.size() == estimator_loss_terms.size()
            assert kl_prior.size() == neg_log_constants.size()

            loss = kl_prior + estimator_loss_terms + neg_log_constants

        assert len(loss.shape) == 1, f'{loss.shape} has more than only batch dim.'

        return loss, {'t': t_int.squeeze(), 'loss_t': loss.squeeze(),
                      'error': error.squeeze()}</code></pre> 
<p></p> 
<p>å¯¹å™ªéŸ³é¢„æµ‹çš„mse/l2æŸå¤±ä»£ç å¦‚ä¸‹ã€‚å…¶ä¸­ï¼Œsum_except_batchå‡½æ•°ä¸ºå°†æ‰¹æ¬¡å±•å¼€è®¡ç®—æŸå¤±ã€‚</p> 
<pre><code class="language-python">def compute_error(self, net_out, gamma_t, eps):
        """Computes error, i.e. the most likely prediction of x."""
        eps_t = net_out
        if self.training and self.loss_type == 'l2':
            denom = (self.n_dims + self.in_node_nf) * eps_t.shape[1]
            error = sum_except_batch((eps - eps_t) ** 2) / denom
        else:
            error = sum_except_batch((eps - eps_t) ** 2)
        return error

def sum_except_batch(x):
    return x.view(x.size(0), -1).sum(-1)</code></pre> 
<p></p> 
<p>è®¡ç®—xhçš„KLæ•£åº¦æŸå¤±ï¼š</p> 
<p>å™ªéŸ³è°ƒåº¦å™¨å¾€xhä¸­æ·»åŠ å™ªéŸ³æ—¶ï¼Œæ˜¯å¦ä¿æŒäº†xhçš„åˆ†å¸ƒçš„æ£€éªŒæƒ©ç½šé¡¹ã€‚å¯¹äºä¸å¯è®­ç»ƒçš„å™ªéŸ³è°ƒåº¦å™¨æ¥è¯´ï¼Œæ˜¯å›ºå®šçš„ã€‚è¿™éƒ¨åˆ†çš„æŸå¤±å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p> 
<pre><code class="language-python">def kl_prior(self, xh, node_mask):
        """Computes the KL between q(z1 | x) and the prior p(z1) = Normal(0, 1).
        å¯¹äºå®é™…ä¸Šè¿™éƒ¨åˆ†æŸå¤±å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè¿™éƒ¨åˆ†è®¡ç®—é‡è¾ƒå¤§ã€‚ ä½†æ˜¯ï¼Œå¯¹å…¶è¿›è¡Œè®¡ç®—ï¼Œä»¥ä¾¿åœ¨å™ªå£°è¡¨ä¸­å‡ºç°é”™è¯¯æ—¶çœ‹åˆ°å®ƒã€‚
        This is essentially a lot of work for something that is in practice negligible in the loss. However, you
        compute it so that you see it when you've made a mistake in your noise schedule.
        """
        # Compute the last alpha value, alpha_T.
        ones = torch.ones((xh.size(0), 1), device=xh.device)
        gamma_T = self.gamma(ones)
        alpha_T = self.alpha(gamma_T, xh)

        # Compute means.
        mu_T = alpha_T * xh
        mu_T_x, mu_T_h = mu_T[:, :, :self.n_dims], mu_T[:, :, self.n_dims:]

        # Compute standard deviations (only batch axis for x-part, inflated for h-part).
        sigma_T_x = self.sigma(gamma_T, mu_T_x).squeeze()  # Remove inflate, only keep batch dimension for x-part.
        sigma_T_h = self.sigma(gamma_T, mu_T_h)

        # Compute KL for h-part.
        zeros, ones = torch.zeros_like(mu_T_h), torch.ones_like(sigma_T_h)
        kl_distance_h = gaussian_KL(mu_T_h, sigma_T_h, zeros, ones, node_mask)

        # Compute KL for x-part.
        zeros, ones = torch.zeros_like(mu_T_x), torch.ones_like(sigma_T_x)
        subspace_d = self.subspace_dimensionality(node_mask)
        kl_distance_x = gaussian_KL_for_dimension(mu_T_x, sigma_T_x, zeros, ones, d=subspace_d)

        return kl_distance_x + kl_distance_h</code></pre> 
<p></p> 
<p>å¸¸æ•°é¡¹è´Ÿå¯¹æ•°æŸå¤±ï¼Œç”±log_pxh_given_z0_without_constantså‡½æ•°å®Œæˆï¼Œä»£ç å¦‚ä¸‹ã€‚</p> 
<pre><code class="language-python">def log_pxh_given_z0_without_constants(
            self, x, h, z_t, gamma_0, eps, net_out, node_mask, epsilon=1e-10):
        # Discrete properties are predicted directly from z_t.
        z_h_cat = z_t[:, :, self.n_dims:-1] if self.include_charges else z_t[:, :, self.n_dims:] # åŸå­ç§ç±»
        z_h_int = z_t[:, :, -1:] if self.include_charges else torch.zeros(0).to(z_t.device) # ç”µè·

        # Take only part over x.
        eps_x = eps[:, :, :self.n_dims] # å™ªéŸ³çš„xéƒ¨åˆ†ï¼Œå³åæ ‡éƒ¨åˆ†
        net_x = net_out[:, :, :self.n_dims] # æ¨¡å‹é¢„æµ‹çš„å™ªéŸ³çš„xéƒ¨åˆ†

        # Compute sigma_0 and rescale to the integer scale of the data.
        sigma_0 = self.sigma(gamma_0, target_tensor=z_t)
        sigma_0_cat = sigma_0 * self.norm_values[1]
        sigma_0_int = sigma_0 * self.norm_values[2]

        # Computes the error for the distribution N(x | 1 / alpha_0 z_0 + sigma_0/alpha_0 eps_0, sigma_0 / alpha_0),
        # the weighting in the epsilon parametrization is exactly '1'.
        log_p_x_given_z_without_constants = -0.5 * self.compute_error(net_x, gamma_0, eps_x)

        # Compute delta indicator masks.
        h_integer = torch.round(h['integer'] * self.norm_values[2] + self.norm_biases[2]).long()
        onehot = h['categorical'] * self.norm_values[1] + self.norm_biases[1]

        estimated_h_integer = z_h_int * self.norm_values[2] + self.norm_biases[2]
        estimated_h_cat = z_h_cat * self.norm_values[1] + self.norm_biases[1]
        assert h_integer.size() == estimated_h_integer.size()

        h_integer_centered = h_integer - estimated_h_integer

        # Compute integral from -0.5 to 0.5 of the normal distribution
        # N(mean=h_integer_centered, stdev=sigma_0_int)
        log_ph_integer = torch.log(
            cdf_standard_gaussian((h_integer_centered + 0.5) / sigma_0_int)
            - cdf_standard_gaussian((h_integer_centered - 0.5) / sigma_0_int)
            + epsilon)
        log_ph_integer = sum_except_batch(log_ph_integer * node_mask)

        # Centered h_cat around 1, since onehot encoded.
        centered_h_cat = estimated_h_cat - 1

        # Compute integrals from 0.5 to 1.5 of the normal distribution
        # N(mean=z_h_cat, stdev=sigma_0_cat)
        log_ph_cat_proportional = torch.log(
            cdf_standard_gaussian((centered_h_cat + 0.5) / sigma_0_cat)
            - cdf_standard_gaussian((centered_h_cat - 0.5) / sigma_0_cat)
            + epsilon)

        # Normalize the distribution over the categories.
        log_Z = torch.logsumexp(log_ph_cat_proportional, dim=2, keepdim=True)
        log_probabilities = log_ph_cat_proportional - log_Z

        # Select the log_prob of the current category usign the onehot
        # representation.
        log_ph_cat = sum_except_batch(log_probabilities * onehot * node_mask)

        # Combine categorical and integer log-probabilities.
        log_p_h_given_z = log_ph_integer + log_ph_cat

        # Combine log probabilities for x and h.
        log_p_xh_given_z = log_p_x_given_z_without_constants + log_p_h_given_z

        return log_p_xh_given_z</code></pre> 
<p></p> 
<p>è‡³æ­¤ï¼ŒSE3ç­‰å˜ç½‘ç»œçš„æ‰©æ•£æ¨¡å‹ï¼Œå°±ä»‹ç»å®Œæˆäº†ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼ŒSE3ç½‘ç»œæ¢æˆå…¶ä»–çš„æ¨¡å‹éƒ½æ˜¯å¯ä»¥ï¼Œä¸ä¸€å®šè¦ç”¨EGNN_dynamics_QM9ï¼Œç›´æ¥ä½¿ç”¨ENGGä¹Ÿæ²¡é—®é¢˜ã€‚å› ä¸ºï¼Œåœ¨æ•´ä¸ªè®¡ç®—æŸå¤±è¿‡ç¨‹ä¸­ï¼Œä»…ç”¨åˆ°SE3ç½‘ç»œé¢„æµ‹å™ªéŸ³è€Œå·²ï¼Œè¾“å…¥æ˜¯éšæœºæ·»åŠ å™ªéŸ³çš„z_x, z_hã€‚</p> 
<p></p> 
<p>æ¥ä¸‹æ¥æ˜¯å…³äºé‡‡æ ·ï¼Œè¿™é‡Œçš„SE3ç­‰å˜æ‰©æ•£æ¨¡å‹é‡‡æ ·çš„æ˜¯zå’Œhçš„éšå‘é‡ï¼Œå³z_xå’Œz_hï¼Œç”±sample/sample_chainå‡½æ•°å®Œæˆï¼ˆsample_chainä¿ç•™äº†ä¸­é—´çš„é‡‡æ ·çŠ¶æ€ï¼‰ã€‚</p> 
<p>ï¼ˆ1ï¼‰sample_combined_position_feature_noiseéšæœºåˆå§‹åŒ–é‡‡æ ·z_tï¼›</p> 
<p>ï¼ˆ2ï¼‰ç”±tåˆ°0é€æ­¥ä½¿ç”¨sample_p_zs_given_ztå‡½æ•°å¯¹z_tè¿›è¡Œå»å™ªï¼›</p> 
<p>ï¼ˆ3ï¼‰sample_p_xh_given_z0å‡½æ•°z_0æ—¶åˆ»ï¼Œå»å™ªï¼›</p> 
<p>ï¼ˆ4ï¼‰æ£€æŸ¥xéƒ¨åˆ†çš„è´¨å¿ƒï¼Œè¿”å›xï¼Œhï¼›</p> 
<p>ä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">@torch.no_grad()
    def sample(self, n_samples, n_nodes, node_mask, edge_mask, context, fix_noise=False):
        """
        Draw samples from the generative model.
        """
        if fix_noise:
            # æ¯ä¸€ä¸ªåˆ†å­çš„z_tç›¸åŒ
            # Noise is broadcasted over the batch axis, useful for visualizations.
            z = self.sample_combined_position_feature_noise(1, n_nodes, node_mask)
        else:
            # æ¯ä¸€ä¸ªåˆ†å­z_tä¸åŒ
            z = self.sample_combined_position_feature_noise(n_samples, n_nodes, node_mask)

        diffusion_utils.assert_mean_zero_with_mask(z[:, :, :self.n_dims], node_mask)

        # Iteratively sample p(z_s | z_t) for t = 1, ..., T, with s = t - 1.
        # é€æ­¥å»å™ª z_t -&gt; z_t-1
        for s in reversed(range(0, self.T)):
            s_array = torch.full((n_samples, 1), fill_value=s, device=z.device)
            t_array = s_array + 1
            s_array = s_array / self.T
            t_array = t_array / self.T

            z = self.sample_p_zs_given_zt(s_array, t_array, z, node_mask, edge_mask, context, fix_noise=fix_noise)

        # Finally sample p(x, h | z_0).
        # z_0 å»å™ª
        x, h = self.sample_p_xh_given_z0(z, node_mask, edge_mask, context, fix_noise=fix_noise)

        diffusion_utils.assert_mean_zero_with_mask(x, node_mask)

        #è´¨å¿ƒ
        max_cog = torch.sum(x, dim=1, keepdim=True).abs().max().item()
        if max_cog &gt; 5e-2:
            print(f'Warning cog drift with error {max_cog:.3f}. Projecting '
                  f'the positions down.')
            # å»è´¨å¿ƒ
            x = diffusion_utils.remove_mean_with_mask(x, node_mask)

        return x, h</code></pre> 
<p></p> 
<p>SE3ç­‰å˜æ‰©æ•£æ¨¡å‹å‰©ä¸‹è¿˜æœ‰å¾ˆå¤šå‡½æ•°ï¼Œä¸»è¦éƒ½æ˜¯ä¸ºäº†å®Œæˆè¿™ä¸¤ä¸ªä»»åŠ¡ï¼ˆè®¡ç®—æŸå¤±å’Œé‡‡æ ·ï¼‰çš„æ”¯æŒå‡½æ•°ï¼ŒåŒ…æ‹¬åˆšæ‰æåˆ°çš„å»å™ªå‡½æ•°sample_p_zs_given_ztï¼Œz_tåˆå§‹åŒ–å‡½æ•°sample_combined_position_feature_noiseï¼Œå™ªéŸ³è°ƒåº¦å™¨sigmaï¼Œ gammaç­‰ã€‚</p> 
<p></p> 
<p>SE3ç­‰å˜æ‰©æ•£æ¨¡å‹çš„å®Œæ•´ä»£ç ï¼š</p> 
<pre><code class="language-python">class EnVariationalDiffusion(torch.nn.Module):
    """
    ç­‰å˜æ‰©æ•£ç±»
    The E(n) Diffusion Module.
    """
    def __init__(
            self,
            dynamics: models.EGNN_dynamics_QM9, in_node_nf: int, n_dims: int,
            timesteps: int = 1000, parametrization='eps', noise_schedule='learned',
            noise_precision=1e-4, loss_type='vlb', norm_values=(1., 1., 1.),
            norm_biases=(None, 0., 0.), include_charges=True):
        super().__init__()

        # æŸå¤±ç±»å‹
        assert loss_type in {'vlb', 'l2'}
        self.loss_type = loss_type
        # èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦åŒ…å«ç”µè·charge
        self.include_charges = include_charges
        # å™ªéŸ³è°ƒåº¦å™¨noise_scheduleæ˜¯å¦å¯è®­ç»ƒï¼›
        # å¯è®­ç»ƒä¸º GammaNetwork()
        # ä¸å¯è®­ç»ƒä¸º PredefinedNoiseSchedule()
        if noise_schedule == 'learned':
            assert loss_type == 'vlb', 'A noise schedule can only be learned' \
                                       ' with a vlb objective.'

        # Only supported parametrization.
        assert parametrization == 'eps'

        if noise_schedule == 'learned':
            self.gamma = GammaNetwork()
        else:
            self.gamma = PredefinedNoiseSchedule(noise_schedule, timesteps=timesteps,
                                                 precision=noise_precision)

        # The network that will predict the denoising.
        # é¢„æµ‹å™ªéŸ³çš„æ¨¡å‹ï¼Œå³SE3æ¨¡å‹
        self.dynamics = dynamics

        # èŠ‚ç‚¹åŸå­ç±»å‹æ•°ï¼Œå«ç”µè·
        self.in_node_nf = in_node_nf
        # åæ ‡ç»´åº¦ï¼Œ3
        self.n_dims = n_dims
        # åŸå­ç§ç±»æ•°
        self.num_classes = self.in_node_nf - self.include_charges

        # æ‰©æ•£æ­¥æ•°
        self.T = timesteps
        self.parametrization = parametrization

        # æ­£åˆ™åŒ–å‚æ•°ï¼Œ norm_valuesï¼Œ norm_biases
        self.norm_values = norm_values
        self.norm_biases = norm_biases
        self.register_buffer('buffer', torch.zeros(1))

        # æ£€æŸ¥æ­£åˆ™åŒ–norm_valuesæ˜¯å¦åˆé€‚
        if noise_schedule != 'learned':
            self.check_issues_norm_values()

    def check_issues_norm_values(self, num_stdevs=8):
        # æ£€æŸ¥æ­£åˆ™åŒ–norm_valuesæ˜¯å¦åˆé€‚
        zeros = torch.zeros((1, 1))
        gamma_0 = self.gamma(zeros)
        sigma_0 = self.sigma(gamma_0, target_tensor=zeros).item()

        # Checked if 1 / norm_value is still larger than 10 * standard
        # deviation.
        max_norm_value = max(self.norm_values[1], self.norm_values[2])

        if sigma_0 * num_stdevs &gt; 1. / max_norm_value:
            raise ValueError(
                f'Value for normalization value {max_norm_value} probably too '
                f'large with sigma_0 {sigma_0:.5f} and '
                f'1 / norm_value = {1. / max_norm_value}')

    def phi(self, x, t, node_mask, edge_mask, context):
        # é¢„æµ‹è¾“å…¥xä¸­çš„å™ªéŸ³
        net_out = self.dynamics._forward(t, x, node_mask, edge_mask, context)

        return net_out

    def inflate_batch_array(self, array, target):
        """
        Inflates the batch array (array) with only a single axis (i.e. shape = (batch_size,), or possibly more empty
        axes (i.e. shape (batch_size, 1, ..., 1)) to match the target shape.
        """
        target_shape = (array.size(0),) + (1,) * (len(target.size()) - 1)
        return array.view(target_shape)

    def sigma(self, gamma, target_tensor):
        """Computes sigma given gamma."""
        return self.inflate_batch_array(torch.sqrt(torch.sigmoid(gamma)), target_tensor)

    def alpha(self, gamma, target_tensor):
        """Computes alpha given gamma."""
        return self.inflate_batch_array(torch.sqrt(torch.sigmoid(-gamma)), target_tensor)

    def SNR(self, gamma):
        """Computes signal to noise ratio (alpha^2/sigma^2) given gamma."""
        return torch.exp(-gamma)

    def subspace_dimensionality(self, node_mask):
        """Compute the dimensionality on translation-invariant linear subspace where distributions on x are defined."""
        number_of_nodes = torch.sum(node_mask.squeeze(2), dim=1)
        return (number_of_nodes - 1) * self.n_dims

    def normalize(self, x, h, node_mask):
        '''
        xå’Œhçš„æ­£åˆ™åŒ–ï¼Œ
        x/norm_values[0]
        h_cat = (h_cat-norm_biases[1])/norm_values[1]
        h_int = (h_int-norm_biases[1])/norm_values[1]
        '''
        x = x / self.norm_values[0]
        delta_log_px = -self.subspace_dimensionality(node_mask) * np.log(self.norm_values[0])

        # Casting to float in case h still has long or int type.
        h_cat = (h['categorical'].float() - self.norm_biases[1]) / self.norm_values[1] * node_mask
        h_int = (h['integer'].float() - self.norm_biases[2]) / self.norm_values[2]

        if self.include_charges:
            h_int = h_int * node_mask

        # Create new h dictionary.
        h = {'categorical': h_cat, 'integer': h_int}

        return x, h, delta_log_px

    def unnormalize(self, x, h_cat, h_int, node_mask):
        # xï¼Œhçš„å»æ­£åˆ™åŒ–
        x = x * self.norm_values[0]
        h_cat = h_cat * self.norm_values[1] + self.norm_biases[1]
        h_cat = h_cat * node_mask
        h_int = h_int * self.norm_values[2] + self.norm_biases[2]

        if self.include_charges:
            h_int = h_int * node_mask

        return x, h_cat, h_int

    def unnormalize_z(self, z, node_mask):
        # z å»æ­£åˆ™åŒ–
        # Parse from z
        x, h_cat = z[:, :, 0:self.n_dims], z[:, :, self.n_dims:self.n_dims+self.num_classes]
        h_int = z[:, :, self.n_dims+self.num_classes:self.n_dims+self.num_classes+1]
        assert h_int.size(2) == self.include_charges

        # Unnormalize
        x, h_cat, h_int = self.unnormalize(x, h_cat, h_int, node_mask)
        output = torch.cat([x, h_cat, h_int], dim=2)
        return output

    def sigma_and_alpha_t_given_s(self, gamma_t: torch.Tensor, gamma_s: torch.Tensor, target_tensor: torch.Tensor):
        """
        Computes sigma t given s, using gamma_t and gamma_s. Used during sampling.

        These are defined as:
            alpha t given s = alpha t / alpha s,
            sigma t given s = sqrt(1 - (alpha t given s) ^2 ).
        """
        sigma2_t_given_s = self.inflate_batch_array(
            -expm1(softplus(gamma_s) - softplus(gamma_t)), target_tensor
        )

        # alpha_t_given_s = alpha_t / alpha_s
        log_alpha2_t = F.logsigmoid(-gamma_t)
        log_alpha2_s = F.logsigmoid(-gamma_s)
        log_alpha2_t_given_s = log_alpha2_t - log_alpha2_s

        alpha_t_given_s = torch.exp(0.5 * log_alpha2_t_given_s)
        alpha_t_given_s = self.inflate_batch_array(
            alpha_t_given_s, target_tensor)

        sigma_t_given_s = torch.sqrt(sigma2_t_given_s)

        return sigma2_t_given_s, sigma_t_given_s, alpha_t_given_s

    def kl_prior(self, xh, node_mask):
        """Computes the KL between q(z1 | x) and the prior p(z1) = Normal(0, 1).
        å¯¹äºå®é™…ä¸Šè¿™éƒ¨åˆ†æŸå¤±å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè¿™éƒ¨åˆ†è®¡ç®—é‡è¾ƒå¤§ã€‚ ä½†æ˜¯ï¼Œå¯¹å…¶è¿›è¡Œè®¡ç®—ï¼Œä»¥ä¾¿åœ¨å™ªå£°è¡¨ä¸­å‡ºç°é”™è¯¯æ—¶çœ‹åˆ°å®ƒã€‚
        This is essentially a lot of work for something that is in practice negligible in the loss. However, you
        compute it so that you see it when you've made a mistake in your noise schedule.
        """
        # Compute the last alpha value, alpha_T.
        ones = torch.ones((xh.size(0), 1), device=xh.device)
        gamma_T = self.gamma(ones)
        alpha_T = self.alpha(gamma_T, xh)

        # Compute means.
        mu_T = alpha_T * xh
        mu_T_x, mu_T_h = mu_T[:, :, :self.n_dims], mu_T[:, :, self.n_dims:]

        # Compute standard deviations (only batch axis for x-part, inflated for h-part).
        sigma_T_x = self.sigma(gamma_T, mu_T_x).squeeze()  # Remove inflate, only keep batch dimension for x-part.
        sigma_T_h = self.sigma(gamma_T, mu_T_h)

        # Compute KL for h-part.
        zeros, ones = torch.zeros_like(mu_T_h), torch.ones_like(sigma_T_h)
        kl_distance_h = gaussian_KL(mu_T_h, sigma_T_h, zeros, ones, node_mask)

        # Compute KL for x-part.
        zeros, ones = torch.zeros_like(mu_T_x), torch.ones_like(sigma_T_x)
        subspace_d = self.subspace_dimensionality(node_mask)
        kl_distance_x = gaussian_KL_for_dimension(mu_T_x, sigma_T_x, zeros, ones, d=subspace_d)

        return kl_distance_x + kl_distance_h

    def compute_x_pred(self, net_out, zt, gamma_t):
        """Commputes x_pred, i.e. the most likely prediction of x."""
        if self.parametrization == 'x':
            x_pred = net_out
        elif self.parametrization == 'eps':
            sigma_t = self.sigma(gamma_t, target_tensor=net_out)
            alpha_t = self.alpha(gamma_t, target_tensor=net_out)
            eps_t = net_out
            x_pred = 1. / alpha_t * (zt - sigma_t * eps_t)
        else:
            raise ValueError(self.parametrization)

        return x_pred

    def compute_error(self, net_out, gamma_t, eps):
        """Computes error, i.e. the most likely prediction of x."""
        eps_t = net_out
        if self.training and self.loss_type == 'l2':
            denom = (self.n_dims + self.in_node_nf) * eps_t.shape[1]
            error = sum_except_batch((eps - eps_t) ** 2) / denom
        else:
            error = sum_except_batch((eps - eps_t) ** 2)
        return error

    def log_constants_p_x_given_z0(self, x, node_mask):
        """Computes p(x|z0)."""
        batch_size = x.size(0)

        n_nodes = node_mask.squeeze(2).sum(1)  # N has shape [B]
        assert n_nodes.size() == (batch_size,)
        degrees_of_freedom_x = (n_nodes - 1) * self.n_dims

        zeros = torch.zeros((x.size(0), 1), device=x.device)
        gamma_0 = self.gamma(zeros)

        # Recall that sigma_x = sqrt(sigma_0^2 / alpha_0^2) = SNR(-0.5 gamma_0).
        log_sigma_x = 0.5 * gamma_0.view(batch_size)

        return degrees_of_freedom_x * (- log_sigma_x - 0.5 * np.log(2 * np.pi))

    def sample_p_xh_given_z0(self, z0, node_mask, edge_mask, context, fix_noise=False):
        """Samples x ~ p(x|z0)."""
        zeros = torch.zeros(size=(z0.size(0), 1), device=z0.device)
        gamma_0 = self.gamma(zeros)
        # Computes sqrt(sigma_0^2 / alpha_0^2)
        sigma_x = self.SNR(-0.5 * gamma_0).unsqueeze(1)
        net_out = self.phi(z0, zeros, node_mask, edge_mask, context)

        # Compute mu for p(zs | zt).
        mu_x = self.compute_x_pred(net_out, z0, gamma_0)
        xh = self.sample_normal(mu=mu_x, sigma=sigma_x, node_mask=node_mask, fix_noise=fix_noise)

        x = xh[:, :, :self.n_dims]

        h_int = z0[:, :, -1:] if self.include_charges else torch.zeros(0).to(z0.device)
        x, h_cat, h_int = self.unnormalize(x, z0[:, :, self.n_dims:-1], h_int, node_mask)

        h_cat = F.one_hot(torch.argmax(h_cat, dim=2), self.num_classes) * node_mask
        h_int = torch.round(h_int).long() * node_mask
        h = {'integer': h_int, 'categorical': h_cat}
        return x, h

    def sample_normal(self, mu, sigma, node_mask, fix_noise=False):
        """Samples from a Normal distribution."""
        bs = 1 if fix_noise else mu.size(0)
        eps = self.sample_combined_position_feature_noise(bs, mu.size(1), node_mask)
        return mu + sigma * eps

    def log_pxh_given_z0_without_constants(
            self, x, h, z_t, gamma_0, eps, net_out, node_mask, epsilon=1e-10):
        # Discrete properties are predicted directly from z_t.
        z_h_cat = z_t[:, :, self.n_dims:-1] if self.include_charges else z_t[:, :, self.n_dims:] # åŸå­ç§ç±»
        z_h_int = z_t[:, :, -1:] if self.include_charges else torch.zeros(0).to(z_t.device) # ç”µè·

        # Take only part over x.
        eps_x = eps[:, :, :self.n_dims] # å™ªéŸ³çš„xéƒ¨åˆ†ï¼Œå³åæ ‡éƒ¨åˆ†
        net_x = net_out[:, :, :self.n_dims] # æ¨¡å‹é¢„æµ‹çš„å™ªéŸ³çš„xéƒ¨åˆ†

        # Compute sigma_0 and rescale to the integer scale of the data.
        sigma_0 = self.sigma(gamma_0, target_tensor=z_t)
        sigma_0_cat = sigma_0 * self.norm_values[1]
        sigma_0_int = sigma_0 * self.norm_values[2]

        # Computes the error for the distribution N(x | 1 / alpha_0 z_0 + sigma_0/alpha_0 eps_0, sigma_0 / alpha_0),
        # the weighting in the epsilon parametrization is exactly '1'.
        log_p_x_given_z_without_constants = -0.5 * self.compute_error(net_x, gamma_0, eps_x)

        # Compute delta indicator masks.
        h_integer = torch.round(h['integer'] * self.norm_values[2] + self.norm_biases[2]).long()
        onehot = h['categorical'] * self.norm_values[1] + self.norm_biases[1]

        estimated_h_integer = z_h_int * self.norm_values[2] + self.norm_biases[2]
        estimated_h_cat = z_h_cat * self.norm_values[1] + self.norm_biases[1]
        assert h_integer.size() == estimated_h_integer.size()

        h_integer_centered = h_integer - estimated_h_integer

        # Compute integral from -0.5 to 0.5 of the normal distribution
        # N(mean=h_integer_centered, stdev=sigma_0_int)
        log_ph_integer = torch.log(
            cdf_standard_gaussian((h_integer_centered + 0.5) / sigma_0_int)
            - cdf_standard_gaussian((h_integer_centered - 0.5) / sigma_0_int)
            + epsilon)
        log_ph_integer = sum_except_batch(log_ph_integer * node_mask)

        # Centered h_cat around 1, since onehot encoded.
        centered_h_cat = estimated_h_cat - 1

        # Compute integrals from 0.5 to 1.5 of the normal distribution
        # N(mean=z_h_cat, stdev=sigma_0_cat)
        log_ph_cat_proportional = torch.log(
            cdf_standard_gaussian((centered_h_cat + 0.5) / sigma_0_cat)
            - cdf_standard_gaussian((centered_h_cat - 0.5) / sigma_0_cat)
            + epsilon)

        # Normalize the distribution over the categories.
        log_Z = torch.logsumexp(log_ph_cat_proportional, dim=2, keepdim=True)
        log_probabilities = log_ph_cat_proportional - log_Z

        # Select the log_prob of the current category usign the onehot
        # representation.
        log_ph_cat = sum_except_batch(log_probabilities * onehot * node_mask)

        # Combine categorical and integer log-probabilities.
        log_p_h_given_z = log_ph_integer + log_ph_cat

        # Combine log probabilities for x and h.
        log_p_xh_given_z = log_p_x_given_z_without_constants + log_p_h_given_z

        return log_p_xh_given_z

    def compute_loss(self, x, h, node_mask, edge_mask, context, t0_always):
        # æ‰©æ•£æ¨¡å‹çš„æŸå¤±ï¼Œè¾“å…¥x, h
        """Computes an estimator for the variational lower bound, or the simple loss (MSE)."""

        # This part is about whether to include loss term 0 always.
        if t0_always:
            # loss_term_0 will be computed separately.
            # estimator = loss_0 + loss_t,  where t ~ U({1, ..., T})
            lowest_t = 1
        else:
            # estimator = loss_t,           where t ~ U({0, ..., T})
            lowest_t = 0
        # 1. éšæœºåˆå§‹åŒ–tï¼Œç”Ÿæˆå™ªéŸ³
        # Sample a timestep t.
        t_int = torch.randint(
            lowest_t, self.T + 1, size=(x.size(0), 1), device=x.device).float()
        s_int = t_int - 1
        t_is_zero = (t_int == 0).float()  # Important to compute log p(x | z0).

        # Normalize t to [0, 1]. Note that the negative
        # step of s will never be used, since then p(x | z0) is computed.
        s = s_int / self.T
        t = t_int / self.T

        # Compute gamma_s and gamma_t via the network.
        gamma_s = self.inflate_batch_array(self.gamma(s), x)
        gamma_t = self.inflate_batch_array(self.gamma(t), x)

        # Compute alpha_t and sigma_t from gamma.
        alpha_t = self.alpha(gamma_t, x)
        sigma_t = self.sigma(gamma_t, x)

        # Sample zt ~ Normal(alpha_t x, sigma_t) å™ªéŸ³
        eps = self.sample_combined_position_feature_noise(
            n_samples=x.size(0), n_nodes=x.size(1), node_mask=node_mask)

        # Concatenate x, h[integer] and h[categorical].
        xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
        # 2. å°†å™ªéŸ³æ·»åŠ åˆ°xhä¸­ï¼Œç”Ÿæˆz_xh
        # Sample z_t given x, h for timestep t, from q(z_t | x, h)
        z_t = alpha_t * xh + sigma_t * eps

        diffusion_utils.assert_mean_zero_with_mask(z_t[:, :, :self.n_dims], node_mask)

        # 3. se3ç½‘ç»œé¢„æµ‹å™ªéŸ³
        # Neural net prediction.
        net_out = self.phi(z_t, t, node_mask, edge_mask, context)

        # 4. se3ç½‘ç»œè®¡ç®—æŸå¤± l2 æˆ–è€… mseæŸå¤±ï¼›
        # 4.1 L2 æˆ–è€… mse æŸå¤±
        # Compute the error.
        error = self.compute_error(net_out, gamma_t, eps)

        if self.training and self.loss_type == 'l2':
            SNR_weight = torch.ones_like(error)
        else:
            # Compute weighting with SNR: (SNR(s-t) - 1) for epsilon parametrization.
            SNR_weight = (self.SNR(gamma_s - gamma_t) - 1).squeeze(1).squeeze(1)
        assert error.size() == SNR_weight.size()
        loss_t_larger_than_zero = 0.5 * SNR_weight * error

        # 4.2 è´Ÿå¯¹æ•°å¸¸æ•°æŸå¤±
        # The _constants_ depending on sigma_0 from the
        # cross entropy term E_q(z0 | x) [log p(x | z0)].
        neg_log_constants = -self.log_constants_p_x_given_z0(x, node_mask)

        # Reset constants during training with l2 loss.
        if self.training and self.loss_type == 'l2':
            neg_log_constants = torch.zeros_like(neg_log_constants)

        # 4.3 xhçš„KLæ•£åº¦æŸå¤±
        # The KL between q(z1 | x) and p(z1) = Normal(0, 1). Should be close to zero. 
        kl_prior = self.kl_prior(xh, node_mask)

        # Combining the terms
        if t0_always:
            loss_t = loss_t_larger_than_zero
            num_terms = self.T  # Since t=0 is not included here.
            estimator_loss_terms = num_terms * loss_t

            # Compute noise values for t = 0.
            t_zeros = torch.zeros_like(s)
            gamma_0 = self.inflate_batch_array(self.gamma(t_zeros), x)
            alpha_0 = self.alpha(gamma_0, x)
            sigma_0 = self.sigma(gamma_0, x)

            # Sample z_0 given x, h for timestep t, from q(z_t | x, h)
            eps_0 = self.sample_combined_position_feature_noise(
                n_samples=x.size(0), n_nodes=x.size(1), node_mask=node_mask)
            z_0 = alpha_0 * xh + sigma_0 * eps_0

            net_out = self.phi(z_0, t_zeros, node_mask, edge_mask, context)

            # z_0æ—¶åˆ»çš„å‰¯é˜Ÿè´Ÿå¯¹æ•°å¸¸æ•°æŸå¤±
            loss_term_0 = -self.log_pxh_given_z0_without_constants(
                x, h, z_0, gamma_0, eps_0, net_out, node_mask)

            assert kl_prior.size() == estimator_loss_terms.size()
            assert kl_prior.size() == neg_log_constants.size()
            assert kl_prior.size() == loss_term_0.size()

            loss = kl_prior + estimator_loss_terms + neg_log_constants + loss_term_0

        else:
            # Computes the L_0 term (even if gamma_t is not actually gamma_0)
            # and this will later be selected via masking.
            loss_term_0 = -self.log_pxh_given_z0_without_constants(
                x, h, z_t, gamma_t, eps, net_out, node_mask)

            t_is_not_zero = 1 - t_is_zero

            loss_t = loss_term_0 * t_is_zero.squeeze() + t_is_not_zero.squeeze() * loss_t_larger_than_zero

            # Only upweigh estimator if using the vlb objective.
            if self.training and self.loss_type == 'l2':
                estimator_loss_terms = loss_t
            else:
                num_terms = self.T + 1  # Includes t = 0.
                estimator_loss_terms = num_terms * loss_t

            assert kl_prior.size() == estimator_loss_terms.size()
            assert kl_prior.size() == neg_log_constants.size()

            loss = kl_prior + estimator_loss_terms + neg_log_constants

        assert len(loss.shape) == 1, f'{loss.shape} has more than only batch dim.'

        return loss, {'t': t_int.squeeze(), 'loss_t': loss.squeeze(),
                      'error': error.squeeze()}

    def forward(self, x, h, node_mask=None, edge_mask=None, context=None):
        """
        è®­ç»ƒæ—¶ï¼Œè®¡ç®—l2æŸå¤±æˆ–è€…æ˜¯è´Ÿå¯¹æ•°æŸå¤±NLLï¼Œå¦åˆ™è®¡ç®—è´Ÿå¯¹æ•°æŸå¤±
        Computes the loss (type l2 or NLL) if training. And if eval then always computes NLL.
        """
        # Normalize data, take into account volume change in x.
        # æ­£åˆ™åŒ–è¾“å…¥æ•°æ®ï¼Œx,hï¼Œä¸ºäº†å‡å°‘ä½“ç§¯çš„å½±å“
        x, h, delta_log_px = self.normalize(x, h, node_mask)

        # Reset delta_log_px if not vlb objective.
        if self.training and self.loss_type == 'l2':
            delta_log_px = torch.zeros_like(delta_log_px)

        # è®¡ç®—æŸå¤±
        if self.training:
            # Only 1 forward pass when t0_always is False.
            loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context, t0_always=False)
        else:
            # Less variance in the estimator, costs two forward passes.
            loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context, t0_always=True)

        neg_log_pxh = loss

        # Correct for normalization on x.
        assert neg_log_pxh.size() == delta_log_px.size()
        neg_log_pxh = neg_log_pxh - delta_log_px

        return neg_log_pxh

    def sample_p_zs_given_zt(self, s, t, zt, node_mask, edge_mask, context, fix_noise=False):
        """Samples from zs ~ p(zs | zt). Only used during sampling."""
        gamma_s = self.gamma(s)
        gamma_t = self.gamma(t)

        sigma2_t_given_s, sigma_t_given_s, alpha_t_given_s = \
            self.sigma_and_alpha_t_given_s(gamma_t, gamma_s, zt)

        sigma_s = self.sigma(gamma_s, target_tensor=zt)
        sigma_t = self.sigma(gamma_t, target_tensor=zt)

        # Neural net prediction.
        eps_t = self.phi(zt, t, node_mask, edge_mask, context)

        # Compute mu for p(zs | zt).
        diffusion_utils.assert_mean_zero_with_mask(zt[:, :, :self.n_dims], node_mask)
        diffusion_utils.assert_mean_zero_with_mask(eps_t[:, :, :self.n_dims], node_mask)
        mu = zt / alpha_t_given_s - (sigma2_t_given_s / alpha_t_given_s / sigma_t) * eps_t

        # Compute sigma for p(zs | zt).
        sigma = sigma_t_given_s * sigma_s / sigma_t

        # Sample zs given the paramters derived from zt.
        zs = self.sample_normal(mu, sigma, node_mask, fix_noise)

        # Project down to avoid numerical runaway of the center of gravity.
        zs = torch.cat(
            [diffusion_utils.remove_mean_with_mask(zs[:, :, :self.n_dims],
                                                   node_mask),
             zs[:, :, self.n_dims:]], dim=2
        )
        return zs

    def sample_combined_position_feature_noise(self, n_samples, n_nodes, node_mask):
        """
        # å¯¹ z_x é‡‡æ ·ä»¥å‡å€¼ä¸ºä¸­å¿ƒçš„æ­£æ€å™ªå£°ï¼Œå¯¹ z_h é‡‡æ ·æ ‡å‡†æ­£æ€å™ªå£°
        Samples mean-centered normal noise for z_x, and standard normal noise for z_h.
        """
        z_x = utils.sample_center_gravity_zero_gaussian_with_mask(
            size=(n_samples, n_nodes, self.n_dims), device=node_mask.device,
            node_mask=node_mask)
        z_h = utils.sample_gaussian_with_mask(
            size=(n_samples, n_nodes, self.in_node_nf), device=node_mask.device,
            node_mask=node_mask)
        z = torch.cat([z_x, z_h], dim=2)
        return z

    @torch.no_grad()
    def sample(self, n_samples, n_nodes, node_mask, edge_mask, context, fix_noise=False):
        """
        Draw samples from the generative model.
        """
        if fix_noise:
            # æ¯ä¸€ä¸ªåˆ†å­çš„z_tç›¸åŒ
            # Noise is broadcasted over the batch axis, useful for visualizations.
            z = self.sample_combined_position_feature_noise(1, n_nodes, node_mask)
        else:
            # æ¯ä¸€ä¸ªåˆ†å­z_tä¸åŒ
            z = self.sample_combined_position_feature_noise(n_samples, n_nodes, node_mask)

        diffusion_utils.assert_mean_zero_with_mask(z[:, :, :self.n_dims], node_mask)

        # Iteratively sample p(z_s | z_t) for t = 1, ..., T, with s = t - 1.
        # é€æ­¥å»å™ª z_t -&gt; z_t-1
        for s in reversed(range(0, self.T)):
            s_array = torch.full((n_samples, 1), fill_value=s, device=z.device)
            t_array = s_array + 1
            s_array = s_array / self.T
            t_array = t_array / self.T

            z = self.sample_p_zs_given_zt(s_array, t_array, z, node_mask, edge_mask, context, fix_noise=fix_noise)

        # Finally sample p(x, h | z_0).
        # z_0 å»å™ª
        x, h = self.sample_p_xh_given_z0(z, node_mask, edge_mask, context, fix_noise=fix_noise)

        diffusion_utils.assert_mean_zero_with_mask(x, node_mask)

        #è´¨å¿ƒ
        max_cog = torch.sum(x, dim=1, keepdim=True).abs().max().item()
        if max_cog &gt; 5e-2:
            print(f'Warning cog drift with error {max_cog:.3f}. Projecting '
                  f'the positions down.')
            # å»è´¨å¿ƒ
            x = diffusion_utils.remove_mean_with_mask(x, node_mask)

        return x, h

    @torch.no_grad()
    def sample_chain(self, n_samples, n_nodes, node_mask, edge_mask, context, keep_frames=None):
        """
        Draw samples from the generative model, keep the intermediate states for visualization purposes.
        """
        z = self.sample_combined_position_feature_noise(n_samples, n_nodes, node_mask)

        diffusion_utils.assert_mean_zero_with_mask(z[:, :, :self.n_dims], node_mask)

        if keep_frames is None:
            keep_frames = self.T
        else:
            assert keep_frames &lt;= self.T
        chain = torch.zeros((keep_frames,) + z.size(), device=z.device)

        # Iteratively sample p(z_s | z_t) for t = 1, ..., T, with s = t - 1.
        for s in reversed(range(0, self.T)):
            s_array = torch.full((n_samples, 1), fill_value=s, device=z.device)
            t_array = s_array + 1
            s_array = s_array / self.T
            t_array = t_array / self.T

            z = self.sample_p_zs_given_zt(
                s_array, t_array, z, node_mask, edge_mask, context)

            diffusion_utils.assert_mean_zero_with_mask(z[:, :, :self.n_dims], node_mask)

            # Write to chain tensor.
            write_index = (s * keep_frames) // self.T
            chain[write_index] = self.unnormalize_z(z, node_mask)

        # Finally sample p(x, h | z_0).
        x, h = self.sample_p_xh_given_z0(z, node_mask, edge_mask, context)

        diffusion_utils.assert_mean_zero_with_mask(x[:, :, :self.n_dims], node_mask)

        xh = torch.cat([x, h['categorical'], h['integer']], dim=2)
        chain[0] = xh  # Overwrite last frame with the resulting x and h.

        chain_flat = chain.view(n_samples * keep_frames, *z.size()[1:])

        return chain_flat

    def log_info(self):
        """
        Some info logging of the model.
        """
        gamma_0 = self.gamma(torch.zeros(1, device=self.buffer.device))
        gamma_1 = self.gamma(torch.ones(1, device=self.buffer.device))

        log_SNR_max = -gamma_0
        log_SNR_min = -gamma_1

        info = {
            'log_SNR_max': log_SNR_max.item(),
            'log_SNR_min': log_SNR_min.item()}
        print(info)

        return info</code></pre> 
<p></p> 
<h5></h5> 
<h5>2.2.2.3 VAEæ¨¡å‹ç»“æ„ ï¼ˆEnHierarchicalVAEï¼‰</h5> 
<p>get_autoencoderå‡½æ•°åœ¨3.2åŠ è½½æ¨¡å‹å’Œ3.2.1 GeoLDMéƒ¨åˆ†å‡å‡ºç°è¿‡ï¼Œå…¶ç”¨é€”æ˜¯åŠ è½½ä¸€ä¸ªVAEæ¨¡å‹ã€‚</p> 
<p></p> 
<p>VAEæ¨¡å‹ç”±get_autoencoderå‡½æ•°å¯¼å…¥ï¼Œä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">def get_autoencoder(args, device, dataset_info, dataloader_train):
    histogram = dataset_info['n_nodes']
    in_node_nf = len(dataset_info['atom_decoder']) + int(args.include_charges)
    nodes_dist = DistributionNodes(histogram)

    prop_dist = None
    if len(args.conditioning) &gt; 0:
        prop_dist = DistributionProperty(dataloader_train, args.conditioning)

    # if args.condition_time:
    #     dynamics_in_node_nf = in_node_nf + 1
    # else:
    print('Autoencoder models are _not_ conditioned on time.')
        # dynamics_in_node_nf = in_node_nf
    
    # ç¼–ç å™¨ï¼Œ ä¹Ÿæ˜¯ä¸€ä¸ªENGGç½‘ç»œï¼Œæ³¨æ„è¾“å‡ºç»´åº¦æ˜¯args.latent_nfï¼Œ è¾“å…¥ç»´åº¦æ˜¯in_node_nf
    encoder = EGNN_encoder_QM9(
        in_node_nf=in_node_nf, context_node_nf=args.context_node_nf, out_node_nf=args.latent_nf,
        n_dims=3, device=device, hidden_nf=args.nf,
        act_fn=torch.nn.SiLU(), n_layers=1,
        attention=args.attention, tanh=args.tanh, mode=args.model, norm_constant=args.norm_constant,
        inv_sublayers=args.inv_sublayers, sin_embedding=args.sin_embedding,
        normalization_factor=args.normalization_factor, aggregation_method=args.aggregation_method,
        include_charges=args.include_charges
        )
    
    # è§£ç å™¨ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªENGGç½‘ç»œï¼Œæ³¨æ„ï¼Œè¾“å…¥çš„ç»´åº¦æ˜¯args.latent_nfï¼Œ è¾“å‡ºç»´åº¦æ˜¯in_node_nf
    decoder = EGNN_decoder_QM9(
        in_node_nf=args.latent_nf, context_node_nf=args.context_node_nf, out_node_nf=in_node_nf,
        n_dims=3, device=device, hidden_nf=args.nf,
        act_fn=torch.nn.SiLU(), n_layers=args.n_layers,
        attention=args.attention, tanh=args.tanh, mode=args.model, norm_constant=args.norm_constant,
        inv_sublayers=args.inv_sublayers, sin_embedding=args.sin_embedding,
        normalization_factor=args.normalization_factor, aggregation_method=args.aggregation_method,
        include_charges=args.include_charges
        )

    vae = EnHierarchicalVAE(
        encoder=encoder,
        decoder=decoder,
        in_node_nf=in_node_nf,
        n_dims=3,
        latent_node_nf=args.latent_nf,
        kl_weight=args.kl_weight,
        norm_values=args.normalize_factors,
        include_charges=args.include_charges
        )

    return vae, nodes_dist, prop_dist</code></pre> 
<p></p> 
<p>VAEæ¨¡å‹çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œéƒ½æ˜¯ä¸€ä¸ªEGNN_encoder_QM9ç½‘ç»œã€‚è¦æ³¨æ„çš„æ˜¯ç¼–ç å™¨å’Œè§£ç å™¨çš„è¾“å…¥å’Œè¾“å‡ºç»´åº¦æ˜¯å¯¹åº”çš„ã€‚ç¼–ç å™¨å°†åˆ†å­ä¸­å„åŸå­ç‰¹å¾åµŒå…¥è‡³args.latent_nfç»´ï¼Œè§£ç å™¨å°†å…¶è¿˜åŸè‡³in_node_nfç»´ã€‚ç¼–ç å™¨å’Œè§£ç å™¨çš„ç»“æ„æ˜¯å®Œå…¨å¯¹åº”çš„ï¼ŒåŒ…æ‹¬å±‚æ•°ç­‰ã€‚</p> 
<p></p> 
<p>æ•´ä¸ªVAEæ¨¡å‹ç”±EnHierarchicalVAEå®ç°ï¼Œä¸VAEæ¨¡å‹æ¶æ„åŸºæœ¬ä¸€è‡´ï¼ŒåŒ…å«äº†ï¼ˆæ³¨ï¼šè¿™éƒ¨åˆ†å¹¶æœªåŒ…å«diffusionéƒ¨åˆ†ï¼‰ã€‚VAEéƒ¨åˆ†çš„ä»£ç æ¯”è¾ƒç®€å•ï¼Œå°±ä¸è¯¦ç»†ä»‹ç»äº†ï¼š</p> 
<p>1. compute_reconstruction_errorè®¡ç®—é‡æ„æŸå¤±ï¼›</p> 
<ol><li>compute_lossè®¡ç®—é‡æ„æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ä¹‹å’Œï¼›</li><li>forwardå‡½æ•°è®¡ç®—é‡æ„æŸå¤±å’ŒKLæ•£åº¦æŸå¤±ä¹‹å’Œï¼›</li><li>ç¼–ç å™¨ encodeï¼›</li><li>è§£ç å™¨ decodeï¼ˆè¿”å›åŸå­ç±»å‹one-hotï¼‰ï¼›</li></ol> 
<p>EnHierarchicalVAEä»£ç å¦‚ä¸‹ï¼š</p> 
<pre><code class="language-python">class EnHierarchicalVAE(torch.nn.Module):
    """
    VAE æ¨¡å—
    The E(n) Hierarchical VAE Module.
    """
    def __init__(
            self,
            encoder: models.EGNN_encoder_QM9,
            decoder: models.EGNN_decoder_QM9,
            in_node_nf: int, n_dims: int, latent_node_nf: int,
            kl_weight: float,
            norm_values=(1., 1., 1.), norm_biases=(None, 0., 0.), 
            include_charges=True):
        super().__init__()

        self.include_charges = include_charges

        self.encoder = encoder
        self.decoder = decoder

        self.in_node_nf = in_node_nf
        self.n_dims = n_dims
        self.latent_node_nf = latent_node_nf
        self.num_classes = self.in_node_nf - self.include_charges
        self.kl_weight = kl_weight

        self.norm_values = norm_values
        self.norm_biases = norm_biases
        self.register_buffer('buffer', torch.zeros(1))

    def subspace_dimensionality(self, node_mask):
        # è®¡ç®—å®šä¹‰ x åˆ†å¸ƒçš„å¹³ç§»ä¸å˜çº¿æ€§å­ç©ºé—´çš„ç»´æ•°ï¼Œå³
        """Compute the dimensionality on translation-invariant linear subspace where distributions on x are defined."""
        number_of_nodes = torch.sum(node_mask.squeeze(2), dim=1)
        return (number_of_nodes - 1) * self.n_dims

    def compute_reconstruction_error(self, xh_rec, xh):
        # è®¡ç®—é‡æ„æŸå¤±
        """Computes reconstruction error."""

        bs, n_nodes, dims = xh.shape

        # Error on positions. åŸå­åæ ‡ä½ç½®æŸå¤±
        x_rec = xh_rec[:, :, :self.n_dims]
        x = xh[:, :, :self.n_dims]
        error_x = sum_except_batch((x_rec - x) ** 2) # sum_except_batchå‡½æ•° ä¸€ç»´æ±‚å’Œ
        
        # Error on classes. åŸå­ç±»å‹åˆ†ç±»ï¼Œäº¤å‰ç†µ
        h_cat_rec = xh_rec[:, :, self.n_dims:self.n_dims + self.num_classes]
        h_cat = xh[:, :, self.n_dims:self.n_dims + self.num_classes]
        h_cat_rec = h_cat_rec.reshape(bs * n_nodes, self.num_classes)
        h_cat = h_cat.reshape(bs * n_nodes, self.num_classes)
        error_h_cat = F.cross_entropy(h_cat_rec, h_cat.argmax(dim=1), reduction='none')
        error_h_cat = error_h_cat.reshape(bs, n_nodes, 1)
        error_h_cat = sum_except_batch(error_h_cat)
        # error_h_cat = sum_except_batch((h_cat_rec - h_cat) ** 2)

        # Error on charges. # ç”µè·ç±»å‹æŸå¤±
        if self.include_charges:
            h_int_rec = xh_rec[:, :, -self.include_charges:]
            h_int = xh[:, :, -self.include_charges:]
            error_h_int = sum_except_batch((h_int_rec - h_int) ** 2)
        else:
            error_h_int = 0.
        
        # æŸå¤±åˆè®¡
        error = error_x + error_h_cat + error_h_int

        if self.training:
            denom = (self.n_dims + self.in_node_nf) * xh.shape[1]
            error = error / denom

        return error
    
    def sample_normal(self, mu, sigma, node_mask, fix_noise=False):
        """Samples from a Normal distribution."""
        bs = 1 if fix_noise else mu.size(0)
        eps = self.sample_combined_position_feature_noise(bs, mu.size(1), node_mask)
        return mu + sigma * eps
    
    def compute_loss(self, x, h, node_mask, edge_mask, context):
        # è®¡ç®—å˜åˆ†ä¸‹ç•Œçš„ä¼°è®¡é‡
        """Computes an estimator for the variational lower bound."""

        # Concatenate x, h[integer] and h[categorical].
        xh = torch.cat([x, h['categorical'], h['integer']], dim=2)

        # Encoder output. ç¼–ç å™¨è¾“å‡º
        z_x_mu, z_x_sigma, z_h_mu, z_h_sigma = self.encode(x, h, node_mask, edge_mask, context)
        
        # KL distance. KLæ•£åº¦ï¼ˆä¸¤ä¸ªæ­£æ€åˆ†å¸ƒä¹‹é—´çš„æ•£åº¦ï¼‰
        # KL for invariant features. ä¸å˜ç‰¹å¾ï¼Œå³h
        zeros, ones = torch.zeros_like(z_h_mu), torch.ones_like(z_h_sigma)
        loss_kl_h = gaussian_KL(z_h_mu, ones, zeros, ones, node_mask)
        # KL for equivariant features. ç­‰å˜ç‰¹å¾ï¼Œå³x
        assert z_x_sigma.mean(dim=(1,2), keepdim=True).expand_as(z_x_sigma).allclose(z_x_sigma, atol=1e-7)
        zeros, ones = torch.zeros_like(z_x_mu), torch.ones_like(z_x_sigma.mean(dim=(1,2)))
        subspace_d = self.subspace_dimensionality(node_mask)
        loss_kl_x = gaussian_KL_for_dimension(z_x_mu, ones, zeros, ones, subspace_d)
        loss_kl = loss_kl_h + loss_kl_x

        # Infer latent z.
        z_xh_mean = torch.cat([z_x_mu, z_h_mu], dim=2)
        diffusion_utils.assert_correctly_masked(z_xh_mean, node_mask)
        z_xh_sigma = torch.cat([z_x_sigma.expand(-1, -1, 3), z_h_sigma], dim=2)
        # é‡‡æ ·z_xh
        z_xh = self.sample_normal(z_xh_mean, z_xh_sigma, node_mask) 
        # z_xh = z_xh_mean
        diffusion_utils.assert_correctly_masked(z_xh, node_mask)
        diffusion_utils.assert_mean_zero_with_mask(z_xh[:, :, :self.n_dims], node_mask)

        # Decoder output (reconstruction). è§£ç å™¨è¾“å‡ºï¼Œé‡æ„æŸå¤±
        x_recon, h_recon = self.decoder._forward(z_xh, node_mask, edge_mask, context)
        xh_rec = torch.cat([x_recon, h_recon], dim=2)
        # é‡æ„æŸå¤±
        loss_recon = self.compute_reconstruction_error(xh_rec, xh)

        # Combining the terms æŸå¤±ï¼šKLæ•£åº¦+é‡æ„æŸå¤±
        assert loss_recon.size() == loss_kl.size()
        loss = loss_recon + self.kl_weight * loss_kl

        assert len(loss.shape) == 1, f'{loss.shape} has more than only batch dim.'

        return loss, {'loss_t': loss.squeeze(), 'rec_error': loss_recon.squeeze()}

    def forward(self, x, h, node_mask=None, edge_mask=None, context=None):
        # è®¡ç®—è®­ç»ƒçš„ ELBOWã€‚ å¦‚æœ eval åˆ™æ€»æ˜¯è®¡ç®— Nll
        """
        Computes the ELBO if training. And if eval then always computes NLL.
        """

        loss, loss_dict = self.compute_loss(x, h, node_mask, edge_mask, context)

        neg_log_pxh = loss

        return neg_log_pxh

    def sample_combined_position_feature_noise(self, n_samples, n_nodes, node_mask):
        # å¯¹ z_x é‡‡æ ·ä»¥è´¨å¿ƒä¸ºé›¶çš„æ­£æ€å™ªå£°ï¼Œå¯¹ z_h é‡‡æ ·æ ‡å‡†æ­£æ€å™ªå£°
        """
        Samples mean-centered normal noise for z_x, and standard normal noise for z_h.
        """
        z_x = utils.sample_center_gravity_zero_gaussian_with_mask(
            size=(n_samples, n_nodes, self.n_dims), device=node_mask.device,
            node_mask=node_mask)
        z_h = utils.sample_gaussian_with_mask(
            size=(n_samples, n_nodes, self.latent_node_nf), device=node_mask.device,
            node_mask=node_mask)
        z = torch.cat([z_x, z_h], dim=2)
        return z
    
    def encode(self, x, h, node_mask=None, edge_mask=None, context=None):
        # ç¼–ç å™¨
        """Computes q(z|x)."""

        # Concatenate x, h[integer] and h[categorical].
        xh = torch.cat([x, h['categorical'], h['integer']], dim=2)

        # æ£€æŸ¥è´¨å¿ƒæ˜¯å¦ä¸º0
        diffusion_utils.assert_mean_zero_with_mask(xh[:, :, :self.n_dims], node_mask)

        # Encoder output. ç¼–ç å™¨è¾“å‡º
        z_x_mu, z_x_sigma, z_h_mu, z_h_sigma = self.encoder._forward(xh, node_mask, edge_mask, context)

        bs, _, _ = z_x_mu.size()
        sigma_0_x = torch.ones(bs, 1, 1).to(z_x_mu) * 0.0032 # æ ‡å‡†å·®ï¼Ÿ
        sigma_0_h = torch.ones(bs, 1, self.latent_node_nf).to(z_h_mu) * 0.0032 # æ ‡å‡†å·®ï¼Ÿ

        return z_x_mu, sigma_0_x, z_h_mu, sigma_0_h
    
    def decode(self, z_xh, node_mask=None, edge_mask=None, context=None):
        # è§£ç å™¨ï¼Œ è¿”å›åŸå­åæ ‡xå’ŒåŸå­ç±»å‹h,ä¸åŒ…æ‹¬ç”µè·ç‰¹å¾
        """Computes p(x|z)."""

        # Decoder output (reconstruction).
        x_recon, h_recon = self.decoder._forward(z_xh, node_mask, edge_mask, context)
        # æ£€æŸ¥é‡æ„åxçš„è´¨å¿ƒæ˜¯å¦ä¸º0
        diffusion_utils.assert_mean_zero_with_mask(x_recon, node_mask)

        xh = torch.cat([x_recon, h_recon], dim=2)

        x = xh[:, :, :self.n_dims]
        diffusion_utils.assert_correctly_masked(x, node_mask)

        h_int = xh[:, :, -1:] if self.include_charges else torch.zeros(0).to(xh)
        h_cat = xh[:, :, self.n_dims:-1]  # TODO: have issue when include_charges is False
        h_cat = F.one_hot(torch.argmax(h_cat, dim=2), self.num_classes) * node_mask
        h_int = torch.round(h_int).long() * node_mask
        h = {'integer': h_int, 'categorical': h_cat}

        return x, h

    @torch.no_grad()
    def reconstruct(self, x, h, node_mask=None, edge_mask=None, context=None):
        pass

    def log_info(self):
        """
        Some info logging of the model.
        """
        info = None
        print(info)

        return info

def disabled_train(self, mode=True):
    """Overwrite model.train with this function to make sure train/eval mode
    does not change anymore."""
    return self</code></pre> 
<p></p> 
<h5></h5> 
<p>2.3 GeoLDMåˆ†å­ç”Ÿæˆä»£ç </p> 
<p>ä½œè€…å¹¶æ²¡æœ‰ç›´æ¥æä¾›ç”Ÿæˆåˆ†å­çš„ä»£ç ã€‚è€Œæ˜¯é€šè¿‡è¯„ä¼°æ¨¡å‹çš„å½¢å¼ï¼Œå‘å¸ƒäº†ç”Ÿæˆåˆ†å­çš„æ–¹æ³•ï¼ˆå³åˆ†å­ç”Ÿæˆå’Œæ¨¡å‹è¯„ä¼°æ”¾åœ¨äº†ä¸€èµ·ï¼‰ã€‚è¦ç”Ÿæˆåˆ†å­å¹¶è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œéœ€è¦åœ¨./ç›®å½•ä¸‹æ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼š</p> 
<pre><code class="language-python">python eval_analyze.py \
  --model_path outputs/pretrained/drugs_latent2 \
  --n_samples 10
</code></pre> 
<p></p> 
<p>--model_pathï¼šè¢«è¯„ä¼°çš„æ¨¡å‹æ˜¯é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä¿å­˜åœ¨outputs/pretrained/drugs_latent2è·¯å¾„ã€‚</p> 
<p></p> 
<p>ç°åœ¨æ¥çœ‹ä¸€ä¸‹eval_analyze.pyä»£ç ã€‚é¦–å…ˆæ˜¯__mian__éƒ¨åˆ†ã€‚</p> 
<pre><code class="language-python">if __name__ == "__main__":
    main()
    
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_path', type=str, default="outputs/edm_1",
                        help='Specify model path')
    parser.add_argument('--n_samples', type=int, default=100,
                        help='Specify model path')
    parser.add_argument('--batch_size_gen', type=int, default=100,
                        help='Specify model path')
    parser.add_argument('--save_to_xyz', type=eval, default=False,
                        help='Should save samples to xyz files.')

    eval_args, unparsed_args = parser.parse_known_args()

    assert eval_args.model_path is not None

    # åŠ è½½æ¨¡å‹å‚æ•°pickleæ–‡ä»¶
    with open(join(eval_args.model_path, 'args.pickle'), 'rb') as f:
        args = pickle.load(f)

    # CAREFUL with this --&gt;
    if not hasattr(args, 'normalization_factor'):
        args.normalization_factor = 1
    if not hasattr(args, 'aggregation_method'):
        args.aggregation_method = 'sum'
    ####################### by wufeil ####################################
    # args.cuda = not args.no_cuda and torch.cuda.is_available()
    # device = torch.device("cuda" if args.cuda else "cpu")
    # å› ä¸ºæ˜¯macOSæ‰€ä»¥è®¾ç½®ä¸ºmps
    args.cuda = not args.no_cuda and torch.backends.mps.is_available() 
    device = torch.device("mps" if args.cuda else "cpu")
    args.device = device
    #####################################################################
    dtype = torch.float32
    utils.create_folders(args)
    print(args)

    # Retrieve QM9 dataloadersæ•°æ® dataloader
    dataloaders, charge_scale = dataset.retrieve_dataloaders(args)

    dataset_info = get_dataset_info(args.dataset, args.remove_h)

    # Load model åŠ è½½æ¨¡å‹
    generative_model, nodes_dist, prop_dist = get_latent_diffusion(args, device, dataset_info, dataloaders['train'])
    if prop_dist is not None:
        property_norms = compute_mean_mad(dataloaders, args.conditioning, args.dataset)
        prop_dist.set_normalizer(property_norms)
    generative_model.to(device)

    # åŠ è½½æ¨¡å‹çš„æƒé‡
    fn = 'generative_model_ema.npy' if args.ema_decay &gt; 0 else 'generative_model.npy'
    flow_state_dict = torch.load(join(eval_args.model_path, fn), map_location=device)
    generative_model.load_state_dict(flow_state_dict)

    # ç”Ÿæˆåˆ†å­ï¼Œå¹¶è®¡ç®— ç¨³å®šç‡ï¼Œæœ‰æ•ˆç‡ï¼Œç‹¬ç‰¹ç‡ï¼Œæ–°é¢–ç‡
    # Analyze stability, validity, uniqueness and novelty
    stability_dict, rdkit_metrics = analyze_and_save(
        args, eval_args, device, generative_model, nodes_dist,
        prop_dist, dataset_info, n_samples=eval_args.n_samples,
        batch_size=eval_args.batch_size_gen, save_to_xyz=eval_args.save_to_xyz)
    print(stability_dict)

    # æ‰“å°ç»“æœ
    if rdkit_metrics is not None:
        rdkit_metrics = rdkit_metrics[0]
        print("Validity %.4f, Uniqueness: %.4f, Novelty: %.4f" % (rdkit_metrics[0], rdkit_metrics[1], rdkit_metrics[2]))
    else:
        print("Install rdkit roolkit to obtain Validity, Uniqueness, Novelty")

    # In GEOM-Drugs the validation partition is named 'val', not 'valid'.
    if args.dataset == 'geom':
        val_name = 'val'
        num_passes = 1
    else:
        val_name = 'valid'
        num_passes = 5

    # è¯„ä¼°éªŒè¯é›†å’Œæµ‹è¯•é›†çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ŒæŸå¤±
    # Evaluate negative log-likelihood for the validation and test partitions
    val_nll = test(args, generative_model, nodes_dist, device, dtype,
                   dataloaders[val_name],
                   partition='Val')
    print(f'Final val nll {val_nll}')
    test_nll = test(args, generative_model, nodes_dist, device, dtype,
                    dataloaders['test'],
                    partition='Test', num_passes=num_passes)
    print(f'Final test nll {test_nll}')

    print(f'Overview: val nll {val_nll} test nll {test_nll}', stability_dict)
    with open(join(eval_args.model_path, 'eval_log.txt'), 'w') as f:
        print(f'Overview: val nll {val_nll} test nll {test_nll}',
              stability_dict,
              file=f)</code></pre> 
<p></p> 
<p>__mian__å‡½æ•°ä¸­ï¼Œä¼šåŠ è½½model_pathè·¯å¾„ä¸‹çš„ï¼Œæ¨¡å‹é…ç½®å‚æ•°pickleæ–‡ä»¶ï¼Œä¹Ÿä¼šåŠ è½½å…¶ä¸­çš„é¢„è®­ç»ƒçš„æƒé‡æ–‡ä»¶ã€‚è°ƒç”¨analyze_and_saveå‡½æ•°ï¼Œç”Ÿæˆåˆ†å­å¹¶è®¡ç®—ç¨³å®šç‡ï¼Œæœ‰æ•ˆç‡ï¼Œç‹¬ç‰¹ç‡ï¼Œæ–°é¢–ç‡ã€‚ä¸‹é¢è¯¦ç»†ä»‹ç»analyze_and_saveå‡½æ•°ã€‚</p> 
<p></p> 
<p>analyze_and_saveå‡½æ•°é¦–å…ˆä¼šé‡‡æ ·æ¯ä¸ªèŠ‚ç‚¹çš„åŸå­æ•°ï¼Œå³nodesxsampleã€‚ç„¶åï¼Œä½¿ç”¨qm9ã€‚samplingä¸­çš„sampleå‡½æ•°ï¼Œè°ƒç”¨GeoLDMæ¨¡å‹çš„sampleå‡½æ•°ï¼Œä¸ºåŸå­å¡«ä¸ŠåŸå­åæ ‡å’ŒåŸå­å±æ€§ã€‚ç„¶åï¼Œä¿å­˜æˆxyzæ–‡ä»¶æ ¼å¼ï¼ˆæ–‡ä»¶åç¼€åä¸ºtxtï¼‰ã€‚æœ€åï¼Œè°ƒç”¨analyze_stability_for_moleculeså‡½æ•°è®¡ç®—æŒ‡æ ‡ç»“æœï¼Œè¿”å›æµ‹è¯•æŒ‡æ ‡ç»“æœã€‚</p> 
<pre><code class="language-python">def analyze_and_save(args, eval_args, device, generative_model,
                     nodes_dist, prop_dist, dataset_info, n_samples=10,
                     batch_size=10, save_to_xyz=False):
    # æ‰¹æ¬¡å¤§å°
    batch_size = min(batch_size, n_samples)
    assert n_samples % batch_size == 0
    molecules = {'one_hot': [], 'x': [], 'node_mask': []}
    start_time = time.time()
    for i in range(int(n_samples/batch_size)):
        # æ¨¡ç‰ˆé‡‡æ ·ï¼Œæ¯ä¸ªåˆ†å­çš„å¤§å°åˆå§‹åŒ–å‡½æ•°
        nodesxsample = nodes_dist.sample(batch_size)
        # åˆ†å­ç”Ÿæˆ
        one_hot, charges, x, node_mask = sample(
            args, device, generative_model, dataset_info, prop_dist=prop_dist, nodesxsample=nodesxsample)

        molecules['one_hot'].append(one_hot.detach().cpu())
        molecules['x'].append(x.detach().cpu())
        molecules['node_mask'].append(node_mask.detach().cpu())

        current_num_samples = (i+1) * batch_size
        secs_per_sample = (time.time() - start_time) / current_num_samples
        print('\t %d/%d Molecules generated at %.2f secs/sample' % (
            current_num_samples, n_samples, secs_per_sample))

        # ä¿å­˜æˆxyzæ–‡ä»¶
        if save_to_xyz:
            id_from = i * batch_size
            qm9_visualizer.save_xyz_file(
                join(eval_args.model_path, 'eval/analyzed_molecules/'),
                one_hot, charges, x, dataset_info, id_from, name='molecule',
                node_mask=node_mask)

    molecules = {key: torch.cat(molecules[key], dim=0) for key in molecules}
    # è¯„ä¼°åˆ†å­
    stability_dict, rdkit_metrics = analyze_stability_for_molecules(
        molecules, dataset_info)
    # return æŒ‡æ ‡
    return stability_dict, rdkit_metrics</code></pre> 
<p></p> 
<p>æ¥ä¸‹æ¥ï¼Œçœ‹ä¸€ä¸‹å…¶ä¸­çš„å…³é”®ï¼Œsampleã€‚ sampleå‡½æ•°æ¥è‡ªäºqm9.samplingå‡½æ•°ã€‚qm9.samplingå‡½æ•°åŸºäºä¹‹å‰é‡‡æ ·çš„ï¼Œè®°å½•æ¯ä¸ªåˆ†å­æœ‰å‡ ä¸ªåŸå­çš„nodesxsampleï¼Œç”Ÿæˆnode_maskï¼ˆè®°å½•æ¯ä¸ªåˆ†å­æœ‰å‡ ä¸ªåŸå­ï¼Œæœ‰å‡ ä¸ªdummyåŸå­ï¼‰ã€‚åŸºäºnode_maskç”Ÿæˆedge_maskã€‚ç„¶åï¼Œæ ¹æ®æ˜¯å¦æœ‰æ¡ä»¶ï¼Œåˆå§‹åŒ–contextã€‚æœ‰äº†node_maskï¼Œedge_maskå’Œcontext è¿™äº›åˆ†å­ç”Ÿæˆçš„æ¨¡ç‰ˆï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨GeoLDMæ¨¡å‹ä¸­smapleå‡½æ•°ï¼Œå¯¹å¡«å……åˆ†å­çš„xå’Œhã€‚</p> 
<pre><code class="language-python">def sample(args, device, generative_model, dataset_info,
           prop_dist=None, nodesxsample=torch.tensor([10]), context=None,
           fix_noise=False):
    max_n_nodes = dataset_info['max_n_nodes']  # this is the maximum node_size in QM9

    # åˆ†å­å¯ç”Ÿæˆçš„æœ€å¤§èŠ‚ç‚¹ï¼ˆåŸå­ï¼‰æ•°æ£€æŸ¥
    assert int(torch.max(nodesxsample)) &lt;= max_n_nodes
    batch_size = len(nodesxsample)

    node_mask = torch.zeros(batch_size, max_n_nodes)
    # node_maskåˆå§‹åŒ–
    for i in range(batch_size):
        node_mask[i, 0:nodesxsample[i]] = 1

    # Compute edge_mask
    # ç”Ÿæˆå¯¹åº”edge_mask
    edge_mask = node_mask.unsqueeze(1) * node_mask.unsqueeze(2)
    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)
    edge_mask *= diag_mask
    edge_mask = edge_mask.view(batch_size * max_n_nodes * max_n_nodes, 1).to(device)
    node_mask = node_mask.unsqueeze(2).to(device)

    # TODO FIX: This conditioning just zeros.
    if args.context_node_nf &gt; 0:
        if context is None:
            context = prop_dist.sample_batch(nodesxsample)
        context = context.unsqueeze(1).repeat(1, max_n_nodes, 1).to(device) * node_mask
    else:
        context = None

    if args.probabilistic_model == 'diffusion':
        # GeoLDMåŸºäºæ¨¡ç‰ˆ(node_mask, edge_mask, context) é‡‡æ ·æ¯ä¸ªèŠ‚ç‚¹çš„åæ ‡å’ŒèŠ‚ç‚¹ç±»å‹ï¼Œ
        x, h = generative_model.sample(batch_size, max_n_nodes, node_mask, edge_mask, context, fix_noise=fix_noise)

        assert_correctly_masked(x, node_mask)
        assert_mean_zero_with_mask(x, node_mask)

        one_hot = h['categorical']
        charges = h['integer']

        assert_correctly_masked(one_hot.float(), node_mask)
        if args.include_charges:
            assert_correctly_masked(charges.float(), node_mask)

    else:
        raise ValueError(args.probabilistic_model)

    return one_hot, charges, x, node_mask</code></pre> 
<p></p> 
<p>å› æ­¤ï¼Œæ¥ä¸‹æ¥ï¼Œä»‹ç»ä¸€ä¸‹generative_model.sampleï¼Œå³GeoLDMä¸­çš„sampleï¼Œå…¶ä»£ç å¦‚ä¸‹ã€‚åœ¨ä»£ç ä¸­ï¼Œsampleå‡½æ•°ç›´æ¥é€šè¿‡super().sample()çš„è°ƒç”¨æ–¹å¼ï¼Œè°ƒç”¨äº†å…¶çˆ¶ç±»ï¼ˆEnVariationalDiffusionï¼‰çš„sampleå‡½æ•°ã€‚æ³¨æ„ï¼Œåœ¨EnVariationalDiffusionçš„sampleå‡½æ•°è¿”å›çš„xå’Œhä¸æ˜¯æœ€ç»ˆçš„åŸå­åæ ‡å’ŒåŸå­ç±»å‹ï¼Œè¿˜è¦ç»è¿‡è§£ç å™¨ï¼Œå³self.vae.decodeçš„è§£ç ï¼ŒçŒœå¾—åˆ°æœ€åçš„xå’Œhã€‚</p> 
<pre><code class="language-python">    @torch.no_grad()
    def sample(self, n_samples, n_nodes, node_mask, edge_mask, context, fix_noise=False):
        """
        Draw samples from the generative model.
        """
        # super().sampleï¼ˆï¼‰è°ƒç”¨çˆ¶ç±»çš„sampleï¼Œå³EnVariationalDiffusionçš„sample 
        z_x, z_h = super().sample(n_samples, n_nodes, node_mask, edge_mask, context, fix_noise)

        z_xh = torch.cat([z_x, z_h['categorical'], z_h['integer']], dim=2)
        diffusion_utils.assert_correctly_masked(z_xh, node_mask)
        x, h = self.vae.decode(z_xh, node_mask, edge_mask, context)

        return x, h</code></pre> 
<p></p> 
<p>åœ¨EnVariationalDiffusionçš„sampleå‡½æ•°ä¸­ï¼Œå…ˆè¿›è¡Œå™ªéŸ³é‡‡æ ·ï¼ŒæŒ‰ç…§fix_noiseå‚æ•°è®¾ç½®ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªåˆ†å­æ˜¯å¦ä½¿ç”¨ç›¸åŒçš„åˆå§‹åŒ–å™ªéŸ³ã€‚ç„¶åé€æ­¥è¿›è¡Œå»å™ªï¼ˆåˆå§‹åŒ–æ—¶é—´æ­¥sï¼Œç„¶åè°ƒç”¨sample_p_zs_given_ztå‡½æ•°ï¼Œé€æ­¥é¢„æµ‹å»å™ªåçš„zï¼‰ã€‚ä¸æ–­è¿­ä»£ï¼ŒçŸ¥é“s=0ï¼Œè¿›è¡Œæœ€åçš„å»å™ªï¼ˆsample_p_xh_given_z0ï¼‰ï¼Œç„¶åè¿”å›æ— å™ªéŸ³çŠ¶æ€ä¸‹çš„xå’Œhã€‚å®é™…ä¸Šè¿™ä¹Ÿæ˜¯xå’Œhçš„éšå‘é‡z_xå’Œz_hçš„å»å™ªè¿‡ç¨‹ã€‚</p> 
<pre><code class="language-python">    @torch.no_grad()
    def sample(self, n_samples, n_nodes, node_mask, edge_mask, context, fix_noise=False):
        """
        Draw samples from the generative model.
        """
        if fix_noise:
            # æ¯ä¸€ä¸ªåˆ†å­çš„z_tç›¸åŒ
            # Noise is broadcasted over the batch axis, useful for visualizations.
            z = self.sample_combined_position_feature_noise(1, n_nodes, node_mask)
        else:
            # æ¯ä¸€ä¸ªåˆ†å­z_tä¸åŒ
            z = self.sample_combined_position_feature_noise(n_samples, n_nodes, node_mask)

        diffusion_utils.assert_mean_zero_with_mask(z[:, :, :self.n_dims], node_mask)

        # Iteratively sample p(z_s | z_t) for t = 1, ..., T, with s = t - 1.
        # é€æ­¥å»å™ª z_t -&gt; z_t-1
        for s in reversed(range(0, self.T)):
            s_array = torch.full((n_samples, 1), fill_value=s, device=z.device)
            t_array = s_array + 1
            s_array = s_array / self.T
            t_array = t_array / self.T
            # é¢„æµ‹å»å™ªä»¥åçš„z_tï¼Œå³z_sã€‚
            z = self.sample_p_zs_given_zt(s_array, t_array, z, node_mask, edge_mask, context, fix_noise=fix_noise)

        # Finally sample p(x, h | z_0).
        # z_0 å»å™ª
        x, h = self.sample_p_xh_given_z0(z, node_mask, edge_mask, context, fix_noise=fix_noise)

        diffusion_utils.assert_mean_zero_with_mask(x, node_mask)

        #è´¨å¿ƒ
        max_cog = torch.sum(x, dim=1, keepdim=True).abs().max().item()
        if max_cog &gt; 5e-2:
            print(f'Warning cog drift with error {max_cog:.3f}. Projecting '
                  f'the positions down.')
            # å»è´¨å¿ƒ
            x = diffusion_utils.remove_mean_with_mask(x, node_mask)

        return x, h</code></pre> 
<p></p> 
<p>è‡³æ­¤ï¼ŒGeoLDMçš„ä¸»è¦ä»£ç å·²ç»åˆ†æå®Œæ¯•äº†ã€‚</p> 
<p>æ³¨ï¼šåŸä»£ç ä¸­ï¼Œå­˜åœ¨å°é”™è¯¯ï¼Œå¯èƒ½æ˜¯ä¸åŒæœºå™¨çš„åŸå› ã€‚å¦å¤–æˆ‘è¿™ä¸ªè®­ç»ƒçš„æœºå™¨æ˜¯mpsï¼Œä¸æ˜¯cu da.</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/98b736ad4ca5cdf323d98250ea941a07/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">ã€comp221ã€‘Flask Python Web</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3175da9b8ec16e7b6ac8e5b70fcd27ae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;Â»</span>
			<p class="pager__title">C&#43;&#43;çš„ä»‹ç»å‰æ™¯å’Œåœ¨å®é™…å¼€å‘ä¸­çš„è¿ç”¨åŠä¸javaå¯¹æ¯”ï¼ˆåµŒå…¥å¼ï¼Œæ¸¸æˆï¼‰</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 ç¼–ç¨‹å¤§ç™½çš„åšå®¢.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>