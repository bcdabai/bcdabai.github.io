<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>​Linux开源存储漫谈（3）Linux-IO iSCSI Target vs NFS - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="​Linux开源存储漫谈（3）Linux-IO iSCSI Target vs NFS" />
<meta property="og:description" content="SCSI &amp; iSCSI SCSI是Small Computer System Interface（小型计算机系统接口）的缩写，基于client-server模型，SCSI client称为initiator，server称为target。iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是一种基于因特网及SCSI-3协议下的存储技术。iSCSI是建立在TCP协议之上的块传输层协议，是一组基于块的命令，iSCSI协议使用Initiator将一组SCSI命令发送到target上执行某种操作以实现存储和数据读写。
iSCSI及基本概念 两个重要的组件启动器（initiator）和目标（target）
目标，iSCSI target是存储设备端或模拟存储设备，是存储设备，其目的是为其他主机提供网络磁盘。Linux内核LIO模块即实现iSCSI target的模拟， 详见Linux Storage Stack Diagram 。
启动器，iSCSI initiator就是能够使用target的客户端，通常是服务器。也就是说，想要连接到iscsi target的服务器，需要安装iSCSI initiator相关的软件包，如open-iscsi
Network Portal: 网络端口。在 initiator端用IP地址标识，在target端IP地址及端口IP:PORT
Session: 连接initiator和target的一组TCP连接构成。可以向 session 添加 TCP 连接，一个session中是可以有多个连接，一个 session只能看到同一个target（多网卡应用）
Connection : Initiator和target间的一个TCP连接
CID(Connection ID): 每个connection都有一个CID ，该标识在session范围内是唯一。CID由 initiator产生，在 login 请求和使用 logout 关闭 连接时传递给 target。
SSID（Session ID）：一个 iSCSI Initiator 与 iSCSI Target 之间的会话（Session）由会话ID（SSID）定义，该会话ID是一个由发起方部分（ISID）和目标部分（Target Portal Group Tag）组成的元组。 ISID 在会话建立时由发起者明确指定。 Target Portal Group Tag 由发起者在连接建立时选择的 TCP端口来隐式指定" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f8cd21596b37893f0b1d12f8d2be52eb/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-04T09:26:07+08:00" />
<meta property="article:modified_time" content="2023-06-04T09:26:07+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">​Linux开源存储漫谈（3）Linux-IO iSCSI Target vs NFS</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>SCSI &amp; iSCSI</h3> 
<p>SCSI是Small Computer System Interface（<a href="https://baike.baidu.com/item/%E5%B0%8F%E5%9E%8B%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%8E%A5%E5%8F%A3/10638565?fromModule=lemma_inlink" rel="nofollow" title="小型计算机系统接口">小型计算机系统接口</a>）的缩写，基于client-server模型，SCSI client称为initiator，server称为target。iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是一种基于因特网及SCSI-3协议下的存储技术。iSCSI是建立在TCP协议之上的块传输层协议，是一组基于块的命令，iSCSI协议使用Initiator将一组SCSI命令发送到target上执行某种操作以实现存储和数据读写。</p> 
<h3>iSCSI及基本概念</h3> 
<p>两个重要的组件启动器（initiator）和目标（target）<br><strong>目标</strong>，iSCSI target是存储设备端或模拟存储设备，是存储设备，其目的是为其他主机提供网络磁盘。Linux内核LIO模块即实现iSCSI target的模拟， 详见<a class="link-info" href="https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram" rel="nofollow" title="Linux Storage Stack Diagram">Linux Storage Stack Diagram</a> 。</p> 
<p><strong>启动器</strong>，iSCSI initiator就是能够使用target的客户端，通常是服务器。也就是说，想要连接到iscsi target的服务器，需要安装iSCSI initiator相关的软件包，如open-iscsi</p> 
<p><strong>Network Portal</strong>: 网络端口。在 initiator端用IP地址标识，在target端IP地址及端口IP:PORT</p> 
<p><strong>Session</strong>: 连接initiator和target的一组TCP连接构成。可以向 session 添加 TCP 连接，一个session中是可以有多个连接，一个 session只能看到同一个target（多网卡应用）</p> 
<p><strong>Connection </strong>: Initiator和target间的一个TCP连接</p> 
<p><strong>CID(Connection ID)</strong>: 每个connection都有一个CID ，该标识在session范围内是唯一。CID由 initiator产生，在 login 请求和使用 logout 关闭 连接时传递给 target。</p> 
<p><strong>SSID（Session ID）</strong>：一个 iSCSI Initiator 与 iSCSI Target 之间的会话（Session）由会话ID（SSID）定义，该会话ID是一个由发起方部分（ISID）和目标部分（Target Portal Group Tag）组成的元组。 ISID 在会话建立时由发起者明确指定。 Target Portal Group Tag 由发起者在连接建立时选择的 TCP端口来隐式指定</p> 
<p><strong>Portal Groups</strong>: 网络端口组。iSCSI session 支持多连接，一些实现能把通过多个端口建立的多个连接捆绑到一个 session。 一个 iSCSI 网络实体的多个网络端口被定义为一个网络端口组，把该组和一个 session 联系起来，该 session 就可以捆绑通过该组内多个端口建立的多个连接，再使它们一起协同工作以达到捆绑的目的。每一个该组的 session 并不需要包括该组的所有网络端口。一个 iSCSI 节点可能有一或者多个网络端口组，但是每一个 iSCSI 使用的网络端口只能属于 iSCSI 节点的一个组。</p> 
<p><strong>Target Portal Group Tag</strong>: 网络端口组标识。使用 16 比特的数标识一个网络端口组。在 一个 iSCSI 节点里，所有具有同样组标志的端口构成一个网络端口组</p> 
<p><strong>iSCSI Task</strong>: 一个 iSCSI 任务是指一个需要响应的 iSCSI 请求</p> 
<h3>搭建基于Linux-IO的iSCSI target</h3> 
<p>1.  安装targetcli并启动target服务</p> 
<p>ubuntu-live-server 22.04安装targetcli并启动target.service服务（或参考<a class="link-info" href="https://mp.csdn.net/mp_blog/creation/editor/130570984" title="环境准备篇">环境准备篇</a>）</p> 
<pre><code class="language-bash">root@nvme:~# apt install -y targetcli-fb
root@nvme:~# systemctl start target.service
root@nvme:~# systemctl enable target.service
</code></pre> 
<p>2. 运行targetcli配置iscsi target，先运行lsblk查看块设备</p> 
<pre><code class="language-bash">root@nvme:~# lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
    ......
sda       8:0    0   1.8T  0 disk
├─sda2    8:2    0   200G  0 part /
└─sda5    8:5    0  1007K  0 part
nvme0n1 259:0    0 931.5G  0 disk
</code></pre> 
<p>运行targetcli进入交互窗口(# 手动添加说明信息 #)</p> 
<pre><code class="language-bash">root@nvme:~# targetcli
targetcli shell version 2.1.53
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/&gt; ls
o- / ................................................................................ [...]
  o- backstores ..........# iscsi后端支持的存储类型 #................................. [...]
  | o- block .............# 块设备，磁盘驱动器、磁盘分区、逻辑卷等 #.... [Storage Objects: 0]
  | o- fileio ............# 指定大小的文件 #........................... [Storage Objects: 0]
  | o- pscsi .............# 物理SCSI设备 #............................. [Storage Objects: 0]
  | o- ramdisk ...........# 内存盘，重启失效 #......................... [Storage Objects: 0]
  o- iscsi ................................................. [mutual disc auth, Targets: 0]
  o- loopback ................................................................ [Targets: 0]
  o- vhost ................................................................... [Targets: 0]
/&gt;</code></pre> 
<p>第一步，基于/dev/nvme0n1创建名为iscsi_nvme的backstores/block</p> 
<pre><code class="language-bash">/&gt; cd backstores/block/
/backstores/block&gt;
/backstores/block&gt; create iscsi_nvme /dev/nvme0n1
Created block storage object iscsi_nvme using /dev/nvme0n1.</code></pre> 
<p>第二步，创建iSCSI target，名称可由系统自动生成，是一串用于描述共享资源的唯一字符串，iSCSI target 命名约定格式，iqn.yyyy-mm.*:*，iqn：ISCSI Qualified Name，如果没有关闭（<strong>/&gt; set group=global auto_add_default_portal=false</strong>），则自动创建网络端口组(portal groups）及网络端口（portal，0.0.0.0:3260）</p> 
<pre><code class="language-bash">/&gt; get global
GLOBAL CONFIG GROUP
===================
auto_add_default_portal=true
----------------------------
If true, adds a portal listening on all IPs to new targets.

auto_add_mapped_luns=true
-------------------------
If true, automatically create node ACLs mapped LUNs after creating a new target LUN or a new node ACL

auto_cd_after_create=false
--------------------------
If true, changes current path to newly created objects.

auto_enable_tpgt=true
---------------------
If true, automatically enables TPGTs upon creation.

auto_save_on_exit=true
----------------------
If true, saves configuration on exit.

auto_use_daemon=false
---------------------
If true, commands will be sent to targetclid.

color_command=cyan
------------------
Color to use for command completions.

color_default=none
------------------
Default text display color.

color_keyword=cyan
------------------
Color to use for keyword completions.

color_mode=true
---------------
Console color display mode.

color_parameter=magenta
-----------------------
Color to use for parameter completions.

color_path=magenta
------------------
Color to use for path completions

daemon_use_batch_mode=false
---------------------------
If true, use batch mode for daemonized approach.

export_backstore_name_as_model=true
-----------------------------------
If true, the backstore name is used for the scsi inquiry model name.

logfile=/root/.targetcli/log.txt
--------------------------------
Logfile to use.

loglevel_console=info
---------------------
Log level for messages going to the console.

loglevel_file=debug
-------------------
Log level for messages going to the log file.

max_backup_files=10
-------------------
Max no. of configurations to be backed up in /etc/rtslib-fb-target/backup/ directory.

prompt_length=30
----------------
Max length of the shell prompt path, 0 for infinite.

tree_max_depth=0
----------------
Maximum depth of displayed node tree.

tree_round_nodes=true
---------------------
Tree node display style.

tree_show_root=true
-------------------
Whether or not to display tree root.

tree_status_mode=true
---------------------
Whether or not to display status in tree.

/&gt;

/&gt; cd /iscsi
/iscsi&gt;
/iscsi&gt; create iqn.2023-05.nvme.iscsi.com:nvmeiscsi
Created target iqn.2023-05.nvme.iscsi.com:nvmeiscsi.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.
</code></pre> 
<p>第三步，创建LUN</p> 
<pre><code class="language-bash">/&gt; cd iscsi/iqn.2023-05.nvme.iscsi.com:nvmeiscsi/tpg1/luns
/iscsi/iqn.20...csi/tpg1/luns&gt; create /backstores/block/iscsi_nvme
Created LUN 0.
</code></pre> 
<p>第四步，配置discovery_auth &amp; acl，"# 注释信息" </p> 
<pre><code class="language-bash">
# 设置discovery认证信息

/iscsi&gt; set discovery_auth userid=disUser password=disUserPwd
Parameter userid is now 'disUser'.
Parameter password is now 'disUserPwd'.
/iscsi&gt; set discovery_auth mutual_userid=mDisUser mutual_password=mDisUserPwd
Parameter mutual_userid is now 'mDisUser'.
Parameter mutual_password is now 'mDisUserPwd'.
/iscsi&gt; get discovery_auth
DISCOVERY_AUTH CONFIG GROUP
===========================
enable=True
-----------
The enable discovery_auth parameter.

mutual_password=mDisUserPwd
---------------------------
The mutual_password discovery_auth parameter.

mutual_userid=mDisUser
----------------------
The mutual_userid discovery_auth parameter.

password=disUserPwd
-------------------
The password discovery_auth parameter.

userid=disUser
--------------
The userid discovery_auth parameter.

/iscsi&gt;

# 设置target 认证信息（只读权限）

/iscsi&gt; cd iqn.2023-05.nvme.iscsi.com:nvmeiscsi/tpg1/
/iscsi/iqn.20...vmeiscsi/tpg1&gt; set attribute authentication=1 generate_node_acls=1
Parameter authentication is now '1'.
Parameter generate_node_acls is now '1'.
/iscsi/iqn.20...vmeiscsi/tpg1&gt; set auth userid=clientUser password=clientUserPwd
Parameter userid is now 'clientUser'.
Parameter password is now 'clientUserPwd'.
/iscsi/iqn.20...vmeiscsi/tpg1&gt; set auth mutual_userid=mClientUser mutual_password=mClientUserPwd
Parameter mutual_userid is now 'mClientUser'.
Parameter mutual_password is now 'mClientUserPwd'.
/iscsi/iqn.20...vmeiscsi/tpg1&gt; get auth
AUTH CONFIG GROUP
=================
mutual_password=mClientUserPwd
------------------------------
The mutual_password auth parameter.

mutual_userid=mClientUser
-------------------------
The mutual_userid auth parameter.

password=clientUserPwd
----------------------
The password auth parameter.

userid=clientUser
-----------------
The userid auth parameter.
/iscsi/iqn.20...vmeiscsi/tpg1&gt; 
/iscsi/iqn.20...vmeiscsi/tpg1&gt; cd acls/

# 创建acl，及配置认证信息，acl的名称将用于InitiatorName(读写权限)
/iscsi/iqn.20...csi/tpg1/acls&gt; create iqn.2023-05.vm-nvme.iscsi.com:client001
Created Node ACL for iqn.2023-05.vm-nvme.iscsi.com:client001
Created mapped LUN 0.
/iscsi/iqn.20...csi/tpg1/acls&gt; cd iqn.2023-05.vm-nvme.iscsi.com:client001/
/iscsi/iqn.20...com:client001&gt; set auth userid=rwClientUser password=rwClientUserPwd
Parameter userid is now 'rwClientUser'.
Parameter password is now 'rwClientUserPwd'.
/iscsi/iqn.20...com:client001&gt; set auth mutual_userid=mRwClientUser mutual_password=mRwClientUserPwd
Parameter mutual_userid is now 'mRwClientUser'.
Parameter mutual_password is now 'mRwClientUserPwd'.
/iscsi/iqn.20...com:client001&gt; get auth
AUTH CONFIG GROUP
=================
mutual_password=mRwClientUserPwd
--------------------------------
The mutual_password auth parameter.

mutual_userid=mRwClientUser
---------------------------
The mutual_userid auth parameter.

password=rwClientUserPwd
------------------------
The password auth parameter.

userid=rwClientUser
-------------------
The userid auth parameter.

/iscsi/iqn.20...com:client001&gt;
/iscsi/iqn.20...com:client001&gt; cd /
</code></pre> 
<p>第四步，最后查看iscsi配置信息</p> 
<pre><code class="language-bash">/&gt; cd iscsi/iqn.2023-05.nvme.iscsi.com:nvmeiscsi/tpg1/
/iscsi/iqn.20...vmeiscsi/tpg1&gt; cd /
/&gt; ls
o- / ......................................................................................................................... [...]
  o- backstores .............................................................................................................. [...]
  | o- block .................................................................................................. [Storage Objects: 1]
  | | o- iscsi_nvme ................................................................. [/dev/nvme0n1 (931.5GiB) write-thru activated]
  | |   o- alua ................................................................................................... [ALUA Groups: 1]
  | |     o- default_tg_pt_gp ....................................................................... [ALUA state: Active/optimized]
  | o- fileio ................................................................................................. [Storage Objects: 0]
  | o- pscsi .................................................................................................. [Storage Objects: 0]
  | o- ramdisk ................................................................................................ [Storage Objects: 0]
  o- iscsi .......................................................................................... [mutual disc auth, Targets: 1]
  | o- iqn.2023-05.nvme.iscsi.com:nvmeiscsi .............................................................................. [TPGs: 1]
  |   o- tpg1 .................................................................................... [gen-acls, tpg-auth, mutual auth]
  |     o- acls .......................................................................................................... [ACLs: 1]
  |     | o- iqn.2023-05.vm-nvme.iscsi.com:client001 ................................................ [auth via tpg, Mapped LUNs: 1]
  |     |   o- mapped_lun0 ............................................................................ [lun0 block/iscsi_nvme (rw)]
  |     o- luns .......................................................................................................... [LUNs: 1]
  |     | o- lun0 ............................................................. [block/iscsi_nvme (/dev/nvme0n1) (default_tg_pt_gp)]
  |     o- portals .................................................................................................... [Portals: 1]
  |       o- 0.0.0.0:3260 ..................................................................................................... [OK]
  o- loopback ......................................................................................................... [Targets: 0]
  o- vhost ............................................................................................................ [Targets: 0]
/&gt; exit
</code></pre> 
<p>第五步，设置防火墙（开放端口，或直接关闭ubuntu防火墙）</p> 
<pre><code class="language-bash"># 开放3260端口
root@nvme:~# ufw allow 3260
Rules updated
Rules updated (v6)

# 关闭Ubuntu防火墙
root@nvme:~# ufw disable
Firewall stopped and disabled on system startup
root@nvme:~#
</code></pre> 
<h3>配置iSCSI Initiator</h3> 
<p>1. 启动qemu-kvm虚拟机，参见<a class="link-info" href="https://mp.csdn.net/mp_blog/creation/editor/130570984" title="环境准备篇">环境准备篇</a>启动qemu虚拟机</p> 
<p>2. 进入root用户，安装并配置open-iscsi</p> 
<pre><code class="language-bash">root@nvme:~# ssh ubuntu@192.168.2.112
Welcome to Ubuntu 22.04.2 LTS (GNU/Linux 5.15.0-71-generic x86_64)

    ......

Expanded Security Maintenance for Applications is not enabled.

5 updates can be applied immediately.
To see these additional updates run: apt list --upgradable

Enable ESM Apps to receive additional future security updates.
See https://ubuntu.com/esm or run: sudo pro status


Last login: Fri May 12 03:27:02 2023 from 192.168.2.111
ubuntu@vm-nvme:~$ sudo su -
root@vm-nvme:~#
root@vm-nvme:~# apt install -y open-iscsi

# 修改/etc/iscsi/initiatorname.iscsi，修改后内容如下
root@vm-nvme:~# cat /etc/iscsi/initiatorname.iscsi | grep -v -E "^$|^#"
InitiatorName=iqn.2023-05.vm-nvme.iscsi.com:client001
root@vm-nvme:~#


# 修改/etc/iscsi/iscsi.conf，修改后内容如下
root@vm-nvme:~# cat  /etc/iscsi/iscsid.conf | grep -v -E "^$|^#" | grep auth
node.session.auth.authmethod = CHAP
node.session.auth.username = rwClientUser
node.session.auth.password = rwClientUserPwd
node.session.auth.username_in = mRwClientUser
node.session.auth.password_in = mRwClientUserPwd
discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = disUser
discovery.sendtargets.auth.password = disUserPwd
discovery.sendtargets.auth.username_in = mDisUser
discovery.sendtargets.auth.password_in = mDisUserPwd

</code></pre> 
<p>3. 发现iSCSI target，然后login，之后运行lsblk查看iSCSI设备（/dev/sdc）</p> 
<pre><code class="language-bash">root@vm-nvme:~# iscsiadm -m discovery -t sendtargets -p 192.168.2.111:3260
192.168.2.111:3260,1 iqn.2023-05.nvme.iscsi.com:nvmeiscsi
root@vm-nvme:~# 
root@vm-nvme:~# iscsiadm -m node -T  iqn.2023-05.nvme.iscsi.com:nvmeiscsi --login
Logging in to [iface: default, target: iqn.2023-05.nvme.iscsi.com:nvmeiscsi, portal: 192.168.2.111,3260]
Login to [iface: default, target: iqn.2023-05.nvme.iscsi.com:nvmeiscsi, portal: 192.168.2.111,3260] successful.
root@vm-nvme:~# 
root@vm-nvme:~# iscsiadm -m session show
tcp: [2] 192.168.2.111:3260,1 iqn.2023-05.nvme.iscsi.com:nvmeiscsi (non-flash)
root@vm-nvme:~# 
root@vm-nvme:~# lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
fd0       2:0    1     4K  0 disk
   ......
sdb       8:16   0  32.2G  0 disk
├─sdb1    8:17   0  32.1G  0 part /
├─sdb14   8:30   0     4M  0 part
└─sdb15   8:31   0   106M  0 part /boot/efi
sdc       8:32   0 931.5G  1 disk
vda     252:0    0   366K  0 disk
root@vm-nvme:~# 

</code></pre> 
<p>4. 来一波fio性能测试，性能测试之前，先看一下网络，宿主机（192.168.2.111）和虚拟机（192.168.2.112）通过宿主机虚拟网桥kvmbr0连接，先使用iperf测试一下两个系统之间的网络带宽，两个OS都执行，apt install -y iperf，如果虚拟机执行失败，使用宿主机/etc/apt/sources.list覆盖虚拟机/etc/apt/sources.list，在执行apt install -y iperf</p> 
<pre><code class="language-bash"># 宿主机192.168.2.111，安装并启动iperf server
root@nvme:~# apt install -y iperf
root@nvme:~# iperf -s -f M
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size: 9.54 MByte (default)
------------------------------------------------------------
[  1] local 192.168.2.111 port 5001 connected with 192.168.2.112 port 49926
[ ID] Interval       Transfer     Bandwidth
[  1] 0.0000-10.0011 sec  37436 MBytes  3743 MBytes/sec
root@nvme:~#


# 虚拟机192.168.2.112
root@vm-nvme:~# apt install -y iperf
root@vm-nvme:~# iperf -f G -c 192.168.2.111
------------------------------------------------------------
Client connecting to 192.168.2.111, TCP port 5001
TCP window size:  595 KByte (default)
------------------------------------------------------------
[  1] local 192.168.2.112 port 56570 connected with 192.168.2.111 port 5001
[ ID] Interval       Transfer     Bandwidth
[  1] 0.0000-10.0116 sec  36.5 GBytes  3.64 GBytes/sec
root@vm-nvme:~#

</code></pre> 
<p>iperf测试结果可以看出宿主机与虚拟机的网络带宽可达到36Gbps，3700M字节，可以断定网络带宽足够，不会成为iSCSI数据传输瓶颈</p> 
<p>确认网络带宽后，启动fio测试iSCSI的性能，使用<a class="link-info" href="https://blog.csdn.net/arvey8888/article/details/130598516" title="​Linux开源存储漫谈——IO性能测试利器">​Linux开源存储漫谈——IO性能测试利器</a>中本地测试相同fio参数</p> 
<pre><code class="language-bash">root@vm-nvme:~/fio# lsscsi -l
[0:0:1:0]    disk    ATA      QEMU HARDDISK    2.5+  /dev/sda
  state=running queue_depth=1 scsi_level=6 type=0 device_blocked=0 timeout=30
[1:0:0:0]    disk    ATA      QEMU HARDDISK    2.5+  /dev/sdb
  state=running queue_depth=1 scsi_level=6 type=0 device_blocked=0 timeout=30
[2:0:0:0]    disk    LIO-ORG  iscsi_nvme       4.0   /dev/sdc
  state=running queue_depth=128 scsi_level=7 type=0 device_blocked=0 timeout=30
root@vm-nvme:~/fio# 
root@vm-nvme:~/fio# cat nread.fio
[global]
bs=4096
rw=read
ioengine=libaio
size=50G
direct=1
iodepth=256
iodepth_batch=128
iodepth_low=128
iodepth_batch_complete=128
userspace_reap
group_reporting
[test]
numjobs=1
filename=/dev/sdc
root@vm-nvme:~/fio# fio nread.fio
test: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=256
fio-3.28
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=1183MiB/s][r=303k IOPS][eta 00m:00s]
test: (groupid=0, jobs=1): err= 0: pid=2448: Fri May 12 06:25:25 2023
  read: IOPS=299k, BW=1168MiB/s (1225MB/s)(50.0GiB/43839msec)
    slat (usec): min=69, max=1855, avg=93.69, stdev=16.30
    clat (usec): min=368, max=12692, avg=745.73, stdev=115.29
     lat (usec): min=457, max=13858, avg=839.43, stdev=119.77
    clat percentiles (usec):
     |  1.00th=[  586],  5.00th=[  685], 10.00th=[  693], 20.00th=[  709],
     | 30.00th=[  717], 40.00th=[  725], 50.00th=[  734], 60.00th=[  742],
     | 70.00th=[  758], 80.00th=[  775], 90.00th=[  807], 95.00th=[  865],
     | 99.00th=[  979], 99.50th=[ 1074], 99.90th=[ 1778], 99.95th=[ 2311],
     | 99.99th=[ 4359]
   bw (  MiB/s): min= 1001, max= 1217, per=100.00%, avg=1168.30, stdev=27.87, samples=87
   iops        : min=256256, max=311552, avg=299084.48, stdev=7134.34, samples=87
  lat (usec)   : 500=0.22%, 750=66.40%, 1000=32.58%
  lat (msec)   : 2=0.74%, 4=0.05%, 10=0.01%, 20=0.01%
  cpu          : usr=7.08%, sys=23.74%, ctx=535537, majf=0, minf=266
  IO depths    : 1=0.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=100.0%
     submit    : 0=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=100.0%
     complete  : 0=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=100.0%
     issued rwts: total=13107200,0,0,0 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=256

Run status group 0 (all jobs):
   READ: bw=1168MiB/s (1225MB/s), 1168MiB/s-1168MiB/s (1225MB/s-1225MB/s), io=50.0GiB (53.7GB), run=43839-43839msec

Disk stats (read/write):
  sdc: ios=203884/0, merge=12844944/0, ticks=149896/0, in_queue=149896, util=99.82%
root@vm-nvme:~/fio#</code></pre> 
<p>测试数据可以看出，内核NVME驱动 + LIO iSCSI Target，性能损耗，iops：299k/728k，BW：1183MiB/2842MiB，无论是iops还是BW都损耗60%，latency则增长近1.5倍，338增长到839</p> 
<h3>iSCSI混合随机读写（读写比例80/20）</h3> 
<pre><code class="language-bash">[global]
bs=4096
rw=randrw
rwmixwrite=20
ioengine=libaio
direct=1
group_reporting
iodepth=32
iodepth_batch=8
iodepth_low=8
iodepth_batch_complete=8
userspace_reap
runtime=60
[test]
numjobs=16
filename=/dev/sdc</code></pre> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>iodepth/_low/_batch/_complete</td><td>numjobs</td><td>iops(read)</td><td>bw(read)</td><td>iops(write)</td><td>bw(write)</td></tr><tr><td>64/16/16/16</td><td>16</td><td>29.4k</td><td>115MiB/s</td><td>7355</td><td>28.7MiB/s</td></tr><tr><td>32/8/8/8</td><td>16</td><td>29.8k</td><td>117MiB/s</td><td>7464</td><td>29.2MiB/s</td></tr><tr><td>32/8/8/8</td><td>8</td><td>28.9k</td><td>113MiB/s</td><td>7229</td><td>28.2MiB/s</td></tr></tbody></table> 
<p>对比<a class="link-info" href="https://blog.csdn.net/arvey8888/article/details/130598516" title="​Linux开源存储漫谈——IO性能测试利器">​Linux开源存储漫谈——IO性能测试利器</a>内核nvme驱动的测试结果相同参数总体上损耗掉25%的性能</p> 
<h3>NFS性能测试</h3> 
<p>Linux内核中实现了nfs，nfs具体是用rpc来实现的，linux内核的rpc模块实现大致三个模块：一是与用户层的接口；二是逻辑控制框架；三是通信框架，其中，用户层的接口及通信框架都是可插拔可替换，nfs就是作为一个用户接口子的rpc应用，介绍完了NFS的实现原理，开始搭建NFS服务</p> 
<p><strong>铲除iscsi配置</strong></p> 
<p><strong>虚拟机</strong>，注销iscsi session禁掉iscsi服务</p> 
<pre><code class="language-bash">root@vm-nvme:~# iscsiadm -m node --logout
root@vm-nvme:~# systemctl stop iscsi.service
root@vm-nvme:~# systemctl disable iscsi.service
</code></pre> 
<p><strong>宿主机</strong>，进入targetcli delete掉target 及backstores/block</p> 
<pre><code class="language-bash"># 清除iscsi target
root@nvme:~# targetcli
/&gt; cd iscsi
/iscsi&gt; delete iqn.2023-05.nvme.iscsi.com:nvmeiscsi
/iscsi&gt; cd /backstores/block
/backstores/block&gt; delete iscsi_nvme
/backstores/block&gt; exit
root@nvme:~# 

# 停止并禁用target服务
root@nvme:~# systemctl stop target.service
root@nvme:~# systemctl disable target.service

</code></pre> 
<p><strong>宿主机配置NFS服务（见：<a class="link-info" href="https://ubuntu.com/server/docs/service-nfs" rel="nofollow" title="Install the NFS Server on Ubuntu">Install the NFS Server on Ubuntu</a>）</strong></p> 
<pre><code class="language-bash"># 安装nfs-server
root@nvme:~# apt install -y nfs-kernel-server

# mkfs nvme盘，并挂载至/mnt/nvme
root@nvme:~# mkdir -p /mnt/nvme
root@nvme:~# mkfs -t ext4 /dev/nvme0n1
root@nvme:~# mount -t ext4 /dev/nvme0n1 /mnt/nvme
root@nvme:~# mkdir /mnt/nvme/share
root@nvme:~# chown nobody:nogroup /mnt/nvme/share
root@nvme:~# chmod 777 /mnt/nvme/share

# /etc/exports 文件追加"/mnt/nvme/share *(rw,sync,fsid=0,crossmnt,no_subtree_check)"
root@nvme:~# cat /etc/exports
# /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) 
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
/mnt/nvme/share *(rw,sync,fsid=0,crossmnt,no_subtree_check)

# 启动nfs服务
root@nvme:~# systemctl start nfs-kernel-server </code></pre> 
<p><strong>虚拟机 （见：<strong><a class="link-info" href="https://ubuntu.com/server/docs/service-nfs" rel="nofollow" title="Install the NFS Server on Ubuntu">Install the NFS Server on Ubuntu</a></strong>）</strong></p> 
<pre><code class="language-bash">root@vm-nvme:~# apt install -y nfs-common
root@vm-nvme:~# mkdir -p /mnt/nvme
root@vm-nvme:~# mount -t nfs 192.168.2.111:/mnt/nvme/share /mnt/nvme
</code></pre> 
<p><strong>fio测试NFS性能</strong></p> 
<pre><code class="language-bash"># 宿主机与虚拟机分别执行fio -name=write -rw=write -ioengine=libaio -direct=1 -iodepth=64 -numjobs=1 -size=5G

# 先在宿主机执行 

root@nvme:~# fio -name=write -rw=write -ioengine=libaio -direct=1 -iodepth=64 -numjobs=1 -size=5G -filename=/mnt/nvme/share/local-fio-5g
write: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
......
Jobs: 1 (f=1): [W(1)][100.0%][w=527MiB/s][w=135k IOPS][eta 00m:00s]
write: (groupid=0, jobs=1): err= 0: pid=2458: Sun May 14 07:05:39 2023
  write: IOPS=135k, BW=527MiB/s (553MB/s)(5120MiB/9715msec); 0 zone resets
......
Run status group 0 (all jobs):
  WRITE: bw=527MiB/s (553MB/s), 527MiB/s-527MiB/s (553MB/s-553MB/s), io=5120MiB (5369MB), run=9715-9715msec

Disk stats (read/write):
  nvme0n1: ios=0/1300604, merge=0/420, ticks=0/28956, in_queue=28957, util=98.93%
root@nvme:~#



# 切换到vm中执行
root@vm-nvme:~# fio -name=write -rw=write -ioengine=libaio -direct=1 -iodepth=64 -numjobs=1 -size=5G -filename=/mnt/nvme/nfs-fio-5g
......
Jobs: 1 (f=1): [W(1)][100.0%][w=26.8MiB/s][w=6857 IOPS][eta 00m:00s]
write: (groupid=0, jobs=1): err= 0: pid=2022: Sun May 14 07:10:07 2023
  write: IOPS=6815, BW=26.6MiB/s (27.9MB/s)(5120MiB/192325msec); 0 zone resets
......
Run status group 0 (all jobs):
  WRITE: bw=26.6MiB/s (27.9MB/s), 26.6MiB/s-26.6MiB/s (27.9MB/s-27.9MB/s), io=5120MiB (5369MB), run=192325-192325msec
root@vm-nvme:~#</code></pre> 
<p>可以看出，差距相当明显，iops：135k/6815，bw：527MiB/26.8MiB，再测试顺序读，差距没有顺序写那么夸张，但也很大，同时，如果你多一些测试，同时使用top、vmstat、sar等性能观测工具，你就会发现，宿主机的负载会因iowait增高而快速攀升，也就是说，如果存在资源竞争或资源有限的情况下，NFS性能问题会加剧</p> 
<h3>NFS混合随机读写（读写比例80/20）</h3> 
<p>为了排除宿主机Page Cache对测试结果的影响，每次执行fio前，先在宿主机上执行echo 3&gt;/proc/sys/vm/drop_caches，如果宿主机不清缓存，数据会漂亮一些（1.8倍左右）</p> 
<pre><code class="language-bash">[global]
bs=4096
rw=randrw
rwmixwrite=20
ioengine=libaio
direct=1
group_reporting
iodepth=32
iodepth_batch=8
iodepth_low=8
iodepth_batch_complete=8
userspace_reap
runtime=60
[test]
numjobs=16
filename=/mnt/nvme/nfs-fio-10g</code></pre> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td>iodepth/_low/_batch/_complete</td><td>numjobs</td><td> <p>iops(read)</p> <p>iscsi/nfs</p> </td><td> <p>bw(read)</p> <p>iscsi/nfs</p> </td><td> <p>iops(write)</p> <p>iscsi/nfs</p> </td><td> <p>bw(write)</p> <p>iscsi/nfs</p> </td></tr><tr><td>64/16/16/16</td><td>16</td><td>29.4k/10.9k</td><td>115MiB/42.6MiB</td><td>7355/2728</td><td>28.7MiB/10.7MiB</td></tr><tr><td>32/8/8/8</td><td>16</td><td>29.8k/11.2k</td><td>117MiB/43.6MiB</td><td>7464/2798</td><td>29.2MiB/10.9MiB</td></tr><tr><td>32/8/8/8</td><td>8</td><td>28.9k/10.8k</td><td>113MiB/42.3MiB</td><td>7229/2711</td><td>28.2MiB/10.6MiB</td></tr></tbody></table> 
<p>相比于NFS，iscsi几乎不会引起主机负载表高，cache也几乎没什么变化，这得益于Linux-IO及Zero-Copy技术，在后续的spdk及IO 虚拟化vhost中会再次见证Zero-Copy带来性能提升</p> 
<p></p> 
<p>欢迎转载，请说明出处</p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/78dd5cf2a24dbcb5e6ff916637eb14d6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux——TCP协议2</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a529f020b17c3d5106b9dffe4dd9a492/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Mysql必知必会之聚簇索引</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>