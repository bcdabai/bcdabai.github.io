<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>CV【3】：drop_out &amp; drop_path - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="CV【3】：drop_out &amp; drop_path" />
<meta property="og:description" content="文章目录 前言1. drop_out1.1. 出现的原因1.2. 概念1.3. 工作原理1.4. 尺度匹配问题1.5. 有效缓解过拟合的原因1.6. 代码实现 2. drop_path2.1. 与 drop_out 的差异2.2. 工作原理2.3. 在网络中的应用2.4. 代码实现 前言 本文主要对比了两种正则化方法：drop_out 和 drop_path
1. drop_out 1.1. 出现的原因 在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。
过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。
正则化可以有效地缓解上述两个问题
1.2. 概念 drop_out 是作为缓解卷积神经网络CNN过拟合而被提出的一种正则化方法，也叫做随机失活。
drop_out 可以作为训练深度神经网络的一种 trick 供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为 0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。
简单来说就是在模型训练阶段的前向传播过程中，让某些神经元的激活值以一定的概率停止工作。
drop_out 确实能够有效缓解过拟合现象的发生，但是可能会减缓模型收敛的速度，因为每次迭代只有一部分参数更新，可能导致梯度下降变慢。
1.3. 工作原理 通过一个三层的简单神经网络来介绍 drop_out的工作原理，输入是 X X X，输出是 Y Y Y。正常的训练流程是首先把输入X通过网络进行前向传播，然后把误差反向传播以决定如何更新参数。
遍历神经网络的每一层节点，设置节点保留概率 keep_prob，即该层的节点有 keep_prob 的概率被保留，keep_prob 的取值范围在 0 到 1 之间，假设 keep_prob = 0.5 通过设置神经网络该层节点的保留概率，使得神经网络不会去偏向于某一个节点（因为该节点有可能被删除），从而使得每一个节点的权重不会过大，来减轻神经网络的过拟合 删除神经网络的节点，并删除网络与移除节点之间的连接
输入样本，使用简化后的网络进行训练 让输入 X X X 通过部分神经元失活的新网络（如上右图）进行前向传播，然后计算损失并把损失反向传播，一小批样本执行完这个过程后，根据梯度下降算法更新参数 不断重复这一过程： 恢复失活的神经元重新让所有神经元以一定概率 p 失活（这次失活的和上次失活的神经元并不一定相同）让输入通过部分神经元失活的新网络进行前向传播，然后计算损失并把损失反向传播，新的一批样本执行完这个过程后，根据梯度下降算法更新参数 需要注意的是，drop_out 一般只在网络的训练阶段使用，而测试阶段不使用drop_out。这是因为如果在测试阶段使用 drop_out 可能会导致预测值产生随机变化（因为 drop_out 使节点随机失活）。而且，在训练阶段已经将权重参数除以 keep_prob 来保证输出的期望值不变，所以在测试阶段没必要再使用 drop_out" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/852832254dda69b6ed416601f04ef683/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-17T12:00:00+08:00" />
<meta property="article:modified_time" content="2023-01-17T12:00:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">CV【3】：drop_out &amp; drop_path</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_5" rel="nofollow">前言</a></li><li><a href="#1_drop_out_11" rel="nofollow">1. drop_out</a></li><li><ul><li><a href="#11__12" rel="nofollow">1.1. 出现的原因</a></li><li><a href="#12__19" rel="nofollow">1.2. 概念</a></li><li><a href="#13__28" rel="nofollow">1.3. 工作原理</a></li><li><a href="#14__46" rel="nofollow">1.4. 尺度匹配问题</a></li><li><a href="#15___64" rel="nofollow">1.5. 有效缓解过拟合的原因</a></li><li><a href="#16__69" rel="nofollow">1.6. 代码实现</a></li></ul> 
  </li><li><a href="#2_drop_path_103" rel="nofollow">2. drop_path</a></li><li><ul><li><a href="#21__drop_out__104" rel="nofollow">2.1. 与 drop_out 的差异</a></li><li><a href="#22__109" rel="nofollow">2.2. 工作原理</a></li><li><a href="#23__120" rel="nofollow">2.3. 在网络中的应用</a></li><li><a href="#24__135" rel="nofollow">2.4. 代码实现</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<hr> 
<h2><a id="_5"></a>前言</h2> 
<p>本文主要对比了两种正则化方法：<code>drop_out</code> 和 <code>drop_path</code></p> 
<hr> 
<h2><a id="1_drop_out_11"></a>1. drop_out</h2> 
<h3><a id="11__12"></a>1.1. 出现的原因</h3> 
<p>在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p> 
<p>过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。</p> 
<p>正则化可以有效地缓解上述两个问题</p> 
<h3><a id="12__19"></a>1.2. 概念</h3> 
<p><code>drop_out</code> 是作为缓解卷积神经网络CNN过拟合而被提出的一种正则化方法，也叫做随机失活。</p> 
<p><code>drop_out</code> 可以作为训练深度神经网络的一种 trick 供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为 0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。</p> 
<p>简单来说就是在模型训练阶段的前向传播过程中，让某些神经元的激活值以一定的概率停止工作。</p> 
<p><code>drop_out</code> 确实能够有效缓解过拟合现象的发生，但是可能会减缓模型收敛的速度，因为每次迭代只有一部分参数更新，可能导致梯度下降变慢。</p> 
<h3><a id="13__28"></a>1.3. 工作原理</h3> 
<p>通过一个三层的简单神经网络来介绍 <code>drop_out</code>的工作原理，输入是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span>，输出是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         Y 
        
       
      
        Y 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">Y</span></span></span></span></span>。正常的训练流程是首先把输入X通过网络进行前向传播，然后把误差反向传播以决定如何更新参数。</p> 
<p><img src="https://images2.imgbox.com/24/0c/ZFuh9vkD_o.jpg" alt="在这里插入图片描述" width="400"></p> 
<ul><li>遍历神经网络的每一层节点，设置节点保留概率 keep_prob，即该层的节点有 keep_prob 的概率被保留，keep_prob 的取值范围在 0 到 1 之间，假设 keep_prob = 0.5 
  <ul><li>通过设置神经网络该层节点的保留概率，使得神经网络不会去偏向于某一个节点（因为该节点有可能被删除），从而使得每一个节点的权重不会过大，来减轻神经网络的过拟合</li></ul> </li><li>删除神经网络的节点，并删除网络与移除节点之间的连接<br> <img src="https://images2.imgbox.com/a5/74/6YNCG2vD_o.jpg" alt="在这里插入图片描述" width="600"></li><li>输入样本，使用简化后的网络进行训练 
  <ul><li>让输入 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            X 
           
          
         
           X 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span> 通过部分神经元失活的新网络（如上右图）进行前向传播，然后计算损失并把损失反向传播，一小批样本执行完这个过程后，根据梯度下降算法更新参数</li></ul> </li><li>不断重复这一过程： 
  <ul><li>恢复失活的神经元</li><li>重新让所有神经元以一定概率 p 失活（这次失活的和上次失活的神经元并不一定相同）</li><li>让输入通过部分神经元失活的新网络进行前向传播，然后计算损失并把损失反向传播，新的一批样本执行完这个过程后，根据梯度下降算法更新参数</li></ul> </li></ul> 
<p>需要注意的是，<code>drop_out</code> 一般只在网络的训练阶段使用，而测试阶段不使用<code>drop_out</code>。这是因为如果在测试阶段使用 <code>drop_out</code> 可能会导致预测值产生随机变化（因为 <code>drop_out</code> 使节点随机失活）。而且，在训练阶段已经将权重参数除以 keep_prob 来保证输出的期望值不变，所以在测试阶段没必要再使用 <code>drop_out</code></p> 
<h3><a id="14__46"></a>1.4. 尺度匹配问题</h3> 
<p>上面提到 <code>drop_out</code> 一般只在网络的训练阶段使用，而测试阶段不使用<code>drop_out</code>，也就是说训练时前向传播只使用没有失活的那部分神经元，而测试时使用的是全部的神经元，那么训练和测试阶段就会出现数据尺度不同的问题。</p> 
<p>所以测试时，所有权重参数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         W 
        
       
      
        W 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span> 都要乘以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         − 
        
       
         p 
        
       
      
        1 - p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>，以保证训练和测试时尺度变化一致。</p> 
<p>举例来说，假设输入是 100 个特征，没有使用 <code>drop_out</code> 之前，隐藏层第一层的第一个神经元的值可以表示为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           Z 
          
         
           1 
          
         
           1 
          
         
        
          = 
         
         
          
          
            ∑ 
           
           
           
             i 
            
           
             = 
            
           
             1 
            
           
          
            100 
           
          
          
          
            w 
           
          
            1 
           
          
            i 
           
          
          
          
            x 
           
          
            1 
           
          
            i 
           
          
         
        
       
         Z_1^1 = \displaystyle\sum^{100}_{i=1} w_1^i x_1^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1111em; vertical-align: -0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8641em;"><span class="" style="top: -2.453em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3.0788em; vertical-align: -1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8011em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">100</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: -0.0269em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>不妨取 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ω 
         
        
          1 
         
        
          i 
         
        
        
        
          x 
         
        
          1 
         
        
          i 
         
        
       
         = 
        
       
         a 
        
       
      
        \omega _{1}^{i}x_{1}^{i}=a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0728em; vertical-align: -0.2481em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -2.4519em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -2.4519em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span>，那么此时 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Z 
         
        
          1 
         
        
          1 
         
        
       
         = 
        
       
         100 
        
       
         a 
        
       
      
        Z_{1}^{1}=100a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0622em; vertical-align: -0.2481em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -2.4519em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">100</span><span class="mord mathnormal">a</span></span></span></span></span></p> 
<ul><li>训练阶段使用 <code>drop_out</code> 时，若失活率 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          p 
         
        
          = 
         
        
          0.3 
         
        
       
         p = 0.3 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0.3</span></span></span></span></span>，可以理解成只有 70 个神经元起作用，此时 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           Z 
          
         
           1 
          
         
           1 
          
         
        
          = 
         
         
          
          
            ∑ 
           
           
           
             i 
            
           
             = 
            
           
             1 
            
           
          
            70 
           
          
          
          
            ω 
           
          
            1 
           
          
            i 
           
          
          
          
            x 
           
          
            1 
           
          
            i 
           
          
         
           = 
          
         
           70 
          
         
           a 
          
         
        
       
         Z_{1}^{1}=\displaystyle\sum_{i=1}^{70}\omega _{1}^{i}x_{1}^{i}=70a 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0622em; vertical-align: -0.2481em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -2.4519em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3.0788em; vertical-align: -1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8011em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">70</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">70</span><span class="mord mathnormal">a</span></span></span></span></span></li><li>而测试时没有 <code>drop_out</code> ，使用的是全部神经元，也就是 100a，不难发现使用 <code>drop_out</code> 后少了 30a，这就是训练阶段和测试阶段数据的尺度不一致</li></ul> 
<p>为了保证尺度的一致性，测试时所有权重参数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         W 
        
       
      
        W 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span> 都要乘以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         1 
        
       
         − 
        
       
         p 
        
       
      
        1 - p 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Z 
         
        
          1 
         
        
          1 
         
        
       
         = 
        
        
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           100 
          
         
        
          ( 
         
        
          0.7 
         
         
         
           ω 
          
         
           1 
          
         
           i 
          
         
        
          ) 
         
         
         
           x 
          
         
           1 
          
         
           i 
          
         
        
          = 
         
        
          70 
         
        
          a 
         
        
       
      
        Z_{1}^{1}=\displaystyle\sum_{i=1}^{100} (0.7\omega _{1}^{i})x_{1}^{i}=70a 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0622em; vertical-align: -0.2481em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8141em;"><span class="" style="top: -2.4519em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 3.0788em; vertical-align: -1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8011em;"><span class="" style="top: -1.8723em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.3em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">100</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em;"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord">0.7</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -2.453em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">70</span><span class="mord mathnormal">a</span></span></span></span></span>，这样使用 <code>drop_out</code> 的训练集和不使用 <code>drop_out</code> 的测试集的尺度就一致了。所以 <code>drop_out</code> 在训练和测试时是不一样的</p> 
<p>代码实现时要注意这点，即训练前要用 <code>train()</code> 函数，表示模型进入训练阶段，该阶段 <code>drop_out</code> 是正常工作的，测试前要用 <code>eval()</code> 函数，表示模型进入测试阶段，<code>drop_out</code> 就会停止工作</p> 
<h3><a id="15___64"></a>1.5. 有效缓解过拟合的原因</h3> 
<ul><li>使用 <code>drop_out</code> 可以使得部分节点失活，可以起到简化神经网络结构的作用，从而起到正则化的作用</li><li>取平均的策略通常可以有效防止过拟合问题，<code>drop_out</code> 掉不同的隐藏神经元就类似在训练不同的网络，随机删掉部分隐藏神经元导致网络结构已经不同，整个 <code>drop_out</code> 过程就相当于对很多个不同的神经网络取平均</li><li>使用 <code>drop_out</code> 可以使得神经网络的节点随机失活，这样会让神经网络在训练的时候不会使得某一个节点权重过大，让神经网络的节点不会依赖任何输入的特征</li></ul> 
<h3><a id="16__69"></a>1.6. 代码实现</h3> 
<p><code>nn.Dropout</code> 和 <code>nn.functional.dropout</code> 两种具体的实现方法：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Dropout1</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token builtin">super</span><span class="token punctuation">(</span>Dropout1<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
       self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span>
 
   <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
       out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
       out <span class="token operator">=</span> F<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>out<span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> training<span class="token operator">=</span>self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>  <span class="token comment"># 这里必须给traning设置为True</span>
       <span class="token keyword">return</span> out
<span class="token comment"># 如果设置为F.dropout(out, p=0.5)实际上是没有任何用的, 因为它的training状态一直是默认值False. 由于F.dropout只是相当于引用的一个外部函数, 模型整体的training状态变化也不会引起F.dropout这个函数的training状态发生变化. 所以,在训练模式下out = F.dropout(out) 就是 out = out. </span>
Net <span class="token operator">=</span> Dropout1<span class="token punctuation">(</span><span class="token punctuation">)</span>
Net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#或者直接使用nn.Dropout() (nn.Dropout()实际上是对F.dropout的一个包装, 自动将self.training传入，两者没有本质的差别)</span>
<span class="token keyword">class</span> <span class="token class-name">Dropout2</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token builtin">super</span><span class="token punctuation">(</span>Dropout2<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
 
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
      out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
      <span class="token keyword">return</span> out
Net <span class="token operator">=</span> Dropout2<span class="token punctuation">(</span><span class="token punctuation">)</span>
Net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<h2><a id="2_drop_path_103"></a>2. drop_path</h2> 
<h3><a id="21__drop_out__104"></a>2.1. 与 drop_out 的差异</h3> 
<p><code>drop_path</code> 将深度学习模型中的多分支结构随机 “失效”，而 <code>drop_out</code> 是对神经元随机 “失效”。换句话说，<code>drop_out</code> 是随机的点对点路径的关闭，<code>drop_path</code> 是随机的点对层之间的关闭</p> 
<p>假设有一个 Linear 层输入4结点，输出5结点，那么一共有 20 个点对点路径。<code>drop_out</code> 会随机关闭这些路径，而 <code>drop_path</code> 会随机选择输入结点，使其与之相连的 5 条路径全部关闭</p> 
<h3><a id="22__109"></a>2.2. 工作原理</h3> 
<p><code>drop_path</code> 和 <code>drop_out</code> 其实数学原理类似：通过范围是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         0 
        
       
         , 
        
       
         1 
        
       
         ) 
        
       
      
        (0,1) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> 的随机 rand 值，当施加了一个 drop_rate 后，被关闭的概率 p = rand + drop_rate</p> 
<ul><li>只需要对 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          p 
         
        
       
         p 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span> 进行下取整，即可得到服从 0-1 分布的数据，通过与权重的点乘，即可关闭 drop_rate 比例的结点</li><li>但是在传播过程中，总结点数没有改变（仍然包含被关闭的结点），因此输入的数据均值<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          u 
         
        
          = 
         
        
          s 
         
        
          u 
         
        
          m 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          / 
         
        
          N 
         
        
       
         u=sum(x)/N 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span>，就被放大了</li></ul> 
<p>假设原始的数据 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         X 
        
       
      
        X 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span>，结点数为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         N 
        
       
      
        N 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span>，均值为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         u 
        
       
      
        u 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span></span></p> 
<ul><li>经过比例为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          r 
         
        
       
         r 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span></span></span></span></span> 的 drop 操作后，总数据有 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          n 
         
        
          = 
         
        
          N 
         
        
          ∗ 
         
        
          r 
         
        
       
         n = N * r 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span></span></span></span></span> 个结点被置 0</li><li>因此新的均值为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           u 
          
         
           ′ 
          
         
        
          = 
         
        
          ( 
         
        
          N 
         
        
          − 
         
        
          n 
         
        
          ) 
         
        
          ∗ 
         
        
          u 
         
        
          / 
         
        
          N 
         
        
       
         u' = (N-n)*u/N 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7519em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">u</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span>，显然均值发生变化，数据分布以及梯度也随之发生变化</li><li>为了让数据保持一致性，需要将均值拉回来：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           u 
          
         
           ′ 
          
         
        
          ÷ 
         
        
          ( 
         
        
          N 
         
        
          − 
         
        
          n 
         
        
          ) 
         
        
          / 
         
        
          N 
         
        
       
         u' ÷ (N-n)/N 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8352em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">÷</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span>，即 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           u 
          
         
           ′ 
          
         
        
          ÷ 
         
        
          r 
         
        
       
         u' ÷ r 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8352em; vertical-align: -0.0833em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">÷</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span></span></span></span></span></li><li>但是 <code>drop_path</code> 输出，是对原始数据的调整，通过激活函数来完成 drop 的功能</li></ul> 
<h3><a id="23__120"></a>2.3. 在网络中的应用</h3> 
<p>假设在前向传播中有如下的代码：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token punctuation">)</span>
</code></pre> 
<p>那么在 <code>drop_path</code> 分支中，每个 batch 有 drop_prob 的概率样本在 <code>self.conv(x)</code> 不会 “执行”，会以 0 直接传递。</p> 
<p>若 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></span> 为输入的张量，其通道为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         [ 
        
       
         B 
        
       
         , 
        
       
         C 
        
       
         , 
        
       
         H 
        
       
         , 
        
       
         W 
        
       
         ] 
        
       
      
        [B,C,H,W] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right: 0.0502em;">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span><span class="mclose">]</span></span></span></span></span>，那么 <code>drop_path</code> 的含义为在一个 batch_size中，随机有 drop_prob 的样本，不经过主干，而直接由分支进行恒等映射</p> 
<p>需要注意的是，<code>drop_path</code> 不能直接这样使用：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="24__135"></a>2.4. 代码实现</h3> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">drop_path</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> drop_prob<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0.</span><span class="token punctuation">,</span> training<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> drop_prob <span class="token operator">==</span> <span class="token number">0.</span> <span class="token keyword">or</span> <span class="token keyword">not</span> training<span class="token punctuation">:</span>
        <span class="token keyword">return</span> x
    keep_prob <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> drop_prob
    shape <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>ndim <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># work with diff dim tensors, not just 2D ConvNets</span>
    random_tensor <span class="token operator">=</span> keep_prob <span class="token operator">+</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>shape<span class="token punctuation">,</span> dtype<span class="token operator">=</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    random_tensor<span class="token punctuation">.</span>floor_<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># binarize</span>
    output <span class="token operator">=</span> x<span class="token punctuation">.</span>div<span class="token punctuation">(</span>keep_prob<span class="token punctuation">)</span> <span class="token operator">*</span> random_tensor
    <span class="token keyword">return</span> output


<span class="token keyword">class</span> <span class="token class-name">DropPath</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> drop_prob<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DropPath<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop_prob <span class="token operator">=</span> drop_prob

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> drop_path<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>drop_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>training<span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/4196d13fa57b93a6d5e9fe5d5e9a436b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker快速安装Oracle11g</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/beca2f5a7af7fa8818e3895806cc581c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【面试题】2023年前端最新面试题-webpack篇</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>