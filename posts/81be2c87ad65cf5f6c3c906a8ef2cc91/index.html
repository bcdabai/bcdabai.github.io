<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用 TensorFlow Serving 和 Docker 快速部署机器学习服务 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用 TensorFlow Serving 和 Docker 快速部署机器学习服务" />
<meta property="og:description" content="从实验到生产，简单快速部署机器学习模型一直是一个挑战。这个过程要做的就是将训练好的模型对外提供预测服务。在生产中，这个过程需要可重现，隔离和安全。这里，我们使用基于Docker的TensorFlow Serving来简单地完成这个过程。TensorFlow 从1.8版本开始支持Docker部署，包括CPU和GPU，非常方便。
获得训练好的模型 获取模型的第一步当然是训练一个模型，但是这不是本篇的重点，所以我们使用一个已经训练好的模型，比如ResNet。TensorFlow Serving 使用SavedModel这种格式来保存其模型，SavedModel是一种独立于语言的，可恢复，密集的序列化格式，支持使用更高级别的系统和工具来生成，使用和转换TensorFlow模型。这里我们直接下载一个预训练好的模型：
$ mkdir /tmp/resnet $ curl -s https://storage.googleapis.com/download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | tar --strip-components=2 -C /tmp/resnet -xvz 复制代码 如果是使用其他框架比如Keras生成的模型，则需要将模型转换为SavedModel格式，比如：
from keras.models import Sequential from keras import backend as K import tensorflow as tf model = Sequential() # 中间省略模型构建 # 模型转换为SavedModel signature = tf.saved_model.signature_def_utils.predict_signature_def( inputs={&#39;input_param&#39;: model.input}, outputs={&#39;type&#39;: model.output}) builder = tf.saved_model.builder.SavedModelBuilder(&#39;/tmp/output_model_path/1/&#39;) builder.add_meta_graph_and_variables( sess=K.get_session(), tags=[tf.saved_model.tag_constants.SERVING], signature_def_map={ tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature }) builder.save() 复制代码 下载完成后，文件目录树为：
$ tree /tmp/resnet /tmp/resnet └── 1538687457 ├── saved_model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/81be2c87ad65cf5f6c3c906a8ef2cc91/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-04T04:41:27+08:00" />
<meta property="article:modified_time" content="2019-03-04T04:41:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用 TensorFlow Serving 和 Docker 快速部署机器学习服务</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="article-content"> 
 <p>从实验到生产，简单快速部署机器学习模型一直是一个挑战。这个过程要做的就是将训练好的模型对外提供预测服务。在生产中，这个过程需要可重现，隔离和安全。这里，我们使用基于Docker的TensorFlow Serving来简单地完成这个过程。TensorFlow 从1.8版本开始支持Docker部署，包括CPU和GPU，非常方便。</p> 
 <h3 class="heading">获得训练好的模型</h3> 
 <p>获取模型的第一步当然是训练一个模型，但是这不是本篇的重点，所以我们使用一个已经训练好的模型，比如ResNet。TensorFlow Serving 使用SavedModel这种格式来保存其模型，SavedModel是一种独立于语言的，可恢复，密集的序列化格式，支持使用更高级别的系统和工具来生成，使用和转换TensorFlow模型。这里我们直接下载一个预训练好的模型：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> mkdir /tmp/resnet</span>
<span class="hljs-meta">$</span><span class="bash"> curl <span class="hljs-_">-s</span> https://storage.googleapis.com/download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | tar --strip-components=2 -C /tmp/resnet -xvz</span>
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>如果是使用其他框架比如Keras生成的模型，则需要将模型转换为SavedModel格式，比如：</p> 
 <pre><code class="hljs py copyable" lang="py"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model = Sequential()
<span class="hljs-comment"># 中间省略模型构建</span>

<span class="hljs-comment"># 模型转换为SavedModel</span>
signature = tf.saved_model.signature_def_utils.predict_signature_def(
    inputs={<!-- --><span class="hljs-string">'input_param'</span>: model.input}, outputs={<!-- --><span class="hljs-string">'type'</span>: model.output})
builder = tf.saved_model.builder.SavedModelBuilder(<span class="hljs-string">'/tmp/output_model_path/1/'</span>)
builder.add_meta_graph_and_variables(
    sess=K.get_session(),
    tags=[tf.saved_model.tag_constants.SERVING],
    signature_def_map={
        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
            signature
    })
builder.save()
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>下载完成后，文件目录树为：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> tree /tmp/resnet</span>
/tmp/resnet
└── 1538687457
    ├── saved_model.pb
    └── variables
        ├── variables.data-00000-of-00001
        └── variables.index
<span class="copy-code-btn">复制代码</span></code></pre> 
 <h3 class="heading">部署模型</h3> 
 <p>使用Docker部署模型服务：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> docker pull tensorflow/serving</span>
<span class="hljs-meta">$</span><span class="bash"> docker run -p 8500:8500 -p 8501:8501 --name tfserving_resnet \</span>
--mount type=bind,source=/tmp/resnet,target=/models/resnet \
-e MODEL_NAME=resnet -t tensorflow/serving
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>其中，<code>8500</code>端口对于TensorFlow Serving提供的gRPC端口，<code>8501</code>为REST API服务端口。<code>-e MODEL_NAME=resnet</code>指出TensorFlow Serving需要加载的模型名称，这里为<code>resnet</code>。上述命令输出为</p> 
 <pre><code class="hljs shell copyable">2019-03-04 02:52:26.610387: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: resnet model_base_path: /models/resnet
2019-03-04 02:52:26.618200: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.
2019-03-04 02:52:26.618628: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: resnet
2019-03-04 02:52:26.745813: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: resnet version: 1538687457}
2019-03-04 02:52:26.745901: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: resnet version: 1538687457}
2019-03-04 02:52:26.745935: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: resnet version: 1538687457}
2019-03-04 02:52:26.747590: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/resnet/1538687457
2019-03-04 02:52:26.747705: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/resnet/1538687457
2019-03-04 02:52:26.795363: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }
2019-03-04 02:52:26.828614: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-03-04 02:52:26.923902: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.
2019-03-04 02:52:28.098479: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key saved_model_main_op on SavedModel bundle.
2019-03-04 02:52:28.144510: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 1396689 microseconds.
2019-03-04 02:52:28.146646: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /models/resnet/1538687457/assets.extra/tf_serving_warmup_requests
2019-03-04 02:52:28.168063: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1538687457}
2019-03-04 02:52:28.174902: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...
[warn] getaddrinfo: address family for nodename not supported
2019-03-04 02:52:28.186724: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 237] RAW: Entering the event loop ...
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>我们可以看到，TensorFlow Serving使用<code>1538687457</code>作为模型的版本号。我们使用curl命令来查看一下启动的服务状态，也可以看到提供服务的模型版本以及模型状态。</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> curl http://localhost:8501/v1/models/resnet</span>
{
 "model_version_status": [
  {
   "version": "1538687457",
   "state": "AVAILABLE",
   "status": {
    "error_code": "OK",
    "error_message": ""
   }
  }
 ]
}
<span class="copy-code-btn">复制代码</span></code></pre> 
 <h3 class="heading">查看模型输入输出</h3> 
 <p>很多时候我们需要查看模型的输出和输出参数的具体形式，TensorFlow提供了一个<code>saved_model_cli</code>命令来查看模型的输入和输出参数：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> saved_model_cli show --dir /tmp/resnet/1538687457/ --all</span>

MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['image_bytes'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: input_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['classes'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: ArgMax:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1001)
        name: softmax_tensor:0
  Method name is: tensorflow/serving/predict

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['image_bytes'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: input_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['classes'] tensor_info:
        dtype: DT_INT64
        shape: (-1)
        name: ArgMax:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1001)
        name: softmax_tensor:0
  Method name is: tensorflow/serving/predict
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>注意到<code>signature_def</code>，<code>inputs</code>的名称，类型和输出，这些参数在接下来的模型预测请求中需要。</p> 
 <h3 class="heading">使用模型接口预测：REST和gRPC</h3> 
 <p>TensorFlow Serving提供REST API和gRPC两种请求方式，接下来将具体这两种方式。</p> 
 <h4 class="heading">REST</h4> 
 <p>我们下载一个客户端脚本，这个脚本会下载一张猫的图片，同时使用这张图片来计算服务请求时间。</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> curl -o /tmp/resnet/resnet_client.py https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/example/resnet_client.py</span>
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>以下脚本使用<code>requests</code>库来请求接口，使用图片的base64编码字符串作为请求内容，返回图片分类，并计算了平均处理时间。</p> 
 <pre><code class="hljs py copyable" lang="py"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function

<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">import</span> requests

<span class="hljs-comment"># The server URL specifies the endpoint of your server running the ResNet</span>
<span class="hljs-comment"># model with the name "resnet" and using the predict interface.</span>
SERVER_URL = <span class="hljs-string">'http://localhost:8501/v1/models/resnet:predict'</span>

<span class="hljs-comment"># The image URL is the location of the image we should send to the server</span>
IMAGE_URL = <span class="hljs-string">'https://tensorflow.org/images/blogs/serving/cat.jpg'</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">()</span>:</span>
  <span class="hljs-comment"># Download the image</span>
  dl_request = requests.get(IMAGE_URL, stream=<span class="hljs-keyword">True</span>)
  dl_request.raise_for_status()

  <span class="hljs-comment"># Compose a JSON Predict request (send JPEG image in base64).</span>
  jpeg_bytes = base64.b64encode(dl_request.content).decode(<span class="hljs-string">'utf-8'</span>)
  predict_request = <span class="hljs-string">'{"instances" : [{"b64": "%s"}]}'</span> % jpeg_bytes

  <span class="hljs-comment"># Send few requests to warm-up the model.</span>
  <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>):
    response = requests.post(SERVER_URL, data=predict_request)
    response.raise_for_status()

  <span class="hljs-comment"># Send few actual requests and report average latency.</span>
  total_time = <span class="hljs-number">0</span>
  num_requests = <span class="hljs-number">10</span>
  <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(num_requests):
    response = requests.post(SERVER_URL, data=predict_request)
    response.raise_for_status()
    total_time += response.elapsed.total_seconds()
    prediction = response.json()[<span class="hljs-string">'predictions'</span>][<span class="hljs-number">0</span>]

  print(<span class="hljs-string">'Prediction class: {}, avg latency: {} ms'</span>.format(
      prediction[<span class="hljs-string">'classes'</span>], (total_time*<span class="hljs-number">1000</span>)/num_requests))


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
  main()
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>输出结果为</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> python resnet_client.py</span>
Prediction class: 286, avg latency: 210.12310000000002 ms
<span class="copy-code-btn">复制代码</span></code></pre> 
 <h4 class="heading">gRPC</h4> 
 <p>让我们下载另一个客户端脚本，这个脚本使用gRPC作为服务，传入图片并获取输出结果。这个脚本需要安装<code>tensorflow-serving-api</code>这个库。</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> curl -o /tmp/resnet/resnet_client_grpc.py https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/example/resnet_client_grpc.py</span>
<span class="hljs-meta">$</span><span class="bash"> pip install tensorflow-serving-api</span>
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>脚本内容：</p> 
 <pre><code class="hljs py copyable" lang="py"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function

<span class="hljs-comment"># This is a placeholder for a Google-internal import.</span>

<span class="hljs-keyword">import</span> grpc
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-keyword">from</span> tensorflow_serving.apis <span class="hljs-keyword">import</span> predict_pb2
<span class="hljs-keyword">from</span> tensorflow_serving.apis <span class="hljs-keyword">import</span> prediction_service_pb2_grpc

<span class="hljs-comment"># The image URL is the location of the image we should send to the server</span>
IMAGE_URL = <span class="hljs-string">'https://tensorflow.org/images/blogs/serving/cat.jpg'</span>

tf.app.flags.DEFINE_string(<span class="hljs-string">'server'</span>, <span class="hljs-string">'localhost:8500'</span>,
                           <span class="hljs-string">'PredictionService host:port'</span>)
tf.app.flags.DEFINE_string(<span class="hljs-string">'image'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'path to image in JPEG format'</span>)
FLAGS = tf.app.flags.FLAGS


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">(_)</span>:</span>
  <span class="hljs-keyword">if</span> FLAGS.image:
    <span class="hljs-keyword">with</span> open(FLAGS.image, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> f:
      data = f.read()
  <span class="hljs-keyword">else</span>:
    <span class="hljs-comment"># Download the image since we weren't given one</span>
    dl_request = requests.get(IMAGE_URL, stream=<span class="hljs-keyword">True</span>)
    dl_request.raise_for_status()
    data = dl_request.content

  channel = grpc.insecure_channel(FLAGS.server)
  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
  <span class="hljs-comment"># Send request</span>
  <span class="hljs-comment"># See prediction_service.proto for gRPC request/response details.</span>
  request = predict_pb2.PredictRequest()
  request.model_spec.name = <span class="hljs-string">'resnet'</span>
  request.model_spec.signature_name = <span class="hljs-string">'serving_default'</span>
  request.inputs[<span class="hljs-string">'image_bytes'</span>].CopyFrom(
      tf.contrib.util.make_tensor_proto(data, shape=[<span class="hljs-number">1</span>]))
  result = stub.Predict(request, <span class="hljs-number">10.0</span>)  <span class="hljs-comment"># 10 secs timeout</span>
  print(result)


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
  tf.app.run()

<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>输出的结果可以看到图片的分类，概率和使用的模型信息：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">$</span><span class="bash"> python resnet_client_grpc.py</span>
outputs {
  key: "classes"
  value {
    dtype: DT_INT64
    tensor_shape {
      dim {
        size: 1
      }
    }
    int64_val: 286
  }
}
outputs {
  key: "probabilities"
  value {
    dtype: DT_FLOAT
    tensor_shape {
      dim {
        size: 1
      }
      dim {
        size: 1001
      }
    }
    float_val: 2.4162832232832443e-06
    float_val: 1.9012182974620373e-06
    float_val: 2.7247710022493266e-05
    float_val: 4.426385658007348e-07
    ...(中间省略)
    float_val: 1.4636580090154894e-05
    float_val: 5.812107133351674e-07
    float_val: 6.599806511076167e-05
    float_val: 0.0012952701654285192
  }
}
model_spec {
  name: "resnet"
  version {
    value: 1538687457
  }
  signature_name: "serving_default"
}
<span class="copy-code-btn">复制代码</span></code></pre> 
 <h3 class="heading">性能</h3> 
 <h4 class="heading">通过编译优化的TensorFlow Serving二进制来提高性能</h4> 
 <p>TensorFlows serving有时会有输出如下的日志：</p> 
 <pre><code class="hljs shell copyable">Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>TensorFlow Serving已发布Docker镜像旨在尽可能多地使用CPU架构，因此省略了一些优化以最大限度地提高兼容性。如果你没有看到此消息，则你的二进制文件可能已针对你的CPU进行了优化。根据你的模型执行的操作，这些优化可能会对你的服务性能产生重大影响。幸运的是，编译优化的TensorFlow Serving二进制非常简单。官方已经提供了自动化脚本，分以下两部进行：</p> 
 <pre><code class="hljs shell copyable"><span class="hljs-meta">#</span><span class="bash"> 1. 编译开发版本</span>
<span class="hljs-meta">$</span><span class="bash"> docker build -t <span class="hljs-variable">$USER</span>/tensorflow-serving-devel <span class="hljs-_">-f</span> Dockerfile.devel https://github.com/tensorflow/serving.git<span class="hljs-comment">#:tensorflow_serving/tools/docker</span></span>
<span class="hljs-meta">
#</span><span class="bash"> 2. 生产新的镜像</span>
<span class="hljs-meta">$</span><span class="bash"> docker build -t <span class="hljs-variable">$USER</span>/tensorflow-serving --build-arg TF_SERVING_BUILD_IMAGE=<span class="hljs-variable">$USER</span>/tensorflow-serving-devel https://github.com/tensorflow/serving.git<span class="hljs-comment">#:tensorflow_serving/tools/docker</span></span>
<span class="copy-code-btn">复制代码</span></code></pre> 
 <p>之后，使用新编译的<code>$USER/tensorflow-serving</code>重新启动服务即可。</p> 
 <h3 class="heading">总结</h3> 
 <p>上面我们快速实践了使用TensorFlow Serving和Docker部署机器学习服务的过程，可以看到，TensorFlow Serving提供了非常方便和高效的模型管理，配合Docker，可以快速搭建起机器学习服务。</p> 
 <h3 class="heading">参考</h3> 
 <ul><li><a href="https://link.juejin.im?target=https%3A%2F%2Fmedium.com%2Ftensorflow%2Fserving-ml-quickly-with-tensorflow-serving-and-docker-7df7094aa008" rel="nofollow">Serving ML Quickly with TensorFlow Serving and Docker</a></li><li><a href="https://link.juejin.im?target=https%3A%2F%2Fwww.tensorflow.org%2Ftfx%2Fserving%2Ftutorials%2FServing_REST_simple" rel="nofollow">Train and serve a TensorFlow model with TensorFlow Serving</a></li></ul> 
 <blockquote> 
  <p>GitHub repo: <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fqiwihui%2Fblog" rel="nofollow">qiwihui/blog</a></p> 
  <p>Follow me: <a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fqiwihui" rel="nofollow">@qiwihui</a></p> 
  <p>Site: <a href="https://link.juejin.im?target=https%3A%2F%2Fqiwihui.com" rel="nofollow">QIWIHUI</a></p> 
 </blockquote> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/fb6339ce99707ac2d290eb30d33c525a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Docker常用命令（四）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/37f6a4c6f9070bada2a24b447ea9da75/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用git命令 提交多个项目到同一个仓库中</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>