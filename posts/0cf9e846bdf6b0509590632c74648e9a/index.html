<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>阅读论文记忆博客 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="阅读论文记忆博客" />
<meta property="og:description" content="Previous papers [arXiv:1706.03762] Attention Is All You Need
动机：RNN中顺序性质使训练无法并行化；注意力机制（Attention）对序列信息处理很好，但依赖RNN。挑战：只靠Attention机制，不使用RNN和CNN，提高并行度；充分发挥Attention抓长距离依赖关系比RNN强的优势。方法：Transformer代替RNN；
Masked Attention: 不让Decoder在训练的时候用后文的信息生成前文的信息；
Feed Forward: 非线性处理；
残差计算：更有效的反向传播；
Position Coding： “位置编码”（不同频率的正弦和余弦函数）与Embedding相加；
Scaled Dot-Product Attention：
A t t e n t i o n ( Q , K , V ) = s o f t m a x ( Q K T d k ) V Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V Attention(Q,K,V)=softmax(dk​ ​QKT​)V
多头注意力（Multi-Head Attention）：
M u l t i H e a d ( Q , K , V ) = C o n c a t ( h e a d 1 , ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0cf9e846bdf6b0509590632c74648e9a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-30T20:51:53+08:00" />
<meta property="article:modified_time" content="2022-05-30T20:51:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">阅读论文记忆博客</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Previous_papers_1"></a>Previous papers</h2> 
<p>[arXiv:1706.03762] Attention Is All You Need</p> 
<ul><li>动机：RNN中顺序性质使训练无法并行化；注意力机制（Attention）对序列信息处理很好，但依赖RNN。</li><li>挑战：只靠Attention机制，不使用RNN和CNN，提高并行度；充分发挥Attention抓长距离依赖关系比RNN强的优势。</li><li>方法：Transformer代替RNN；<br> Masked Attention: 不让Decoder在训练的时候用后文的信息生成前文的信息；<br> Feed Forward: 非线性处理；<br> 残差计算：更有效的反向传播；<br> Position Coding： “位置编码”（不同频率的正弦和余弦函数）与Embedding相加；<br> Scaled Dot-Product Attention：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           A 
          
         
           t 
          
         
           t 
          
         
           e 
          
         
           n 
          
         
           t 
          
         
           i 
          
         
           o 
          
         
           n 
          
         
           ( 
          
         
           Q 
          
         
           , 
          
         
           K 
          
         
           , 
          
         
           V 
          
         
           ) 
          
         
           = 
          
         
           s 
          
         
           o 
          
         
           f 
          
         
           t 
          
         
           m 
          
         
           a 
          
         
           x 
          
         
           ( 
          
          
           
           
             Q 
            
            
            
              K 
             
            
              T 
             
            
           
           
            
            
              d 
             
            
              k 
             
            
           
          
         
           ) 
          
         
           V 
          
         
        
          Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.44833em; vertical-align: -0.93em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.51833em;"><span class="" style="top: -2.25278em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.85722em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.81722em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                    <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                     <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                    </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.18278em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.93em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span></span></span></span></span></span><br> 多头注意力（Multi-Head Attention）：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           M 
          
         
           u 
          
         
           l 
          
         
           t 
          
         
           i 
          
         
           H 
          
         
           e 
          
         
           a 
          
         
           d 
          
         
           ( 
          
         
           Q 
          
         
           , 
          
         
           K 
          
         
           , 
          
         
           V 
          
         
           ) 
          
         
           = 
          
         
           C 
          
         
           o 
          
         
           n 
          
         
           c 
          
         
           a 
          
         
           t 
          
         
           ( 
          
         
           h 
          
         
           e 
          
         
           a 
          
          
          
            d 
           
          
            1 
           
          
         
           , 
          
         
           . 
          
         
           . 
          
         
           . 
          
         
           , 
          
         
           h 
          
         
           e 
          
         
           a 
          
          
          
            d 
           
          
            h 
           
          
         
           ) 
          
          
          
            W 
           
          
            O 
           
          
         
        
          MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.10903em;">M</span><span class="mord mathdefault">u</span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.14133em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></span><br> 其中<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
       
        
         
         
           h 
          
         
           e 
          
         
           a 
          
          
          
            d 
           
          
            i 
           
          
         
           = 
          
         
           A 
          
         
           t 
          
         
           t 
          
         
           e 
          
         
           n 
          
         
           t 
          
         
           i 
          
         
           o 
          
         
           n 
          
         
           ( 
          
         
           Q 
          
          
          
            W 
           
          
            i 
           
          
            Q 
           
          
         
           , 
          
         
           K 
          
          
          
            W 
           
          
            i 
           
          
            K 
           
          
         
           , 
          
         
           V 
          
          
          
            W 
           
          
            i 
           
          
            V 
           
          
         
           ) 
          
         
        
          head_i=Attention(QW^Q_i,KW^K_i,VW^V_i) 
         
        
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.2361em; vertical-align: -0.276864em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.959239em;"><span class="" style="top: -2.42314em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.18091em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.276864em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -2.453em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.891331em;"><span class="" style="top: -2.453em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></li></ul> 
<p>[arXiv:1807.03748] Representation Learning with Contrastive Predictive Coding</p> 
<ul><li>动机：很多时候，很多数据维度高、label相对少，我们并不希望浪费掉没有label的那部分data；所以在label少的时候，可以利用无监督学习帮助我们学到数据本身的高级信息，从而对下游任务有很大的帮助。</li><li>挑战：从原始观察数据中对高级表示（特征）建模很难；什么是理想的表示；在没有对特定数据模式进行额外监督或专门化的情况下，能否学习这种表示（能否无监督）。</li><li>方法：将高维数据压缩到一个更紧凑的隐嵌入空间中，其中条件预测更容易建模；在这个隐空间中使用强大的自回归模型对未来的许多步骤进行预测；采用与自然语言模型中词嵌入学习类似的方法，对损失函数进行噪声对比估计（Noise-Contrastive Estimation），从而允许对整个模型进行端到端的训练。</li></ul> 
<p>[Interspeech 2021] Self-Supervised Learning Based Phone-Fortified Speech Enhancement</p> 
<ul><li>动机：基于深度复数网络的方法由于其在处理复数频谱方面的有效性而显示出良好的性能；最近的语音增强方法侧重于进一步优化网络结构和超参数，忽略了固有的语音特征（例如phonetic特征）。</li><li>方法：采用复数网络来保存语音幅度和相位；采用复数卷积网络来估计用于噪声信息滤波的复比率掩码（complex ratio mask）；预训练的CPC模型使用 wav2vec；采用一种简单的特征嵌入网络，将语音（phonetic）特征与频谱特征融合；在复数U-NET中应用一个自注意力层，进一步改善语音表示学习；采用PFP损失计算干净语音向量和增强语音phonetic向量之间的绝对距离。</li></ul> 
<p>[ICASSP 2018] x-vectors: Robust DNN Embeddings For Speaker Recognition</p> 
<ul><li>动机：在大规模训练数据集上：Embedding比i-vector好。</li><li>方法：增加训练数据量；训练x-vector（Embedding从segment6输出的仿射分量中提取）；对i-vector系统中的增广进行了研究。</li></ul> 
<p>[Interspeech 2020] ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification</p> 
<ul><li>动机：对时延神经网络（TDNN）体系结构的多项增强。</li><li>方法：将最初的TDNN块（即Conv1D）重构为具有跳过连接的一维Res2Net模块，引入压缩-激励块；聚集和传播不同层次的特征；对统计池模块进行了改进（引入Attention）；网络创新点：SE-Res2Block。<br> Squeeze-Excitation 压缩-激励：压缩：[N,C,L] -&gt; [N,C,1]；激励：[N,C,1] -&gt; [N,R,1] -&gt; [N,C,1]， [N,C,1]*[N,C,L] -&gt; [N,C,L]；<br> Res2Net：用一个较小的3×3过滤器取代了过滤器组，将不同的过滤器组以层级残差式风格连接。它可以通过在内部构造层次（hierarchical）残差连接来处理多尺度特征在粒度级别上表示了多尺度特征，并增加了每层的感受野（模块内部的连接形式与残差网络类似，故命名为Res2Net）。<br> 多层特征聚合（MFA）：将所有SE-Res2Blocks的输出特征映射连接（而非直接串联）。<br> 池化层的最终输出是通过将<strong>加权均值</strong>和<strong>加权标准差</strong>的向量串联起来而给出的。</li></ul> 
<p>[Interspeech 2021] Speech Enhancement with Weakly Labelled Data from AudioSet</p> 
<ul><li>动机：收集干净的语音和背景噪声训练神经网络非常耗时（用新数据集）；针对特定背景噪声训练的语音增强系统可能无法很好地推广到广泛的噪声（提高泛用性）。</li><li>方法：弱标记数据的语音增强：如果s1同时包含“语音”和“水”当调节音频标签概率c时，通过将条件向量c设置为“语音”，系统将分离“语音”和“水”。</li></ul> 
<p>[Interspeech 2020] A Deep Learning-based Kalman Filter for Speech Enhancement</p> 
<ul><li>动机：现有的卡尔曼滤波器（KF）在实际噪声条件下对噪声方差和线性预测系数（LPC）的估计较差。这会导致语音增强性能下降。</li><li>方法：基于MMSE，使用DNN估计噪声功率谱密度（PSD）以计算噪声方差（白化滤波器）。然后将其应用于带噪语音，产生预白化语音以计算LPC。用改进后参数构造的KF来增强语音。</li></ul> 
<p>[ICASSP 2021] High Fidelity Speech Regeneration with Application to Speech Enhancement</p> 
<ul><li>动机：为了增强语音，使其超越原始信号的限制，我们采用了一种再生方法，即从语音的本质（包括半识别语音、韵律特征和身份）重新生成语音。</li><li>方法：我们提出了一种wav-to-wav语音生成模型，该模型可以实时生成24khz语音，并利用由ASR和身份特征组成的紧凑语音表示，以实现更高级别的可懂度。受语音转换方法的启发，我们使用辅助身份网络来增强语音特征，同时保持源的身份。</li></ul> 
<p>[ICASSP 2021] Densely Connected Multi-Stage Model With Channel Wise Subband Feature for Real-Time Speech Enhancement</p> 
<ul><li>动机： 低延迟总是带来质量损失，很难在增强质量和计算效率之间取得平衡。 唱歌和情感演讲等特定场景中的增强对于传统方法很复杂。</li><li>方法： 提出了一种高效的实时语音增强网络，具有多级密集连接结构，可逐步增强信道方向的子带语音。为了获得从粗到精的估计，利用早期的增强语音来指导深层的处理。此外，对所有中间结果进行监督，以稳定训练并加速收敛。此外，对特定场景的一些小数据集采用了自适应微调步骤，在相应场景下取得了极好的改善。</li></ul> 
<p>[ICASSP 2021] TSTNN: Two-Stage Transformer Based Neural Network for Speech Enhancement in The Time Domain</p> 
<ul><li>动机： T-F域方法仍存在两个主要局限性：STFT是额外的开销，在去噪过程中通常忽略了噪声相位信息。Transformer神经网络能够有效地解决长依赖问题，并行运行良好，在许多NLP任务中表现出良好的性能。</li><li>方法： 提出了两级变压器神经网络（TSTNN）用于时域的端到端语音去噪。该模型由编码器、两级Transformer模块、Mask模块和译码器组成。编码器将输入的噪声语音映射为特征表示。TSTM利用四个堆叠的两级Transformer块，逐级有效地从编码器输出中提取局部和全局信息。Mask模块创建一个Mask，该Mask将与编码器输出相乘。最后，解码器使用掩蔽编码器特征来重建增强语音。</li></ul> 
<p>[IEEE Signal Processing Letters] Additive Margin Softmax for Face Verification</p> 
<ul><li>动机：一般来说，人脸验证任务可以看作是一个度量学习问题，因此学习类内变化小、类间差异大的大边缘人脸特征对于获得良好的性能具有重要意义。最近，大余量Softmax和角度Softmax被提议以乘法方式合并角度余量。</li><li>方法：为Softmax损耗引入了一种新的附加角裕度，它比现有的工作直观且更具解释性。此外还强调并讨论了特征规范化的重要性。</li></ul> 
<p>[Interspeech 2021] Personalized Speech Enhancement through Self-Supervised Data Augmentation and Purification</p> 
<ul><li>动机：由于隐私限制和目标用户对无噪声语音的访问受限，训练个性化语音增强模型本质上是一个no-shot学习问题。如果有大量来自测试时用户的未标记噪声语音，可以使用自监督学习来训练个性化语音增强模型。伪源（目标说话人的嘈杂录音）的质量决定个性化语音增强模型的质量。</li><li>方法：提出了一个数据净化步骤，它改进了自监督方法。首先训练一个信噪比预测模型来估计伪源的逐帧信噪比。然后，我们将预测器的估计值转换为权重，以调整伪源对训练个性化模型的逐帧贡献。</li></ul> 
<p>[ICASSP 2021] Speech Enhancement Aided End-To-End Multi-Task Learning for Voice Activity Detection</p> 
<ul><li>动机：近年来的研究表明，语音增强有助于VAD的实现，但对VAD性能的改善有限。</li><li>方法：提出了一个语音增强辅助的端到端多任务VAD模型。该模型有两个解码器，一个用于语音增强，另一个用于VAD。这两个解码器共享相同的编码器和语音分离网络。本文提出了一种新的联合优化目标VAD-mask尺度不变信源失真比（mSI-SDR）。mSI-SDR在训练过程中使用VAD信息来屏蔽语音增强解码器的输出。它使得VAD和语音增强任务不仅在共享编码器和分离网络上进行联合优化，而且在目标层次上进行联合优化。理论上满足实时工作要求。</li></ul> 
<p>[Speech Communication 08/2019] Deep learning for minimum mean-square error approaches to speech enhancement</p> 
<ul><li>动机：语音增强研究的重点已经从最小均方误差（MMSE）方法，如MMSE短时谱幅度（MMSE-STSA）估计器，转移到最新的基于Mask和映射的深度学习方法。我们的目标是弥合这两种不同的语音增强方法之间的差距。</li><li>方法：使用残差长短时记忆（ResLSTM）网络来精确估计（映射的）先验信噪比，作为MMSE方法参数恢复语音。</li></ul> 
<p>[ICASSP 2021] Fullsubnet: A Full-Band and Sub-Band Fusion Model for Real-Time Single-Channel Speech Enhancement</p> 
<ul><li>动机：子带模型满足实时性，性能有竞争力。然而，由于它不能对全局谱进行建模，也不能利用长距离跨带依赖性。对于信噪比极低的子带，子带模型很难恢复干净语音，而在全频带下可以。另外，训练全频带模型学习高维输入和输出之间的回归，缺乏专门用于子频带信息（如信号平稳性）的机制。</li><li>方法：本文提出了一种用于单通道实时语音增强的全频带和子频带融合模型FullSubNet。子带模型独立处理每个频率。FullSubNet将纯全频带模型和纯子频带模型依次连接起来，使用联合训练来整合这两种模型的优点。</li></ul> 
<p>[ICASSP 2021] Monaural Speech Enhancement with Complex Convolutional Block Attention Module and Joint Time Frequency Losses</p> 
<ul><li>动机： Deep complex U-Net结构和卷积递归网络（CRN）结构实现了单耳语音增强的最新性能。 Deep complex U-Net和CRN都是具有跳跃连接的编码器和解码器结构，这在很大程度上依赖于复数卷积层的表示能力。</li><li>方法：提出了一个复卷积块注意模块（CCBAM），通过构造更多信息特征来提高复卷积层的表示能力。CCBAM是一个轻量级的通用模块，可以轻松集成到任何复值卷积层中。我们将CCBAM与Deep complex U-Net和CRN相结合，以提高它们的语音增强性能。我们进一步提出了一种混合损耗函数，用于在时频（TF）域和时域中联合优化复杂模型。通过整合CCBAM和混合损耗，我们形成了一个新的端到端（E2E）复杂语音增强框架。</li></ul> 
<p>[IEEE/ACM 2020] DeepMMSE: A Deep Learning Approach to MMSE-Based Noise Power Spectral Density Estimation</p> 
<ul><li>动机：精确的噪声功率谱密度(PSD)跟踪器是单通道语音增强系统不可或缺的组成部分。基于贝叶斯驱动的最小均方误差(MMSE)的噪声PSD估计器是近年来最突出的指标。然而，由于现有的信噪比（SNR）估计方法，它们缺乏跟踪高度非平稳噪声源的能力，这是由于噪声信号的变化速率比语音信号的变化速率慢这一基本假设造成的。结果，基于MMSE的噪声PSD跟踪器表现出较大的跟踪延迟，并且产生需要偏置补偿的噪声PSD估计。</li><li>方法：提出了一种基于MMSE的噪声PSD跟踪器，该跟踪器采用时间卷积网络（TCN）作为先验信噪比估计器。提出的噪声PSD跟踪器，称为DeepMMSE，不假设噪声或语音的特性，没有跟踪延迟。</li></ul> 
<p>[ICASSP 2021] A Modulation-Domain Loss for Neural-Network-Based Real-Time Speech Enhancement</p> 
<ul><li>动机：调制与语音清晰度密切相关。然而，目前调制域方法假设语音和噪声在调制域中是可分离的。此外，它们通常需要一套完整的光谱时间感受野（STRF），以便将处理后的调制光谱反转回TF域。这可能在计算上不适用于实时应用。</li><li>方法：描述了一种基于深度学习的语音增强系统的调制域损失函数。可学习的谱-时间感受野（STRF）被用来优化说话人识别任务。然后使用学习的STRF计算调制域中的加权均方误差（MSE），以训练语音增强系统。</li></ul> 
<p>[ICASSP 2021] Speech Enhancement with Mixture of Deep Experts with Clean Clustering Pre-Training</p> 
<ul><li>动机：在为自动语音识别（ASR）应用引入了基于音素的体系结构中，如果ASR系统不正确，则会激活错误的DNN。此外，ASR系统中的错误会中断语音的连续性。最后，ASR不是作为训练阶段的一部分学习的。</li><li>方法：提出了一个混合的深度专家（模式）神经网络架构的单麦克风语音增强。我们的架构由一组深层神经网络（DNNs）组成，每一个DNNs都是不同语音频谱模式（如音素）的“专家”。一个选通DNN负责潜在变量，这些变量是分配给每个专家给定一个语音片段的输出的权重。专家从噪声输入中估计出一个掩模，然后将最终的掩模作为专家估计的加权平均，权重由选通DNN确定。然后，基于估计的掩模，采用软谱衰减对噪声语音信号进行增强。</li></ul> 
<h3><a id="2022118_95"></a>2022/1/18</h3> 
<p>[Speech Communication 12/2020] Masked multi-head self-attention for causal speech enhancement</p> 
<ul><li>动机：对带噪语音长期依赖性建模对语音增强非常重要，RNN和TCN在建模长期依赖性时都显示出不足。多头注意力（MHA）在机器翻译等任务中优于RNN和TCN。</li><li>方法：构建MHANet，网络在TCN的基础上将残差块换为多头注意力块（多头注意力层+两层前馈层），使用mask来确保因果关系（确保实时）。<br> 语音增强思路沿用Deep Xi框架：使用网络预测映射的先验SNR，之后使用MMSE方法预测纯净语音。</li></ul> 
<p>[Interspeech 2020] DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement</p> 
<ul><li>动机：最近的一些研究使用复数谱图作为训练目标，但在实数网络中进行训练，分别预测幅值和相位分量（实部和虚部）。需要提出一个新的网络，其中的CNN和RNN结构都可以直接处理复数运算。</li><li>方法：设计了新的复数语音增强网络，称为DCCRN（深复卷积递归网络），优化了SI-SNR loss。网络结合了DCUNET和CRN的优点，使用LSTM建模时间上下文，显著降低了可训练参数和计算成本。</li></ul> 
<h3><a id="2022125_104"></a>2022/1/25</h3> 
<p>[IEEE Access 2021] DeepLPC: A Deep Learning Approach to Augmented Kalman Filter-Based Single-Channel Speech Enhancement</p> 
<ul><li>动机：AKF使用LPC和预测误差方差两参数完成语音增强。Deep Xi和DeepMMSE不能直接从带噪语音中估计纯净语音的LPC，因使用白化滤波器估计线性预测系数(LPC)有偏差，严重降低了AKF（增广卡尔曼滤波器）产生的增强语音的感知质量和可懂度。</li><li>方法：提出的框架称为DeepLPC，可直接一并估计纯净语音和噪声LPC功率谱（LPC-PS），网络选用Resnet TCN。之后采用统计模型方法Levinson-Durbin和AKF得出增强的语音。</li></ul> 
<p>[Interspeech 2018] A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement</p> 
<ul><li>动机：许多语音增强的实际应用，如助听器和人工耳蜗，要求实时处理，无延迟或低延迟。</li><li>方法：将卷积编码器-解码器（CED）和长短时存储器（LSTM）结合，形成自然适合实时处理的因果语音增强。</li></ul> 
<h3><a id="202221_113"></a>2022/2/1</h3> 
<p>[IEEE Access 2021] DeepLPC-MHANet: Multi-Head Self-Attention for Augmented Kalman Filter-Based Speech Enhancement</p> 
<ul><li>动机：当前基于增强卡尔曼滤波器（AKF）的语音增强算法使用时间卷积网络（TCN）来估计纯净语音和噪声线性预测系数（LPC）。然而，与TCN相比，多头注意网络（MHANet）已证明能够更有效地建模带噪语音的长期相关性。</li><li>方法：研究了用于LPC估计的MHANet。通过以迄今为止最小的偏差生成LPC估计值，DeepLPC MHANet使AKF能够以比以往任何基于KF或AKF的方法更高的质量和可懂度生成增强语音。</li></ul> 
<h3><a id="202228_118"></a>2022/2/8</h3> 
<p>[Interspeech 2020] Noisy-Reverberant Speech Enhancement Using DenseUNet with Time-Frequency Attention</p> 
<ul><li>动机：背景噪声和室内混响是现实环境中语音信号的两种主要失真。每种方法都会降低语音清晰度和质量，其综合影响尤其有害。</li><li>方法：提出了一种T-F注意机制来改进基于DenseUNet的噪声混响语音增强，通过隐式估计复比率掩码（cRM）在复数域中执行增强。其中引入了一种新的时频（T-F）注意机制来有效地聚合不同T-F单元之间的上下文信息，并开发了一种通道式注意来合并不同特征映射之间的信息源。此外还引入了规范化激活策略，以缓解小批量训练的性能下降。</li></ul> 
<p>[Preprint submitted to Elsevier] On Training Targets for Supervised LPC Estimation to Augmented Kalman Filter-based Speech Enhancement</p> 
<ul><li>动机：在实际应用中，语音编码、语音识别和语音增强的性能在很大程度上取决于纯净语音和噪声的线性预测系数（LPC）的准确性。语音和噪声LPC估计作为一个有监督的学习问题已经显示出相当大的前景。</li><li>方法：本研究旨在确定哪种训练目标以及DNN方法在实践中产生更准确的LPC。我们使用大数据集为每个训练目标训练ResNet TCN和MHANet。</li></ul> 
<h3><a id="2022215_127"></a>2022/2/15</h3> 
<p>[arXiv:2010.12713] Dual-path Self-Attention RNN for Real-Time Speech Enhancement</p> 
<ul><li>动机：双路递归神经网络（DP-RNN）最近被提出用于时域说话人分离，具有最先进的性能。基于DP-RNN在说话人分离和语音增强中的注意机制的成功，在这项工作中，我们建议用注意来增强DP-RNN。</li><li>方法：利用长短时记忆（LSTM）RNN和组块间SARNN中的因果注意，开发了一个实时DP-SARNN。提出了一种用于时域语音增强的双路径自注意递归神经网络（DP-SARNN）。通过最近提出的一种高效注意机制来增强块间和块内RNN。DP-SARNN通过使用比DP-RNN大四倍的帧位。</li></ul> 
<p>[arXiv:2111.07518] Time-Frequency Attention for Monaural Speech Enhancement</p> 
<ul><li>动机：语音增强的研究大多不考虑语音在时间-频率（T-F）表示中的能量分布，这对于准确地预测Mask或谱（spectra）是很重要的。</li><li>方法：提出了一个简单而有效的T-F注意（TFA）模块，其中生成了一个二维Attention图，使用残差时间卷积网络（ResTCN）作为主干网络。<br> 二维Attention：<br> <img src="https://images2.imgbox.com/30/0f/GYUiAWrN_o.png" width="50%"><br> 我们提议的TFA模块示意图，其中TA和FA模块分别以黑色和蓝色虚线框显示。<br> AvgPool和Conv1D分别代表平均池和一维卷积运算。<br> ⊗ 和“圈点”分别表示矩阵乘法和元素乘积。</li></ul> 
<h3><a id="2022222_142"></a>2022/2/22</h3> 
<p>[arXiv:1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p> 
<ul><li>动机：将预训练好的语言表达应用于下游任务有两种现有策略：基于特征（feature）和微调（fine-tuning），目前它们都使用单向（预测未来）语言模型来学习一般的语言表征。当前的技术限制了预训练的表示的能力，尤其是对于微调方法。在完形填空（Cloze）任务（Taylor，1953）的启发下，我们通过提出BERT。</li><li>方法：通过使用“masked语言模型”（MLM）预训练目标，实现预训练的深层双向表达，缓解了单向性约束。使用“下一个句子预测”任务来联合预训练文本对表示，是第一个基于微调的表示模型。https://github.com/google-research/bert提供代码和预训练的模型。</li></ul> 
<h3><a id="202232_147"></a>2022/3/2</h3> 
<p>[IEEE/ACM Trans. Audio Speech Lang. Process.] Dense CNN With Self-Attention for Time-Domain Speech Enhancement</p> 
<ul><li>动机：相位对增强语音的质量起着重要作用，尤其是在低信噪比条件下。最近的研究已经成功地将自注意用于语音增强，并取得了很好的效果。时域网络训练中提出的频谱幅度（SM）损失获得了更好的客观可懂度和质量分数，但在增强的话语中引入了之前未知的伪影（artifact）。</li><li>方法：提出了一种具有自注意的稠密卷积网络（DCN），用于时域语音增强。<br> DCN基于带跳过连接的编码器-解码器体系结构。<br> 编码器和解码器中的每一层包括稠密块和注意块。<br> 稠密块用于更好的特征提取，并在更深的网络中进行特征重用，注意模块用于话语级（utterance level）上下文聚合。<br> 发现在语音增强方面，注意力优于扩张卷积。<br> 提出了相位约束幅度（PCM）损失来消除这种伪影，从而获得一致的信噪比改善。</li></ul> 
<h3><a id="202238_157"></a>2022/3/8</h3> 
<p>[arXiv: 2111.06015] Uformer: A Unet based dilated complex &amp; real dual-path conformer network for simultaneous speech enhancement and dereverberation</p> 
<ul><li>动机：在复数谱中，幅度和相位之间存在着密切的关系。卓越的幅度估计有利于相位更好的恢复，反之亦然。TeCANet将Transformer应用于上下文窗口内的帧，从而估计帧之间的相关性。为了进一步实现更强的语境建模能力，Conformer和双路径方法的结合是一种本能的想法。</li><li>方法：提出了Uformer，这是一种基于Unet的扩展复数实双路径Conformer网络，用于同时进行语音增强和去混响。在编码器和解码器之间的瓶颈特征上应用了扩展的复实双路径Conformer。<br> 在Conformer中，使用带有上下文窗口的时间注意（TA）对局部时间依赖性进行建模，而使用扩展卷积（DC）对全局时间依赖性进行建模。频率注意（FA）用于建模子带信息。<br> 采用融合编码器和解码器来同时建模复数的频谱和幅度。<br> 利用编码器-解码器注意代替跳过连接来估计注意Mask。</li></ul> 
<h3><a id="2022315_165"></a>2022/3/15</h3> 
<p>[ICASSP 2022] MMLatch: Bottom-Up Top-Down Fusion For Multimodal Sentiment Analysis</p> 
<ul><li> <p>动机：反馈回路已被证明存在于人脑中，例如在发声或视觉运动协调的情况下。人类感知模型强调了自上而下融合的重要性，即高级表征会影响感知感官输入的方式，即认知会影响感知。这些自上而下的互动在当前的深度学习模型中没有得到体现。</p> </li><li> <p>方法： 提出了一种神经体系结构，在网络训练过程中，使用前向传递中的反馈机制来捕获自上而下的跨模态交互。提出的机制提取每个模态的高级表示，并使用这些表示来屏蔽（mask）感官输入，从而允许模型执行自上而下的特征屏蔽。</p> </li></ul> 
<h3><a id="2022322_172"></a>2022/3/22</h3> 
<p>[arXiv:2110.04474] A Mutual learning framework for Few-shot Sound Event Detection</p> 
<ul><li> <p>动机：原型网络（Prototypic network，简称ProtoNet）已被证明是一种有效的少样本（few-shot）VAD方法，但仍存在两个问题：首先，小规模的支持集不足，因此类原型（class prototypes）可能无法准确地表示类中心。 其次，特征提取器是任务不可知的（或类不可知的）： 特征提取器使用基类数据进行训练，并直接应用于看不见的类数据。</p> </li><li> <p>方法： 提出了一个相互学习(mutual learning)的框架，以不断更新特征提取器和类原型。 更具体地说，我们首先使用基类数据训练特征提取器，并使用类原型初始化分类器。 然后，我们利用未标记音频的统计信息，用转换推理更新分类器。 为了获得特定于任务的特征提取器，我们进一步使用更新的类原型作为监督信息来微调特征提取器。 这些过程可以重复几次，以便特征提取器和分类器可以不断更新。 我们的贡献可以总结如下：<br> （1）为了解决类原型不能准确地代表真实的类中心的问题，我们提出用跨导（transductive）学习更新类原型。<br> （2）为了使特征提取器具有任务特定性，我们提出了一种新的方法来微调特征提取器。<br> （3）与最先进的方法相比，我们的相互学习框架显著提高了少样本生物声学事件检测的性能。</p> </li></ul> 
<h3><a id="2022329_183"></a>2022/3/29</h3> 
<p>[arXiv:2110.05588] DeepFilterNet: A Low Complexity Speech Enhancement Framework for Full-Band Audio based on Deep Filtering</p> 
<ul><li> <p>动机：最近的工作建议使用复数滤波器代替带掩码的逐点乘法。噪声和语音通常具有平滑的频谱包络（spectral envelope）这一事实。深度滤波（DF）增强仅适用于较低频率，因为周期性语音成分包含较低频率的大部分能量。</p> </li><li> <p>方法： 提出了DeepFilterNet，这是一个利用深度滤波的两阶段语音增强框架。首先，我们使用模拟人类频率感知（frequency perception）的ERB标度增益（ERB-scaled gains）来增强频谱包络（spectral envelope）。第二阶段采用深度滤波来增强语音的周期成分（periodic components）。<br> 除了利用语音的感知特性（perceptual properties），我们还通过可分离（separable）卷积和线性层和递归层的广泛分组（extensive grouping）来增强网络稀疏性（sparsity），以设计一个低复杂度的体系结构。</p> </li></ul> 
<h3><a id="202245_191"></a>2022/4/5</h3> 
<p>[arXiv:2203.07960] Investigating self-supervised learning for speech enhancement and separation</p> 
<ul><li> <p>动机：将自监督学习（SSL）应用于增强和分离的研究非常有限。</p> </li><li> <p>方法： 评估了语音增强和分离下游任务的13种SSL上游方法。分析了现有SSL框架难以应用于语音增强和分离的因素，并讨论了这两项任务所需的表示属性。</p> </li><li> <p>结论：<br> 主要实验：<br> <img src="https://images2.imgbox.com/66/c5/Bf46W1SJ_o.png" width="60%"></p> </li></ul> 
<p>一些SSL表示始终优于基线（STFT、对数梅尔滤波器库[FBBank]）。<br> 与ASR等其他任务相比，SSL在增强和分离方面的改进并没有那么大。比FBANK基线提高了0.05 PESQ以上的模型包括 HuBERT/UniSpeech-SAT/WavLM Large 和 UniSpeech-SAT Base+ 。<br> SSL模型性能不佳的可能原因：<br> 1）SSL模型从未见过噪声和说话人重叠；<br> 2）只关注全局结构并建立长期依赖关系，丢失信号重建所需的一些局部信息。<br> 矢量量化（VQ）似乎会降低分离性能（文章解释：将连续语音表示转换为离散语音表示不利于语音分离等连续序列生成任务）。</p> 
<p>消融研究：<br> 步幅：步幅大小对语音增强和分离性能有很大影响（提高步幅会导致性能降低）。<br> 不同层做表示：对于HuBERT，第12层对于语音增强获得最佳PESQ和STOI数。第1层对于语音分离性能最好（第一层的性能比最后一层高4.21dB）。<br> 对不同层加权求和的表示进一步改善了增强和分离结果，对于大多数SSL模型，较低的层通常获得更高的权重。文章解释为：增强和分离需要细粒度的波形信息来重建干净的信号，而这些信号通常在SSL模型的深层中丢失。</p> 
<h3><a id="2022413_215"></a>2022/4/13</h3> 
<p>[ICASSP 2022] SNRi Target Training for Joint Speech Enhancement and Recognition</p> 
<ul><li> <p>动机：典型的单通道SE前端旨在完美地去除噪声，但在实践中，它们会在经过去噪的语音中产生伪影（影响下游任务）。本文希望以数据驱动的方式估计每个噪声输入的适当降噪水平。</p> </li><li> <p>方法：提出“信噪比改善（signal-to-noise ratio improvement，SNRi）目标训练”——SNRi-Net。解决了以下问题：对于给定的噪声输入，每个任务需要多少信噪比改善（SNRi）？ 产生具有指定目标SNRi的输出信号。另外，在ASR联合训练中，目标SNRi值由一个辅助网络SNRi-Pred-Net估计。</p> </li></ul> 
<h3><a id="202253_222"></a>2022/5/3</h3> 
<p>[ICASSP 2022] BLOOM-NET: BLOCKWISE OPTIMIZATION FOR MASKING NETWORKS TOWARD SCALABLE AND EFFICIENT SPEECH ENHANCEMENT</p> 
<ul><li> <p>动机：[模型压缩\语音增强] 在多阶段语音增强中，时域分离模型（基线1）和时域分块优化（基线2）都显示出了模块冗余问题。一个可扩展且高效的系统必须通过自适应模型体系结构覆盖边缘计算中与资源相关的广泛多样性，而不是训练不同块配置的多个版本，以适应各种应用和硬件需求。</p> </li><li> <p>方法：提出可伸缩语音增强模型（BLOOM-Net），实现了特征空间而非原始信号域的可伸缩性。具体方法为：</p> </li></ul> 
<img src="https://images2.imgbox.com/01/69/0VpnC1Aq_o.png" width="60%"> 
<p>对于所有的块序列，Encoder都是共享和重用的。BLOOM-Net执行特定于块的Mask和Decoder只是为了计算特定于块的loss。<br> 这样，在测试期间，实际推理涉及Enc，Sep（1~l）, Mas（l）和Dec（l），即上图中的阴影块。<br> 另外，微调（所有L块中的所有模块都使用所有损失函数的组合进行更新）可以进一步细化经过充分训练的BLOOM-Net，使其从全局中学习，性能接近理论上限。<br> 结果，所提出的分块优化方法在性能略有下降的情况下实现了所需的可伸缩性。</p> 
<h3><a id="2022510_237"></a>2022/5/10</h3> 
<p>[ICASSP 2022] Low Resources Online Single-Microphone Speech Enhancement with Harmonic Emphasis</p> 
<ul><li> <p>动机：[损失函数] 许多语音增强算法的基音谐波之间的降噪能力很低，在严重的情况下，谐波结构甚至可能会丢失。认识到这个缺点，我们提出了一种新的加权Loss，强调了基音占主导地位的频带。</p> </li><li> <p>方法：引入了一种新的加权损失来训练基于DNN的语音增强算法，该算法强调由人类语音典型的谐波结构控制的频带。首先提出了一种利用输入信号的自相关来检测谐波频带的方法。语音片段周期性可以通过语音信号的自相关（以语音周期的典型滞后（lags typical to the speech periods）计算）与其方差之间的比率来评估。然后，通过增加相应的权重来强调这些检测到的频带对整体损耗函数的贡献。</p> </li></ul> 
<h3><a id="2022517_245"></a>2022/5/17</h3> 
<p>[ICASSP 2022] Multi-Scale Temporal Frequency Convolutional Network with Axial Attention for Multi-Channel Speech Enhancement</p> 
<p>Department of Speech Technology, Baidu Inc, Beijing, 100085, China</p> 
<ul><li> <p>动机：[多通道语音增强] 全DNN方法通常比信号处理与DNN结合的方法具有更好的性能。</p> </li><li> <p>方法：设计了多尺度时频卷积网络（MTFAA-Net）。采用等效矩形带宽（equivalent rectangular bandwidth，ERB）的频带合并模块处理输入。模型包括多通道相位编码器（复数的卷积层）、多尺度（multi-scale）时频处理（在时间维度上进行了扩张卷积，在频率维度上进行了上下采样）、轴向自注意（axial self-attention，ASA）和两级掩蔽等策略。</p> </li></ul> 
<img src="https://images2.imgbox.com/ff/53/i5OUpdCr_o.png" width="60%"> 
<p>图1 所提出的MTFAA-Net的架构</p> 
<img src="https://images2.imgbox.com/b8/38/pVi1GnQe_o.png" width="60%"> 
<p>图2 相位编码器（a）、TF卷积模块（b）、频率提升采样模块（c）、掩模估计和应用模块（d）和轴向自注意模块（e）的流程图</p> 
<h3><a id="2022524_263"></a>2022/5/24</h3> 
<p>[arXiv:2111.04436] Low Resources Online Single-Microphone Speech Enhancement with Harmonic Emphasis</p> 
<ul><li> <p>动机：[网络浮点数量化] 许多压缩和加速策略在分类任务中取得了显著的效果，但在回归任务中产生了不满足的（ungratified）性能。</p> </li><li> <p>方法：提出了一种新的仅符号指数浮点网络（sign-exponent-only floating-point network，SEOFP-NET）技术，其参数由符号指数浮点表示。SEOFP-NET通过量化原始单精度浮点表示的分数位（fraction bits），并将浮点乘法器（floating-point multiplier）替换为整数加法器（integer-adder）。</p> </li></ul> 
<img src="https://images2.imgbox.com/08/35/mPtCUX8g_o.jpg" width="60%"> 
<img src="https://images2.imgbox.com/39/e8/AoudQLaU_o.jpg" width="60%"> 
<img src="https://images2.imgbox.com/72/0a/meUNY7U3_o.jpg" width="60%">
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f80e11814a511ea6919c121952e2ce20/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">《通信原理》课后题 樊昌信</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/315ede013d3e369e0d52057fbc88b870/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python &#43; Django &#43; MySQL 系列学习(二)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>