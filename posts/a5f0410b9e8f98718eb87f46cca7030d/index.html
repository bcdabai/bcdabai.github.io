<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 爬虫入门的教程（2小时快速入门、简单易懂、快速上手） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python 爬虫入门的教程（2小时快速入门、简单易懂、快速上手）" />
<meta property="og:description" content="http://c.biancheng.net/view/2011.html
这是一篇详细介绍 Python 爬虫入门的教程，从实战出发，适合初学者。读者只需在阅读过程紧跟文章思路，理清相应的实现代码，30 分钟即可学会编写简单的 Python 爬虫。
这篇 Python 爬虫教程主要讲解以下 5 部分内容：
了解网页；使用 requests 库抓取网站数据；使用 Beautiful Soup 解析网页；清洗和组织数据；爬虫攻防战； 了解网页 以中国旅游网首页（http://www.cntour.cn/）为例，抓取中国旅游网首页首条信息（标题和链接），数据以明文的形式出面在源码中。在中国旅游网首页，按快捷键【Ctrl&#43;U】打开源码页面，如图 1 所示。
图 1 中国旅游网首页源码
认识网页结构 网页一般由三部分组成，分别是 HTML（超文本标记语言）、CSS（层叠样式表）和 JScript（活动脚本语言）。
HTML
HTML 是整个网页的结构，相当于整个网站的框架。带“＜”、“＞”符号的都是属于 HTML 的标签，并且标签都是成对出现的。
常见的标签如下：
&lt;html&gt;..&lt;/html&gt; 表示标记中间的元素是网页
&lt;body&gt;..&lt;/body&gt; 表示用户可见的内容
&lt;div&gt;..&lt;/div&gt; 表示框架
&lt;p&gt;..&lt;/p&gt; 表示段落
&lt;li&gt;..&lt;/li&gt;表示列表
&lt;img&gt;..&lt;/img&gt;表示图片
&lt;h1&gt;..&lt;/h1&gt;表示标题
&lt;a href=&#34;&#34;&gt;..&lt;/a&gt;表示超链接
CSS
CSS 表示样式，图 1 中第 13 行＜style type=＂text/css＂＞表示下面引用一个 CSS，在 CSS 中定义了外观。
JScript
JScript 表示功能。交互的内容和各种特效都在 JScript 中，JScript 描述了网站中的各种功能。
如果用人体来比喻，HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里。CSS 是人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等。JScript 表示人的技能，例如跳舞、唱歌或者演奏乐器等。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a5f0410b9e8f98718eb87f46cca7030d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-08-30T21:32:57+08:00" />
<meta property="article:modified_time" content="2019-08-30T21:32:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 爬虫入门的教程（2小时快速入门、简单易懂、快速上手）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><a href="http://c.biancheng.net/view/2011.html" rel="nofollow">http://c.biancheng.net/view/2011.html</a></p> 
<p>这是一篇详细介绍 <a href="http://c.biancheng.net/python/" rel="nofollow">Python</a> 爬虫入门的教程，从实战出发，适合初学者。读者只需在阅读过程紧跟文章思路，理清相应的实现代码，30 分钟即可学会编写简单的 Python 爬虫。<br><br> 这篇 Python 爬虫教程主要讲解以下 5 部分内容：</p> 
<ol><li>了解网页；</li><li>使用 requests 库抓取网站数据；</li><li>使用 Beautiful Soup 解析网页；</li><li>清洗和组织数据；</li><li>爬虫攻防战；</li></ol> 
<h3>了解网页</h3> 
<p>以中国旅游网首页（<a href="http://www.cntour.cn/" rel="nofollow">http://www.cntour.cn/</a>）为例，抓取中国旅游网首页首条信息（标题和链接），数据以明文的形式出面在源码中。在中国旅游网首页，按快捷键【Ctrl+U】打开源码页面，如图 1 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/3c/5f/ZiDhJ4iI_o.png"><br> 图 1 中国旅游网首页源码</p> 
<h4>认识网页结构</h4> 
<p>网页一般由三部分组成，分别是 HTML（超文本标记语言）、CSS（层叠样式表）和 JScript（活动脚本语言）。</p> 
<p>HTML</p> 
<p>HTML 是整个网页的结构，相当于整个网站的框架。带“＜”、“＞”符号的都是属于 HTML 的标签，并且标签都是成对出现的。<br><br> 常见的标签如下：</p> 
<p>&lt;html&gt;..&lt;/html&gt; 表示标记中间的元素是网页<br> &lt;body&gt;..&lt;/body&gt; 表示用户可见的内容<br> &lt;div&gt;..&lt;/div&gt; 表示框架<br> &lt;p&gt;..&lt;/p&gt; 表示段落<br> &lt;li&gt;..&lt;/li&gt;表示列表<br> &lt;img&gt;..&lt;/img&gt;表示图片<br> &lt;h1&gt;..&lt;/h1&gt;表示标题<br> &lt;a href=""&gt;..&lt;/a&gt;表示超链接</p> 
<p>CSS</p> 
<p>CSS 表示样式，图 1 中第 13 行＜style type=＂text/css＂＞表示下面引用一个 CSS，在 CSS 中定义了外观。</p> 
<p>JScript</p> 
<p>JScript 表示功能。交互的内容和各种特效都在 JScript 中，JScript 描述了网站中的各种功能。<br><br> 如果用人体来比喻，HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里。CSS 是人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等。JScript 表示人的技能，例如跳舞、唱歌或者演奏乐器等。</p> 
<h4>写一个简单的 HTML</h4> 
<p>通过编写和修改 HTML，可以更好地理解 HTML。首先打开一个记事本，然后输入下面的内容：</p> 
<p>&lt;html&gt;<br> &lt;head&gt;<br>     &lt;title&gt; Python 3 爬虫与数据清洗入门与实战&lt;/title&gt;<br> &lt;/head&gt;<br> &lt;body&gt;<br>     &lt;div&gt;<br>         &lt;p&gt;Python 3爬虫与数据清洗入门与实战&lt;/p&gt;<br>     &lt;/div&gt;<br>     &lt;div&gt;<br>         &lt;ul&gt;<br>             &lt;li&gt;&lt;a href="http://c.biancheng.net"&gt;爬虫&lt;/a&gt;&lt;/li&gt;<br>             &lt;li&gt;数据清洗&lt;/li&gt;<br>         &lt;/ul&gt;<br>     &lt;/div&gt;<br> &lt;/body&gt;</p> 
<p>输入代码后，保存记事本，然后修改文件名和后缀名为"HTML.html"；<br><br> 运行该文件后的效果，如图 2 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/47/29/HJY4mlQY_o.gif"><br> 图 2</p> 
<p><br> 这段代码只是用到了 HTML，读者可以自行修改代码中的中文，然后观察其变化。</p> 
<h4>关于爬虫的合法性</h4> 
<p>几乎每一个网站都有一个名为 robots.txt 的文档，当然也有部分网站没有设定 robots.txt。对于没有设定 robots.txt 的网站可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。如果网站有 robots.txt 文档，就要判断是否有禁止访客获取的数据。<br><br> 以淘宝网为例，在浏览器中访问 <a href="https://www.taobao.com/robots.txt" rel="nofollow">https://www.taobao.com/robots.txt</a>，如图  3 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/63/fe/hnQY2BBd_o.gif"><br> 图 3 淘宝网的robots.txt文件内容</p> 
<p><br> 淘宝网允许部分爬虫访问它的部分路径，而对于没有得到允许的用户，则全部禁止爬取，代码如下：</p> 
<p>User-Agent:*<br> Disallow:/</p> 
<p>这一句代码的意思是除前面指定的爬虫外，不允许其他爬虫爬取任何数据。</p> 
<h3>使用 requests 库请求网站</h3> 
<h4>安装 requests 库</h4> 
<p>首先在 PyCharm 中安装 requests 库，为此打开 PyCharm，单击“File”（文件）菜单，选择“Setting for New Projects...”命令，如图 4 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/3b/02/kbbz34z8_o.png"><br> 图 4</p> 
<p><br> 选择“Project Interpreter”（项目编译器）命令，确认当前选择的编译器，然后单击右上角的加号，如图 5 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/04/59/ZdRgPSfI_o.png"><br> 图 5</p> 
<p><br> 在搜索框输入：requests（注意，一定要输入完整，不然容易出错），然后单击左下角的“Install Package”（安装库）按钮。如图 6 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/ff/65/tKSDfHia_o.png"><br> 图 6</p> 
<p><br> 安装完成后，会在 Install Package 上显示“Package‘requests’ installed successfully”（库的请求已成功安装），如图 7 所示；如果安装不成功将会显示提示信息。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/56/b8/kaDeMCw7_o.png"><br> 图 7 安装成功</p> 
<h4>爬虫的基本原理</h4> 
<p>网页请求的过程分为两个环节：</p> 
<ol><li>Request （请求）：每一个展示在用户面前的网页都必须经过这一步，也就是向服务器发送访问请求。</li><li>Response（响应）：服务器在接收到用户的请求后，会验证请求的有效性，然后向用户（客户端）发送响应的内容，客户端接收服务器响应的内容，将内容展示出来，就是我们所熟悉的网页请求，如图 8 所示。</li></ol> 
<p><img alt="" class="has" src="https://images2.imgbox.com/02/75/RhsLH1DF_o.gif"><br> 图 8 Response相应</p> 
<p><br> 网页请求的方式也分为两种：</p> 
<ol><li>GET：最常见的方式，一般用于获取或者查询资源信息，也是大多数网站使用的方式，响应速度快。</li><li>POST：相比 GET 方式，多了以表单形式上传参数的功能，因此除查询信息外，还可以修改信息。</li></ol> 
<p><br> 所以，在写爬虫前要先确定向谁发送请求，用什么方式发送。</p> 
<h4>使用 GET 方式抓取数据</h4> 
<p>复制任意一条首页首条新闻的标题，在源码页面按【Ctrl+F】组合键调出搜索框，将标题粘贴在搜索框中，然后按【Enter】键。<br><br> 如图 8 所示，标题可以在源码中搜索到，请求对象是www.cntour.cn，请求方式是GET（所有在源码中的数据请求方式都是GET），如图 9 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/f9/7c/yEutQNHP_o.png"><br> 图 9（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11G6464b94.jpg" rel="nofollow">点此查看高清大图</a>）</p> 
<p>确定好请求对象和方式后，在 PyCharm 中输入以下代码：</p> 
<pre> </pre> 
<ol><li>import requests #导入requests包</li><li>url = 'http://www.cntour.cn/'</li><li>strhtml = requests.get(url) #Get方式获取网页数据</li><li>print(strhtml.text)</li></ol> 
<p>运行结果如图 10 所示：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/7a/03/xEMOYLUj_o.png"><br> 图 10 运行结果效果图（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11G64K5445.jpg" rel="nofollow">点此查看高清大图</a>）</p> 
<p><br> 加载库使用的语句是 import+库的名字。在上述过程中，加载 requests 库的语句是：import requests。<br><br> 用 GET 方式获取数据需要调用 requests 库中的 get 方法，使用方法是在 requests 后输入英文点号，如下所示：</p> 
<p>requests.get</p> 
<p>将获取到的数据存到 strhtml 变量中，代码如下：</p> 
<p>strhtml = request.get(url)</p> 
<p>这个时候 strhtml 是一个 URL 对象，它代表整个网页，但此时只需要网页中的源码，下面的语句表示网页源码：</p> 
<p>strhtml.text</p> 
<h4>使用 POST 方式抓取数据</h4> 
<p>首先输入有道翻译的网址：<a href="http://fanyi.youdao.com/" rel="nofollow">http://fanyi.youdao.com/</a>，进入有道翻译页面。<br><br> 按快捷键 F12，进入开发者模式，单击 Network，此时内容为空，如图 11 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/c1/89/6XhdeUYV_o.gif"><br> 图 11</p> 
<p><br> 在有道翻译中输入“我爱中国”，单击“翻译”按钮，如图 12 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/e7/cc/TRVfkI2a_o.gif"><br> 图 12</p> 
<p><br> 在开发者模式中，依次单击“Network”按钮和“XHR”按钮，找到翻译数据，如图 13 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/3e/69/80fgJiFf_o.gif"><br> 图 13</p> 
<p><br> 单击 Headers，发现请求数据的方式为 POST。如图 14 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/0a/1c/ac3Afj5K_o.gif"><br> 图 14</p> 
<p><br> 找到数据所在之处并且明确请求方式之后，接下来开始撰写爬虫。<br><br> 首先，将 Headers 中的 URL 复制出来，并赋值给 url，代码如下：</p> 
<p>url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'</p> 
<p>POST 的请求获取数据的方式不同于 GET，POST 请求数据必须构建请求头才可以。<br><br> Form Data 中的请求参数如图 15 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/43/21/NM4ZouCC_o.gif"><br> 图 15</p> 
<p><br> 将其复制并构建一个新字典：</p> 
<p>From_data={'i':'我愛中國','from':'zh-CHS','to':'en','smartresult':'dict','client':'fanyideskweb','salt':'15477056211258','sign':'b3589f32c38bc9e3876a570b8a992604','ts':'1547705621125','bv':'b33a2f3f9d09bde064c9275bcb33d94e','doctype':'json','version':'2.1','keyfrom':'fanyi.web','action':'FY_BY_REALTIME','typoResult':'false'}</p> 
<p>接下来使用 requests.post 方法请求表单数据，代码如下：</p> 
<p>import requests        #导入requests包<br> response = requests.post(url,data=payload)</p> 
<p>将字符串格式的数据转换成 JSON 格式数据，并根据<a href="http://c.biancheng.net/data_structure/" rel="nofollow">数据结构</a>，提取数据，并将翻译结果打印出来，代码如下：</p> 
<pre> </pre> 
<ol><li>import json</li><li>content = json.loads(response.text)</li><li>print(content['translateResult'][0][0]['tgt'])</li></ol> 
<p>使用 requests.post 方法抓取有道翻译结果的完整代码如下：</p> 
<pre> </pre> 
<ol><li>import requests #导入requests包</li><li>import json</li><li>def get_translate_date(word=None):</li><li>url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'</li><li>From_data={'i':word,'from':'zh-CHS','to':'en','smartresult':'dict','client':'fanyideskweb','salt':'15477056211258','sign':'b3589f32c38bc9e3876a570b8a992604','ts':'1547705621125','bv':'b33a2f3f9d09bde064c9275bcb33d94e','doctype':'json','version':'2.1','keyfrom':'fanyi.web','action':'FY_BY_REALTIME','typoResult':'false'}</li><li>#请求表单数据</li><li>response = requests.post(url,data=From_data)</li><li>#将Json格式字符串转字典</li><li>content = json.loads(response.text)</li><li>print(content)</li><li>#打印翻译后的数据</li><li>#print(content['translateResult'][0][0]['tgt'])</li><li>if __name__=='__main__':</li><li>get_translate_date('我爱中国')</li></ol> 
<h3>使用 Beautiful Soup 解析网页</h3> 
<p>通过 requests 库已经可以抓到网页源码，接下来要从源码中找到并提取数据。Beautiful Soup 是 python 的一个库，其最主要的功能是从网页中抓取数据。Beautiful Soup 目前已经被移植到 bs4 库中，也就是说在导入 Beautiful Soup 时需要先安装 bs4 库。<br><br> 安装 bs4 库的方式如图 16 所示:</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/30/fa/pQkKgYBj_o.gif"><br> 图 16</p> 
<p><br> 安装好 bs4 库以后，还需安装 lxml 库。如果我们不安装 lxml 库，就会使用 Python 默认的解析器。尽管 Beautiful Soup 既支持 Python 标准库中的 HTML 解析器又支持一些第三方解析器，但是 lxml 库具有功能更加强大、速度更快的特点，因此笔者推荐安装 lxml 库。<br><br> 安装 Python 第三方库后，输入下面的代码，即可开启 Beautiful Soup 之旅：</p> 
<pre> </pre> 
<ol><li>import requests #导入requests包</li><li>from bs4 import BeautifulSoup</li><li>url='http://www.cntour.cn/'</li><li>strhtml=requests.get(url)</li><li>soup=BeautifulSoup(strhtml.text,'lxml')</li><li>data = soup.select('#main&gt;div&gt;div.mtop.firstMod.clearfix&gt;div.centerBox&gt;ul.newsList&gt;li&gt;a')</li><li>print(data)</li></ol> 
<p>代码运行结果如图 17 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/c6/da/6xA9SHuE_o.png"><br> 图 17（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11GA30cY.jpg" rel="nofollow">点此查看高清大图</a>）</p> 
<p><br> Beautiful Soup 库能够轻松解析网页信息，它被集成在 bs4 库中，需要时可以从 bs4 库中调用。其表达语句如下：</p> 
<p>from bs4 import BeautifulSoup</p> 
<p>首先，HTML 文档将被转换成 Unicode 编码格式，然后 Beautiful Soup 选择最合适的解析器来解析这段文档，此处指定 lxml 解析器进行解析。解析后便将复杂的 HTML 文档转换成树形结构，并且每个节点都是 Python 对象。这里将解析后的文档存储到新建的变量 soup 中，代码如下：</p> 
<p>soup=BeautifulSoup(strhtml.text,'lxml')</p> 
<p>接下来用 select（选择器）定位数据，定位数据时需要使用浏览器的开发者模式，将鼠标光标停留在对应的数据位置并右击，然后在快捷菜单中选择“检查”命令，如图 18 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/b4/bf/Tfys4jvx_o.gif"><br> 图 18</p> 
<p><br> 随后在浏览器右侧会弹出开发者界面，右侧高亮的代码（参见图  19(b)）对应着左侧高亮的数据文本（参见图 19(a)）。右击右侧高亮数据，在弹出的快捷菜单中选择“Copy”➔“Copy Selector”命令，便可以自动复制路径。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/4f/a4/1P1PPJGk_o.gif"><br> 图 19 复制路径</p> 
<p>将路径粘贴在文档中，代码如下:</p> 
<p>#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li:nth-child(1) &gt; a</p> 
<p>由于这条路径是选中的第一条的路径，而我们需要获取所有的头条新闻，因此将 li：nth-child（1）中冒号（包含冒号）后面的部分删掉，代码如下：</p> 
<p>#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li &gt; a</p> 
<p>使用 soup.select 引用这个路径，代码如下：</p> 
<p>data = soup.select('#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li &gt; a')</p> 
<h3>清洗和组织数据</h3> 
<p>至此，获得了一段目标的 HTML 代码，但还没有把数据提取出来，接下来在 PyCharm 中输入以下代码：</p> 
<pre> </pre> 
<ol><li>for item in data:</li><li>result={<!-- --></li><li>'title':item.get_text(),</li><li>'link':item.get('href')</li><li>}</li><li>print(result)</li></ol> 
<p>代码运行结果如图 20 所示：</p> 
<p><img alt="" class="has" src="https://images2.imgbox.com/fa/19/K8GXLmWF_o.gif"><br> 图 20（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11GA629403.jpg" rel="nofollow">点此查看高清大图</a>）</p> 
<p><br> 首先明确要提取的数据是标题和链接，标题在＜a＞标签中，提取标签的正文用 get_text() 方法。链接在＜a＞标签的 href 属性中，提取标签中的 href 属性用 get() 方法，在括号中指定要提取的属性数据，即 get(＇href＇)。<br><br> 从图 20 中可以发现，文章的链接中有一个数字 ID。下面用正则表达式提取这个 ID。需要使用的正则符号如下:</p> 
<p>\d匹配数字<br> +匹配前一个字符1次或多次</p> 
<p>在 Python 中调用正则表达式时使用 re 库，这个库不用安装，可以直接调用。在 PyCharm 中输入以下代码:</p> 
<pre> </pre> 
<ol><li>import re</li><li>for item in data:</li><li>result={<!-- --></li><li>"title":item.get_text(),</li><li>"link":item.get('href'),</li><li>'ID':re.findall('\d+',item.get('href'))</li><li>}</li><li>print(result)</li></ol> 
<p>运行结果如图 21 所示：</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/30/ad/hp3pHC4B_o.png"><br> 图 21</p> 
<p><br> 这里使用 re 库的 findall 方法，第一个参数表示正则表达式，第二个参数表示要提取的文本。</p> 
<h3>爬虫攻防战</h3> 
<p>爬虫是模拟人的浏览访问行为，进行数据的批量抓取。当抓取的数据量逐渐增大时，会给被访问的服务器造成很大的压力，甚至有可能崩溃。换句话就是说，服务器是不喜欢有人抓取自己的数据的。那么，网站方面就会针对这些爬虫者，采取一些反爬策略。<br><br> 服务器第一种识别爬虫的方式就是通过检查连接的 useragent 来识别到底是浏览器访问，还是代码访问的。如果是代码访问的话，访问量增大时，服务器会直接封掉来访 IP。<br><br> 那么应对这种初级的反爬机制，我们应该采取何种举措？<br><br> 还是以前面创建好的爬虫为例。在进行访问时，我们在开发者环境下不仅可以找到 URL、Form Data，还可以在 Request headers 中构造浏览器的请求头，封装自己。服务器识别浏览器访问的方法就是判断 keyword 是否为 Request headers 下的 User-Agent，如图 22 所示。</p> 
<p><br><img alt="" class="has" src="https://images2.imgbox.com/e9/66/dGEGnd0m_o.png"><br> 图 22</p> 
<p><br> 因此，我们只需要构造这个请求头的参数。创建请求头部信息即可，代码如下：</p> 
<p>headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'}<br> response = request.get(url,headers=headers)</p> 
<p>写到这里，很多读者会认为修改 User-Agent 很太简单。确实很简单，但是正常人1秒看一个图，而个爬虫1秒可以抓取好多张图，比如 1 秒抓取上百张图，那么服务器的压力必然会增大。也就是说，如果在一个 IP 下批量访问下载图片，这个行为不符合正常人类的行为，肯定要被封 IP。<br><br> 其原理也很简单，就是统计每个IP的访问频率，该频率超过阈值，就会返回一个验证码，如果真的是用户访问的话，用户就会填写，然后继续访问，如果是代码访问的话，就会被封 IP。<br><br> 这个问题的解决方案有两个，第一个就是常用的增设延时，每 3 秒钟抓取一次，代码如下：</p> 
<p>import time<br> time.sleep(3)</p> 
<p>但是，我们写爬虫的目的是为了高效批量抓取数据，这里设置 3 秒钟抓取一次，效率未免太低。其实，还有一个更重要的解决办法，那就是从本质上解决问题。<br><br> 不管如何访问，服务器的目的就是查出哪些为代码访问，然后封锁 IP。解决办法：为避免被封 IP，在数据采集时经常会使用代理。当然，requests 也有相应的 proxies 属性。<br><br> 首先，构建自己的代理 IP 池，将其以字典的形式赋值给 proxies，然后传输给 requests，代码如下：</p> 
<pre> </pre> 
<ol><li>proxies={<!-- --></li><li>"http":"http://10.10.1.10:3128",</li><li>"https":"http://10.10.1.10:1080",</li><li>}</li><li>response = requests.get(url, proxies=proxies)</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d3198542ffb20427beef49edb23d1f7f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">游戏：Java编程实现猜数（0——100），最多猜10次</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f3a14493c8301e9a97970fc8eabc82be/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python3 websocket客户端</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>