<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>android 解码出来的视频frame数据，是如何一步步的传递到显示端的(使用 GPU offline 合成) - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="android 解码出来的视频frame数据，是如何一步步的传递到显示端的(使用 GPU offline 合成)" />
<meta property="og:description" content="经过一段时间的研究，对android视频解码，直到显示端的整体流程，有了浅薄的理解
这里总结一下，解码出来的视频帧，是怎么一步步走到显示的。
平台: Exynos 4412 android 4.4
1, Exynos 4412 视频编解码依赖于硬件MFC。 解码出来的数据，不会进行原始数据的搬运。实际传递的是这些解码完数据的物理地址。
2，显示端硬件获得这些数据的物理地址，用这些物理地址取数据进行显示
3，这里不研究解码驱动和显示驱动。内核态的东西留到以后再研究。这里只研究了android中的流程
4，整体上，可以理解为
MFC-&gt; V4L2 -&gt; OMX -&gt; awesomeplayer -&gt; sufrfaceflinger -&gt; bufferqueue-&gt; sufrfaceflinger -&gt; HWcomposer -&gt; GPU -&gt; DISPLAY
1，初始化的时候，
-----&gt; allocateOutputBuffersFromNativeWindow
-----&gt; status_t OMXNodeInstance::useGraphicBuffer
-----&gt; virtual status_t useGraphicBuffer
-----&gt; status_t OMXNodeInstance::useGraphicBuffer2_l
-----&gt; OMX_UseBuffer -----&gt; SEC_OMX_UseBuffer //temp_bufferHeader-&gt;pBuffer = pBuffer; //这里就是把graphicBuffer-&gt;handle复制给了bufferHeader-&gt;pBuffer，也就是data_ptr，也就是存贮一些地址值的内存，也就是graphicBuffer-&gt;handle中最终存储了解码完毕数据存放地址信息，最后传给显示
说到底，就是MediaBuffer中指向了有效的data_ptr， MediaBuffer指向了GraphicBuffer，graphicBuffer-&gt;handle实际上就是data_ptr
所以一开始传的是MediaBuffer，最后又传的GraphicBuffer， 但是都能使用到data_ptr
2，解码过程运转起来之后
-----&gt; SEC_OutputBufferGetQueue里面，把pSECComponent-&gt;processData[OUTPUT_PORT_INDEX].dataBuffer = dataBuffer-&gt;bufferHeader-&gt;pBuffer;
-----&gt; SEC_MFC_H264_Decode_Nonblock里面
-----&gt; SsbSipMfcDecGetOutBuf 获取到解码完的数据物理地址填充到dataBuffer，也就是填充到了dataBuffer-&gt;bufferHeader-&gt;pBuffer，然后一步步往上送
-----&gt; sec_mfc_bufferProcess" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/15057864c270f626b0aa1bf6de421d71/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2015-12-11T21:52:13+08:00" />
<meta property="article:modified_time" content="2015-12-11T21:52:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">android 解码出来的视频frame数据，是如何一步步的传递到显示端的(使用 GPU offline 合成)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 经过一段时间的研究，对android视频解码，直到显示端的整体流程，有了浅薄的理解</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 这里总结一下，解码出来的视频帧，是怎么一步步走到显示的。</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 平台: Exynos 4412  android 4.4</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 1,  Exynos 4412 视频编解码依赖于硬件MFC。  解码出来的数据，不会进行原始数据的搬运。实际传递的是这些解码完数据的物理地址。</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 2，显示端硬件获得这些数据的物理地址，用这些物理地址取数据进行显示</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 3，这里不研究解码驱动和显示驱动。内核态的东西留到以后再研究。这里只研究了android中的流程</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 4，整体上，可以理解为</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> <span style="white-space:pre"></span>MFC-&gt;   V4L2  -&gt;  OMX -&gt;    <span style="font-size:13.3333px"> awesomeplayer -&gt;  sufrfaceflinger <span style="font-size:13.3333px"> -&gt;   <span style="font-size:13.3333px"> bufferqueue-&gt;   <span style="font-size:13.3333px">sufrfaceflinger </span><span style="font-size:13.3333px"> </span><span style="font-size:13.3333px"> -&gt;    <span style="font-size:13.3333px"> </span></span></span></span></span><span style="font-size:13.3333px">HWcomposer -&gt;  GPU</span><span style="font-size:13.3333px"> -&gt; DISPLAY</span></p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> <br> </p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> <strong>1，初始化的时候，</strong><br> -----&gt; allocateOutputBuffersFromNativeWindow<br> -----&gt; status_t OMXNodeInstance::useGraphicBuffer<br> -----&gt; virtual status_t useGraphicBuffer<br> -----&gt; status_t OMXNodeInstance::useGraphicBuffer2_l<span style="white-space:pre"></span><br> -----&gt; OMX_UseBuffer<span style="white-space:pre"> </span><br> -----&gt; SEC_OMX_UseBuffer   //temp_bufferHeader-&gt;pBuffer        = pBuffer; <br> <span style="white-space:pre"></span>//这里就是把graphicBuffer-&gt;handle复制给了bufferHeader-&gt;pBuffer，也就是data_ptr，也就是存贮一些地址值的内存，也就是graphicBuffer-&gt;handle中最终存储了解码完毕数据存放地址信息，最后传给显示<br> <span style="white-space:pre"></span>说到底，就是MediaBuffer中指向了有效的data_ptr， MediaBuffer指向了GraphicBuffer，graphicBuffer-&gt;handle实际上就是data_ptr<br> <span style="white-space:pre"></span>所以一开始传的是MediaBuffer，最后又传的GraphicBuffer， 但是都能使用到data_ptr</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> <br> <strong>2，解码过程运转起来之后</strong><br> -----&gt; SEC_OutputBufferGetQueue里面，把pSECComponent-&gt;processData[OUTPUT_PORT_INDEX].dataBuffer = dataBuffer-&gt;bufferHeader-&gt;pBuffer;<br> -----&gt; SEC_MFC_H264_Decode_Nonblock里面<br> -----&gt; SsbSipMfcDecGetOutBuf 获取到解码完的数据物理地址填充到dataBuffer，也就是填充到了dataBuffer-&gt;bufferHeader-&gt;pBuffer，然后一步步往上送<br> -----&gt; sec_mfc_bufferProcess<br> -----&gt; SEC_Postprocess_OutputData<br> -----&gt; SEC_OutputBufferReturn<br> -----&gt; FillBufferDone<br> -----&gt; OMX_ERRORTYPE OMXNodeInstance::OnFillBufferDone<br> -----&gt; OMX_ERRORTYPE OMX::OnFillBufferDone(   node_id node, OMX_IN OMX_BUFFERHEADERTYPE *pBuffer)<br>  {<!-- --><br>     ALOGV("OnFillBufferDone buffer=%p", pBuffer);<br>     omx_message msg;<br>     msg.type = omx_message::FILL_BUFFER_DONE;<br>     msg.node = node;<br>     msg.u.extended_buffer_data.buffer = pBuffer;<br>     msg.u.extended_buffer_data.range_offset = pBuffer-&gt;nOffset;<br>     msg.u.extended_buffer_data.range_length = pBuffer-&gt;nFilledLen;<br>     msg.u.extended_buffer_data.flags = pBuffer-&gt;nFlags;<br>     msg.u.extended_buffer_data.timestamp = pBuffer-&gt;nTimeStamp;<br>     msg.u.extended_buffer_data.platform_private = pBuffer-&gt;pPlatformPrivate;<br>     msg.u.extended_buffer_data.data_ptr = pBuffer-&gt;pBuffer;      ///pBuffer-&gt;pBuffer实际上就是dataBuffer-&gt;bufferHeader-&gt;pBuffer;  也就是  pSECComponent-&gt;processData[OUTPUT_PORT_INDEX].dataBuffer，  也就是SEC_MFC_H264_Decode_Nonblock里面的pOutputData-&gt;dataBuffer;<br> <span style="white-space:pre"></span>//    void *pOutputBuf = (void *)pOutputData-&gt;dataBuffer; 这里的pOutputBuf的内容就传给了data_ptr<br> <span style="white-space:pre"></span>//    SEC_OSAL_Memcpy(pYUVBuf[0], &amp;(outputInfo.YPhyAddr), sizeof(outputInfo.YPhyAddr));<br>     //    SEC_OSAL_Memcpy((unsigned char *)pYUVBuf[0] + (sizeof(void *) * 1), &amp;(outputInfo.CPhyAddr), sizeof(outputInfo.CPhyAddr));<br>     //    SEC_OSAL_Memcpy((unsigned char *)pYUVBuf[0] + (sizeof(void *) * 2), &amp;(outputInfo.YVirAddr), sizeof(outputInfo.YVirAddr));<br>     //    SEC_OSAL_Memcpy((unsigned char *)pYUVBuf[0] + (sizeof(void *) * 3), &amp;(outputInfo.CVirAddr), sizeof(outputInfo.CVirAddr));<br> <span style="white-space:pre"></span>//    data_ptr 里面存的就是几个虚拟地址<br>     findDispatcher(node)-&gt;post(msg);<br>     return OMX_ErrorNone;<br> }<br> -----&gt; void OMX::CallbackDispatcher::post(const omx_message &amp;msg) {<!-- --><br>    Mutex::Autolock autoLock(mLock);<br>     mQueue.push_back(msg);<br>     mQueueChanged.signal();<br> }<br> <br> <strong>3，stagefright的某个循环：</strong><br>   void OMXCodec::on_message(const omx_message &amp;msg) {<!-- --><br>     switch (msg.type) {<!-- --><br>         case omx_message::FILL_BUFFER_DONE:<br>         {<!-- --><br>                 mFilledBuffers.push_back(i);<br>                 mBufferFilled.signal();<br>                 if (mIsEncoder) {<!-- --><br>                     sched_yield();<br>                 }<br>         }<br> }<br> FillBufferDone时候最终会通过OMX的消息机制走到OMXCodec::on_message中进入case omx_message::FILL_BUFFER_DONE:主要工作是，记录这个已经填充的buffer的索引号，然后将这个索引号保存在<br>                 mFilledBuffers.push_back(i);<br>                 mBufferFilled.signal();<br> 看到mBufferFilled这个Condition被signal了，那么谁wait在这个Condition上呢？看上OMXCodec：：read函数中我红色标注的部分。原来在OMXCodec::read函数的后半段中会一直<br> while (mState != ERROR &amp;&amp; !mNoMoreOutputData &amp;&amp; mFilledBuffers.empty()) {<!-- --><br>         if ((err = waitForBufferFilled_l()) != OK) {<!-- --><br>             return err;<br>         }<br>     }<br>        那么FILL_BUFFER_DONE之后，在OMXCodec::read函数中被返回给AwesomePlayer的mVideoBuffer就是在OMX框架中outPutPort上的buffer。<br> 通过上面的分析可以看到，虽然在OMX框架中 Input/OutPutPort上的buffer的生产和消费是异步，但是还是通过了一个Condition mBufferFilled来做同步。这个生产者消费者的问题，来用一个信号量做同步，还是比较好的。<br>          <br> <strong>4，stagefright的某个循环：   </strong><br> void AwesomePlayer::onVideoEvent()   <br> -----&gt; status_t err = mVideoSource-&gt;read(&amp;mVideoBuffer, &amp;options);<br> -----&gt; mVideoRenderer-&gt;render(mVideoBuffer);<br> -----&gt; virtual void render(MediaBuffer *buffer) {<!-- --><br>         ATRACE_CALL();<br>         int64_t timeUs;<br>         CHECK(buffer-&gt;meta_data()-&gt;findInt64(kKeyTime, &amp;timeUs));<br>         native_window_set_buffers_timestamp(mNativeWindow.get(), timeUs * 1000);<br>         status_t err = mNativeWindow-&gt;queueBuffer(<br>                 mNativeWindow.get(), buffer-&gt;graphicBuffer().get(), -1);  //在此之前，传递的都是MediaBuffer，在allocateOutputBuffersFromNativeWindow初始化的时候，MediaBuffer中的成员已经指向。graphicBuffer。从这一步开始，一步步传递的是graphicBuffer。但是前面讲了，graphicBuffer-&gt;handle实际上就是要传递的buffer。里面包含了解码物理地址<br>         if (err != 0) {<!-- --><br>             ALOGE("queueBuffer failed with error %s (%d)", strerror(-err),<br>                     -err);<br>             return;<br>         }<br>         sp&lt;MetaData&gt; metaData = buffer-&gt;meta_data();<br>         metaData-&gt;setInt32(kKeyRendered, 1);<br>     }<span style="white-space:pre"> </span><br> <br> ×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××以上为stagefright的内容，以下为surfaceflinger的内容×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××<br> <br> -----&gt; mNativeWindow-&gt;queueBuffer<br> -----&gt; ANativeWindow::queueBuffer      = hook_queueBuffer;<br>  -----&gt; Surface::hook_queueBuffer()<br> -----&gt; int Surface::queueBuffer(android_native_buffer_t* buffer, int fenceFd) //查出32个slot buffer中的哪一个，把index传下去<br>  -----&gt; virtual status_t queueBuffer<br> -----&gt; status_t BnGraphicBufferProducer::onTransact<br> <span style="white-space:pre"></span>       case QUEUE_BUFFER: {<!-- --><br>             CHECK_INTERFACE(IGraphicBufferProducer, data, reply);<br>             int buf = data.readInt32();<br>             QueueBufferInput input(data);<br>             QueueBufferOutput* const output =<br>                     reinterpret_cast&lt;QueueBufferOutput *&gt;(<br>                             reply-&gt;writeInplace(sizeof(QueueBufferOutput)));<br>             status_t result = queueBuffer(buf, input, output);<br>             reply-&gt;writeInt32(result);<br>             return NO_ERROR;<br>         } break;<br>    -----&gt;  status_t BufferQueue::queueBuffer   //item.mBuf = buf; mQueue.push_back(item); listener-&gt;onFrameAvailable();<br>   -----&gt;  void FramebufferSurface::onFrameAvailable()  里面nextBuffer获得GraphicBuffer<br>   -----&gt;  mHwc.fbPost<br>   -----&gt;  int HWComposer::fbPost<br>   -----&gt;  status_t HWComposer::setFramebufferTarget   //disp.fbTargetHandle = buf-&gt;handle;  disp.framebufferTarget-&gt;handle = disp.fbTargetHandle;  前面已经讲了，graphicBuffer-&gt;handle 存的就是有效数据。 也就是最终disp.framebufferTarget-&gt;handle指向有效数据<br>   在初始化的时候，HWComposer::createWorkList里面  disp.framebufferTarget = &amp;disp.list-&gt;hwLayers[numLayers - 1];让framebufferTarget指向了disp.list-&gt;hwLayers[numLayers - 1]。 所以这里最终disp.list-&gt;hwLayers[numLayers - 1]-&gt;handle指向有效数据<br>     <br> <strong> 5， Vsync事件触发</strong><br>   -----&gt; dispatchRefresh<br>   -----&gt; SurfaceFlinger::onMessageReceived<br>   ----&gt; handleMessageRefresh<br> -----&gt; doComposition<br> <span style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px">-----&gt; </span> doDisplayComposition<br> <span style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px">-----&gt; </span> clip.isEmpty<br> <span style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px">-----&gt; </span> onDraw<br> <span style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px">-----&gt; </span> drawWithOpenGL 0,0,1280,720,1280,720,<br> <span style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px">-----&gt; GPU</span><br> <br> 总结：<br> 1，graphicBuffer-&gt;handle 存放有效数据。具体在哪里初始化申请的，待研究<br> 2，OMX，stagefright，surfaceflinger,HWcomposer说到底就是在传递这个graphicBuffer-&gt;handle<br> 3，传到HWcomposer后，并不是马上刷新屏幕。而是先用setFramebufferTarget存好。等Vsync时机到了，刷新<br>   </p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 以上只是对流程进行了简单梳理</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 其中涉及到很多surfaceflinger，bufferqueue,binder的东西。</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 让人看的头疼。需要后面继续深入研究</p> 
<p style="color:rgb(51,51,51); font-family:Arial; font-size:14px; line-height:26px"> 其中难免有错误。请大家指出</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6835799f68d3933e5ed23d51d22708f8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">stagefright使用surfaceflinger buffer 的分析</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fd881e314ca0463f43183678e8bc13b1/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">UrlRewrite的使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>