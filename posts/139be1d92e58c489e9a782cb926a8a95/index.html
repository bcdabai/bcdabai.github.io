<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>多目标学习(Multi-task Learning)-网络设计和损失函数优化 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="多目标学习(Multi-task Learning)-网络设计和损失函数优化" />
<meta property="og:description" content="目前多目标学习主要从两个方向展开，一个是网络结构设计，一个是损失函数优化；
一、MTL网络设计 MTL网络通常可分为两种，一种是hard-parameter sharing不同任务间共用底部的隐层，另一种是soft-parameter sharing，形式较为多样，如两个任务参数不共享，但对不同任务的参数增加L2范数的限制；也有一些对每个任务分别生成各自的隐层，学习所有隐层的组合；这两种方式各有优劣，hard类的网络较soft不容易陷入过拟合，但如果任务差异较大，模型结果较差，但soft类网络通常参数较多，结构比较复杂，线上部署困难；
1、hard-parameter sharing models hard-parameter sharing models为不同任务底层共享模型结构和参数，顶层分为几个不同的目标进行网络训练
这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响
基本上，只要是能预测单模型的模型，都可以很简单的转化为hard-parameter sharing models的结构，只需要将共享层的最后一层与多个输出层拼接即可。
2 soft-parameter sharing models soft-parameter sharing models不同于hard-parameter sharing model，每个任务有自己的参数，最后通过对不同任务的参数之间的差异加约束，表达相似性。比如可以使用L2, trace norm（迹范数）等。
网络结构如下：
3、相关技术 3.1 MMoE 谷歌2018 其中(a)是传统的硬共享参数模型，(b)是MoE模型，使用单个gate控制多个任务的参数，（c）是MMoE模型，在MoE的基础上，每个任务使用一个gate控制其权重。
名词解释：
a、expert：指对模型输入进行不同方式的变换处理的网络层，每个Expert表示一种网络（Expert也可以都一样）
b、gate：控制每个Expert权重的变量，对于每一个任务，不同Expert的权重可能是不一样的，因此使用gate来控制权重，类似于attention
MoE模型对于不同的任务的gate权重是一样的，其函数表达式如下：
y k = h k ∑ i = 1 n g i f i ( x ) y^k=h^k\sum_{i=1}^{n}g_if_i(x) yk=hki=1∑n​gi​fi​(x)
其中k表示第k个任务，n表示n个expert网络
MMoE是在MoE的基础上提出的方法，作者认为对于不同的任务，模型的权重选择是不同的，所以为每个任务分配一个gate模型。对于不同的任务，gate k的输出表示不同Expert被选择的概率，将多个Expert加权求和，得到f_k(x)，并输出给特点的Tower模型，用于最终的输出。
MMoE模型的表达式如下：
f k ( x ) = ∑ i = 1 n g i k ( x ) f i ( x ) f^k(x)=\sum_{i=1}^{n} g_i^k(x)f_i(x) fk(x)=i=1∑n​gik​(x)fi​(x)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/139be1d92e58c489e9a782cb926a8a95/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-01T17:12:02+08:00" />
<meta property="article:modified_time" content="2020-12-01T17:12:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">多目标学习(Multi-task Learning)-网络设计和损失函数优化</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>目前多目标学习主要从两个方向展开，一个是网络结构设计，一个是损失函数优化；</p> 
<h2><a id="MTL_3"></a>一、MTL网络设计</h2> 
<p>MTL网络通常可分为两种，一种是hard-parameter sharing不同任务间共用底部的隐层，另一种是soft-parameter sharing，形式较为多样，如两个任务参数不共享，但对不同任务的参数增加L2范数的限制；也有一些对每个任务分别生成各自的隐层，学习所有隐层的组合；这两种方式各有优劣，hard类的网络较soft不容易陷入过拟合，但如果任务差异较大，模型结果较差，但soft类网络通常参数较多，结构比较复杂，线上部署困难；</p> 
<h3><a id="1hardparameter_sharing_models_6"></a>1、hard-parameter sharing models</h3> 
<p>hard-parameter sharing models为不同任务底层共享模型结构和参数，顶层分为几个不同的目标进行网络训练<br> <img src="https://images2.imgbox.com/17/31/VvWzW6mJ_o.png" alt="在这里插入图片描述"><br> 这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响</p> 
<p>基本上，只要是能预测单模型的模型，都可以很简单的转化为hard-parameter sharing models的结构，只需要将共享层的最后一层与多个输出层拼接即可。</p> 
<h3><a id="2_softparameter_sharing_models_14"></a>2 soft-parameter sharing models</h3> 
<p>soft-parameter sharing models不同于hard-parameter sharing model，每个任务有自己的参数，最后通过对不同任务的参数之间的差异加约束，表达相似性。比如可以使用L2, trace norm（迹范数）等。</p> 
<p>网络结构如下：<br> <img src="https://images2.imgbox.com/dc/5a/dQhdO05t_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3_22"></a>3、相关技术</h3> 
<h4><a id="31_MMoE_2018_23"></a>3.1 MMoE 谷歌2018</h4> 
<p><img src="https://images2.imgbox.com/b4/53/cSTp6gGC_o.png" alt="在这里插入图片描述"><br> 其中(a)是传统的硬共享参数模型，(b)是MoE模型，使用单个gate控制多个任务的参数，（c）是MMoE模型，在MoE的基础上，每个任务使用一个gate控制其权重。<br> 名词解释：<br> a、expert：指对模型输入进行不同方式的变换处理的网络层，每个Expert表示一种网络（Expert也可以都一样）<br> b、gate：控制每个Expert权重的变量，对于每一个任务，不同Expert的权重可能是不一样的，因此使用gate来控制权重，类似于attention<br> MoE模型对于不同的任务的gate权重是一样的，其函数表达式如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           y 
          
         
           k 
          
         
        
          = 
         
         
         
           h 
          
         
           k 
          
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
         
         
           g 
          
         
           i 
          
         
         
         
           f 
          
         
           i 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
       
         y^k=h^k\sum_{i=1}^{n}g_if_i(x) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.09355em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.92907em; vertical-align: -1.27767em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.87233em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05001em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.30001em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.10764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><br> 其中k表示第k个任务，n表示n个expert网络</p> 
<p>MMoE是在MoE的基础上提出的方法，作者认为对于不同的任务，模型的权重选择是不同的，所以为每个任务分配一个gate模型。对于不同的任务，gate k的输出表示不同Expert被选择的概率，将多个Expert加权求和，得到f_k(x)，并输出给特点的Tower模型，用于最终的输出。</p> 
<p>MMoE模型的表达式如下：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           f 
          
         
           k 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
         
         
           g 
          
         
           i 
          
         
           k 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
         
         
           f 
          
         
           i 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
       
         f^k(x)=\sum_{i=1}^{n} g_i^k(x)f_i(x) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.14911em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.92907em; vertical-align: -1.27767em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em;"><span class="" style="top: -1.87233em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -3.05001em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="top: -4.30001em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -2.453em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.10764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span></span><br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           g 
          
         
           k 
          
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
         
           W 
          
          
           
           
             g 
            
           
             k 
            
           
          
            ( 
           
          
            x 
           
          
            ) 
           
          
         
        
          ) 
         
        
       
         g^k(x)=softmax(W_{g^k(x)}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.14911em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.899108em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.12762em; vertical-align: -0.37762em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.49738em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.782029em;"><span class="" style="top: -2.786em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.37762em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span><br> 类似于MoE，k表示第k个任务，每个任务对应一个gate。<br> MMoE的底层参数仍然是共享的，但是通过目标和网络参数直接的gate来学习，让每部分网络充分学习到对每个目标的贡献最大的一组参数结构，通过这种方式来保证，底层网络参数共享的时候，不会出现目标之间相互抵消的作用。</p> 
<h4><a id="32_SNR_2019_43"></a>3.2 SNR 谷歌2019</h4> 
<p>论文地址为：http://www.jiaqima.com/papers/SNR.pdf</p> 
<p>SNR 是MMOE作者对多任务学习的进一步工作,为了尽量共享不同任务之间的信息，提高精度，也节约线上服务代价而提出的一种算法。<br> 其结构图如下<br> <img src="https://images2.imgbox.com/19/4a/EjYytP8O_o.png" alt="在这里插入图片描述"><br> <strong>主要优化点：</strong><br> 把网络结构的跨任务参数共享抽象为网络子结构的路由问题；<br> 引入0-1隐变量对路由作最优化；<br> 通过L1正则化（可以得到参数量更小的网络）,学到稀疏解，同等精度下节约11%的网络结构开销<br> SNR-Trans 可形式化为如下表达<br> <img src="https://images2.imgbox.com/7a/03/n63LG0MZ_o.png" alt="在这里插入图片描述"><br> hard concrete distribution 可形式化为<img src="https://images2.imgbox.com/41/60/AbTmIif0_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/bd/2a/5GJqNgBn_o.png" alt="在这里插入图片描述"><br> u是0，1之间的随机分布函数 β, γ, ζ 都是超参数<br> 加L1正则化的损失函数可表示为<br> <img src="https://images2.imgbox.com/39/13/TqapZztI_o.png" alt="在这里插入图片描述"><br> 其中，q是s的累计概率分布函数</p> 
<p>线上的serving的时候，z简化为<br> <img src="https://images2.imgbox.com/c8/07/w9odxw14_o.png" alt="在这里插入图片描述"><br> 可参见：网友的中文翻译 <a href="https://zhuanlan.zhihu.com/p/87288876" rel="nofollow">https://zhuanlan.zhihu.com/p/87288876</a></p> 
<h2><a id="_68"></a>二、损失函数优化</h2> 
<p>现有的损失函数优化方法大概有如下几类</p> 
<h3><a id="1Gradient_Normalization_71"></a>1、Gradient Normalization——梯度标准化</h3> 
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks》 ICML 2018</p> 
<p>论文参考链接：http://proceedings.mlr.press/v80/chen18a/chen18a.pdf<br> 希望不同的任务Loss量级接近；不同的任务以相近的速度来进行学习。<br> <strong>【评价】</strong><br> <strong>优点</strong>：Gradient Normalization既考虑了loss的量级，又考虑了不同任务的训练速度。<br> <strong>缺点</strong>：每一步迭代都需要额外计算梯度，当W选择的参数多的时候，会影响训练速度；<br> 此外，该loss依赖于参数的初始值；如果初始值变化很大的话，paper建议可以采用其他值代替，比如分类任务，可以用<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         l 
        
       
         o 
        
       
         g 
        
       
         ( 
        
       
         C 
        
       
         ) 
        
       
      
        log(C) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.07153em;">C</span><span class="mclose">)</span></span></span></span></span>来代替初始值，C是类别数量。<br> 其梯度损失（Gradient Loss）函数定义为，各个任务实际的梯度范数与理想的梯度范数的差的绝对值和,可表示为：<br> <img src="https://images2.imgbox.com/61/b6/piPmfAkj_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/80/79/oeV7sWOG_o.png" alt="在这里插入图片描述"><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          G 
         
        
          i 
         
        
          w 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        G_i^w(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00866em; vertical-align: -0.258664em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -2.44134em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.258664em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>是任务i梯度标准化的值，是任务i的权重<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          w 
         
        
          i 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        w_i(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>与loss L的乘积对<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          L 
         
        
          i 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        L_i(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>参数W求梯度的L2范数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          G 
         
        
          i 
         
        
          w 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        G_i^w(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00866em; vertical-align: -0.258664em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -2.44134em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.258664em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>可以衡量某个任务loss的量级；<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           G 
          
         
           ‾ 
          
         
        
          W 
         
        
       
         ( 
        
       
         t 
        
       
         ） 
        
       
      
        \overline G_W(t） 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.13333em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.88333em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mathdefault">G</span></span><span class="" style="top: -3.80333em;"><span class="pstrut" style="height: 3em;"></span><span class="overline-line" style="border-bottom-width: 0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mord cjk_fallback">）</span></span></span></span></span>是全局梯度标准化的值（即所有任务梯度标准化值的期望值），通过所有<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          G 
         
        
          i 
         
        
          w 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        G_i^w(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00866em; vertical-align: -0.258664em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -2.44134em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.258664em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>求均值实现。</p> 
<p>计算完Gradient Loss后，通过以下函数对 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          w 
         
        
          i 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        w_i(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span> 进行更新(GL指Gradient Loss)：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           w 
          
         
           i 
          
         
        
          ( 
         
        
          t 
         
        
          + 
         
        
          1 
         
        
          ) 
         
        
          = 
         
         
         
           w 
          
         
           i 
          
         
        
          ( 
         
        
          t 
         
        
          ) 
         
        
          + 
         
        
          λ 
         
        
          ∗ 
         
        
          G 
         
        
          r 
         
        
          a 
         
        
          d 
         
        
          i 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          ( 
         
        
          G 
         
        
          L 
         
        
          , 
         
         
         
           w 
          
         
           i 
          
         
        
          ( 
         
        
          t 
         
        
          ) 
         
        
          ) 
         
        
       
         w_i(t+1)=w_i(t)+\lambda*Gradient(GL,w_i(t)) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">G</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">i</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mord mathdefault">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></span><br> α是超参数，α 越大，对训练速度的平衡限制越强</p> 
<h3><a id="2__Dynamic_Weight_Averaging__88"></a>2 、Dynamic Weight Averaging ——动态加权平均</h3> 
<p>参见论文:《End-to-End Multi-Task Learning with Attention》，CVPR 2019<br> 论文作者知乎：https://zhuanlan.zhihu.com/p/138597214（Multi-task Learning and Beyond: 过去，现在与未来）<br> 直观来看，loss缩小快的任务，则权重会变小；反之权重会变大。<br> <img src="https://images2.imgbox.com/5b/51/dTYp8UOw_o.png" alt="在这里插入图片描述"><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          w 
         
        
          k 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        w_k(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.02691em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span> 代表了每个任务i的权重，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          L 
         
        
          n 
         
        
       
         ( 
        
       
         t 
        
       
         ) 
        
       
      
        L_n(t) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          α 
         
        
          n 
         
        
       
         t 
        
       
      
        \alpha_{n}t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.76508em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.0037em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">t</span></span></span></span></span>分别代表任务k在t时刻的loss，和训练速度，n代表任务数，t足够大时，w趋近于1<br> <strong>【评价】</strong><br> 优点：只需要记录不同step的loss值，从而避免了为了获取不同任务的梯度，实现简单，运算较快。<br> 缺点：没有考虑不同任务的loss的量级，需要额外的操作把各个任务的量级调整到差不多。</p> 
<h3><a id="3_Dynamic_Task_Prioritization__97"></a>3、 Dynamic Task Prioritization ——动态任务优先级</h3> 
<p>参见论文：《Dynamic task prioritization for multitask learning》，ECCV 2018，Cites：53<br> DTP希望让更难学的任务具有更高的权重<br> 其不同任务的权重为<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
          L 
         
        
          ( 
         
         
         
           p 
          
         
           c 
          
         
        
          ; 
         
         
         
           γ 
          
         
           0 
          
         
        
          ) 
         
        
          = 
         
        
          − 
         
        
          ( 
         
        
          1 
         
        
          − 
         
         
         
           p 
          
         
           c 
          
         
         
         
           ) 
          
          
          
            γ 
           
          
            0 
           
          
         
        
          l 
         
        
          o 
         
        
          g 
         
        
          ( 
         
         
         
           p 
          
         
           c 
          
         
        
          ) 
         
        
       
         FL(p_c;\gamma_0)=-(1-p_c)^{\gamma_0} log(p_c) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.05556em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">−</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714392em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05556em;">γ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em;"><span class="" style="top: -2.357em; margin-left: -0.05556em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></p> 
<ul><li>FL(pc ; γ0)代表不同任务的权重，</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           p 
          
         
           c 
          
         
        
       
         p_c 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>指的是KPI，即任务c的衡量指标，KPI要选择一个衡量任务的有意义的指标，比如分类模型中的准确率等，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           p 
          
         
           c 
          
         
        
       
         p_c 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>在[0,1]之间，KPI与任务的训练难度成反比，即KPI越高，任务越好学；</li><li>γ0允许为特定的任务i来调整权重；原文讲到：These focusing parameters γ0, …, γt are not the actual weights applied to the loss (i.e., not the mixing parameters) but rather adjust the rate at which easy examples and tasks are down-weighted.<br> 直观来看，KPI高的任务，学习起来比较简单，则权重会变小；反之，难学的任务权重会变大。<br> <strong>【评价】</strong><br> 优点：需要获取不同step的KPI值，从而避免了为了获取不同任务的梯度，运算较快<br> 缺点：DTP没有考虑不同任务的loss的量级，需要额外的操作把各个任务的量级调整到差不多；且需要经常计算KPI…</li></ul> 
<p>4、Uncertainty Weighting——不确定性加权<br> 论文地址：《Multi-task learning using uncertainty to weigh losses for scene geometry and semantics》<br> Keras 实现参见：<a href="https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example.ipynb">https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example.ipynb</a></p> 
<p>其他有价值的参考链接：<a href="https://engineering.taboola.com/deep-multi-task-learning-3-lessons-learned/" rel="nofollow">https://engineering.taboola.com/deep-multi-task-learning-3-lessons-learned/</a><br> 让“简单”的任务有更高的权重<br> 最终的loss为<br> <img src="https://images2.imgbox.com/39/60/JhCYGeyl_o.png" alt="在这里插入图片描述"><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          δ 
         
        
          1 
         
        
       
      
        \delta_1 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.03785em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         δ 
        
       
         2 
        
       
      
        \delta2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span><span class="mord">2</span></span></span></span></span> 是两个任务中，各自存在的不确定性。<br> 因此， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          δ 
         
        
          2 
         
        
       
      
        \delta^2 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03785em;">δ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>越大，任务的不确定性越大，则任务的权重越小，即噪声大且难学的任务权重会变小。</p> 
<p><strong>【评价】</strong><br> 优点：不确定性建模似乎可以适用于标签噪声更大的数据，而DTP可能在干净的标注数据里效果更好<br> 缺点：参考：<a href="https://zhuanlan.zhihu.com/p/138597214" rel="nofollow">https://zhuanlan.zhihu.com/p/138597214</a><br> <img src="https://images2.imgbox.com/91/46/RkclRotu_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5Pareto_optimum_125"></a>5、帕累托最优（Pareto optimum）</h3> 
<p>参见论文：Multi-Task Learning as Multi-Objective Optimization （https://arxiv.org/abs/1810.04650）</p> 
<p>gitlab地址为：<a href="https://github.com/JayYip/deep-learning-nlp-notes">https://github.com/JayYip/deep-learning-nlp-notes</a></p> 
<p>task-weighting 方法都基于一些特定的 heuristic，很难保证在 MTL optimisation 取得 optimum. 在该论文中，作者将 MTL 问题看做是多目标优化问题，其目标为取得 Pareto optimum.<br> 论文的损失函数为<br> <img src="https://images2.imgbox.com/fb/18/0fYAXUUb_o.png" alt="在这里插入图片描述"><br> Pareto optimum 是指任何对其中一个任务的效果变好的情况，一定会对其他剩余所有任务的效果变差。作者利用了一个叫 multiple gradient descent algorithm (MGDA) 的方法来寻找这个 Pareto stationary point。大致方式是，在每次计算 task-specific gradients 后，其得到 common direction 来更新共享参数。这个 common direction 如果存在，则整个 optimisation 并未收敛到 Pareto optimum。这样的收敛方法保证了 共享参数不会出现 conflicting gradients 让每一个任务的 loss 收敛更加平滑。</p> 
<h2><a id="MTL_learning_Tips_137"></a>三、MTL learning Tips</h2> 
<p>可参见：<a href="https://zhuanlan.zhihu.com/p/59413549" rel="nofollow">https://zhuanlan.zhihu.com/p/59413549</a><br> 部分tip罗列如下：</p> 
<ul><li>1 紧凑分布均匀的label的辅助任务更好（from POS in NLP）[When is multitask learning effective? Multitask learning for semantic sequence prediction under varying data conditions]</li><li>2 主任务训练曲线更快平稳，辅助任务平稳慢（还未平稳）[ Identifying beneficial task relations for multi-task learning in deep neural networks]</li><li>3 不同任务尺度不一样，任务最优学习率可能不同</li><li>4 某个任务的输出可以作为某些任务的输入</li><li>5 某些任务的迭代周期不同，可能需要异步训练（后验信息；特征选择，特征衍生任务等）</li><li>6 整体loss可能被某些任务主导，需要整个周期对参数进行动态调整（通过引入一些不确定性,每个任务学习一个噪声参数，统一所有损失 [Multitask learning using uncertainty to weigh losses for scene geometry and senantics]</li></ul> 
<p>7 某些估计作为特征（交替训练）</p> 
<p><strong>【参考链接】</strong></p> 
<p>1、https://zhuanlan.zhihu.com/p/269492239</p> 
<p>2、https://zhuanlan.zhihu.com/p/138597214</p> 
<p>3、https://zhuanlan.zhihu.com/p/68846373（帕累托最优）</p> 
<p>4、https://zhuanlan.zhihu.com/p/59413549</p> 
<p>5、新火试茶</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/597e8fec11fbafbe7a5ebdb8dbebbb4d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java中byte为何范围是-128~127</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/69e7014ccd0a77f8ad450e814ddb14ee/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">liunx安装nginx全步骤</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>