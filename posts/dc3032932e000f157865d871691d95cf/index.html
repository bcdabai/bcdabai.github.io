<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ceph优化总结 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Ceph优化总结" />
<meta property="og:description" content="Ceph优化总结 一. 硬件层面1、 CPU2、 内存3、 网络4、 SSD5、 BIOS 二. 软件层面1、 Kernel pid max2、 设置MTU，交换机端需要支持该功能，系统网卡设置才有效果3、 read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作4、 swappiness, 主要控制系统对swap的使用5、 I/O Scheduler，SSD要用noop，SATA/SAS使用deadline6、 ceph.conf配置选项7、 PG Number8、 修改crush map9、 其他因素 参考文章： https://www.jianshu.com/p/dd572541df2e https://blog.csdn.net/fuzhongfaya/article/details/80932766 一. 硬件层面 硬件规划
SSD选择
BIOS设置
1、 CPU ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。
ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。
ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。
2、 内存 ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存。
3、 网络 万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。网络接口上可以使用bond来提供高可用或负载均衡。
4、 SSD SSD在ceph中的使用可以有几种架构
a、 ssd作为Journal
b、 ssd作为高速ssd pool(需要更改crushmap)
c、 ssd做为tier pool
5、 BIOS a、 开启VT和HT，VH是虚拟化云平台必备的，HT是开启超线程单个处理器都能使用线程级并行计算。
b、 关闭节能设置，可有一定的性能提升。
c、 NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存， NUMA可能会在某些情况下影响ceph-osd 。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。
二. 软件层面 Linux OS" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/dc3032932e000f157865d871691d95cf/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-02-28T10:33:06+08:00" />
<meta property="article:modified_time" content="2019-02-28T10:33:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ceph优化总结</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>Ceph优化总结</h4> 
 <ul><li><a href="#__4" rel="nofollow">一. 硬件层面</a></li><li><ul><li><a href="#1__CPU_9" rel="nofollow">1、 CPU</a></li><li><a href="#2___14" rel="nofollow">2、 内存</a></li><li><a href="#3___17" rel="nofollow">3、 网络</a></li><li><a href="#4__SSD_20" rel="nofollow">4、 SSD</a></li><li><a href="#5__BIOS_26" rel="nofollow">5、 BIOS</a></li></ul> 
  </li><li><a href="#__30" rel="nofollow">二. 软件层面</a></li><li><ul><li><a href="#1__Kernel_pid_max_37" rel="nofollow">1、 Kernel pid max</a></li><li><a href="#2__MTU_39" rel="nofollow">2、 设置MTU，交换机端需要支持该功能，系统网卡设置才有效果</a></li><li><a href="#3__read_ahead__41" rel="nofollow">3、 read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作</a></li><li><a href="#4__swappiness_swap_43" rel="nofollow">4、 swappiness, 主要控制系统对swap的使用</a></li><li><a href="#5__IO_SchedulerSSDnoopSATASASdeadline_45" rel="nofollow">5、 I/O Scheduler，SSD要用noop，SATA/SAS使用deadline</a></li><li><a href="#6__cephconf_48" rel="nofollow">6、 ceph.conf配置选项</a></li><li><a href="#7__PG_Number_123" rel="nofollow">7、 PG Number</a></li><li><a href="#8__crush_map_133" rel="nofollow">8、 修改crush map</a></li><li><a href="#9___137" rel="nofollow">9、 其他因素</a></li></ul> 
 </li></ul> 
</div> 
<br> 参考文章： 
<br> 
<a href="https://www.jianshu.com/p/dd572541df2e" rel="nofollow">https://www.jianshu.com/p/dd572541df2e</a> 
<br> 
<a href="https://blog.csdn.net/fuzhongfaya/article/details/80932766">https://blog.csdn.net/fuzhongfaya/article/details/80932766</a> 
<p></p> 
<h2><a id="__4"></a>一. 硬件层面</h2> 
<p>硬件规划<br> SSD选择<br> BIOS设置</p> 
<h3><a id="1__CPU_9"></a>1、 CPU</h3> 
<p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。<br> ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。<br> ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p> 
<h3><a id="2___14"></a>2、 内存</h3> 
<p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存。</p> 
<h3><a id="3___17"></a>3、 网络</h3> 
<p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。网络接口上可以使用bond来提供高可用或负载均衡。</p> 
<h3><a id="4__SSD_20"></a>4、 SSD</h3> 
<p>SSD在ceph中的使用可以有几种架构<br> a、 ssd作为Journal<br> b、 ssd作为高速ssd pool(需要更改crushmap)<br> c、 ssd做为tier pool</p> 
<h3><a id="5__BIOS_26"></a>5、 BIOS</h3> 
<p>a、 开启VT和HT，VH是虚拟化云平台必备的，HT是开启超线程单个处理器都能使用线程级并行计算。<br> b、 关闭节能设置，可有一定的性能提升。<br> c、 NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存， NUMA可能会在某些情况下影响ceph-osd 。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p> 
<h2><a id="__30"></a>二. 软件层面</h2> 
<p>Linux OS<br> Ceph Configurations<br> PG Number调整<br> CRUSH Map<br> 其他因素</p> 
<h3><a id="1__Kernel_pid_max_37"></a>1、 Kernel pid max</h3> 
<p>echo 4194303 &gt; /proc/sys/kernel/pid_max</p> 
<h3><a id="2__MTU_39"></a>2、 设置MTU，交换机端需要支持该功能，系统网卡设置才有效果</h3> 
<p>配置文件追加MTU=9000</p> 
<h3><a id="3__read_ahead__41"></a>3、 read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作</h3> 
<p>echo “8192” &gt; /sys/block/sda/queue/read_ahead_kb</p> 
<h3><a id="4__swappiness_swap_43"></a>4、 swappiness, 主要控制系统对swap的使用</h3> 
<p>echo “vm.swappiness = 0”/etc/sysctl.conf ; sysctl –p</p> 
<h3><a id="5__IO_SchedulerSSDnoopSATASASdeadline_45"></a>5、 I/O Scheduler，SSD要用noop，SATA/SAS使用deadline</h3> 
<p>echo “deadline” &gt;/sys/block/vd[x]/queue/scheduler<br> echo “noop” &gt;/sys/block/vd[x]/queue/scheduler</p> 
<h3><a id="6__cephconf_48"></a>6、 ceph.conf配置选项</h3> 
<pre><code>[global]#全局设置
fsid = 41a38739-fb23-469c-b504-5c934ffe6026             #集群标识ID 
mon initial members = ceph1,ceph2,ceph3 #初始monitor (由创建monitor命令而定)
mon host = ip1,ip2,ip3       #monitor IP 地址
auth cluster required = cephx                  			#集群认证
auth service required = cephx                           #服务认证
auth client required = cephx                            #客户端认证
osd pool default size = 2                               #最小副本数
osd pool default min size = 1                           #PG 处于 degraded 状态不影响其 IO 能力,min_size是一个PG能接受IO的最小副本数
osd pool default pg num = 128                           #pool的pg数量
osd pool default pgp num = 128                          #pool的pgp数量
public network = 10.0.1.0/24                            #公共网络(monitorIP段) 
cluster network = 10.0.1.0/24                           #集群网络
#max open files = 131072                                 #默认0#如果设置了该选项，Ceph会设置系统的max open fds 注释掉使用系统默认优化过的

##############################################################
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
mon clock drift allowed = 1                             #默认值0.05#monitor间的clock drift
mon osd min down reporters = 3                          #默认值1#向monitor报告down的最小OSD数
mon osd down out interval = 600      					#默认值300 #标记一个OSD状态为down和out之前ceph等待的秒数
##############################################################
[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 20000 								#默认5120 #osd journal大小
osd journal = /var/lib/ceph/osd/$cluster-$id/journal    #osd journal 位置
osd mkfs type = xfs                                     #格式化系统类型（当Ceph挂载一个OSD时）
osd mkfs options xfs = -f -i size=2048                  #强制格式化
osd mount options xfs = "rw,noexec,nodev,noatime,nodiratime,nobarrier" #默认值rw,noatime,inode64 #Ceph OSD xfs Mount选项
filestore xattr use omap = true                         #默认false #为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用
filestore min sync interval = 10                        #默认0.1 #从日志到数据盘最小同步间隔(seconds)
filestore max sync interval = 15                        #默认5 #从日志到数据盘最大同步间隔(seconds)
filestore queue max ops = 25000                         #默认500 #在阻塞新operation加入队列之前，数据盘最大接受的操作数
filestore queue max bytes = 1048576000      			#默认100 #数据盘一次操作最大字节数(bytes)
filestore queue committing max ops = 50000 				#默认500 #数据盘能够commit的操作数
filestore queue committing max bytes = 10485760000 		#默认100 #数据盘能够commit的最大字节数(bytes)10G
filestore split multiple = 8 							#默认值2 #前一个子目录分裂成子目录中的文件的最大数量
filestore merge threshold = 40 							#默认值10 #前一个子类目录中的文件合并到父类的最小数量
filestore fd cache size = 1024 							#默认值128 #对象文件句柄缓存大小
journal max write bytes = 1073714824 					#默认值1048560 #journal一次性写入的最大字节数(bytes)
journal max write entries = 10000 						#默认值100 #journal一次性写入的最大记录数
journal queue max ops = 50000  							#默认值50 #journal一次性最大在队列中的操作数
journal queue max bytes = 10485760000 					#默认值33554432 #journal一次性最大在队列中的字节数(bytes)10G
osd max write size = 512 								#默认值90 #OSD一次可写入的最大值(MB)
osd client message size cap = 2147483648 				#默认值100 	#客户端允许在内存中的最大数据(bytes)2G
osd deep scrub stride = 131072 							#默认值524288 #在Deep Scrub时候允许读取的字节数(bytes)
osd op threads = 16 									#默认值2 #并发文件系统操作数
osd disk threads = 4 									#默认值1 #OSD密集型操作例如恢复和Scrubbing时的线程
osd map cache size = 1024 								#默认值500 #保留OSD Map的缓存(MB)
osd map cache bl size = 128 							#默认值50 #OSD进程在内存中的OSD Map缓存(MB)
osd recovery op priority = 2 							#默认值10 #恢复操作优先级，取值1-63，值越高占用资源越高
osd recovery max active = 10 							#默认值15 #同一时间内活跃的恢复请求数 
osd max backfills = 4  									#默认值10 #一个OSD允许的最大backfills数
osd min pg log entries = 30000 							#默认值3000 #修建PGLog是保留的最大PGLog数
osd max pg log entries = 100000 						#默认值10000 #修建PGLog是保留的最大PGLog数
osd mon heartbeat interval = 40 						#默认值30 #OSD ping一个monitor的时间间隔（默认30s）
ms dispatch throttle bytes = 1048576000 				#默认值 104857600 #等待派遣的最大消息数
objecter inflight ops = 819200 							#默认值1024 #客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限
osd op log threshold = 50 								#默认值5 #一次显示多少操作的log
osd crush chooseleaf type = 0 							#默认值为1 #CRUSH规则用到chooseleaf时的bucket的类型
##############################################################
[client]
rbd cache = true 										#默认值 true #RBD缓存
rbd cache size = 335544320 								#默认值33554432 #RBD缓存大小(bytes)
rbd cache max dirty = 134217728 						#默认值25165824 #缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through
rbd cache max dirty age = 30 							#默认值1 #在被刷新到存储盘前dirty数据存在缓存的时间(seconds)
rbd cache writethrough until flush = false 				#默认值true #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写
             											#设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。
rbd cache max dirty object = 2 							#默认值0 #最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分
      													#每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能pid_max
rbd cache target dirty = 235544320 						#默认值16777216 #开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty
</code></pre> 
<h3><a id="7__PG_Number_123"></a>7、 PG Number</h3> 
<p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。<br> Total PGs = （(Total_number_of_OSD * 100) / max_replication_count ）/ pool_count</p> 
<p>例：<br> 有100个osd，2副本，5个pool<br> Total PGs =100*100/2=5000<br> 每个pool 的PG=5000/5=1000，那么创建pool的时候就指定pg为1024<br> ceph osd pool create pool_name 1024</p> 
<h3><a id="8__crush_map_133"></a>8、 修改crush map</h3> 
<p>Crush map可以设置不同的osd对应到不同的pool，也可以修改每个osd的weight<br> 配置可参考：<a href="http://linuxnote.blog.51cto.com/9876511/1790758" rel="nofollow">http://linuxnote.blog.51cto.com/9876511/1790758</a></p> 
<h3><a id="9___137"></a>9、 其他因素</h3> 
<p>ceph osd perf<br> 通过osd perf可以提供磁盘latency的状况，如果延时过长，应该剔除osd</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/603cb0e70bf8b999fed8c612e02d937d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">异常解决：java.lang.IllegalStateException: Failed to introspect Class</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7d3bd74fd2a9674b4d0a9369e79fd4a5/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">34 文件地理数据库（GDB）变文件夹了怎么办</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>