<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【pytorch笔记】（五）自定义损失函数、学习率衰减、模型微调 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【pytorch笔记】（五）自定义损失函数、学习率衰减、模型微调" />
<meta property="og:description" content="本文目录： 1. 自定义损失函数2. 动态调整学习率3. 模型微调-torchvision3.1 使用已有模型3.2 训练特定层 1. 自定义损失函数 虽然pytorch提供了许多常用的损失函数，但很多时候我们需要自定义一些新的损失函数来满足某些特定任务的需求，这时就需要我们自己写损失函数了。
pytorch 自定义损失函数主要有两种方式：
自定义函数：这种方法比较简单，写个函数即可。但需要注意的是输入输出的变量都得是torch.tensor类型的数据，且尽量采用torch自带的函数计算loss，否则可能会无法求导。 import torch def my_loss(x, y): loss = torch.mean((x - y)**2) return loss 自定义类：这种方式虽然需要提前实例化类，但更为常用，显得专业。自定义类继承于nn.Module，相当于定义了一个网络层，可以维护状态和存储参数信息。自定义类有两个关键要点，一是定义初始化方法（init），二是定义前向计算方式（forward）。
以下给出一个我自己写的分位数回归损失函数——pinball loss： import torch.nn as nn import torch class PinballLoss(nn.Module): &#34;&#34;&#34; Pinball loss at all confidence levels. &#34;&#34;&#34; def __init__(self, quantiles): &#34;&#34;&#34; Initialize Parameters ---------- quantiles : the list of quantiels, [1, n], list or ndarray. For example, quantiles = [0.025 0.05 0.075 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/78eac392eabc3e567951824fcfd6f442/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-22T16:06:29+08:00" />
<meta property="article:modified_time" content="2022-06-22T16:06:29+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【pytorch笔记】（五）自定义损失函数、学习率衰减、模型微调</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>本文目录：</h4> 
 <ul><li><a href="#1__3" rel="nofollow">1. 自定义损失函数</a></li><li><a href="#2__70" rel="nofollow">2. 动态调整学习率</a></li><li><a href="#3_torchvision_140" rel="nofollow">3. 模型微调-torchvision</a></li><li><ul><li><a href="#31__155" rel="nofollow">3.1 使用已有模型</a></li><li><a href="#32__170" rel="nofollow">3.2 训练特定层</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="1__3"></a>1. 自定义损失函数</h2> 
<p>虽然pytorch提供了许多常用的损失函数，但很多时候我们需要自定义一些新的损失函数来满足某些特定任务的需求，这时就需要我们自己写损失函数了。</p> 
<p>pytorch 自定义损失函数主要有两种方式：</p> 
<ul><li><strong>自定义函数</strong>：这种方法比较简单，写个函数即可。但需要注意的是输入输出的变量都得是torch.tensor类型的数据，且尽量采用torch自带的函数计算loss，否则可能会无法求导。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token keyword">def</span> <span class="token function">my_loss</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>x <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss
</code></pre> 
<ul><li><strong>自定义类</strong>：这种方式虽然需要提前实例化类，但更为常用，显得专业。自定义类继承于<code>nn.Module</code>，相当于定义了一个网络层，可以维护状态和存储参数信息。自定义类有两个关键要点，一是定义<strong>初始化方法（<strong>init</strong>）</strong>，二是定义<strong>前向计算方式（forward）</strong>。<br> 以下给出一个我自己写的分位数回归损失函数——pinball loss：</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">PinballLoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Pinball loss at all confidence levels.
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> quantiles<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Initialize

        Parameters
        ----------
        quantiles : the list of quantiels, [1, n], list or ndarray. 
        For example, quantiles = [0.025 0.05  0.075 0.925 0.95  0.975].

        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PinballLoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>quantiles <span class="token operator">=</span> quantiles

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> preds<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Compute the pinball loss

        Parameters
        ----------
        preds : the predictions at all quantiles, torch.tensor, [n, len(quantiles)].
        target : the ground truth of y, torch.tensor, [n, 1].

        Returns
        -------
        loss : the value of the pinball loss.

        """</span>

        <span class="token keyword">assert</span> preds<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> target<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> target<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            target <span class="token operator">=</span> target<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> q <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>quantiles<span class="token punctuation">)</span><span class="token punctuation">:</span>
            errors <span class="token operator">=</span> target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> preds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">(</span>q <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> errors<span class="token punctuation">,</span> q <span class="token operator">*</span> errors<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>losses<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> loss
</code></pre> 
<h2><a id="2__70"></a>2. 动态调整学习率</h2> 
<p>学习率（learning rate）本质上就是优化方法里的<strong>搜索步长</strong>，不同优化方法之间最大的差异在于他们的<strong>搜索方向和搜索步长不同</strong>。深度学习通过不断更新优化参数来减小loss，该过程的本质其实也是最优化里<strong>寻找全局最优解</strong>（最小的loss）的过程，只是实际情况中经常无法寻找到全局最优解或成本太高，往往采用表现还行的局部最优解。</p> 
<p>那么，为什么要动态调整学习率呢？来看下图</p> 
<p><img src="https://images2.imgbox.com/bf/bf/z2RunuUb_o.png" alt="图源见水印"><br> （图源见水印）</p> 
<p>我们的目标是到达这条曲线的“谷底”（局部最优解），如果学习率太大，会经常跨过这个谷底，难以收敛到局部最优解，导致模型表现一般；如果学习率太小，虽然最终也能到达谷底，但是这个过程耗时漫长，等得花都谢了。这说明<strong>太大或太小的学习率都不合适</strong>，但这个合适的学习率到底多大合适呢？我们无从得知，只能凭经验判断。<strong>神经网络参数的初始权重不同，且每一次优化过程的初始起点、搜索方向都可能不同，这就导致每一次跑出来的结果都有所不同</strong>，论文里的完美结果经常难以复现（指定随机数种子可以改善），这也是深度学习有时候像玄学、炼丹的原因。尽管如此，在实际工作中我们只要不断调参得到一个较为满意的结果，得益于神经网络的强大拟合能力，其结果往往就已经超越了许多传统方法。</p> 
<p>咳咳，扯远了。在实际的模型训练中，我们一般采用<strong>动态调整学习率</strong>，也称<strong>学习率衰减</strong>的策略来加速算法收敛。其思想为：<strong>先取一个较大的学习率加速收敛，然后随着遍历次数（epoch）的增加逐步减小学习率，防止跳出局部最优解。</strong></p> 
<p>pytorch官方在<code>torch.optim.lr_schduler</code>里提供了许多学习率衰减的方法，这里列出常用的几种方式：</p> 
<table><thead><tr><th align="left">Scheduler</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">StepLR</td><td align="left">等间隔调整学习率，如每隔10个epoch衰减一次学习率</td></tr><tr><td align="left">MultiStepLR</td><td align="left">多间隔调整学习率，如epoch分别在10，30，70时衰减一次学习率</td></tr><tr><td align="left">ExponentialLR</td><td align="left">指数衰减学习率，每训练一个epoch调整一次学习率，即<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
        
         
          
          
            l 
           
           
           
             r 
            
           
             ∗ 
            
           
          
            = 
           
          
            l 
           
          
            r 
           
          
            × 
           
          
            g 
           
          
            a 
           
          
            m 
           
          
            m 
           
           
           
             a 
            
            
            
              e 
             
            
              p 
             
            
              o 
             
            
              c 
             
            
              h 
             
            
           
          
         
           lr^*=lr \times gamma^{epoch} 
          
         
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.688696em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.01968em;">l</span><span class="mord mathdefault" style="margin-right: 0.02778em;">r</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1.04355em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">g</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord mathdefault">m</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span></td></tr><tr><td align="left">CosineAnnealingLR</td><td align="left">余弦退火函数调整学习率，随着epoch增加，学习率呈余弦函数状衰减</td></tr></tbody></table> 
<p>更多学习率衰减方法参考<a href="https://pytorch.org/docs/stable/optim.html?highlight=scheduler" rel="nofollow">pytorch官网</a>。</p> 
<p>以下给出一个学习率衰减的模板：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">import</span> SGD
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> ExponentialLR

model <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#可更换为任意模型</span>
optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment">#参数优化器</span>
scheduler <span class="token operator">=</span> ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>  <span class="token comment">#学习率衰减控制器</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> <span class="token builtin">input</span><span class="token punctuation">,</span> target <span class="token keyword">in</span> dataset<span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>以上是pytorch官网给出的方法。当然，如果官方给的方法无法满足我们的需求，那我们可以自己写一个函数来调整学习率。</p> 
<p>假设我们现在需要学习率每隔30轮下降为原来的1/10，假设官网没有符合我们需求的schduler（实际上StepLR符合），那我们可以自己写一个函数：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">adjust_learning_rate</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    lr <span class="token operator">=</span> args<span class="token punctuation">.</span>lr <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">0.1</span> <span class="token operator">**</span> <span class="token punctuation">(</span>epoch <span class="token operator">//</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
        param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr
</code></pre> 
<p>然后我们就可以在训练过程中调用我们自定义的函数来实现学习率的动态变化。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">adjust_learning_rate</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> args<span class="token punctuation">.</span>lr<span class="token punctuation">,</span>momentum <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    adjust_learning_rate<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span>epoch<span class="token punctuation">)</span>
</code></pre> 
<h2><a id="3_torchvision_140"></a>3. 模型微调-torchvision</h2> 
<p>深度学习和计算机算力的不断发展催生出了许多包含巨量参数的模型，如GPT-3等，这些大模型花费了创造它们的公司大量的算力、时间成本，我们普通人想要训练出这样的大模型困难重重：</p> 
<ul><li>缺少数据。这些大模型通常是在大量数据的基础上训练出来的，动不动就几百G几百T的，普通电脑难以存下这么多数据。</li><li>缺少算力。大模型通常是用服务器或数据中心的几百张NVIDIA显卡上跑出来的，光这硬件成本就得至少几百上千万，电费也不是一笔小数目（怎么有点像挖矿呢）。</li><li>缺少技术（大佬请无视）。综上，想要训练出大模型，需要大量数据和显卡，要驱动模型在大数据和大量显卡上正常训练需要一点技术，普通人如果对这方面的知识接触少的话一时半会儿很难跑得起来模型。</li></ul> 
<p>那我们的普通人就没法用大模型了吗？答案是否定的，有许多已经训练好的模型已经开源分享出来了，比如<code>torchvision.models</code>里就有许多预训练好的模型，我们只需要下载下来就能用了。但这存在一个问题，假设我们要做的识别猫的种类，这些预训练好的模型通常是针对特定任务的，比如图像识别，他们采用的数据集里面包含猫的照片可能就只有一小部分，无法满足我们的需求。要解决这个问题，我们可以采用<strong>迁移学习（transfer learning）</strong>。</p> 
<p><strong>迁移学习可以将源数据集上学到的知识迁移到目标数据集上</strong>。例如，虽然ImageNet数据集的图像里包含的猫的图片不是很多，但在该数据集上训练的模型可以抽取比较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别猫也可能同样有效。</p> 
<p>迁移学习的一大应用场景是<strong>模型微调</strong>（finetune）。简单来说，就是我们先找到一个同类的别人训练好的模型，把别人现成的训练好了的模型拿过来，换成自己的数据，通过训练调整一下参数。 在PyTorch中提供了许多预训练好的网络模型（VGG，ResNet系列，mobilenet系列…），这些模型都是PyTorch官方在相应的大型数据集训练好的。</p> 
<p>模型微调的主要流程是：<strong>复制预训练好的模型，微调除输出层外的其它层参数，修改输出层参数并随机初始化，在目标数据集上上训练模型。</strong></p> 
<h3><a id="31__155"></a>3.1 使用已有模型</h3> 
<p>以<code>torchvision.models</code>为例，里面列出了许多用于图像分类任务的模型，我们下载导入一下即可使用。</p> 
<p>但要注意的是，Windows环境下 <code>torchvision</code> 下载的预训练模型保存在<code>C:\Users\&lt;username&gt;\.cache</code>，这一点对于C盘空间较小的用户来说很不友好，也不符合使用习惯，如果<code>.cahe</code>里的文件不小心被清理掉了还得重新下载一遍。为此，我们需要修改一下预训练模型的保存路径，以下提供一种修改方式：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models
<span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'TORCH_HOME'</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token string">'E:\pytorch\Data'</span>  <span class="token comment">#修改模型保存路径，下载好后重新加载模型就会在这个目录下加载了</span>

<span class="token comment"># pretrained = True表示下载使用预训练得到的权重，False表示不使用预训练得到的权重</span>
resnet34 <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet34<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> 
</code></pre> 
<h3><a id="32__170"></a>3.2 训练特定层</h3> 
<p>在默认情况下，参数的属性<code>.requires_grad = True</code>，如果我们从头开始训练或微调不需要注意这里。但如果我们正在提取特征并且只想为新初始化的层计算梯度，其他参数不进行改变。那我们就需要通过设置<code>requires_grad = False</code>来冻结部分层。在PyTorch官方中提供了这样一个例程。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">set_parameter_requires_grad</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> feature_extracting<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> feature_extracting<span class="token punctuation">:</span>
        <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre> 
<p>在下面我们仍旧使用<code>resnet18</code>为例的将1000类改为4类，但是仅改变最后一层的模型参数，不改变特征提取的模型参数；注意我们先冻结模型参数的梯度，再对模型输出部分的全连接层进行修改，这样修改后的全连接层的参数就是可计算梯度的。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models

<span class="token comment"># 冻结参数的梯度</span>
feature_extract <span class="token operator">=</span> <span class="token boolean">True</span>
model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet18<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
set_parameter_requires_grad<span class="token punctuation">(</span>model<span class="token punctuation">,</span> feature_extract<span class="token punctuation">)</span>

<span class="token comment"># 修改模型</span>
num_ftrs <span class="token operator">=</span> model<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_features
model<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span>num_ftrs<span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>之后在训练过程中，model仍会进行梯度回传，但是参数更新则只会发生在fc层。通过设定参数的requires_grad属性，我们完成了指定训练模型的特定层的目标，这对实现模型微调非常重要。</p> 
<blockquote> 
 <p>参考资料：<br> [1] <a href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E5%85%AD%E7%AB%A0/6.1%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html#" rel="nofollow">Datawhale_深入浅出pytorch</a><br> [2] https://blog.csdn.net/qq_27825451/article/details/95165265<br> [3] <a href="https://pytorch.org/docs/stable/optim.html?highlight=scheduler" rel="nofollow">pytorch官方文档</a><br> [4] https://www.jianshu.com/p/26a7dbc15246<br> [5] https://blog.csdn.net/yanxiangtianji/article/details/112256618</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/dec7cec001688df5b12a5e079f7a1d27/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">AcWing：第56场周赛</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ed0d3674e530973630b0aea9e5aab886/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java初学之标识符</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>