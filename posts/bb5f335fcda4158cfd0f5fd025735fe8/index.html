<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>大数据技术——spark集群搭建 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="大数据技术——spark集群搭建" />
<meta property="og:description" content="目录
spark概述
spark集群搭建
1.Spark安装
2.环境变量配置
3.Spark集群配置
4.启动Spark集群
存在问题及解决方案
请参考以下文章 spark概述 Spark是一个开源的大数据处理框架，它可以在分布式计算集群上进行高效的数据处理和分析。Spark的特点是速度快、易用性高、支持多种编程语言和数据源。Spark的核心是基于内存的计算模型，可以在内存中快速地处理大规模数据。Spark支持多种数据处理方式，包括批处理、流处理、机器学习和图计算等。Spark的生态系统非常丰富，包括Spark SQL、Spark Streaming、MLlit
GraphX等组件，可以满足不同场景下的数据处理需求。
spark集群搭建 在部署spark集群时，我们知道有三种：
一种是本地模式，一种是Standalone 集群，还有一种是云端。
1.Spark安装 首先我们需要在master节点上进行Spark的安装。
其中1台机器（节点）作为Master节点，主机名为hadoop1,另外两台机器（节点）作为Slave节点（即作为Worker节点），主机名分别为hadoop2和hadoop3。
在Master节点机器上，访问Spark官方下载地址Downloads | Apache Spark，按照如下图下载:
我们选择2.1.0的版本，也可以选择其他的版本，但是需要注意的是，如果你选择的Spark版本过高，可能导致无法与你的hadoop版本适配。
完成下载后，进行如下的命令行操作，和hadoop安装时十分类似。
$ tar -zxvf scala.2.11.8.tgz #解压到当前路径
$ cd /usr/local
$ sudo mv ./spark-2.1.0-bin-without-hadoop/ ./spark #重命名
spark安装与上述Scala步骤一致
2.环境变量配置 同样在master机器上，打开bashrc文件进行环境变量配置。
$ vim ~/.bashrc
在文件中添加如下内容
export PATH=$PATH:/usr/local/scala/bin并使其生效。
保存文件并退出vim编辑器，执行如下命令让改环境变量生效：
$ source ~/.bashrc
设置好后，可以使用Scala命令来检验一下是否设置正确：
$ scala
输入scala命令以后，屏幕上会显示Scala和java版本信息，并进入“scala&gt;”提示符状态，这时就可以开始使用Scala解释器了，可以输入scala语句来调试scala代码。
3.Spark集群配置 进入到/usr/local/spark的conf路径下，进行以下文件的配置。
a)配置slaves文件
但是由于其开始并没有这个文件，而只有slaves.template文件，所以我们需要先拷贝重命名一下。
$ cd /usr/local/spark/conf/
$ cp ./slaves.template ./slaves" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/bb5f335fcda4158cfd0f5fd025735fe8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-13T11:15:00+08:00" />
<meta property="article:modified_time" content="2023-04-13T11:15:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据技术——spark集群搭建</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:0px;"></p> 
<p id="spark%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;"><a href="#spark%E6%A6%82%E8%BF%B0" rel="nofollow">spark概述</a></p> 
<p id="spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-toc" style="margin-left:0px;"><a href="#spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA" rel="nofollow">spark集群搭建</a></p> 
<p id="1.Spark%E5%AE%89%E8%A3%85-toc" style="margin-left:40px;"><a href="#1.Spark%E5%AE%89%E8%A3%85" rel="nofollow">1.Spark安装</a></p> 
<p id="2.%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE-toc" style="margin-left:80px;"><a href="#2.%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE" rel="nofollow">2.环境变量配置</a></p> 
<p id="3.Spark%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE-toc" style="margin-left:80px;"><a href="#3.Spark%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE" rel="nofollow">3.Spark集群配置</a></p> 
<p id="4.%E5%90%AF%E5%8A%A8Spark%E9%9B%86%E7%BE%A4-toc" style="margin-left:80px;"><a href="#4.%E5%90%AF%E5%8A%A8Spark%E9%9B%86%E7%BE%A4" rel="nofollow">4.启动Spark集群</a></p> 
<p id="%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-toc" style="margin-left:0px;"><a href="#%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" rel="nofollow">存在问题及解决方案</a></p> 
<p id="%E8%AF%B7%E5%8F%82%E8%80%83%E4%BB%A5%E4%B8%8B%E6%96%87%E7%AB%A0%C2%A0-toc" style="margin-left:0px;"><a href="#%E8%AF%B7%E5%8F%82%E8%80%83%E4%BB%A5%E4%B8%8B%E6%96%87%E7%AB%A0%C2%A0" rel="nofollow">请参考以下文章 </a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="spark%E6%A6%82%E8%BF%B0">spark概述</h2> 
<p>Spark是一个开源的大数据处理框架，它可以在分布式计算集群上进行高效的数据处理和分析。Spark的特点是速度快、易用性高、支持多种编程语言和数据源。Spark的核心是基于内存的计算模型，可以在内存中快速地处理大规模数据。Spark支持多种数据处理方式，包括批处理、流处理、机器学习和图计算等。Spark的生态系统非常丰富，包括Spark SQL、Spark Streaming、MLlit<br> GraphX等组件，可以满足不同场景下的数据处理需求。</p> 
<h2 id="spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA">spark集群搭建</h2> 
<p style="margin-left:0;text-align:justify;">在部署<a href="https://so.csdn.net/so/search?q=spark&amp;spm=1001.2101.3001.7020" title="spark">spark</a>集群时，我们知道有三种：</p> 
<p style="margin-left:0;text-align:justify;">一种是本地模式，一种是Standalone 集群，还有一种是云端。</p> 
<h3 id="1.Spark%E5%AE%89%E8%A3%85" style="margin-left:0px;text-align:justify;"><a name="_Toc13282"></a><a name="_Toc132227537">1.Spark</a>安装</h3> 
<p style="margin-left:0;text-align:justify;">首先我们需要在master节点上进行Spark的安装。</p> 
<p style="margin-left:0;text-align:justify;">其中1台机器（节点）作为Master节点，主机名为<span style="color:#FF0000;">hadoop1</span>,另外两台机器（节点）作为Slave节点（即作为Worker节点），主机名分别为<span style="color:#FF0000;">hadoop2</span>和<span style="color:#FF0000;">hadoop3</span>。</p> 
<p style="margin-left:0;text-align:justify;">在Master节点机器上，访问Spark官方下载地址<a href="https://spark.apache.org/downloads.html" rel="nofollow" title="Downloads | Apache Spark">Downloads | Apache Spark</a>，按照如下图下载:</p> 
<p><img alt="" height="556" src="https://images2.imgbox.com/da/6d/eluA2qXF_o.png" width="702"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">我们选择2.1.0的版本，也可以选择其他的版本，但是需要注意的是，如果你选择的Spark版本过高，可能导致无法与你的hadoop版本适配。</p> 
<p style="margin-left:0;text-align:justify;">完成下载后，进行如下的命令行操作，和hadoop安装时十分类似。</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ tar -zxvf scala.2.11.8.tgz  #</span><span style="color:#FF0000;">解压到当前路径</span></p> 
<p><img alt="" height="492" src="https://images2.imgbox.com/e7/9c/hTcPDpMA_o.png" width="681"> </p> 
<p> <img alt="" height="154" src="https://images2.imgbox.com/b4/8a/taxxLvaW_o.png" width="669"></p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cd /usr/local</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sudo mv ./spark-2.1.0-bin-without-hadoop/ ./spark #</span><span style="color:#FF0000;">重命名</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="68" src="https://images2.imgbox.com/6e/2e/9iJVOfhe_o.png" width="685"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">spark</span><span style="color:#FF0000;">安装与上述Scala步骤一致</span></p> 
<h4 id="2.%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE" style="margin-left:0;text-align:justify;"><a name="_Toc132227538">2.</a>环境变量配置</h4> 
<p style="margin-left:0;text-align:justify;">同样在master机器上，打开bashrc文件进行环境变量配置。</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ vim ~/.bashrc</span></p> 
<p style="margin-left:0;text-align:justify;">在文件中添加如下内容</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export PATH=$PATH:/usr/local/scala/bin</span>并使其生效。</p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="72" src="https://images2.imgbox.com/3a/92/3pN7xk4P_o.png" width="668"></p> 
<p></p> 
<p style="margin-left:0;text-align:justify;">保存文件并退出vim编辑器，执行如下命令让改环境变量生效：</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ source ~/.bashrc</span></p> 
<p style="margin-left:0;text-align:justify;">设置好后，可以使用Scala命令来检验一下是否设置正确：</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ scala</span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="168" src="https://images2.imgbox.com/5d/8a/jyzgMf4T_o.png" width="680"></p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">输入scala命令以后，屏幕上会显示Scala和java版本信息，并进入“scala&gt;”提示符状态，这时就可以开始使用Scala解释器了，可以输入scala语句来调试scala代码。</p> 
<h4 id="3.Spark%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE" style="margin-left:0;text-align:justify;"><a name="_Toc132227539">3.Spark</a>集群配置</h4> 
<p style="margin-left:0;text-align:justify;">进入到/usr/local/spark的conf路径下，进行以下文件的配置。</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">a)</span><span style="background-color:#FFFF00;">配置slaves文件</span></p> 
<p style="margin-left:0;text-align:justify;">但是由于其开始并没有这个文件，而只有slaves.template文件，所以我们需要先拷贝重命名一下。</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cd /usr/local/spark/conf/</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cp ./slaves.template ./slaves</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="73" src="https://images2.imgbox.com/cd/6b/gh02Ogid_o.png" width="681"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;">然后打开这个slaves文件，并将默认的localhost替换相应的两个slave结点：</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#00FFFF;"><span style="color:#FF0000;">hadoop2</span></span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#00FFFF;"><span style="color:#FF0000;">hadoop3</span></span></p> 
<p style="margin-left:0;text-align:justify;">分别在三台虚拟机上修改slaves文件：</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop1</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="238" src="https://images2.imgbox.com/e4/43/nA90UNiX_o.png" width="668"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop2</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="351" src="https://images2.imgbox.com/0d/c3/GnEPBzxd_o.png" width="670"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop3</span><span style="background-color:#FFFF00;">：</span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="282" src="https://images2.imgbox.com/c3/7e/EPhfhBpF_o.png" width="694"></p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">b)<span style="background-color:#FFFF00;">配置spark-env.sh文件</span></p> 
<p style="margin-left:0;text-align:justify;">同样的，我们需要先将template文件拷贝重命名。</p> 
<p style="margin-left:0;text-align:justify;">将 spark-env.sh.template 拷贝到 spark-env.sh</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cp ./spark-env.sh.template ./spark-env.sh</span></p> 
<p style="margin-left:0;text-align:justify;">然后在文件中添加如下内容</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPAEK_MASTER_HOST=hadoop1</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_MASTER_PORT=7077</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_MASTER_WEBUI_PORT=8080</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_WORKER_MEMORY=1g</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_WORKER_CORES=1</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export SPARK_WORKER_INSTANCES=1</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span>分别在三台虚拟机上修改spark-env.sh文件：</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop1</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="273" src="https://images2.imgbox.com/bc/f6/3RgNY7bA_o.png" width="644"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop2</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="251" src="https://images2.imgbox.com/74/da/IDQWuc2v_o.png" width="669"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop3</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="271" src="https://images2.imgbox.com/2e/60/GgqIyRFb_o.png" width="633"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">c)<span style="background-color:#FFFF00;">集群规划</span></p> 
<table border="1" cellspacing="0"><tbody><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;">节点</p> </td><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;">spark节点</p> </td><td style="border-color:#000000;vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;">hadoop节点</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">hadoop1</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;">master</p> <p style="margin-left:0;text-align:justify;">worker</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;"><span style="background-color:#00FFFF;">datanode </span></p> <p style="margin-left:0;text-align:justify;"><span style="background-color:#00FFFF;">namenode secondarynamenode</span>(<span style="color:#FF0000;">hadoop</span>) <span style="background-color:#FFFF00;">resourcemanager nodemanager</span>(<span style="color:#FF0000;">yarn</span>)</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">hadoop2</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;">worker</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;">datanode nodemanager</p> </td></tr><tr><td style="border-color:#000000;vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">hadoop3</span></p> </td><td style="vertical-align:top;width:138.25pt;"> <p style="margin-left:0;text-align:justify;">worker</p> </td><td style="vertical-align:top;width:138.3pt;"> <p style="margin-left:0;text-align:justify;">datanode nodemanager</p> </td></tr></tbody></table> 
<p style="margin-left:0;text-align:justify;"></p> 
<h4 id="4.%E5%90%AF%E5%8A%A8Spark%E9%9B%86%E7%BE%A4" style="margin-left:0;text-align:justify;"><a name="_Toc132227540">4.</a>启动Spark集群</h4> 
<p style="margin-left:0;text-align:justify;">因为我们的Spark是基于hadoop来运行的，因此我们首先需要将hadoop启动起来。</p> 
<p>启动Hadoop集群</p> 
<p><img alt="" height="355" src="https://images2.imgbox.com/4c/cb/hbQnVvTA_o.png" width="644"></p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">在master机器上运行如下指令启动hadoop集群</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cd /usr/local/hadoop/</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/start-all.sh</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop1</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="246" src="https://images2.imgbox.com/64/08/fSdFy7uT_o.png" width="657"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop2</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="159" src="https://images2.imgbox.com/cb/46/y39wvIWN_o.png" width="666"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop3</span><span style="background-color:#FFFF00;">：</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="164" src="https://images2.imgbox.com/30/28/AolufTsd_o.png" width="665"> </p> 
<p> </p> 
<p>启动spark集群</p> 
<p style="margin-left:0;text-align:justify;">然后我们再再master机器上启动Spark的master进程。</p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cd /usr/local/spark/</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/start-master.sh</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">使用jps命令查看master机器上的进程情况，结果如下。</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop1</span><span style="background-color:#FFFF00;">：</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="271" src="https://images2.imgbox.com/84/8e/fZ3Jk5Oy_o.png" width="652"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">我们发现，除了hadoop的相关进程之外，还多了一个Master进程,证明master节点已经成功启动。</p> 
<p style="margin-left:0;text-align:justify;">然后我们同样在master机器上再启动worker进程。</p> 
<p style="margin-left:0;text-align:justify;">用以下命令启动所有的slave节点</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/start-slaves.sh</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop1</span><span style="background-color:#FFFF00;">：</span></p> 
<p style="margin-left:0;text-align:justify;"><img alt="" height="291" src="https://images2.imgbox.com/55/22/oKAMozc7_o.png" width="665"></p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">分别在<span style="color:#FF0000;">hadoop2</span>、<span style="color:#FF0000;">hadoop3</span>节点上运行jps命令，可以看到多了个Worker进程</p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop2</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="167" src="https://images2.imgbox.com/57/98/0fPSHi2a_o.png" width="679"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="background-color:#FFFF00;">hadoop3</span><span style="background-color:#FFFF00;">：</span></p> 
<p><img alt="" height="171" src="https://images2.imgbox.com/be/dd/r9WbKqod_o.png" width="690"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"> 我们发现，同样的除了hadoop的相关进程，多出来一个Worker进程，证明worker节点也已经成功启动。</p> 
<p style="margin-left:0;text-align:justify;">在浏览器上查看Spark独立集群管理器的集群信息</p> 
<p style="margin-left:0;text-align:justify;">在master主机上打开浏览器，</p> 
<p style="margin-left:0;text-align:justify;">分别访问<a href="http://192.168.43.33:50070" rel="nofollow" title="http://192.168.43.33:50070">http://192.168.43.33:50070</a><span style="color:#0563c1;"><u><strong>,</strong></u></span>如下图：</p> 
<p> <img alt="" height="1029" src="https://images2.imgbox.com/3c/d6/rxs9g7mv_o.png" width="1200"></p> 
<p> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;">分别访问<a href="http://192.168.43.33:8080" rel="nofollow" title="http://192.168.43.33:8080">http://192.168.43.33:8080</a>,如下图：</p> 
<p><img alt="" height="364" src="https://images2.imgbox.com/eb/05/XeoAnXlh_o.png" width="679"> </p> 
<p> </p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ spark-shell  </span>#进入shell</p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="425" src="https://images2.imgbox.com/3b/5f/0DIjjCK3_o.png" width="689"> </p> 
<p> </p> 
<p>关闭spark集群</p> 
<p style="margin-left:0;text-align:justify;">①关闭master节点</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/stop-master.sh</span></p> 
<p style="margin-left:0;text-align:justify;">②关闭worker节点</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/stop-slaves.sh</span></p> 
<p style="margin-left:0;text-align:justify;">③关闭Hadoop集群</p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ cd /usr/local/hadoop/</span></p> 
<p style="margin-left:0;text-align:justify;"><span style="color:#FF0000;">$ sbin/stop-all.sh</span></p> 
<p style="margin-left:0;text-align:justify;"></p> 
<p><img alt="" height="487" src="https://images2.imgbox.com/b2/50/ERJmZWyD_o.png" width="701"> </p> 
<p> </p> 
<h2 id="%E5%AD%98%E5%9C%A8%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" style="text-align:justify;"><a name="_Toc132227541"> 存在问题及解决方案</a></h2> 
<h2 id="%E8%AF%B7%E5%8F%82%E8%80%83%E4%BB%A5%E4%B8%8B%E6%96%87%E7%AB%A0%C2%A0" style="text-align:justify;">请参考以下文章 </h2> 
<p><a href="https://blog.csdn.net/qq_53142796/article/details/129991900?spm=1001.2014.3001.5501" title="大数据技术——搭建spark集群出现的问题_肉肉肉肉肉肉~丸子的博客-CSDN博客">大数据技术——搭建spark集群出现的问题_肉肉肉肉肉肉~丸子的博客-CSDN博客</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1ff6f6db8e19c350b93fbfb8fbff24df/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">redis命令</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0e2e3628c500edb3e3f245c5692aa87f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SqlServer 添加指定用户限制访问表或试图的权限</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>