<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LLM之LangChain（四）| 介绍LangChain 0.1在可观察性、可组合性、流媒体、工具、RAG和代理方面的改进 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LLM之LangChain（四）| 介绍LangChain 0.1在可观察性、可组合性、流媒体、工具、RAG和代理方面的改进" />
<meta property="og:description" content="LangChain是大模型应用中非常火的一个框架，最近发布了LangChain 0.1版本，在以下方面带来显著改进：
ObservabilityComposabilityStreamingTool UsageRAGAgents 在本文中，我们将使用一些代码示例来分别介绍一下这些改进的使用方法。
一、LangChain 0.1概述 在0.1发布之前，LangChain团队与100名开发人员进行了深入交流，从而为LangChain的升级提供了宝贵建议。
为了增强基础功能，他们将框架分为3个包：
langchain-core：构成主干，包括主要的抽象、接口和核心功能，包括langchain表达式语言（LCEL）。
langchain-community：旨在更好地管理依赖关系，该包包括第三方集成，增强langchain的健壮性和可扩展性，并改善整体开发者体验。
langchain：专注于更高级别的应用程序，比如Chain、Agent和检索算法的特定用例。
LangChain 0.1还引入了清晰的版本控制策略，以保持向后兼容性：
小版本更新（第二位数字）表示将提供一个突破性变更的公共API；补丁版本（第三位数字）将表示错误修复或新增功能。 二、可观测性（Observability） 为了深入了解LangChain应用程序的内部工作，可以使用set_verbose和set_debug来了解引擎执行的细节。
from langchain.globals import set_debug​set_verbose(False)set_debug(True)​prompt = hub.pull(&#34;hwchase17/openai-functions-agent&#34;)llm = ChatOpenAI(model=&#34;gpt-3.5-turbo&#34;, temperature=0)search = TavilySearchResults()tools = [search]agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools)​agent_executor.invoke({&#34;input&#34;: &#34;what is the weather in sf?&#34;}) [chain/start] [1:chain:AgentExecutor] Entering Chain run with input:{ &#34;input&#34;: &#34;what is the weather in sf?&#34;}[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence] Entering Chain run with input:{ &#34;input&#34;: &#34;what is the weather in sf?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0761fc08294ec5d22a8b59f7d9e76b3b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-18T15:33:30+08:00" />
<meta property="article:modified_time" content="2024-01-18T15:33:30+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LLM之LangChain（四）| 介绍LangChain 0.1在可观察性、可组合性、流媒体、工具、RAG和代理方面的改进</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>       LangChain是大模型应用中非常火的一个框架，最近发布了LangChain 0.1版本，在以下方面带来显著改进：</p> 
<ul><li>Observability</li><li>Composability</li><li>Streaming</li><li>Tool Usage</li><li>RAG</li><li>Agents</li></ul> 
<p>       在本文中，我们将使用一些代码示例来分别介绍一下这些改进的使用方法。</p> 
<p></p> 
<p class="img-center"><img alt="" height="691" src="https://images2.imgbox.com/14/70/jNfiCgCK_o.png" width="1025"></p> 
<h2><strong>一、LangChain 0.1概述</strong></h2> 
<p>       在0.1发布之前，LangChain团队与100名开发人员进行了深入交流，从而为LangChain的升级提供了宝贵建议。</p> 
<p>       为了增强基础功能，他们将框架分为3个包：</p> 
<p><strong>langchain-core：</strong>构成主干，包括主要的抽象、接口和核心功能，包括langchain表达式语言（LCEL）。</p> 
<p><strong>langchain-community：</strong>旨在更好地管理依赖关系，该包包括第三方集成，增强langchain的健壮性和可扩展性，并改善整体开发者体验。</p> 
<p><strong>langchain</strong>：专注于更高级别的应用程序，比如Chain、Agent和检索算法的特定用例。</p> 
<p></p> 
<p class="img-center"><img alt="" height="1012" src="https://images2.imgbox.com/33/d1/9lH7Lx3X_o.png" width="1000"></p> 
<p>       LangChain 0.1还引入了清晰的版本控制策略，以保持向后兼容性：</p> 
<ul><li>小版本更新（第二位数字）表示将提供一个突破性变更的公共API；</li><li>补丁版本（第三位数字）将表示错误修复或新增功能。</li></ul> 
<h2><strong>二、可观测性（Observability）</strong></h2> 
<p>      为了深入了解LangChain应用程序的内部工作，可以使用<strong>set_verbose</strong>和<strong>set_debug</strong>来了解引擎执行的细节。</p> 
<pre><code>from langchain.globals import set_debug</code><code>​</code><code>set_verbose(False)</code><code>set_debug(True)</code><code>​</code><code>prompt = hub.pull("hwchase17/openai-functions-agent")</code><code>llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)</code><code>search = TavilySearchResults()</code><code>tools = [search]</code><code>agent = create_openai_functions_agent(llm, tools, prompt)</code><code>agent_executor = AgentExecutor(agent=agent, tools=tools)</code><code>​</code><code>agent_executor.invoke({"input": "what is the weather in sf?"})</code></pre> 
<pre><code>[chain/start] [1:chain:AgentExecutor] Entering Chain run with input:</code><code>{<!-- --></code><code>  "input": "what is the weather in sf?"</code><code>}</code><code>[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence] Entering Chain run with input:</code><code>{<!-- --></code><code>  "input": "what is the weather in sf?",</code><code>  "intermediate_steps": []</code><code>}</code><code>[chain/start] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 3:chain:RunnableAssign&lt;agent_scratchpad&gt;] Entering Chain run with input:</code><code>{<!-- --></code><code>  "input": "what is the weather in sf?",</code><code>  "intermediate_steps": []...</code><code>​</code><code>...</code><code>[chain/end] [1:chain:AgentExecutor &gt; 2:chain:RunnableSequence &gt; 6:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:</code><code>{<!-- --></code><code>  "lc": 1,</code><code>  "type": "constructor",</code><code>  "id": [</code><code>    "langchain",</code><code>    "prompts",</code><code>    "chat",</code><code>    "ChatPromptValue"</code><code>  ],</code><code>  "kwargs": {<!-- --></code><code>    "messages": [</code><code>      {<!-- --></code><code>        "lc": 1,</code><code>        "type": "constructor",</code><code>        "id": [</code><code>          "langchain",</code><code>          "schema",...</code></pre> 
<p>       LangChain团队还构建了LangSmith，为LLM应用程序提供一流的调试体验。LangSmith现在是私人测试版，希望几个月后能公开测试。</p> 
<pre><code>import os</code><code>import getpass</code><code>​</code><code>os.environ["LANGCHAIN_TRACING_V2"]="true"</code><code>os.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"</code><code>os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()</code><code>​</code><code>set_debug(False)</code><code>​</code><code>agent_executor.invoke({"input": "what is the weather in sf?"})</code><code>​</code><code># {'input': 'what is the weather in sf?',</code><code># 'output': "I'm sorry, but I couldn't find the current weather in San Francisco. However, you can check the weather forecast for San Francisco on websites like Weather.com or AccuWeather.com."}</code></pre> 
<p></p> 
<p class="img-center"><img alt="" height="702" src="https://images2.imgbox.com/54/2c/MXrzqMwH_o.png" width="1080"></p> 
<p>       如果没有参加LangSmith的私人测试版，也可以使用WandB来跟踪LLM应用程序。</p> 
<p></p> 
<p class="img-center"><img alt="" height="710" src="https://images2.imgbox.com/86/10/8cr34vpY_o.gif" width="1080"></p> 
<h2><strong>三、可组合性（Composability）</strong></h2> 
<p>       最近几个月，该团队在LangChain表达式语言（LCEL）方面投入了大量资金，以实现更好的编排，因为我们都需要一种简单而声明的方式来组合链。</p> 
<p>LCEL提供了许多优点：</p> 
<ul><li>流式支持</li><li>异步支持</li><li>优化的并行执行</li><li>中断和回退</li><li>访问中间结果</li><li>验证输入和输出schema</li></ul> 
<p>        让我们看一个简单的例子：</p> 
<pre><code>from langchain_community.retrievers.tavily_search_api import TavilySearchAPIRetriever</code><code>​</code><code>retriever= TavilySearchAPIRetriever()</code><code>​</code><code>prompt = ChatPromptTemplate.from_template("""Answer the question based only on the context provided:</code><code>​</code><code>Context: {context}</code><code>​</code><code>Question: {question}""")</code><code>chain = prompt | model | output_parser</code><code>question = "what is langsmith"</code><code>context = "langsmith is a testing and observability platform built by the langchain team"</code><code>chain.invoke({"question": question, "context": context})</code><code># 'Langsmith is a testing and observability platform developed by the Langchain team.'</code><code>​</code><code>from langchain_core.runnables import RunnablePassthrough</code><code>​</code><code>retrieval_chain = RunnablePassthrough.assign(</code><code>    context=(lambda x: x["question"]) | retriever</code><code>) | chain</code><code>retrieval_chain.invoke({"question": "what is langsmith"})</code><code># 'LangSmith is a platform that helps trace and evaluate language model applications and intelligent agents. It allows users to debug, test, evaluate, and monitor chains and intelligent agents built on any Language Model (LLM) framework. LangSmith seamlessly integrates with LangChain, an open source framework for building with LLMs.'</code></pre> 
<blockquote> 
 <p>注意：LCEL的组件位于langchain-core中，它们将逐渐取代先前存在的（现在为“Legacy”）链。</p> 
</blockquote> 
<h2><strong>四、流式（Streaming）</strong></h2> 
<p>      用LCEL构建的所有链都使用了标准<strong>stream</strong>和<strong>astream</strong>方法，以及标准<strong>astream_log</strong>方法，该方法流式的传输LCEL链中的所有步骤，这些步骤可以被过滤以便很容易地获得所采取的中间步骤和其他信息。</p> 
<pre><code>from langchain_openai import ChatOpenAI</code><code>from langchain_core.prompts import ChatPromptTemplate</code><code>from langchain_core.output_parsers import StrOutputParser</code><code>prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")</code><code>model = ChatOpenAI()</code><code>output_parser = StrOutputParser()</code><code>chain = prompt | model | output_parser</code><code>for s in chain.stream({"topic": "bears"}):</code><code>    print(s)</code></pre> 
<pre><code>Why</code><code> don</code><code>'t</code><code> bears</code><code> wear</code><code> shoes</code><code>?</code><code>​</code><code>Because</code><code> they</code><code> have</code><code> bear</code><code> feet</code><code>!</code></pre> 
<h2><strong>五、工具使用情况（Tool Usage）</strong></h2> 
<p>      确保LLM返回下游应用程序中使用的结构化信息是至关重要。</p> 
<p>      LangChain提供了许多输出解析器，用于将LLM输出转换为更合适的格式，其中许多解析器支持JSON、XML和CSV等结构化格式的流式部分结果。</p> 
<p></p> 
<p class="img-center"><img alt="" height="312" src="https://images2.imgbox.com/6b/3d/HEY6s2rN_o.png" width="679"></p> 
<p>       可以指定输出格式（使用Pydantic、JSON模式，甚至函数），这样可以很轻松地处理响应。</p> 
<pre><code>from langchain.utils.openai_functions import (</code><code>    convert_pydantic_to_openai_function,</code><code>)</code><code>from langchain_core.prompts import ChatPromptTemplate</code><code>from langchain_core.pydantic_v1 import BaseModel, Field, validator</code><code>​</code><code>class Joke(BaseModel):</code><code>    """Joke to tell user."""</code><code>​</code><code>    setup: str = Field(description="question to set up a joke")</code><code>    punchline: str = Field(description="answer to resolve the joke")</code><code>​</code><code>​</code><code>openai_functions = [convert_pydantic_to_openai_function(Joke)]</code><code>​</code><code>from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser</code><code>​</code><code>parser = JsonOutputFunctionsParser()</code><code>chain = prompt | model.bind(functions=openai_functions) | parser</code><code>chain.invoke({"topic": "bears"})</code><code># {'setup': "Why don't bears wear shoes?",</code><code># 'punchline': 'Because they have bear feet!'}</code></pre> 
<p>      所有输出解析器都内置了一个<strong>get_format_instructions</strong>方法来获取指令，并告诉LLM如何响应。</p> 
<h2><strong>六、检索增强生成（RAG）</strong></h2> 
<p>       现在，让我们来谈谈RAG方面，能够轻松地将用户私有数据与LLM相结合是LangChain极其重要的一部分。</p> 
<p>      在数据加载方面，LangChain提供了15个不同的文本拆分器，并为特定的文档类型（如HTML和Markdown）提供优化。</p> 
<p></p> 
<p class="img-center"><img alt="" height="543" src="https://images2.imgbox.com/58/ed/9PmUsfJd_o.png" width="1080"></p> 
<p>       对于大规模文本任务，LangChain提供了索引API，允许用户在忽略未更改的部分的同时重新插入内容，从而为大容量工作负载节省了时间和成本。</p> 
<p>      在检索方面，LangChain支持来自学术界先进策略，比如<strong>FLARE</strong>和<strong>Hyde</strong>，也支持LangChain的技术，比如<strong>Parent Document</strong>和<strong>Self Query</strong>，以及来自其他行业的解决方案，比如<strong>Multi-Query</strong>。</p> 
<p>      它还支持生成环境中的实际问题，比如按用户检索，这对于将多个用户的文档存储在一起的任何应用程序都至关重要。</p> 
<p></p> 
<p class="img-center"><img alt="" height="554" src="https://images2.imgbox.com/ee/37/9p3dZKpX_o.png" width="1080"></p> 
<p>        还提供了更多的检索方法，如<strong>EmbedChain</strong>和<strong>GPTResearcher</strong>。</p> 
<h2><strong>七、Agent</strong></h2> 
<p>       2024年将是Agent的元年，LangChain在Agent方面支持如下功能：</p> 
<ul><li>与第三方工具集成</li><li>构建LLM响应的方法</li><li>灵活的工具调用方式（LCEL）</li><li>提示策略，如ReAct</li><li>函数和工具调用</li></ul> 
<p>        以下是一个简单的例子，展示了使用LangChain启动Agent程序有多容易：</p> 
<pre><code>from langchain_community.tools.tavily_search import TavilySearchResults</code><code>from langchain_openai import ChatOpenAI</code><code>from langchain import hub</code><code>from langchain.agents import create_openai_functions_agent</code><code>from langchain.agents import AgentExecutor</code><code>​</code><code># Prompt</code><code># Get the prompt to use - you can modify this!</code><code>prompt = hub.pull("hwchase17/openai-functions-agent")</code><code>​</code><code># LLM</code><code>llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)</code><code>​</code><code># Tools</code><code>search = TavilySearchResults()</code><code>tools = [search]</code><code>​</code><code># Agent</code><code>agent = create_openai_functions_agent(llm, tools, prompt)</code><code>result = agent.invoke({"input": "what's the weather in SF?", "intermediate_steps": []})</code><code>result.tool</code><code># 'tavily_search_results_json'</code><code>​</code><code>result.tool_input</code><code># {'query': 'weather in San Francisco'}</code><code>​</code><code>result</code><code># AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'weather in San Francisco'}, log="\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\n\n\n", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "query": "weather in San Francisco"\n}', 'name': 'tavily_search_results_json'}})])</code><code>​</code><code># Agent Executor</code><code>agent_executor = AgentExecutor(agent=agent, tools=tools)</code><code>agent_executor.invoke({"input": "what is the weather in sf?"})</code><code>#{'input': 'what is the weather in sf?',</code><code># 'output': 'The weather in San Francisco is currently not available. However, you can check the weather in San Francisco in January 2024 [here](https://www.whereandwhen.net/when/north-america/california/san-francisco-ca/january/).'}</code><code>​</code><code># Streaming</code><code>for step in agent_executor.stream({"input": "what is the weather in sf?"}):</code><code>    print(step)</code><code>    </code><code># {'actions': [AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'weather in San Francisco'}, log="\nInvoking: `tavily_search_results_json` with `{'query': 'weather in San Francisco'}`\n\n\n", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "query": "weather in San Francisco"\n}', 'name': 'tavily_search_results_json'}})])], 'messages': [AIMe...</code></pre> 
<p>       还有一个值得注意的是langgraph库，它可以将语言Agent创建为图。</p> 
<p>       langgraph允许用户自定义循环行为：</p> 
<ul><li>明确的规划步骤</li><li>显式响应步骤</li></ul> 
<p>       或者先对其进行硬编码以调用特定的工具，使用户能够构建更复杂、更高效的语言模型。</p> 
<pre><code>from langgraph.graph import END, Graph</code><code>​</code><code>workflow = Graph()</code><code>​</code><code># Add the agent node, we give it name `agent` which we will use later</code><code>workflow.add_node("agent", agent)</code><code># Add the tools node, we give it name `tools` which we will use later</code><code>workflow.add_node("tools", execute_tools)</code><code>​</code><code># Set the entrypoint as `agent`</code><code># This means that this node is the first one called</code><code>workflow.set_entry_point("agent")</code><code>​</code><code># We now add a conditional edge</code><code>workflow.add_conditional_edges(</code><code>    # First, we define the start node. We use `agent`.</code><code>    # This means these are the edges taken after the `agent` node is called.</code><code>    "agent",</code><code>    # Next, we pass in the function that will determine which node is called next.</code><code>    should_continue,</code><code>    # Finally we pass in a mapping.</code><code>    # The keys are strings, and the values are other nodes.</code><code>    # END is a special node marking that the graph should finish.</code><code>    # What will happen is we will call `should_continue`, and then the output of that</code><code>    # will be matched against the keys in this mapping.</code><code>    # Based on which one it matches, that node will then be called.</code><code>    {<!-- --></code><code>        # If `tools`, then we call the tool node.</code><code>        "continue": "tools",</code><code>        # Otherwise we finish.</code><code>        "exit": END</code><code>    }</code><code>)</code><code>​</code><code># We now add a normal edge from `tools` to `agent`.</code><code># This means that after `tools` is called, `agent` node is called next.</code><code>workflow.add_edge('tools', 'agent')</code><code>​</code><code># Finally, we compile it!</code><code># This compiles it into a LangChain Runnable,</code><code># meaning you can use it as you would any other runnable</code><code>chain = workflow.compile()
</code></pre> 
<blockquote> 
 <p>langgraph还支持OpenGPTs。</p> 
</blockquote> 
<h2><strong>参考文献：</strong></h2> 
<p>[1] https://medium.com/@datadrifters/langchain-0-1-7b26a6012482</p> 
<p>[2] https://www.youtube.com/watch?v=dlJQ-YiXgCs</p> 
<p>[3] https://www.youtube.com/watch?v=b-jEUMt0ji0</p> 
<p>[4] https://www.youtube.com/watch?v=ipwWmXa904w</p> 
<p>[5] https://www.youtube.com/watch?v=gr5CGL4_jpY</p> 
<p>[6] https://www.youtube.com/watch?v=dbjmhCWPKow</p> 
<p>[7] https://www.youtube.com/watch?v=yK64dws8f6A</p> 
<p>[8] https://www.youtube.com/watch?v=08qXj9w-CG4</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7fbb88aa12c286f4971f4d7fff8af35c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">CGAL最小生成树、可视化</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/677d9699cd0d34aad83f14807917739a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">[二]rtmp服务器搭建</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>