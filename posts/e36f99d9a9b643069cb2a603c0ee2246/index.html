<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习-31-Transformer详解以及我的三个疑惑和解答 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习-31-Transformer详解以及我的三个疑惑和解答" />
<meta property="og:description" content="文章目录 TransformerSequence用CNN取代RNNSelf-AttentionSelf-attention is all you need(重点)Self-attention是如何并行计算的？ 矩阵运算！Multi-head Self-attention(Self-attention的变形 )Position EncodingSeq2Seq with AttentionTransformer(重点)简介基本架构EncoderDecoderAttentionSelf-AttentionContext-AttentionScaled Dot-Product AttentionScaled Dot-Product Attention 代码实现Multi-head attention 代码实现ResnetLayer normalizationMaskPadding MaskSequence mask Positional EmbeddingPosition-wise Feed-Forward network Transformer的实现针对Transformer的三个疑惑疑惑一：Transformer的Decoder的输入输出都是什么？疑惑二：Shifted Right到底是什么？上面两个疑惑的总结疑惑三：Transformer里decoder为什么还需要seq mask？ 参考 Transformer Transformer的知名应用——BERT——无监督的训练的Transformer。
Transformer是BERT的核心模块
Transformer是一个seq2seq模型，并且大量用到了&#34;Self-attention&#34;，接下来就要讲解一下&#34;Self-attention&#34;用到了什么东西
Sequence Sequence就会想到RNN，单方向或者双向的RNN。
RNN输入是一串sequence，输出是另外一串sequence。
RNN常被用于输出是一个序列的情况，但是有一个问题——不容易被平行化（并行）。
单向RNN的时候，想要算出b4，必须先把a1，a2，a3都看过才可以算出a4。双向则得全部看完才会有输出。
用CNN取代RNN 于是有人提出用CNN取代RNN
一个三角形是一个filter，输入为sequence中的一段，此刻是将三个vector作为一个输入，输出一个数值。
将三个vector的内容与filter内部的参数做内积，得到一个数值，将filter扫过sequence，产生一排不同的数值。会有多个filter，产生另外一排不同的数值 我们可以看到，用CNN也可以做到和RNN类似的效果：输入一个sequence，输出一个sequence。
表面上CNN和RNN都可以有同样的输入输出。
但是每个CNN只能考虑很有限的内容（三个vector），而RNN是考虑了整个句子再决定输出。
CNN也可以考虑更长的信息，只要叠加多层CNN，上层的filter就可以考虑更加多的信息。
eg：先叠了第一层CNN后，叠加第二层CNN。第二层的filter会把第一层的output当作输入，相当于看了更多的内容。
CNN的好处在于可以并行化。
CNN的缺点在于必须叠加多层，才可以看到长时间的信息，如果要在第一层filter就要看到长时间的信息，那是无法做到的。
所以，我们引入了 一个新的想法：Self-Attention
Self-Attention Self-Attention做的事情就是取代RNN原本要做的事情。
关键： 有一种新的layer—— Self-Attention，输入输出与RNN一样，都是sequence。
特别的地方在于，和双向RNN有同样的能力，每一个输出都是看过整个input sequence，只不过b1 b2 b3 b4是可以同时算出来的，可以并行计算！
Self-attention is all you need(重点) 在此，我先讲将oogle的论文贴出来吧：👉 Attention Is All You Need" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e36f99d9a9b643069cb2a603c0ee2246/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-12-31T14:43:24+08:00" />
<meta property="article:modified_time" content="2020-12-31T14:43:24+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习-31-Transformer详解以及我的三个疑惑和解答</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><ul><li><a href="#Transformer_7" rel="nofollow">Transformer</a></li><li><ul><li><a href="#Sequence_25" rel="nofollow">Sequence</a></li><li><a href="#CNNRNN_39" rel="nofollow">用CNN取代RNN</a></li><li><a href="#SelfAttention_71" rel="nofollow">Self-Attention</a></li><li><a href="#Selfattention_is_all_you_need_84" rel="nofollow">Self-attention is all you need(重点)</a></li><li><a href="#Selfattention__137" rel="nofollow">Self-attention是如何并行计算的？ 矩阵运算！</a></li><li><a href="#Multihead_SelfattentionSelfattention__167" rel="nofollow">Multi-head Self-attention(Self-attention的变形 )</a></li><li><a href="#Position_Encoding_203" rel="nofollow">Position Encoding</a></li><li><a href="#Seq2Seq_with_Attention_273" rel="nofollow">Seq2Seq with Attention</a></li><li><a href="#Transformer_289" rel="nofollow">Transformer(重点)</a></li><li><ul><li><a href="#_291" rel="nofollow">简介</a></li><li><a href="#_302" rel="nofollow">基本架构</a></li><li><a href="#Encoder_312" rel="nofollow">Encoder</a></li><li><a href="#Decoder_327" rel="nofollow">Decoder</a></li><li><a href="#Attention_343" rel="nofollow">Attention</a></li><li><a href="#SelfAttention_355" rel="nofollow">Self-Attention</a></li><li><a href="#ContextAttention_365" rel="nofollow">Context-Attention</a></li><li><a href="#Scaled_DotProduct_Attention_387" rel="nofollow">Scaled Dot-Product Attention</a></li><li><a href="#Scaled_DotProduct_Attention__424" rel="nofollow">Scaled Dot-Product Attention 代码实现</a></li><li><a href="#Multihead_attention__472" rel="nofollow">Multi-head attention 代码实现</a></li><li><a href="#Resnet_541" rel="nofollow">Resnet</a></li><li><a href="#Layer_normalization_559" rel="nofollow">Layer normalization</a></li><li><a href="#Mask_593" rel="nofollow">Mask</a></li><li><ul><li><a href="#Padding_Mask_599" rel="nofollow">Padding Mask</a></li><li><a href="#Sequence_mask_619" rel="nofollow">Sequence mask</a></li></ul> 
     </li><li><a href="#Positional_Embedding_648" rel="nofollow">Positional Embedding</a></li><li><a href="#Positionwise_FeedForward_network_715" rel="nofollow">Position-wise Feed-Forward network</a></li></ul> 
    </li><li><a href="#Transformer_751" rel="nofollow">Transformer的实现</a></li><li><a href="#Transformer_925" rel="nofollow">针对Transformer的三个疑惑</a></li><li><ul><li><a href="#TransformerDecoder_937" rel="nofollow">疑惑一：Transformer的Decoder的输入输出都是什么？</a></li><li><a href="#Shifted_Right_975" rel="nofollow">疑惑二：Shifted Right到底是什么？</a></li><li><a href="#_1006" rel="nofollow">上面两个疑惑的总结</a></li><li><a href="#Transformerdecoderseq_mask_1018" rel="nofollow">疑惑三：Transformer里decoder为什么还需要seq mask？</a></li></ul> 
    </li><li><a href="#_1052" rel="nofollow">参考</a></li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<br> 
<h3><a id="Transformer_7"></a>Transformer</h3> 
<p>Transformer的知名应用——BERT——无监督的训练的Transformer。</p> 
<p><strong>Transformer是BERT的核心模块</strong></p> 
<center> 
 <img src="https://images2.imgbox.com/f4/b0/1H7vFq8I_o.png" height="300/"> 
</center> 
<p>Transformer是一个seq2seq模型，并且大量用到了"Self-attention"，接下来就要讲解一下"Self-attention"用到了什么东西</p> 
<center> 
 <img src="https://images2.imgbox.com/4a/8d/T8wzEc7Z_o.png" height="300/"> 
</center> 
<br> 
<h4><a id="Sequence_25"></a>Sequence</h4> 
<p>Sequence就会想到RNN，单方向或者双向的RNN。</p> 
<p>RNN输入是一串sequence，输出是另外一串sequence。</p> 
<p>RNN常被用于输出是一个序列的情况，但是有一个问题——不容易被平行化（并行）。</p> 
<p>单向RNN的时候，想要算出b4，必须先把a1，a2，a3都看过才可以算出a4。双向则得全部看完才会有输出。</p> 
<br> 
<h4><a id="CNNRNN_39"></a>用CNN取代RNN</h4> 
<p>于是有人提出<strong>用CNN取代RNN</strong></p> 
<p>一个三角形是一个filter，输入为sequence中的一段，此刻是将三个vector作为一个输入，输出一个数值。</p> 
<ul><li>将三个vector的内容与filter内部的参数做内积，得到一个数值，将filter扫过sequence，产生一排不同的数值。</li><li>会有多个filter，产生另外一排不同的数值</li></ul> 
<center> 
 <img src="https://images2.imgbox.com/21/b9/9ni3BSIx_o.png" height="300/"> 
</center> 
<p>我们可以看到，用CNN也可以做到和RNN类似的效果：输入一个sequence，输出一个sequence。</p> 
<p>表面上CNN和RNN都可以有同样的输入输出。</p> 
<p>但是每个CNN只能考虑很有限的内容（三个vector），而RNN是考虑了整个句子再决定输出。</p> 
<p>CNN也可以考虑更长的信息，只要叠加多层CNN，上层的filter就可以考虑更加多的信息。</p> 
<p>eg：先叠了第一层CNN后，叠加第二层CNN。第二层的filter会把第一层的output当作输入，相当于看了更多的内容。</p> 
<p><strong>CNN的好处在于可以并行化。</strong><br> <strong>CNN的缺点在于必须叠加多层，才可以看到长时间的信息，如果要在第一层filter就要看到长时间的信息，那是无法做到的。</strong></p> 
<center> 
 <img src="https://images2.imgbox.com/c4/fb/MAYiZRd6_o.png" height="300/"> 
</center> 
<p>所以，我们引入了 一个新的想法：Self-Attention</p> 
<br> 
<h4><a id="SelfAttention_71"></a>Self-Attention</h4> 
<p>Self-Attention做的事情就是取代RNN原本要做的事情。<br> <strong>关键： 有一种新的layer—— Self-Attention，输入输出与RNN一样，都是sequence。</strong></p> 
<p><strong>特别的地方在于，和双向RNN有同样的能力，每一个输出都是看过整个input sequence，只不过b1 b2 b3 b4是可以同时算出来的，可以并行计算！</strong></p> 
<center> 
 <img src="https://images2.imgbox.com/3f/0a/81itTgY5_o.png" height="300/"> 
</center> 
<br> 
<h4><a id="Selfattention_is_all_you_need_84"></a>Self-attention is all you need(重点)</h4> 
<blockquote> 
 <p>在此，我先讲将oogle的论文贴出来吧：👉 <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need</a></p> 
</blockquote> 
<p>输入sequence x1~x4，通过乘上一个W matrix来得到embedding a1~a4，丢入Self-attention，每一个输入都分别乘上三个不同的transformation matrix，产生三个不同的vector q，k，v。</p> 
<ul><li>q代表query，用来match其他人</li><li>k代表key，用来被匹配的</li><li>v代表要被抽取出来的信息</li></ul> 
<center> 
 <img src="https://images2.imgbox.com/ee/04/qXRXox17_o.png" height="300/"> 
</center> 
<p>拿每个query q去对每个key k做attention，我们这里用到的计算attention的方法是<code>scaled dot-product</code>，关于这一点，我们，下文中还会有介绍。👉 [scaled dot-product传送门](#scaled dot-product)</p> 
<p>attention本质就是输入两个向量，输出一个分数。</p> 
<p>除以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
       
      
        \sqrt{d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.10778em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.93222em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">d</span></span></span><span class="" style="top: -2.89222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.10778em;"><span class=""></span></span></span></span></span></span></span></span></span>的一个原因是：<code>d是q和k的维度</code>，q和k做inner product，所以q和k的维度是一样的为d。除以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
       
      
        \sqrt{d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.10778em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.93222em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">d</span></span></span><span class="" style="top: -2.89222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.10778em;"><span class=""></span></span></span></span></span></span></span></span></span>的直观解释为q和k做内积/点积的数值会随着维度增大 他的variance越大，所以除以来<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
       
      
        \sqrt{d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em; vertical-align: -0.10778em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.93222em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord mathdefault">d</span></span></span><span class="" style="top: -2.89222em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
           <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
            <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
           </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.10778em;"><span class=""></span></span></span></span></span></span></span></span></span>平衡。</p> 
<center> 
 <img src="https://images2.imgbox.com/b4/4a/Es7YLa7O_o.png" height="300/"> 
</center> 
<blockquote> 
 <p>不除以d会梯度爆炸，不收敛，推一推梯度就能的到结果</p> 
</blockquote> 
<p>通过一个softmax函数</p> 
<center> 
 <img src="https://images2.imgbox.com/e1/75/fwMWwM9k_o.png" height="300/"> 
</center> 
<center> 
 <img src="https://images2.imgbox.com/f4/af/iCnRyjF0_o.png" height="300/"> 
</center> 
<p>上图，产生b1的时候已经考虑了全部句子的信息</p> 
<p>如果现在只想考虑局部的信息，而不是全局的，也是可以做到的，即只需要让右边那些<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>产生出来的值变成0，就只考虑局部了。<br> 如果要考虑全局的信息，就要考虑离他最远的input的vector值的话，只要让那个attention （<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         α 
        
       
      
        \alpha 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span></span></span></span></span>）有值即可。</p> 
<p>相比上面那张图，我还是更喜欢下面这张图：</p> 
<center> 
 <img src="https://images2.imgbox.com/ba/31/eHLwjJWg_o.png" height="300/"> 
</center> 
<p>刚刚只是计算了b1，同时也可以计算其他的b</p> 
<center> 
 <img src="https://images2.imgbox.com/9b/f8/7uT5MbWh_o.png" height="300/"> 
</center> 
<p>我们来看一下最终完整的流程图:</p> 
<p>self-attention做的和RNN的事情是一样的，只不过是平行计算出来的。</p> 
<center> 
 <img height="300" src="https://images2.imgbox.com/cb/9e/AfScHnGQ_o.png"> 
</center> 
<center> 
 <img src="https://images2.imgbox.com/b7/fb/2D1UA59B_o.png" height="300/"> 
</center> 
<br> 
<h4><a id="Selfattention__137"></a>Self-attention是如何并行计算的？ 矩阵运算！</h4> 
<p>self-attention中所有的运算都可以利用矩阵来进行运算，因此我们就可以使用gpu来进行加速,极大的加快了我们的运算速度。</p> 
<p>现在我们就来看看self-attention是如何利用矩阵进行并行计算的吧！！！</p> 
<center> 
 <img src="https://images2.imgbox.com/21/3d/0hsntxrU_o.png" height="300/"> 
</center> 
<p>看到上面这张图我们应该不陌生，这是我们进行self-attention的第一步。</p> 
<p>矩阵运算就是将a1~a4拼起来作为一个matrix I，用 I 再乘以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          q 
         
        
       
      
        W^q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span></span></span></span></span></span></span></span></span>，一次得到matrix Q，里面的每一列代表一个q。同理，将matrix I乘以<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          k 
         
        
       
      
        W^k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          v 
         
        
       
      
        W^v 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">v</span></span></span></span></span></span></span></span></span></span></span></span>可以得到相应的matrix K和marix V。</p> 
<p>接下来，拿query q去对每个key k做attention。并对每一列做softmax。</p> 
<center> 
 <img src="https://images2.imgbox.com/fa/eb/WKQSAsd1_o.png" height="300/"> 
</center> 
<center> 
 <img src="https://images2.imgbox.com/da/4b/Tw1COIL9_o.png" height="300/"> 
</center> 
<p>接下来就是根据我们通过softmax得到的结果，考虑每个信息，从而得到我们的一个输出。</p> 
<center> 
 <img src="https://images2.imgbox.com/a1/29/jUazBYcS_o.png" height="300/"> 
</center> 
<p>最后，我们将所有矩阵运算整合起来，来回顾一下整个流程。</p> 
<center> 
 <img src="https://images2.imgbox.com/0a/cc/ynueixMz_o.png" height="300/"> 
</center> 
<br> 
<h4><a id="Multihead_SelfattentionSelfattention__167"></a>Multi-head Self-attention(Self-attention的变形 )</h4> 
<p>通过增加一种叫做“多头”注意力（“multi-headed” attention）的机制，进一步完善了自注意力层，并在两方面提高了注意力层的性能：</p> 
<ol><li> <p>它扩展了模型专注于不同位置的能力。在上面的例子中，虽然每个编码都在z1中有或多或少的体现，但是它可能被实际的单词本身所支配。如果我们翻译一个句子，比如“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意机制会起到作用。</p> </li><li> <p>它给出了注意力层的多个“表示子空间”（representation subspaces）。接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的，在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)投影到不同的表示子空间中。</p> </li></ol> 
<center> 
 <img src="https://images2.imgbox.com/16/ef/6mHlB4lB_o.png" height="300/"> 
</center> 
<p>注意一点的是，每个头只能和对应的头进行运算。比如：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          q 
         
         
         
           i 
          
         
           , 
          
         
           2 
          
         
        
       
      
        q^{i,2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0191em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>只能和对应的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          k 
         
         
         
           i 
          
         
           , 
          
         
           2 
          
         
        
       
      
        k^{i,2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>以及<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          k 
         
         
         
           j 
          
         
           , 
          
         
           2 
          
         
        
       
      
        k^{j,2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>进行运算，而不能和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          k 
         
         
         
           i 
          
         
           , 
          
         
           1 
          
         
        
       
      
        k^{i,1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>以及<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          k 
         
         
         
           j 
          
         
           , 
          
         
           1 
          
         
        
       
      
        k^{j,1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>进行运算。</p> 
<p>算出了<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
         
         
           i 
          
         
           , 
          
         
           1 
          
         
        
       
      
        b^{i,1} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          b 
         
         
         
           i 
          
         
           , 
          
         
           2 
          
         
        
       
      
        b^{i,2} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>后，给我们带来了一点挑战。前馈层不需要两个矩阵，它只需要一个矩阵！所以我们需要一种方法把这两个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          o 
         
        
       
      
        W^o 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">o</span></span></span></span></span></span></span></span></span></span></span></span>与它们相乘。</p> 
<center> 
 <img src="https://images2.imgbox.com/f5/19/jphWjxCn_o.png" height="300/"> 
</center> 
<p>当然我们上面仅仅是二头。其实还可以有好多头，我们再举一个八头的例子：</p> 
<center> 
 <img src="https://images2.imgbox.com/76/08/EqXuOEd1_o.png" height="300/"> 
</center> 
<p>了解了多头机制后，让我们重温之前的例子，看看我们在例句中编码“it”一词时，不同的注意力“头”集中在哪里。</p> 
<blockquote> 
 <p>“The animal didn’t cross the street because it was too tired”，我们会想知道“it”指的是哪个词?</p> 
</blockquote> 
<center> 
 <img src="https://images2.imgbox.com/c3/72/QzWyqRrx_o.png" height="300/"> 
</center> 
<p>当我们编码“it”一词时，一个注意力头集中在“animal”上，而另一个则集中在“tired”上，从某种意义上说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。</p> 
<p>然而，如果我们把所有的attention都加到图示里，事情就更难解释了：</p> 
<center> 
 <img src="https://images2.imgbox.com/b0/1c/P6E7wkss_o.png" height="300/"> 
</center> 
<br> 
<h4><a id="Position_Encoding_203"></a>Position Encoding</h4> 
<p>到目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。</p> 
<p>为了解决这个问题，Transformer为每个输入的词嵌入添加了一个向量。这些向量遵循模型学习到的特定模式，这有助于确定每个单词的位置，或序列中不同单词之间的距离。这里的直觉是，将位置向量添加到词嵌入中使得它们在接下来的运算中，能够更好地表达的词与词之间的距离。</p> 
<p>在原始paper中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
        
          i 
         
        
       
      
        e^i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>是人手设置的，不是学习出来的。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
        
          i 
         
        
       
      
        e^i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>代表了位置信息，每个位置<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
        
          i 
         
        
       
      
        e^i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>不同。<br> paper中将<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          e 
         
        
          i 
         
        
       
      
        e^i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>加上得到一个<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          a 
         
        
          i 
         
        
       
      
        a^i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>新的vector，之后和Self-attention操作一样。</p> 
<blockquote> 
 <p>Q：那么为什么是<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           e 
          
         
           i 
          
         
        
       
         e^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           a 
          
         
           i 
          
         
        
       
         a^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>相加呢？而不是<strong>拼接</strong>起来呢？<br> A：把再<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
       
         x^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>append一个one-hot向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           p 
          
         
           i 
          
         
        
       
         p^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0191em; vertical-align: -0.19444em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span></span></span></span>，由下图可知，结果是一样的。</p> 
</blockquote> 
<center> 
 <img src="https://images2.imgbox.com/0f/ef/1UWBhcDk_o.png" height="300/"> 
</center> 
<p>为了让模型理解单词的顺序，我们添加了位置编码向量，这些向量的值遵循特定的模式。</p> 
<p>如果我们假设词嵌入的维数为4，则实际的位置编码如下：</p> 
<center> 
 <img src="https://images2.imgbox.com/64/d6/d3Lb5ufA_o.png" height="300/"> 
</center> 
<p>尺寸为4的迷你词嵌入位置编码实例</p> 
<p>这个模式会是什么样子？</p> 
<p>在下图中，每一行对应一个词向量的位置编码，所以第一行对应着输入序列的第一个词。每行包含512个值，每个值介于1和-1之间。我们已经对它们进行了颜色编码，所以图案是可见的。</p> 
<center> 
 <img src="https://images2.imgbox.com/2b/63/o7C2NMjz_o.png"> 
</center> 
<p>20字(行)的位置编码实例，词嵌入大小为512(列)。你可以看到它从中间分裂成两半。这是因为左半部分的值由一个函数(使用正弦)生成，而右半部分由另一个函数(使用余弦)生成。然后将它们拼在一起而得到每一个位置编码向量。</p> 
<p>原始论文里描述了位置编码的公式。你可以在 get_timing_signal_1d()中看到生成位置编码的代码。这不是唯一可能的位置编码方法。然而，它的优点是能够扩展到未知的序列长度(例如，当我们训练出的模型需要翻译远比训练集里的句子更长的句子时)。</p> 
<p>在实现的时候使用正余弦函数。公式如下：</p> 
<center> 
 <img src="https://images2.imgbox.com/2c/f0/ti9bKKy8_o.png"> 
</center> 
<p>其中，pos 是指词语在序列中的位置。可以看出，在<strong>偶数位置，使用正弦编码，在奇数位置，使用余弦编码</strong>。</p> 
<p>从编码公式中可以看出，给定词语的 pos，我们可以把它编码成一个 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
         
         
           m 
          
         
           o 
          
         
           d 
          
         
           e 
          
         
           l 
          
         
        
       
      
        d_{model} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> (维度)的向量。也就是说，位置编码的每一个维度对应正弦曲线，波长构成了从 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         2 
        
       
         π 
        
       
      
        2π 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right: 0.03588em;">π</span></span></span></span></span> 到 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         10000 
        
       
         × 
        
       
         2 
        
       
         π 
        
       
      
        10000 × 2π 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right: 0.03588em;">π</span></span></span></span></span> 的等比数列。</p> 
<p>上面的位置编码是<strong>绝对位置编码</strong>。但是词语的<strong>相对位置</strong>也非常重要。这就是论文为什么要使用三角函数的原因！</p> 
<p>pos + k 位置的encoding可以通过pos位置的encoding线性表示。主要数学依据是以下两个公式：</p> 
<center> 
 <img src="https://images2.imgbox.com/5f/a3/CaP2OK3X_o.png"> 
</center> 
<p>位置为 pos + k 的positional encoding 可以表示如下：</p> 
<center> 
 <img src="https://images2.imgbox.com/7b/2a/CcFV6040_o.png"> 
</center> 
<p>化简如下：</p> 
<center> 
 <img src="https://images2.imgbox.com/fc/f5/fpeUSuG0_o.png"> 
</center> 
<p>其中与k相关的项都是常数，所以 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         P 
        
        
        
          E 
         
         
         
           p 
          
         
           o 
          
         
           s 
          
         
           + 
          
         
           k 
          
         
        
       
      
        PE_{pos+k} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>可以被 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         P 
        
        
        
          E 
         
         
         
           p 
          
         
           o 
          
         
           s 
          
         
        
       
      
        PE_{pos} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">P</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.05764em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>线性表示。</p> 
<p>由于</p> 
<center> 
 <img src="https://images2.imgbox.com/c9/0e/aLNsbZG6_o.png"> 
</center> 
<p>所以i越大，周期就越大。周期的范围从 2π到 2π⋅10000</p> 
<p>具体实现会在下面给出。👉 <a href="#position_code" rel="nofollow">传送门</a></p> 
<br> 
<h4><a id="Seq2Seq_with_Attention_273"></a>Seq2Seq with Attention</h4> 
<center> 
 <img src="https://images2.imgbox.com/0a/2d/I83t2T0J_o.png" height="300/"> 
</center> 
<p>光看上面这张图我们可能并不能直观的看出整个过程，索性谷歌做了一张动图来描述整个过程，让我们一起来看一下吧。</p> 
<center> 
 <img src="https://images2.imgbox.com/3a/a1/jij90lrq_o.gif" height="400/"> 
</center> 
<p>这个图的encoding 过程， 主要是self attention, 有三层。 接下来是decoding过程， 也是有三层， 第一个预测结果 &lt;start&gt; 符号， 是完全通过encoding 里的attention vector 做出的决策。 而第二个预测结果Je, 是基于encoding attention vector &amp; &lt;start&gt; attention vector 做出的决策。按照这个逻辑，新翻译的单词不仅仅依赖 encoding attention vector， 也依赖过去翻译好的单词的attention vector。 随着翻译出来的句子越来越多，翻译下一个单词的运算量也就会相应增加。 如果详细分析，复杂度是 （<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          n 
         
         
         
           2 
          
         
           d 
          
         
        
       
      
        n^{2d} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.849108em; vertical-align: 0em;"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>）, 其中n是翻译句子的长度，d是word vector 的维度。</p> 
<br> 
<h4><a id="Transformer_289"></a>Transformer(重点)</h4> 
<h5><a id="_291"></a>简介</h5> 
<p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder，并在机器翻译中取得了BLEU值得新高。</p> 
<p>作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p> 
<ol><li>时间片 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          t 
         
        
       
         t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em; vertical-align: 0em;"></span><span class="mord mathdefault">t</span></span></span></span></span> 的计算依赖 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          t 
         
        
          − 
         
        
          1 
         
        
       
         t-1 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69841em; vertical-align: -0.08333em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> 时刻的计算结果，这样限制了模型的并行能力；</li><li>顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。</li></ol> 
<p>Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。论文中给出Transformer的定义是：Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。</p> 
<h5><a id="_302"></a>基本架构</h5> 
<p>接下来的这张图才是重点，上面介绍的仅仅是Transformer完整架构中的某个部分，不过也不要慌，基本上面我们都有提及。</p> 
<center> 
 <img src="https://images2.imgbox.com/33/73/DpMwtDmi_o.png" height="400/"> 
</center> 
<p>更加具体一点：</p> 
<center> 
 <img src="https://images2.imgbox.com/9a/c7/szhVOOHu_o.png" height="400/"> 
</center> 
<h5><a id="Encoder_312"></a>Encoder</h5> 
<p>在 Encoder 中，Input 经过 embedding 后，要做 positional encodings</p> 
<p><strong>Encoder由 6 层相同的层组成</strong>，每一层包括2个sub-layers：</p> 
<ul><li>第一部分是 multi-head self-attention</li><li>第二部分是 position-wise feed-forward network，是一个全连接层</li></ul> 
<p>两个部分，都有一个残差连接(residual connection)，然后接着一个 Layer Normalization。</p> 
<br> 
<h5><a id="Decoder_327"></a>Decoder</h5> 
<p>和 encoder 类似，<strong>decoder 也是由6个相同的层组成</strong>，但每一个层包括以下3个sub-layers:</p> 
<ul><li>第一个部分是 multi-head self-attention mechanism</li><li>第二部分是 multi-head context-attention mechanism</li><li>第三部分是一个 position-wise feed-forward network</li></ul> 
<p>和 encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 <strong>Layer Normalization</strong>。</p> 
<p>decoder 和 encoder 不同的地方在 multi-head context-attention mechanism</p> 
<br> 
<h5><a id="Attention_343"></a>Attention</h5> 
<p>这一部分我们在前文中已经提及了，就不多说了，Attention 如果用一句话来描述，那就是 encoder 层的输出经过加权平均后再输入到 decoder 层中。它主要应用在 seq2seq 模型中，这个加权可以用矩阵来表示，也叫 Attention 矩阵。它表示对于某个时刻的输出 y，它在输入 x 上各个部分的注意力。这个注意力就是我们刚才说到的加权。</p> 
<p>Attention 又分为很多种，其中两种比较典型的有加性 Attention 和乘性 Attention。加性 Attention 对于输入的隐状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
        
          t 
         
        
       
      
        h_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 和输出的隐状态 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          s 
         
        
          t 
         
        
       
      
        s_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 直接做 concat 操作，得到<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         [ 
        
        
        
          s 
         
        
          t 
         
        
       
         : 
        
        
        
          h 
         
        
          t 
         
        
       
         ] 
        
       
      
        [s_t:h_t] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> ，乘性 Attention 则是对输入和输出做 dot 操作。</p> 
<p>在 Google 这篇论文中，使用的 Attention 模型是乘性 Attention。</p> 
<br> 
<h5><a id="SelfAttention_355"></a>Self-Attention</h5> 
<p>上面我们说attention机制的时候，都会说到两个隐状态，分别是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          h 
         
        
          i 
         
        
       
      
        h_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          s 
         
        
          t 
         
        
       
      
        s_t 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>。前者是输入序列第 i个位置产生的隐状态，后者是输出序列在第 t 个位置产生的隐状态。所谓 self-attention 实际上就是，输出序列就是输入序列。因而自己计算自己的 attention 得分。</p> 
<p>这里我们在上面也已经提及了，就不细说了。</p> 
<br> 
<h5><a id="ContextAttention_365"></a>Context-Attention</h5> 
<p>context-attention 是 encoder 和 decoder 之间的 attention，是两个不同序列之间的attention，与来源于自身的 self-attention 相区别。</p> 
<p>不管是哪种 attention，我们在计算 attention 权重的时候，可以选择很多方式，常用的方法有</p> 
<ul><li>additive attention</li><li>local-base</li><li>general</li><li>dot-product</li><li>scaled dot-product</li></ul> 
<p>Transformer模型采用的是最后一种：<code>scaled dot-product attention</code>。</p> 
<p><span id="scaled dot-product"></span></p> 
<br> 
<h5><a id="Scaled_DotProduct_Attention_387"></a>Scaled Dot-Product Attention</h5> 
<p>那么什么是 scaled dot-product attention 呢？</p> 
<p>Google 在论文中对 Attention 机制这么来描述：</p> 
<blockquote> 
 <p>An attention function can be described as a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, <strong>where the weight assigned to each value is computed by a compatibility of the query with the corresponding key.</strong></p> 
</blockquote> 
<p>通过 query 和 key 的相似性程度来确定 value 的权重分布。论文中的公式长下面这个样子：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
        
          Q 
         
        
          , 
         
        
          K 
         
        
          , 
         
        
          V 
         
        
          ) 
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
          
          
            Q 
           
           
           
             K 
            
           
             T 
            
           
          
          
           
           
             d 
            
           
             k 
            
           
          
         
        
          ) 
         
        
          V 
         
        
       
         Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">A</span><span class="mord mathdefault">t</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">i</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 2.44833em; vertical-align: -0.93em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.51833em;"><span class="" style="top: -2.25278em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.85722em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord" style="padding-left: 0.833em;"><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.81722em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail" style="min-width: 0.853em; height: 1.08em;"> 
                   <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                    <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.18278em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.93em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span></span></span></span></span></span></p> 
<p>scaled dot-product attention 和 dot-product attention 唯一的区别就是，scaled dot-product attention 有一个缩放因子， 叫</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          1 
         
         
          
          
            d 
           
          
            k 
           
          
         
        
       
      
        \frac{1}{\sqrt{d_k}} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.38311em; vertical-align: -0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.58644em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.862231em;"><span class="svg-align" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord mtight" style="padding-left: 0.833em;"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.34877em; margin-left: 0em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.151229em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -2.82223em;"><span class="pstrut" style="height: 3em;"></span><span class="hide-tail mtight" style="min-width: 0.853em; height: 1.08em;"> 
                   <svg width="400em" height="1.08em" viewbox="0 0 400000 1080" preserveaspectratio="xMinYMin slice"> 
                    <path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,
-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,
-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,
35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,
-221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467
s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422
s-65,47,-65,47z M834 80H400000v40H845z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.177769em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>。<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 表示 Key 的维度，默认用 64。</p> 
<p>论文里对于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 的作用这么来解释：对于 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 很大的时候，点积得到的结果维度很大，使得结果处于softmax函数梯度很小的区域。这时候除以一个缩放因子，可以一定程度上减缓这种情况。</p> 
<p>scaled dot-product attention 的结构图如下所示。</p> 
<center> 
 <img src="https://images2.imgbox.com/3e/25/R5otvfMo_o.png" height="300/"> 
</center> 
<p>现在来说下 K、Q、V 分别代表什么。当然我们在上文的Self-attention is all you need中已经说明了，相信你也应该理解了它们的含义，但是在Transformer的完整架构中，还存在一些处理的细节，请看完下面的内容：</p> 
<ul><li>在 encoder 的 self-attention 中，Q、K、V 都来自同一个地方，它们是上一层 encoder 的输出。对于第一层 encoder，它们就是 word embedding 和 positional encoding 相加得到的输入。</li><li>在 decoder 的 self-attention 中，Q、K、V 也是自于同一个地方，它们是上一层 decoder 的输出。对于第一层 decoder，同样也是 word embedding 和 positional encoding 相加得到的输入。但是对于 decoder，我们不希望它能获得下一个 time step (即将来的信息，不想让他看到它要预测的信息)，因此我们需要进行 sequence masking。</li><li>在 encoder-decoder attention 中，Q 来自于 decoder 的上一层的输出，K 和 V 来自于 encoder 的输出，K 和 V 是一样的。</li><li>Q、K、V 的维度都是一样的，分别用 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           d 
          
         
           Q 
          
         
        
       
         d_Q 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>,<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           d 
          
         
           K 
          
         
        
       
         d_K 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           d 
          
         
           V 
          
         
        
       
         d_V 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.84444em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>来表示</li></ul> 
<p>目前可能描述有有点抽象，不容易理解。结合一些应用来说，比如，如果是在自动问答任务中的话，Q 可以代表答案的词向量序列，取 K = V 为问题的词向量序列，那么输出就是所谓的 Aligned Question Embedding。</p> 
<p>Google 论文的主要贡献之一是它表明了内部注意力在机器翻译 (甚至是一般的Seq2Seq任务）的序列编码上是相当重要的，而之前关于 Seq2Seq 的研究基本都只是把注意力机制用在解码端。</p> 
<br> 
<h5><a id="Scaled_DotProduct_Attention__424"></a>Scaled Dot-Product Attention 代码实现</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">class</span> <span class="token class-name">ScaledDotProductAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Scaled dot-product attention mechanism."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> attention_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ScaledDotProductAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>attention_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        前向传播.
        Args:
        	q: Queries张量，形状为[B, L_q, D_q]
        	k: Keys张量，形状为[B, L_k, D_k]
        	v: Values张量，形状为[B, L_v, D_v]，一般来说就是k
        	scale: 缩放因子，一个浮点标量
        	attn_mask: Masking张量，形状为[B, L_q, L_k]

        Returns:
        	上下文张量和attention张量
        """</span>
        attention <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> scale<span class="token punctuation">:</span>
            attention <span class="token operator">=</span> attention <span class="token operator">*</span> scale
        <span class="token keyword">if</span> attn_mask<span class="token punctuation">:</span>
            <span class="token comment"># 给需要 mask 的地方设置一个负无穷</span>
            attention <span class="token operator">=</span> attention<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">,</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>inf<span class="token punctuation">)</span>
	<span class="token comment"># 计算softmax</span>
        attention <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>
	<span class="token comment"># 添加dropout</span>
        attention <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>
	<span class="token comment"># 和V做点积</span>
        context <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attention<span class="token punctuation">,</span> v<span class="token punctuation">)</span>
        <span class="token keyword">return</span> context<span class="token punctuation">,</span> attention
</code></pre> 
<br> 
<h5><a id="Multihead_attention__472"></a>Multi-head attention 代码实现</h5> 
<p>在上文中，我们已经详细说过<strong>Multi-head attention</strong>了，因此这里就不再陈述了，我们仅仅给出代码实现。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dim_per_head <span class="token operator">=</span> model_dim <span class="token operator">//</span> num_heads
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>linear_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim_per_head <span class="token operator">*</span> num_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim_per_head <span class="token operator">*</span> num_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim_per_head <span class="token operator">*</span> num_heads<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dot_product_attention <span class="token operator">=</span> ScaledDotProductAttention<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_final <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> model_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
	
        <span class="token comment"># multi-head attention之后需要做layer norm</span>
        self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>model_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> query<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token comment"># 残差连接</span>
        residual <span class="token operator">=</span> query
        dim_per_head <span class="token operator">=</span> self<span class="token punctuation">.</span>dim_per_head
        num_heads <span class="token operator">=</span> self<span class="token punctuation">.</span>num_heads
        batch_size <span class="token operator">=</span> key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># linear projection</span>
        key <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_k<span class="token punctuation">(</span>key<span class="token punctuation">)</span>
        value <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_v<span class="token punctuation">(</span>value<span class="token punctuation">)</span>
        query <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_q<span class="token punctuation">(</span>query<span class="token punctuation">)</span>

        <span class="token comment"># split by heads</span>
        key <span class="token operator">=</span> key<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dim_per_head<span class="token punctuation">)</span>
        value <span class="token operator">=</span> value<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dim_per_head<span class="token punctuation">)</span>
        query <span class="token operator">=</span> query<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> num_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dim_per_head<span class="token punctuation">)</span>

        <span class="token keyword">if</span> attn_mask<span class="token punctuation">:</span>
            attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>num_heads<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># scaled dot product attention</span>
        scale <span class="token operator">=</span> <span class="token punctuation">(</span>key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        context<span class="token punctuation">,</span> attention <span class="token operator">=</span> self<span class="token punctuation">.</span>dot_product_attention<span class="token punctuation">(</span>
          query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> scale<span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>

        <span class="token comment"># concat heads</span>
        context <span class="token operator">=</span> context<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dim_per_head <span class="token operator">*</span> num_heads<span class="token punctuation">)</span>

        <span class="token comment"># final linear projection</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_final<span class="token punctuation">(</span>context<span class="token punctuation">)</span>

        <span class="token comment"># dropout</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token comment"># add residual and norm layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>residual <span class="token operator">+</span> output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention
</code></pre> 
<p>代码中用到了residual connect和 Layer normalization，我们接下来就来讲讲。</p> 
<br> 
<h5><a id="Resnet_541"></a>Resnet</h5> 
<center> 
 <img src="https://images2.imgbox.com/f0/1a/ZCcvucaa_o.png" height="300/"> 
</center> 
<p>先抛出一个常规的神经网络结构，如上图所示。</p> 
<center> 
 <img src="https://images2.imgbox.com/54/de/ob9E29gs_o.png" height="300/"> 
</center> 
<p>和常规的神经网络结构不同的是，ResNet 引入了残差网络结构(一个shortcht)，通过残差网络，可以把网络层弄的很深，据说可以达到了1000多层，最终的网络分类的效果也是非常好，残差网络的基本结构如上图所示。</p> 
<p>通过增加一个 shortcut(也称恒等映射)，而不是简单的堆叠网络层，将原始所需要学习的函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        H(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span> 转换成 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         F 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         + 
        
       
         x 
        
       
      
        F(x)+x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span> 。这样可以解决网络由于很深出现梯度消失的问题，从而可以把网络做的很深。</p> 
<blockquote> 
 <p>这里我们就不深入了，想了解更多的请看这篇文章：👉 <a href="https://blog.csdn.net/qq_44766883/article/details/111997122">详解ResNet(深度残差网络)</a></p> 
</blockquote> 
<br> 
<h5><a id="Layer_normalization_559"></a>Layer normalization</h5> 
<blockquote> 
 <p>Normalization 有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为 0 方差为 1 的数据。我们在把数据送入激活函数之前进行 normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。</p> 
</blockquote> 
<p>说到 normalization，那就肯定得提到 Batch Normalization。</p> 
<p>BN 的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。</p> 
<p>BN 的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：</p> 
<center> 
 <img src="https://images2.imgbox.com/39/bb/MFZHZ64X_o.png" height="300/"> 
</center> 
<p>可以看到，右半边求均值是<strong>沿着数据 batch N 的方向进行的</strong>！</p> 
<p>Batch normalization 的计算公式如下：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          B 
         
        
          N 
         
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          ) 
         
        
          = 
         
        
          α 
         
        
          × 
         
         
          
           
           
             x 
            
           
             i 
            
           
          
            − 
           
           
           
             u 
            
           
             b 
            
           
          
          
           
            
            
              σ 
             
            
              B 
             
            
              2 
             
            
           
             + 
            
           
             ϵ 
            
           
          
         
        
          + 
         
        
          β 
         
        
       
         BN(x_i) = \alpha × \frac{x_i - u_b}{\sqrt{\sigma_B^2 + \epsilon}} + \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.05017em;">B</span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 2.39033em; vertical-align: -1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.26033em;"><span class="" style="top: -2.17381em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.936189em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mord" style="padding-left: 1em;"><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.795908em;"><span class="" style="top: -2.40647em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05017em;">B</span></span></span><span class="" style="top: -3.0448em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.293531em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathdefault">ϵ</span></span></span><span class="" style="top: -2.89619em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
                   <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
                    <path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.303811em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.13em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span></span></p> 
<p>那么什么是 Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是<strong>在每一个样本上计算均值和方差，而不是 BN 那种在批方向计算均值和方差</strong>！</p> 
<p>下面是 LN 的示意图：</p> 
<center> 
 <img src="https://images2.imgbox.com/49/ea/G91embbv_o.png"> 
</center> 
<p>和上面的 BN 示意图一比较就可以看出二者的区别啦！</p> 
<p>下面看一下 LN 的公式：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          L 
         
        
          N 
         
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          ) 
         
        
          = 
         
        
          α 
         
        
          × 
         
         
          
           
           
             x 
            
           
             i 
            
           
          
            − 
           
           
           
             u 
            
           
             L 
            
           
          
          
           
            
            
              σ 
             
            
              L 
             
            
              2 
             
            
           
             + 
            
           
             ϵ 
            
           
          
         
        
          + 
         
        
          β 
         
        
       
         LN(x_i) = \alpha × \frac{x_i - u_L}{\sqrt{\sigma_L^2 + \epsilon}} + \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right: 0.10903em;">N</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathdefault" style="margin-right: 0.0037em;">α</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 2.39033em; vertical-align: -1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.26033em;"><span class="" style="top: -2.17381em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.936189em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mord" style="padding-left: 1em;"><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.795908em;"><span class="" style="top: -2.40647em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span><span class="" style="top: -3.0448em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.293531em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord mathdefault">ϵ</span></span></span><span class="" style="top: -2.89619em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
                   <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
                    <path d="M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,
158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067
c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,
175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71
c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,
-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26
s76,-59,76,-59s76,-60,76,-60z M1001 80H40000v40H1012z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.303811em;"><span class=""></span></span></span></span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.13em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.05278em;">β</span></span></span></span></span></span></p> 
<br> 
<h5><a id="Mask_593"></a>Mask</h5> 
<p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p> 
<p>其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</p> 
<h6><a id="Padding_Mask_599"></a>Padding Mask</h6> 
<p>什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。因为这些填充的位置，其实是没什么意义的，所以我们的 attention 机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p> 
<p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p> 
<p>而我们的 padding mask 实际上是一个张量，每个值都是一个 Boolean，值为 false 的地方就是我们要进行处理的地方。</p> 
<p>实现：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">padding_mask</span><span class="token punctuation">(</span>seq_k<span class="token punctuation">,</span> seq_q<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># seq_k 和 seq_q 的形状都是 [B,L]</span>
    len_q <span class="token operator">=</span> seq_q<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># `PAD` is 0</span>
    pad_mask <span class="token operator">=</span> seq_k<span class="token punctuation">.</span>eq<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    pad_mask <span class="token operator">=</span> pad_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> len_q<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shape [B, L_q, L_k]</span>
    <span class="token keyword">return</span> pad_mask
</code></pre> 
<h6><a id="Sequence_mask_619"></a>Sequence mask</h6> 
<p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p> 
<p>那么具体怎么做呢？也很简单：<strong>产生一个上三角矩阵，上三角的值全为 1，下三角的值权威0，对角线也是 0</strong>。把这个矩阵作用在每一个序列上，就可以达到我们的目的啦。</p> 
<p>具体的代码实现如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">sequence_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> seq<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># [B, L, L]</span>
    <span class="token keyword">return</span> mask
</code></pre> 
<p>效果如下，</p> 
<center> 
 <img src="https://images2.imgbox.com/f8/4e/9dfSnifO_o.png" height="300/"> 
</center> 
<ul><li>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个 mask 相加作为attn_mask。</li><li>其他情况，attn_mask 一律等于 padding mask。</li></ul> 
<br> 
<p><span id="position_code"></span></p> 
<h5><a id="Positional_Embedding_648"></a>Positional Embedding</h5> 
<p>现在的 Transformer 架构还没有提取序列顺序的信息，这个信息对于序列而言非常重要，如果缺失了这个信息，可能我们的结果就是：所有词语都对了，但是无法组成有意义的语句。</p> 
<p>为了解决这个问题。论文使用了 Positional Embedding：对序列中的词语出现的位置进行编码。</p> 
<p>这一部分，我们上面已经仔细介绍过了，就不再细说了,这里仅给出实现。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> max_seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""初始化。
        Args:
            d_model: 一个标量。模型的维度，论文默认是512
            max_seq_len: 一个标量。文本序列的最大长度
        """</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 根据论文给的公式，构造出PE矩阵</span>
        position_encoding <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>
          <span class="token punctuation">[</span>pos <span class="token operator">/</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token operator">*</span> <span class="token punctuation">(</span>j <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">]</span>
          <span class="token keyword">for</span> pos <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_seq_len<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 偶数列使用sin，奇数列使用cos</span>
        position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 在PE矩阵的第一行，加上一行全是0的向量，代表这`PAD`的positional encoding</span>
        <span class="token comment"># 在word embedding中也经常会加上`UNK`，代表位置单词的word embedding，两者十分类似</span>
        <span class="token comment"># 那么为什么需要这个额外的PAD的编码呢？很简单，因为文本序列的长度不一，我们需要对齐，</span>
        <span class="token comment"># 短的序列我们使用0在结尾补全，我们也需要这些补全位置的编码，也就是`PAD`对应的位置编码</span>
        pad_row <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">]</span><span class="token punctuation">)</span>
        position_encoding <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>pad_row<span class="token punctuation">,</span> position_encoding<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 嵌入操作，+1是因为增加了`PAD`这个补全位置的编码，</span>
        <span class="token comment"># Word embedding中如果词典增加`UNK`，我们也需要+1。看吧，两者十分相似</span>
        self<span class="token punctuation">.</span>position_encoding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>max_seq_len <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>position_encoding<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>position_encoding<span class="token punctuation">,</span>
                                                     requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""神经网络的前向传播。

        Args:
          input_len: 一个张量，形状为[BATCH_SIZE, 1]。每一个张量的值代表这一批文本序列中对应的长度。

        Returns:
          返回这一批序列的位置编码，进行了对齐。
        """</span>
        
        <span class="token comment"># 找出这一批序列的最大长度</span>
        max_len <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>input_len<span class="token punctuation">)</span>
        tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>LongTensor <span class="token keyword">if</span> input_len<span class="token punctuation">.</span>is_cuda <span class="token keyword">else</span> torch<span class="token punctuation">.</span>LongTensor
        <span class="token comment"># 对每一个序列的位置进行对齐，在原序列位置的后面补上0</span>
        <span class="token comment"># 这里range从1开始也是因为要避开PAD(0)的位置</span>
        input_pos <span class="token operator">=</span> tensor<span class="token punctuation">(</span>
          <span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token builtin">len</span> <span class="token keyword">in</span> input_len<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>position_encoding<span class="token punctuation">(</span>input_pos<span class="token punctuation">)</span>
</code></pre> 
<br> 
<h5><a id="Positionwise_FeedForward_network_715"></a>Position-wise Feed-Forward network</h5> 
<p>这是一个全连接网络，包含两个线性变换和一个非线性函数(实际上就是 ReLU)。公式如下</p> 
<center> 
 <img src="https://images2.imgbox.com/9d/fb/S0vUIpBM_o.png"> 
</center> 
<p>这个线性变换在不同的位置都表现地一样，并且在不同的层之间使用不同的参数。</p> 
<p><strong>这里实现上用到了两个一维卷积。</strong></p> 
<p>实现如下:</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalWiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalWiseFeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>w2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>ffn_dim<span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer_norm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>model_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>w2<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w1<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># add residual and norm layer</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer_norm<span class="token punctuation">(</span>x <span class="token operator">+</span> output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
</code></pre> 
<br> 
<h4><a id="Transformer_751"></a>Transformer的实现</h4> 
<p>现在可以开始完成 Transformer 模型的构建了，encoder 端和 decoder 端分别都有 6 层，实现如下：</p> 
<p>首先是 encoder 端：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token triple-quoted-string string">"""Encoder的一层。"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> PositionalWiseFeedForward<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token comment"># self attention</span>
        context<span class="token punctuation">,</span> attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> padding_mask<span class="token punctuation">)</span>

        <span class="token comment"># feed forward network</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>context<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention


<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token triple-quoted-string string">"""多层EncoderLayer组成Encoder。"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
               vocab_size<span class="token punctuation">,</span>
               max_seq_len<span class="token punctuation">,</span>
               num_layers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>
               model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
               num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
               ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
               dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
          <span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span>
           <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>seq_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> max_seq_len<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> inputs_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>seq_embedding<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        output <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_embedding<span class="token punctuation">(</span>inputs_len<span class="token punctuation">)</span>

        self_attention_mask <span class="token operator">=</span> padding_mask<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>

        attentions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> encoder <span class="token keyword">in</span> self<span class="token punctuation">.</span>encoder_layers<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> attention <span class="token operator">=</span> encoder<span class="token punctuation">(</span>output<span class="token punctuation">,</span> self_attention_mask<span class="token punctuation">)</span>
            attentions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> attentions
</code></pre> 
<p>然后是 Decoder 端：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> PositionalWiseFeedForward<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
              dec_inputs<span class="token punctuation">,</span>
              enc_outputs<span class="token punctuation">,</span>
              self_attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
              context_attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># self attention, all inputs are decoder inputs</span>
        dec_output<span class="token punctuation">,</span> self_attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
          dec_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> dec_inputs<span class="token punctuation">,</span> self_attn_mask<span class="token punctuation">)</span>

        <span class="token comment"># context attention</span>
        <span class="token comment"># query is decoder's outputs, key and value are encoder's inputs</span>
        dec_output<span class="token punctuation">,</span> context_attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
          enc_outputs<span class="token punctuation">,</span> enc_outputs<span class="token punctuation">,</span> dec_output<span class="token punctuation">,</span> context_attn_mask<span class="token punctuation">)</span>

        <span class="token comment"># decoder's output, or context</span>
        dec_output <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>dec_output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> dec_output<span class="token punctuation">,</span> self_attention<span class="token punctuation">,</span> context_attention


<span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
               vocab_size<span class="token punctuation">,</span>
               max_seq_len<span class="token punctuation">,</span>
               num_layers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>
               model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
               num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
               ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
               dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers

        self<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
          <span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span>
           <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>seq_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> max_seq_len<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> inputs_len<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> context_attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>seq_embedding<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        output <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_embedding<span class="token punctuation">(</span>inputs_len<span class="token punctuation">)</span>

        self_attention_padding_mask <span class="token operator">=</span> padding_mask<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span>
        seq_mask <span class="token operator">=</span> sequence_mask<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        self_attn_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>gt<span class="token punctuation">(</span><span class="token punctuation">(</span>self_attention_padding_mask <span class="token operator">+</span> seq_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

        self_attentions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        context_attentions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> decoder <span class="token keyword">in</span> self<span class="token punctuation">.</span>decoder_layers<span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> context_attn <span class="token operator">=</span> decoder<span class="token punctuation">(</span>
            output<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> self_attn_mask<span class="token punctuation">,</span> context_attn_mask<span class="token punctuation">)</span>
            self_attentions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self_attn<span class="token punctuation">)</span>
            context_attentions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>context_attn<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> self_attentions<span class="token punctuation">,</span> context_attentions


组合一下，就是 Transformer 模型。

<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
               src_vocab_size<span class="token punctuation">,</span>
               src_max_len<span class="token punctuation">,</span>
               tgt_vocab_size<span class="token punctuation">,</span>
               tgt_max_len<span class="token punctuation">,</span>
               num_layers<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>
               model_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
               num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
               ffn_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
               dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> src_max_len<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span>
                               num_heads<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span>tgt_vocab_size<span class="token punctuation">,</span> tgt_max_len<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> model_dim<span class="token punctuation">,</span>
                               num_heads<span class="token punctuation">,</span> ffn_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>model_dim<span class="token punctuation">,</span> tgt_vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src_seq<span class="token punctuation">,</span> src_len<span class="token punctuation">,</span> tgt_seq<span class="token punctuation">,</span> tgt_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
        context_attn_mask <span class="token operator">=</span> padding_mask<span class="token punctuation">(</span>tgt_seq<span class="token punctuation">,</span> src_seq<span class="token punctuation">)</span>

        output<span class="token punctuation">,</span> enc_self_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>src_seq<span class="token punctuation">,</span> src_len<span class="token punctuation">)</span>

        output<span class="token punctuation">,</span> dec_self_attn<span class="token punctuation">,</span> ctx_attn <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>
          tgt_seq<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> output<span class="token punctuation">,</span> context_attn_mask<span class="token punctuation">)</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>output<span class="token punctuation">)</span>

        <span class="token keyword">return</span> output<span class="token punctuation">,</span> enc_self_attn<span class="token punctuation">,</span> dec_self_attn<span class="token punctuation">,</span> ctx_attn
</code></pre> 
<br> 
<h4><a id="Transformer_925"></a>针对Transformer的三个疑惑</h4> 
<p>我在学习的时候，其实还是比较顺的，但是还是有三个疑问不得解，查阅了相关资料后，基本有了了解。</p> 
<ul><li>疑惑一：Transformer的Decoder的输入输出都是什么？</li><li>疑问二：Shifted Right到底是什么？</li><li>疑惑三：Transformer里decoder为什么还需要seq mask？</li></ul> 
<br> 
<h5><a id="TransformerDecoder_937"></a>疑惑一：Transformer的Decoder的输入输出都是什么？</h5> 
<p>以翻译为例：</p> 
<ul><li>输入：我爱中国</li><li>输出： I Love China</li></ul> 
<p>因为输入（“我爱中国”）在Encoder中进行了编码，这里我们具体讨论Decoder的操作，也就是如何得到输出（“L Love China”）的过程。</p> 
<p><strong>Decoder执行步骤</strong></p> 
<p><strong>Time Step 1</strong></p> 
<ul><li> 
  <ul><li>初始输入： 起始符&lt;/s&gt; + Positional Encoding（位置编码）</li><li>中间输入：（我爱中国）Encoder Embedding</li><li>最终输出：产生预测“I”</li></ul> </li></ul> 
<p><strong>Time Step 2</strong></p> 
<ul><li> 
  <ul><li>初始输入：起始符&lt;/s&gt; + “I”+ Positonal Encoding</li><li>中间输入：（我爱中国）Encoder Embedding</li><li>最终输出：产生预测“Love”</li></ul> </li></ul> 
<p><strong>Time Step 3</strong></p> 
<ul><li> 
  <ul><li>初始输入：起始符&lt;/s&gt; + “I”+ “Love”+ Positonal Encoding</li><li>中间输入：（我爱中国）Encoder Embedding</li><li>最终输出：产生预测“China”</li></ul> </li></ul> 
<p>【图示】</p> 
<center> 
 <img src="https://images2.imgbox.com/a2/5f/xFrU8HED_o.gif"> 
</center> 
<br> 
<h5><a id="Shifted_Right_975"></a>疑惑二：Shifted Right到底是什么？</h5> 
<p>操作：整体右移一位（Shifted Right）</p> 
<center> 
 <img src="https://images2.imgbox.com/00/fd/lccSiU0a_o.png"> 
</center> 
<p>细心的同学会发现论文在Decoder的输入上，对Outputs有Shifted Right操作。</p> 
<p>Shifted Right 实质上是给输出添加起始符/结束符，方便预测第一个Token/结束预测过程。</p> 
<p>正常的输出序列位置关系如下：</p> 
<ul><li>0-“I”</li><li>1-“Love”</li><li>2-“China”</li></ul> 
<p>但在执行的过程中，我们在初始输出中添加了起始符，相当于将输出整体右移一位（Shifted Right），所以输出序列变成如下情况：</p> 
<ul><li>0-&lt;/s&gt;【起始符】</li><li>1-“I”</li><li>2-“Love”</li><li>3-“China”</li></ul> 
<p>这样我们就可以通过起始符&lt;/s&gt;预测“I”，也就是通过起始符预测实际的第一个输出。</p> 
<br> 
<h5><a id="_1006"></a>上面两个疑惑的总结</h5> 
<p>Transformer Decoder的输入：</p> 
<ul><li>初始输入：前一时刻Decoder输入+前一时刻Decoder的预测结果 + Positional Encoding</li><li>中间输入：Encoder Embedding</li><li>Shifted Right：在输出前添加起始符，方便预测第一个Token</li></ul> 
<br> 
<h5><a id="Transformerdecoderseq_mask_1018"></a>疑惑三：Transformer里decoder为什么还需要seq mask？</h5> 
<p>Transformer在训练的时候是并行执行的，所以在decoder的第一个sublayer里需要seq mask，其目的就是为了在预测未来数据时把这些未来的数据屏蔽掉，防止数据泄露。如果我们非要去串行执行training，seq mask其实就不需要了。比如说我们用transformer做NMT，训练数据里有一个sample是I love China --&gt;我爱中国。利用串行的思维来想，在训练过程中，我们会</p> 
<ol><li> <p>把I love China输入到encoder里去，利用top encoder最终输出的tensor (size: 1X3X512，假设我们采用的embedding长度为512，而且batch size = 1)作为decoder里每一层用到的k和v；</p> </li><li> <p>将&lt;s&gt;作为decoder的输入，将decoder最终的输出和‘我’做cross entropy计算error。</p> </li><li> <p>将&lt;s&gt;，我作为decoder的输入，将decoder最终:输出的最后一个prob. vector和‘爱’做cross entropy计算error。</p> </li><li> <p>将&lt;s&gt;，我，爱 作为decoder的输入，将decoder最终的输出的最后一个prob. vector和‘中’做cross entropy计算error。</p> </li><li> <p>将&lt;s&gt;，我，爱，中 作为decoder的输入，将decoder最终的输出的最后一个prob. vector和‘国’做cross entropy计算error。</p> </li><li> <p>将&lt;s&gt;，我，爱，中，国 作为decoder的输入，将decoder最终的输出的最后一个prob. vector和&lt;/s&gt;做cross entropy计算error。</p> </li></ol> 
<p>2-6里都可以不用seq mask。</p> 
<p>而在transformer实际的training过程中，我们是并行地将2-6在一步中完成，即</p> 
<ol start="7"><li>将&lt;s&gt;，我，爱，中，国 作为decoder的输入，将decoder最终输出的5个prob. vector和我，爱，中，国，&lt;/s&gt;分别做cross entropy计算error。</li></ol> 
<p>比如要想在7中计算第一个prob. vector的整个过程中，都不用到‘我’及其后面字的信息，就必需seq mask。对所有位置的输入，情况都是如此。</p> 
<p>但是，仔细想想，7虽然包括了2-6，不过有一点区别。比如对3来说，我们是可以不用seq mask的，这时 &lt;s&gt;所对应的encoder output是会利用’我’里的信息的；而在并行时，seq mask是必需的，这时&lt;s&gt;所对应的encoder output是不会利用’我’里的信息的。</p> 
<p>如此一来，我们可以看到，在transformer训练时，由于是并行计算，decoder的第i个输入只能用到i，i-1，…, 0这些位置上输入的信息；当训练完成后，在实际预测过程中，虽然理论上decoder的第i个输入可以用到所有位置上输入的信息，但是由于模型在训练过程中是按照前述方式训练的，所以继续使用seq mask会和训练方式匹配，得到更好的预测结果。</p> 
<p>我感觉从理论上看，按照串行方式1-6来训练并且不用seq mask，我们可以把信息用得更足一些，似乎可能模型的效果会好一点，但是计算效率比transformer的并行训练差太多，最终综合来看应该还是并行的综合效果好。</p> 
<br> 
<h4><a id="_1052"></a>参考</h4> 
<p>👉 <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need</a></p> 
<p>👉 <a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解Transformer（完整版）</a></p> 
<p>👉 <a href="https://zhuanlan.zhihu.com/p/47812375" rel="nofollow">[整理] 聊聊 Transformer</a></p> 
<p>👉 <a href="https://baijiahao.baidu.com/s?id=1651219987457222196&amp;wfr=spider&amp;for=pc" rel="nofollow">Transformer 模型详解</a></p> 
<p>👉 <a href="https://zhuanlan.zhihu.com/p/48508221" rel="nofollow">详解Transformer （Attention Is All You Need）</a></p> 
<p>👉 <a href="https://www.jianshu.com/p/ef41302edeef" rel="nofollow">神经机器翻译 之 谷歌 transformer 模型</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8b63d73f00282ceb1e7cbf8b34a498b1/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">QCustomPlot系列： 实现X轴，Y轴的单独滚轮缩放数据</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/737ed79999087fa4be8027efc6d86e67/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数控车椭圆编程实例带图_车铣加工编程、车铣复合实例讲解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>