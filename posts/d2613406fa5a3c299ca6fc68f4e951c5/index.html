<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Nebula Importer 数据导入实践 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Nebula Importer 数据导入实践" />
<meta property="og:description" content="本文首发于 Nebula Graph Community 公众号
前言 Nebula 目前作为较为成熟的产品，已经有着很丰富的生态。数据导入的维度而言就已经提供了多种选择。有大而全的Nebula Exchange，小而精简的Nebula Importer, 还有为 Spark / Flink 引擎提供的Nebula Spark Connector 和 Nebula Flink Connector。
在众多的导入方式中，究竟哪种较为方便呢？
使用场景介绍：
Nebula Exchange 需要将来自 Kafka、Pulsar 平台的流式数据, 导入 Nebula Graph 数据库需要从关系型数据库（如 MySQL）或者分布式文件系统（如 HDFS）中读取批式数据需要将大批量数据生成 Nebula Graph 能识别的 SST 文件 Nebula Importer Importer 适用于将本地 CSV 文件的内容导入至 Nebula Graph 中 Nebula Spark Connector: 在不同的 Nebula Graph 集群之间迁移数据在同一个 Nebula Graph 集群内不同图空间之间迁移数据Nebula Graph 与其他数据源之间迁移数据结合 Nebula Algorithm 进行图计算 Nebula Flink Connector 在不同的 Nebula Graph 集群之间迁移数据在同一个 Nebula Graph 集群内不同图空间之间迁移数据Nebula Graph 与其他数据源之间迁移数据 以上摘自 Nebula 官方文档：https://docs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/d2613406fa5a3c299ca6fc68f4e951c5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-04T17:51:41+08:00" />
<meta property="article:modified_time" content="2022-07-04T17:51:41+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Nebula Importer 数据导入实践</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>本文首发于 <strong><a href="https://c1n.cn/zq4FN" rel="nofollow">Nebula Graph Community 公众号</a></strong></p> 
</blockquote> 
<h3><a id="_3"></a>前言</h3> 
<p>Nebula 目前作为较为成熟的产品，已经有着很丰富的生态。数据导入的维度而言就已经提供了多种选择。有大而全的<a href="https://docs.nebula-graph.com.cn/3.1.0/nebula-exchange/about-exchange/ex-ug-what-is-exchange/" rel="nofollow">Nebula Exchange</a>，小而精简的<a href="https://docs.nebula-graph.com.cn/3.1.0/nebula-importer/use-importer/" rel="nofollow">Nebula Importer</a>, 还有为 Spark / Flink 引擎提供的<a href="https://docs.nebula-graph.com.cn/3.1.0/nebula-spark-connector/" rel="nofollow">Nebula Spark Connector</a> 和 <a href="https://docs.nebula-graph.com.cn/3.1.0/nebula-flink-connector/" rel="nofollow">Nebula Flink Connector</a>。</p> 
<p>在众多的导入方式中，究竟哪种较为方便呢？</p> 
<p>使用场景介绍：</p> 
<ul><li>Nebula Exchange 
  <ul><li>需要将来自 Kafka、Pulsar 平台的流式数据, 导入 Nebula Graph 数据库</li><li>需要从关系型数据库（如 MySQL）或者分布式文件系统（如 HDFS）中读取批式数据</li><li>需要将大批量数据生成 Nebula Graph 能识别的 SST 文件</li></ul> </li><li>Nebula Importer 
  <ul><li>Importer 适用于将本地 CSV 文件的内容导入至 Nebula Graph 中</li></ul> </li><li>Nebula Spark Connector: 
  <ul><li>在不同的 Nebula Graph 集群之间迁移数据</li><li>在同一个 Nebula Graph 集群内不同图空间之间迁移数据</li><li>Nebula Graph 与其他数据源之间迁移数据</li><li>结合 Nebula Algorithm 进行图计算</li></ul> </li><li>Nebula Flink Connector 
  <ul><li>在不同的 Nebula Graph 集群之间迁移数据</li><li>在同一个 Nebula Graph 集群内不同图空间之间迁移数据</li><li>Nebula Graph 与其他数据源之间迁移数据</li></ul> </li></ul> 
<p>以上摘自 Nebula 官方文档：<a href="https://docs.nebula-graph.com.cn/2.6.2/1.introduction/1.what-is-nebula-graph/" rel="nofollow">https://docs.nebula-graph.com.cn/2.6.2/1.introduction/1.what-is-nebula-graph/</a></p> 
<p>总体来说，Exchange 大而全，可以和大部分的存储引擎结合，导入到 Nebula 中，但是需要部署Spark 环境。</p> 
<p><img src="https://images2.imgbox.com/5d/96/UIUQO4ay_o.png" alt="Nebula Importer 数据导入实践"></p> 
<p>Importer 使用简单，所需依赖较少，但需要自己提前生成数据文件，配置好 schema 一劳永逸，但是不支持断点续传，适合数据量中等。</p> 
<p>Spark / Flink Connector 需要和流数据结合。</p> 
<p><strong>不同的场景选择不同的工具，如果作为新人使用 Nebula 在导入数据时，建议使用 Nebula Importer 工具，简单快速上手</strong>。</p> 
<h3><a id="Nebula_Importer__39"></a>Nebula Importer 的使用</h3> 
<p>在我们接触 Nebula Graph 初期，当时生态不够完善, 加上只有部分业务迁移到 Nebula Graph 上，我们对 Nebula Graph 数据的导入不管全量还是增量都是采用 Hive 表推到 Kafka，消费 Kafka 批量写入 Nebula Graph 的方式。后来随着越来越多的数据和业务切换到 Nebula Graph，导入的数据效率问题愈发严峻，导入时长的增加，使得业务高峰期时仍然在全量的数据导入，这是不可接受的。</p> 
<p>针对以上问题，在尝试 Nebula Spark Connector 和 Nebula Importer 之后，由便于维护和迁移多方面考虑，采用 <code>Hive table -&gt; CSV -&gt; Nebula Server -&gt; Nebula Importer</code> 的方式进行全量的导入，整体耗时时长也有较大的提升。</p> 
<h4><a id="Nebula_Importor__45"></a>Nebula Importor 的相关配置</h4> 
<h5><a id="_47"></a>系统环境</h5> 
<pre><code class="prism language-shell"><span class="token punctuation">[</span>root@nebula-server-prod-05 importer<span class="token punctuation">]</span><span class="token comment"># lscpu</span>
Architecture:          x86_64
CPU op-mode<span class="token punctuation">(</span>s<span class="token punctuation">)</span>:        <span class="token number">32</span>-bit, <span class="token number">64</span>-bit
Byte Order:            Little Endian
CPU<span class="token punctuation">(</span>s<span class="token punctuation">)</span>:                <span class="token number">16</span>
On-line CPU<span class="token punctuation">(</span>s<span class="token punctuation">)</span> list:   <span class="token number">0</span>-15
Thread<span class="token punctuation">(</span>s<span class="token punctuation">)</span> per core:    <span class="token number">2</span>
Core<span class="token punctuation">(</span>s<span class="token punctuation">)</span> per socket:    <span class="token number">8</span>
Socket<span class="token punctuation">(</span>s<span class="token punctuation">)</span>:             <span class="token number">1</span>
NUMA node<span class="token punctuation">(</span>s<span class="token punctuation">)</span>:          <span class="token number">1</span>
Vendor ID:             GenuineIntel
CPU family:            <span class="token number">6</span>
Model:                 <span class="token number">85</span>
Model name:            Intel<span class="token punctuation">(</span>R<span class="token punctuation">)</span> Xeon<span class="token punctuation">(</span>R<span class="token punctuation">)</span> Platinum 8269CY CPU @ <span class="token number">2</span>.50GHz
Stepping:              <span class="token number">7</span>
CPU MHz:               <span class="token number">2499.998</span>
BogoMIPS:              <span class="token number">4999.99</span>
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              1024K
L3 cache:              36608K
NUMA node0 CPU<span class="token punctuation">(</span>s<span class="token punctuation">)</span>:     <span class="token number">0</span>-15

Disk：SSD
Memory: 128G
</code></pre> 
<h5><a id="_79"></a>集群环境</h5> 
<ul><li>Nebula Version：v2.6.1</li><li>部署方式：RPM</li><li>集群规模：三副本，六节点</li></ul> 
<h5><a id="_85"></a>数据规模</h5> 
<pre><code class="prism language-shell">+---------+--------------------------+-----------+
<span class="token operator">|</span> <span class="token string">"Space"</span> <span class="token operator">|</span> <span class="token string">"vertices"</span>               <span class="token operator">|</span> <span class="token number">559191827</span> <span class="token operator">|</span>
+---------+--------------------------+-----------+
<span class="token operator">|</span> <span class="token string">"Space"</span> <span class="token operator">|</span> <span class="token string">"edges"</span>                  <span class="token operator">|</span> <span class="token number">722490436</span> <span class="token operator">|</span>
+---------+--------------------------+-----------+
</code></pre> 
<h5><a id="Importer__94"></a>Importer 配置</h5> 
<pre><code class="prism language-shell"><span class="token comment"># Graph版本，连接2.x时设置为v2。</span>
version: v2

description: Relation Space <span class="token function">import</span> data

<span class="token comment"># 是否删除临时生成的日志和错误数据文件。</span>
removeTempFiles: <span class="token boolean">false</span>

clientSettings:

  <span class="token comment"># nGQL语句执行失败的重试次数。</span>
  retry: <span class="token number">3</span>

  <span class="token comment"># Nebula Graph客户端并发数。</span>
  concurrency: <span class="token number">5</span>

  <span class="token comment"># 每个Nebula Graph客户端的缓存队列大小。</span>
  channelBufferSize: <span class="token number">1024</span>

  <span class="token comment"># 指定数据要导入的Nebula Graph图空间。</span>
  space: Relation

  <span class="token comment"># 连接信息。</span>
  connection:
    user: root
    password: ******
    address: <span class="token number">10.0</span>.XXX.XXX:9669,10.0.XXX.XXX:9669

  postStart:
    <span class="token comment"># 配置连接Nebula Graph服务器之后，在插入数据之前执行的一些操作。</span>
    commands: <span class="token operator">|</span>

    <span class="token comment"># 执行上述命令后到执行插入数据命令之间的间隔。</span>
    afterPeriod: 1s

  preStop:
    <span class="token comment"># 配置断开Nebula Graph服务器连接之前执行的一些操作。</span>
    commands: <span class="token operator">|</span>

<span class="token comment"># 错误等日志信息输出的文件路径。    </span>
logPath: /mnt/csv_file/prod_relation/err/test.log
<span class="token punctuation">..</span><span class="token punctuation">..</span>
</code></pre> 
<p>由于篇幅 只展示些全局相关的配置，点边相关的配置较多，不再展开，详情可以参考<a href="https://github.com/vesoft-inc/nebula-importer">GitHub</a>。</p> 
<p>设置 Crontab，Hive 生成表之后传输到 Nebula Server，在夜间流量较低的时候跑起 Nebula Importer 任务：</p> 
<pre><code class="prism language-shell"><span class="token number">50</span> 03 <span class="token number">15</span> * * /mnt/csv_file/importer/nebula-importer -config /mnt/csv_file/importer/rel.yaml <span class="token operator">&gt;&gt;</span> /root/rel.log
</code></pre> 
<p>总共耗时 2h，在六点左右完成全量数据的导入。</p> 
<p>部分 log 如下，导入速度最高维持在 <strong>200,000/s</strong> 左右：</p> 
<pre><code class="prism language-shell"><span class="token number">2022</span>/05/15 03:50:11 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">10</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">1952500</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4232us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4582us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">195248.59</span>/s<span class="token punctuation">)</span>
<span class="token number">2022</span>/05/15 03:50:16 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">15</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">2925600</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4421us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4761us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">195039.12</span>/s<span class="token punctuation">)</span>
<span class="token number">2022</span>/05/15 03:50:21 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">20</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">3927400</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4486us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4818us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">196367.10</span>/s<span class="token punctuation">)</span>
<span class="token number">2022</span>/05/15 03:50:26 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">25</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">5140500</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4327us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4653us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">205619.44</span>/s<span class="token punctuation">)</span>
<span class="token number">2022</span>/05/15 03:50:31 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">30</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">6080800</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4431us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4755us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">202693.39</span>/s<span class="token punctuation">)</span>
<span class="token number">2022</span>/05/15 03:50:36 <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> statsmgr.go:62: Tick: Time<span class="token punctuation">(</span><span class="token number">35</span>.00s<span class="token punctuation">)</span>, Finished<span class="token punctuation">(</span><span class="token number">7087200</span><span class="token punctuation">)</span>, Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Read Failed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>, Latency AVG<span class="token punctuation">(</span>4461us<span class="token punctuation">)</span>, Batches Req AVG<span class="token punctuation">(</span>4784us<span class="token punctuation">)</span>, Rows AVG<span class="token punctuation">(</span><span class="token number">202489.00</span>/s<span class="token punctuation">)</span>
</code></pre> 
<p>然后在七点，根据时间戳，重新消费 Kafka 导入当天凌晨到七点的增量数据, 防止 T+1 的全量数据覆盖当天的增量数据。</p> 
<pre><code class="prism language-shell"><span class="token number">50</span> 07 <span class="token number">15</span> * * python3  /mnt/code/consumer_by_time/relation_consumer_by_timestamp.py
</code></pre> 
<p>增量的消费大概耗时 10-15min。</p> 
<h4><a id="_169"></a>实时性</h4> 
<p>根据 MD5 对比之后得到的增量数据，导入Kafka中，实时消费 Kafka 的数据，确保数据的延迟不超过 1 分钟。</p> 
<p>另外长时间的实时可能会有非预期的数据问题出现而未发现，所以每 30 天会导入一次全量数据，上面介绍的 Importer 导入。然后给 Space 的点边添加 TTL=35 天确保未及时更新的数据会被 Filter 和后续回收。</p> 
<h4><a id="_175"></a>一些注意点</h4> 
<p>论坛帖子 <a href="https://discuss.nebula-graph.com.cn/t/topic/361" rel="nofollow">https://discuss.nebula-graph.com.cn/t/topic/361</a> 这里提到了关于 CSV 导入常遇到的问题，大家可以参考下。另外根据经验这边有几点建议：</p> 
<ol><li>关于并发度，问题中有提到，这个 concurrency 指定为你的 cpu cores 就可以, 表示起多少个 client 连接 Nebula Server。 实际操作中，要去 trade off 导入速度和服务器压力的影响。在我们这边测试，如果并发过高，会导致磁盘 IO 过高，引发设置的一些告警，不建议一下把并发拉太高，可以根据实际业务测试下做权衡。</li><li>Importer 并不能断点续传，如果出现错误，需要手动处理。在实际操作中，我们会程序分析 Importer 的 log，根据情况处理，如果哪部分数据出现了非预期的错误，会告警通知，人工介入，防止出现意外。</li><li>Hive 生成表之后传输到 Nebula Server， 这部分任务 实际耗时是和 Hadoop 资源情况密切相关的，有可能会出现资源不够导致 Hive 和 CSV 表生成时间滞缓，而 Importer 正常在跑的情况，这部分需要提前做好预判。我们这边是根据hive任务结束时间和 Importer 任务开始时间做对比，判断是否需要 Importer 的进程正常运行。</li></ol> 
<hr> 
<p>交流图数据库技术？加入 Nebula 交流群请先<a href="https://wj.qq.com/s2/8321168/8e2f/" rel="nofollow">填写下你的 Nebula 名片</a>，Nebula 小助手会拉你进群~~</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/329cc37218b746a1a9a7310caedc6349/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">formData传递数组</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/bd9883f475317b4ba87712f15c25d485/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">UE4蓝图学习篇（四）--流程控制ForLoop和WhileLoop</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>