<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>tidb集群介绍和物理部署 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="tidb集群介绍和物理部署" />
<meta property="og:description" content="TiDB是NewSQL数据库的技术探路者和引领者，本文简要介绍了TiDB的组件TiDB Server、PD调度和TiKV，并使用TiUP搭建了三节点的TiDB集群环境，部署简单便捷。
TiDB介绍 TiDB是PingCAP公司自主设计、研发的开源分布式关系型数据库，是一款同时支持OLTP和OLAP业务的融合型分布式数据库产品，具备水平扩容或者缩容、金融级高可用、实时HTAP、云原生的分布式数据库、同时兼容MySQL 5.7协议和MySQL生态等重要特性。
如图是TiDB整体架构图，可以看到TiDB核心功能组件包括：
计算层TiDB Server，负责与客户端通信，执行SQL解析和优化存储层TiKV和TiFlash，支持key-value键值对存储和列式存储调度层PDCluster，整个集群的大脑，负责分发调度指令给TiKV分析计算模块TiSPARK，与Spark集群进行联动云原生架构，基于K8S实现容器云平台的自动化部署和运维通过TiDB Binlog实现MySQL数据库的实时同步 TiDB执行流程如图所示：
用户的SQL请求会直接或者通过 Load Balancer发送到TiDB ServerTiDB Server会解析MySQL Protocol Packet，获取请求内容，对SQL进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据PD server根据TiKV节点实时上报的数据分布状态，下发数据调度命令给具体的TiKV节点数据全部存储在TiKV集群中， TiKV根据SQL请求返回数据到TiDB Server最后TiDB Server需要将查询结果返回给用户。 TiDB Server TiDB Server是SQL层，对外暴露MySQL协议的连接endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB层本身是无状态的，TiDB Server本身并不存储数据，只是解析 SQL，将实际的数据读取请求翻译成key-value操作转发给底层的存储节点 TiKV（或 TiFlash），最终查询结果返回给客户端。
SQL层架构如下所示：
用户的SQL请求会直接或者通过 Load Balancer发送到TiDB Server，TiDB Server会解析MySQL Protocol Packet，获取请求内容，对SQL进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据。数据全部存储在TiKV集群中，所以在这个过程中TiDB Server需要和TiKV交互，获取数据。最后 TiDB Server需要将查询结果返回给用户。
PD Server PD server是整个TiDB集群的元信息管理模块，负责存储每个TiKV节点实时的数据分布情况和集群的整体拓扑结构，提供TiDB Dashboard管控界面，并为分布式事务分配事务 ID。PD不仅存储元信息，同时还会根据TiKV节点实时上报的数据分布状态，下发数据调度命令给具体的TiKV节点，可以说是整个集群的“大脑”。
调度信息的收集
调度依赖于整个集群信息的收集，简单来说，调度需要知道每个TiKV节点的状态以及每个 Region的状态。TiKV集群会以心跳包的形式定期向PD汇报两类消息，TiKV节点信息和Region信息：
每个TiKV节点会定期向PD汇报节点的状态信息，包括磁盘容量、承载的Region数量、数据写入/读取速度、发送/接收的Snapshot数量、是否过载等每个Raft group leader会定期向PD汇报Region状态信息，包括Leader的位置、Followers的位置、掉线Replica的数量、数据写入/读取速度等 调度的策略
PD在收集到信息后，会根据一些调度策略来制定具体的调度计划：
Region中副本的数量，当发现副本数量不满足Raft算法要求时，需要通过ADD或Remove副本来调整副本数量Raft Group中多个副本不在同一个位置，一般情况下PD只会保证多个副本不会落在同一个节点上，以避免单个副本失效导致多个副本丢失副本在存储之间均匀分配，使得各节点之间承载的数据更均衡Leader数量在存储之间均匀分配，Raft协议要求读写都是在leader上完成，因此PD会尽量保证leader在节点之间分散开访问热点数据在存储间均匀分配，PD会检测出访问热点，并将其在节点之间分散开各个存储的空间使用大致相等，PD在调度时候会考虑节点的存储空间剩余量控制调度速度，避免影响在线服务，调度操作需要耗费CPU、内存、磁盘IO以及网络带宽，PD会对当前正在进行的操作数量进行控制 TiKV存储 TiKV负责存储数据，从外部看TiKV是一个分布式的提供事务的Key-Value存储引擎。存储数据的基本单位是Region，每个Region负责存储一个Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个TiKV节点会负责多个Region。TiKV的API在KV键值对层面提供对分布式事务的原生支持，默认提供了SI (Snapshot Isolation)的隔离级别，这也是TiDB在SQL层面支持分布式事务的核心。TiDB的SQL层做完SQL解析后，会将SQL的执行计划转换为对TiKV API的实际调用。
Key-value键值对" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/37908a43ec109f3087ea128290a52e38/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-12-04T19:42:08+08:00" />
<meta property="article:modified_time" content="2022-12-04T19:42:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">tidb集群介绍和物理部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>TiDB是NewSQL数据库的技术探路者和引领者，本文简要介绍了TiDB的组件TiDB Server、PD调度和TiKV，并使用TiUP搭建了三节点的TiDB集群环境，部署简单便捷。</p> 
<h2 id="tid-nN72rt">TiDB介绍</h2> 
<p>TiDB是PingCAP公司自主设计、研发的开源分布式关系型数据库，是一款同时支持OLTP和OLAP业务的融合型分布式数据库产品，具备水平扩容或者缩容、金融级高可用、实时HTAP、云原生的分布式数据库、同时兼容MySQL 5.7协议和MySQL生态等重要特性。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2a/39/5yhIwpwD_o.png"></p> 
<p>如图是TiDB整体架构图，可以看到TiDB核心功能组件包括：</p> 
<ul><li>计算层TiDB Server，负责与客户端通信，执行SQL解析和优化</li><li>存储层TiKV和TiFlash，支持key-value键值对存储和列式存储</li><li>调度层PDCluster，整个集群的大脑，负责分发调度指令给TiKV</li><li>分析计算模块TiSPARK，与Spark集群进行联动</li><li>云原生架构，基于K8S实现容器云平台的自动化部署和运维</li><li>通过TiDB Binlog实现MySQL数据库的实时同步</li></ul> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/44/b2/XaQWsqfD_o.png"></p> 
<p>TiDB执行流程如图所示：</p> 
<ol><li>用户的SQL请求会直接或者通过 Load Balancer发送到TiDB Server</li><li>TiDB Server会解析MySQL Protocol Packet，获取请求内容，对SQL进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据</li><li>PD server根据TiKV节点实时上报的数据分布状态，下发数据调度命令给具体的TiKV节点</li><li>数据全部存储在TiKV集群中， TiKV根据SQL请求返回数据到TiDB Server</li><li>最后TiDB Server需要将查询结果返回给用户。</li></ol> 
<h3 id="tid-HfWdHH">TiDB Server</h3> 
<p>TiDB Server是SQL层，对外暴露MySQL协议的连接endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB层本身是无状态的，TiDB Server本身并不存储数据，只是解析 SQL，将实际的数据读取请求翻译成key-value操作转发给底层的存储节点 TiKV（或 TiFlash），最终查询结果返回给客户端。</p> 
<p>SQL层架构如下所示：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ab/4c/Fe0Xn6lO_o.png"></p> 
<p>用户的SQL请求会直接或者通过 Load Balancer发送到TiDB Server，TiDB Server会解析MySQL Protocol Packet，获取请求内容，对SQL进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据。数据全部存储在TiKV集群中，所以在这个过程中TiDB Server需要和TiKV交互，获取数据。最后 TiDB Server需要将查询结果返回给用户。</p> 
<h3 id="tid-ckakSr">PD Server</h3> 
<p>PD server是整个TiDB集群的元信息管理模块，负责存储每个TiKV节点实时的数据分布情况和集群的整体拓扑结构，提供TiDB Dashboard管控界面，并为分布式事务分配事务 ID。PD不仅存储元信息，同时还会根据TiKV节点实时上报的数据分布状态，下发数据调度命令给具体的TiKV节点，可以说是整个集群的“大脑”。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/fc/fe/wia6Ukoo_o.png"></p> 
<p><strong>调度信息的收集</strong></p> 
<p>调度依赖于整个集群信息的收集，简单来说，调度需要知道每个TiKV节点的状态以及每个 Region的状态。TiKV集群会以心跳包的形式定期向PD汇报两类消息，TiKV节点信息和Region信息：</p> 
<ul><li>每个TiKV节点会定期向PD汇报节点的状态信息，包括磁盘容量、承载的Region数量、数据写入/读取速度、发送/接收的Snapshot数量、是否过载等</li><li>每个Raft group leader会定期向PD汇报Region状态信息，包括Leader的位置、Followers的位置、掉线Replica的数量、数据写入/读取速度等</li></ul> 
<p><strong>调度的策略</strong></p> 
<p>PD在收集到信息后，会根据一些调度策略来制定具体的调度计划：</p> 
<ul><li>Region中副本的数量，当发现副本数量不满足Raft算法要求时，需要通过ADD或Remove副本来调整副本数量</li><li>Raft Group中多个副本不在同一个位置，一般情况下PD只会保证多个副本不会落在同一个节点上，以避免单个副本失效导致多个副本丢失</li><li>副本在存储之间均匀分配，使得各节点之间承载的数据更均衡</li><li>Leader数量在存储之间均匀分配，Raft协议要求读写都是在leader上完成，因此PD会尽量保证leader在节点之间分散开</li><li>访问热点数据在存储间均匀分配，PD会检测出访问热点，并将其在节点之间分散开</li><li>各个存储的空间使用大致相等，PD在调度时候会考虑节点的存储空间剩余量</li><li>控制调度速度，避免影响在线服务，调度操作需要耗费CPU、内存、磁盘IO以及网络带宽，PD会对当前正在进行的操作数量进行控制</li></ul> 
<h3 id="tid-TGrJdf">TiKV存储</h3> 
<p>TiKV负责存储数据，从外部看TiKV是一个分布式的提供事务的Key-Value存储引擎。存储数据的基本单位是Region，每个Region负责存储一个Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个TiKV节点会负责多个Region。TiKV的API在KV键值对层面提供对分布式事务的原生支持，默认提供了SI (Snapshot Isolation)的隔离级别，这也是TiDB在SQL层面支持分布式事务的核心。TiDB的SQL层做完SQL解析后，会将SQL的执行计划转换为对TiKV API的实际调用。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/94/3c/4psbbFez_o.png"></p> 
<p><strong>Key-value键值对</strong></p> 
<p>TiDB采用Key-Value模型存储数据，key-value键值对是一个巨大的map，这个map会按照key的二进制顺序有序排列。</p> 
<p><strong>存储引擎RocksDB</strong></p> 
<p>TiDB没有直接向磁盘上写数据，而是把数据保存在RocksDB中，具体的数据落地由RocksDB负责。RocksDB 是由Facebook开源的一个非常优秀的单机KV存储引擎，可以满足TiKV对单机引擎的各种要求。这里可以简单的认为RocksDB是一个单机的持久化Key-Value Map。</p> 
<p><strong>Region</strong></p> 
<p>TiKV将整个key-value空间分成很多段，每一段是连续的key，这些段称为region，目前TiKV中默认Region大小是96MB。TiKV会以region为单位，将数据分散在集群的所有节点上，并且保证每个节点上服务的Region数量差不多，并且以Region为单位做Raft的复制和成员管理。这样做有以下好处：</p> 
<ul><li>TiDB中的PD调度系统会将Region数据尽可能的分布在不同的节点上，可以实现存储数据的水平扩展和负载均衡。同时PD也会记录Region在节点中的分布情况，也就是通过任意一个key就可以查到这个key在哪个region上</li><li>TiKV以Region为单位做数据的复制，这样一个Region的数据会保存多个副本，副本之间通过Raft来保证数据的一致性。一个Region的多个副本会保存在不同的节点上，构成一个Raft Group。其中一个副本会作为这个Group的Leader，其他的副本作为Follower。默认情况下，所有的读和写都是通过Leader进行，读操作在Leader上即可完成，而写操作再由Leader复制给Follower。</li></ul> 
<p><strong>多版本控制MVCC</strong></p> 
<p>TiKV的MVCC是通过在key后面添加版本号实现的，没有MVCC之前，可以把TiKV看做这样的：</p> 
<pre><code>Key1 -&gt; Value
Key2 -&gt; Value
……
KeyN -&gt; Value</code></pre> 
<p>有了MVCC之后，TiKV的Key排列是这样的：</p> 
<pre><code>Key1_Version3 -&gt; Value
Key1_Version2 -&gt; Value
Key1_Version1 -&gt; Value
……
Key2_Version4 -&gt; Value
Key2_Version3 -&gt; Value
Key2_Version2 -&gt; Value
Key2_Version1 -&gt; Value
……
KeyN_Version2 -&gt; Value
KeyN_Version1 -&gt; Value
……</code></pre> 
<h2 id="tid-KdNcDY">TiDB集群环境部署</h2> 
<h3 id="tid-6ai2xF">环境准备</h3> 
<p>1、服务器列表</p> 
<p>TiDB集群最小部署环境需要3个节点，这里将TiDB Server、PD Server和TiKV放在一台虚机上共3台主机，另外一个主机部署控制台和监控节点。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/76/1f/94D9fzGL_o.png"></p> 
<p>TiDB默认端口信息如下：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/0b/6a/4QTSKYwO_o.png"></p> 
<p>注：我们机器不够，都部署在一台机器上，生产环境是不允许的。</p> 
<p>2、检测及安装NTP</p> 
<p>TiDB 是一套分布式数据库系统，需要节点间保证时间的同步，从而确保 ACID 模型的事务线性一致性。目前解决授时的普遍方案是采用 NTP 服务，可以通过互联网中的 pool.ntp.org 授时服务来保证节点的时间同步，也可以使用离线环境自己搭建的 NTP 服务来解决授时。在CentOS 7系统上手动安装 NTP 服务，可执行以下命令：</p> 
<pre><code>yum install ntp ntpdate
systemctl start ntpd.service
systemctl enable ntpd.service
systemctl status ntpd.service</code></pre> 
<p>3、创建用户tidb并设置SUDO免密登录</p> 
<pre><code>useradd -d /home/tidb tidb
passwd tidb</code></pre> 
<p>修改sudo权限</p> 
<pre><code>visudo
## Same thing without a password
# %wheel        ALL=(ALL)       NOPASSWD: ALL
%tidb        ALL=(ALL)       NOPASSWD: ALL</code></pre> 
<h3 id="tid-fyJj83">TiUP离线部署TiDB集群</h3> 
<p>1、下载TiDB安装包</p> 
<p>下载地址为 <a href="https://pingcap.com/download-cn/community/" rel="nofollow" title="TiDB 社区版 | PingCAP">TiDB 社区版 | PingCAP</a>，选择6.4版本</p> 
<p><a href="https://cn.pingcap.com/product-community/#TiDB" rel="nofollow" title="TiDB 社区版 | PingCAP">TiDB 社区版 | PingCAP</a><br><img alt="" height="719" src="https://images2.imgbox.com/44/eb/dKUaApZy_o.png" width="1200"></p> 
<p></p> 
<p>2、将离线包发送到目标集群的中控机后，执行以下命令安装 TiUP 组件：</p> 
<p>参考：<a href="https://www.cnblogs.com/xfeiyun/p/16178686.html" rel="nofollow" title="https://www.cnblogs.com/xfeiyun/p/16178686.html">https://www.cnblogs.com/xfeiyun/p/16178686.html</a><br><br><br><br> 3、tidb 通过br备份到s3存储</p> 
<p>参考：<a href="https://blog.csdn.net/TiDBer/article/details/126523290" title="TiDB BR 备份至 MinIO S3 实战_TiDB 社区干货传送门的博客-CSDN博客">TiDB BR 备份至 MinIO S3 实战_TiDB 社区干货传送门的博客-CSDN博客</a></p> 
<p></p> 
<p>4、日常维护学习</p> 
<p><a href="https://www.modb.pro/db/45952?utm_source=index_ai" rel="nofollow" title="数据库系列之TiDB日常操作维护 - 墨天轮">数据库系列之TiDB日常操作维护 - 墨天轮</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/603de343328389f4e4f54f59aa25834e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【编译原理】6—语法制导翻译Syntax-Directed Translation（SDD、SDT详细介绍）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/33105b120e3541b66f74d953a312e88d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【泛函分析】压缩映射定理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>