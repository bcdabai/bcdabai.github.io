<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络相关的笔试题目集合（一） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络相关的笔试题目集合（一）" />
<meta property="og:description" content=" 在找工作的过程中发现好多公司没有专门的、传统的图像处理岗位，所以只能参加算法类的笔试甚至AI类的笔试。在AI的笔试中几乎全是关于神经网络的问题，其实也都是很基础的一些问题，如果事先做了准备，可以从容应对。而对于我这种从传统图像处理算法向深度学习靠拢的新手，不失为一种很好的入门方法。
既然是考察神经网络，激活函数activation function作为模拟人脑中神经元之间的激活/抑制的关键，经常会被考察。问：常用的激活函数有什么，各自有什么特点。关于这个问题可以从网上找到很多答案，比如参考链接1和2，主要讲了常见的激活函数Sigmoid、tanh、ReLu、Leaky ReLus。Sigmoid范围在0~1（这也是具有压缩数据功能的原因），所以Tanh=2*Sigmoid(2x)-1的范围在[-1,1]，tanh读作Hyperbolic Tangent，解决了Sigmoid的均值非0的问题（这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入），但是梯度消失的问题更加严重（红色梯度曲线下降得更快，参数的学习要靠梯度的后向传播）。而ReLU=max(0,x),在输入很大的情况下梯度也不会像Sigmoid一样饱和，且由于计算简单，利用SGD（随机梯度下降）算法收敛速度更快，但缺点是在输入为复数的时候激活函数直接为0，造成所谓的神经元坏死，不能给之后的神经元传递信息，这时就必须十分小心learning Rate的选取，步长不能太大，也可以使用adagrad自动调节learningrate,同时参数的初始化也很重要，一般使用Xavier初始化。Leaky ReLU是为了解决ReLU神经元坏死的问题的，在输入小于0时，输出不再是0，而是一条斜率较小（a）的通过原点的直线。但是现在还没有足够的理论证明Leaky ReLU一定好于ReLU。也可以将斜率a也作为参数进行训练，即PReLU，kaiming He在2015年的论文中就使用了Parametric ReLU。
Randomized Leaky ReLU. Randomized Leaky ReLU 是 leaky ReLU 的random 版本, 其核心思想就是，在训练过程中，a是从一个高斯分布中随机出来的，然后再在测试过程中进行修正。
这么多的激活函数都有一个共同点，就是他们都是非线性的。抛开用激活函数去模拟神经元的仿生想法，激活函数的作用在数学上到底有什么意义呢？从网上资料看就是为了加入非线性因素。因为感知机其实是最原始的神经网络，同时感知机是一个判别模型，可用于分类，如果没有激活函数，我们就无法实现线性不可分问题，包括简单的异或XOR问题。如果没有激活函数，多个感知机的组合得到的仍然是一个线性分类器，仍然无法解决非线性问题。
参考链接2中知乎有人的回答中提到Google的论文Batch normalization: Accelerating deep network training by reducing internal covariate shift尽可能保证每一层网络的输入具有相同的分布，解决了Sigmoid的saturate的问题，但效果仍然没有ReLU好。正好笔试过程中也遇到了Batch Normalization的问题，那么接下来就看一下它到底为何物。
先来说归一化，归一化是对于输入特征来说的，特征可能是多维的，同时他们之间的量纲可能不一样，就会造成不同维度的大小存在很大的差异，这对于训练来说不太友好，因为我们设置的learning rate不可能适应所有的维度，会导致训练过慢。
归一化不仅能加快训练速度（最快梯度下降法的求解），有时候还可以提高精度。比如KNN分类器中要求样本之间的距离，归一化使得不同维度对距离造成的影响大小相同，而不会使得结果主要取决于某一维度。
归一化常用方法有最大最小归一化（线性归一化），max和min值不稳定时可以取常数；z-score标准化（均值方差标准化），归一化后的数据符合标准正态分布，即进行白化whiten；函数转化（非线性归一化，函数可使用对数、正切等）。
其实之前提到的归一化是相对于机器学习领域说的。在深度学习中由于涉及到了多层的网络模型，归一化会显得更加重要，因为机器学习中有一个数据之间是独立同分布IID的假设（这样才能通过训练样本预测新的数据），而神经网络中由于底层参数的更新会影响之后高层的输入，这在paper中被称为Internal Covariate Shift（ICS）问题。所以我们就需要对每一层的输出都进行归一化。但是底层的输出被归一化也意味着之前学到的特征分布被破坏了，所以batch Normalization（BN）又加入了变换重构，引入了可学习参数缩放因子和平移因子。
这里简单介绍一下batch的概念。首先，我们的训练数据集是有限的，我们会使用这一训练数据集训练多次，相对于巩固学习，但训练太多次又会导致过拟合。每次训练后参数完成一次更新，称这一过程是一个epoch。而在每次epoch中，我们把训练数据集分成几个较小的子集，这就是batch，子集的大小是batch_size。数据集较大不利于通过训练网络，分成子集有利于加快训练。训练数据集分成子集，epoch也分成了几个iteration。根据batch_size的选择，可以把训练情况分成三种，第一种是batch取最大值，即Full Batch Learning，一次性将训练数据通过训练网络，这只适用于训练数据少的情况。第二种是另外一个极端，batch_size取1，即Online Learning在线学习/Stochastic随机学习，这种情况难以收敛，因为每次梯度方向以各自样本的梯度方向收敛。我们看到batch_size其实是在内存利用率和内存容量之间的平衡，增大了batch_size，相对于通过并行化提高了内存利用率，但占用的内存也变大。第三种是Mini-batch，在之前两个极端情况下选择一个平衡点。BN就是基于了Mini-batch SGD。BN适用于mini-batch样本量较大且与各mini-batch分布相近似的场景下（训练前需进行充分的shuffle）。不适用于动态网络结构和RNN。
结合前面讲到的激活函数，BN在归一化的同时还可以解决Sigmoid函数的梯度饱和的问题。因为经过BN后，通过激活函数的输入是标准正态分布，经过非线性变换时的输入大概率保持在0均值附近，反向传播时梯度值较大，从而避免了梯度消失。BN增加scale和shift的原因也可以通过对激活函数的输入的处理来说明。因为在进入激活函数之前进行了归一化，将输入集中在了Sigmoid中心附近，而Sigmoid函数虽然本身是非线性函数，但是中心附近是接近线性的，这样一来我们就失去了非线性函数的优势，而补救措施就是增加scale和shift元素。BN的细节还是要看论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》。
总结一下BN的优势：1.提升了训练速度（避免了梯度双端饱和，保证了IID）2.类似于Dropout，可以防止过拟合（加入了随机性的scale、shift？）3.对初始化的要求没有那么高（因为不必使用ReLU？即使使用ReLU也可以避免梯度为0），可以使用较大的学习率。
除了BN，还有其他的归一化方式。BN针对的是单一的神经元，Layer Normalization考虑一层的信息，并将该层的均值和方差作为归一化标准。weight normalization则将参数也进行了规范化。
Reference：
面面观https://blog.csdn.net/cyh_24/article/details/50593400神经网络激励函数的作用是什么？有没有形象的解释？ - 论智的回答 - 知乎https://www.zhihu.com/question/22334626/answer/465380541https://blog.csdn.net/yangdashi888/article/details/78015448激活函数的作用https://blog.csdn.net/program_developer/article/details/78704224DeeplearningAI实用层面https://www.cnblogs.com/cloud-ken/p/7709447.html归一化优点和方法和结构https://blog.csdn.net/qq_28618765/article/details/78221571Batch_size知乎https://www.zhihu.com/question/32673260深度学习29https://blog.csdn.net/hjimce/article/details/50866313Normalization方式https://blog.csdn.net/qq_24153697/article/details/79880202 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0160d17f8e4c8b520a069b709ae18a7b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-10-15T20:23:43+08:00" />
<meta property="article:modified_time" content="2018-10-15T20:23:43+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络相关的笔试题目集合（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style="margin-left:0cm;">在找工作的过程中发现好多公司没有专门的、传统的图像处理岗位，所以只能参加算法类的笔试甚至AI类的笔试。在AI的笔试中几乎全是关于神经网络的问题，其实也都是很基础的一些问题，如果事先做了准备，可以从容应对。而对于我这种从传统图像处理算法向深度学习靠拢的新手，不失为一种很好的入门方法。</p> 
<p style="margin-left:0cm;">既然是考察神经网络，激活函数activation function作为模拟人脑中神经元之间的激活/抑制的关键，经常会被考察。问：常用的激活函数有什么，各自有什么特点。关于这个问题可以从网上找到很多答案，比如参考链接1和2，主要讲了常见的激活函数Sigmoid、tanh、ReLu、Leaky ReLus。Sigmoid范围在0~1（这也是具有压缩数据功能的原因），所以Tanh=2*Sigmoid(2x)-1的范围在[-1,1]，tanh读作Hyperbolic Tangent，解决了Sigmoid的均值非0的问题（这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入），但是梯度消失的问题更加严重（红色梯度曲线下降得更快，参数的学习要靠梯度的后向传播）。而ReLU=max(0,x),在输入很大的情况下梯度也不会像Sigmoid一样饱和，且由于计算简单，利用SGD（随机梯度下降）算法收敛速度更快，但缺点是在输入为复数的时候激活函数直接为0，造成所谓的神经元坏死，不能给之后的神经元传递信息，这时就必须十分小心learning Rate的选取，步长不能太大，也可以使用adagrad自动调节learningrate,同时参数的初始化也很重要，一般使用Xavier初始化。Leaky ReLU是为了解决ReLU神经元坏死的问题的，在输入小于0时，输出不再是0，而是一条斜率较小（a）的通过原点的直线。但是现在还没有足够的理论证明Leaky ReLU一定好于ReLU。也可以将斜率a也作为参数进行训练，即PReLU，kaiming He在2015年的论文中就使用了Parametric ReLU。</p> 
<p style="margin-left:0cm;"> <img alt="" class="has" height="172" src="https://images2.imgbox.com/df/36/N1PxboWO_o.jpg" width="260"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="180" src="https://images2.imgbox.com/c9/3a/7L3LBXPZ_o.jpg" width="273"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="186" src="https://images2.imgbox.com/3c/0e/qXIc3Od5_o.jpg" width="282"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="158" src="https://images2.imgbox.com/df/eb/7wGePUrY_o.png" width="445"></p> 
<p style="margin-left:0cm;">Randomized Leaky ReLU. Randomized Leaky ReLU 是 leaky ReLU 的random 版本, 其核心思想就是，在训练过程中，a是从一个高斯分布中随机出来的，然后再在测试过程中进行修正。</p> 
<p style="margin-left:0cm;">这么多的激活函数都有一个共同点，就是他们都是非线性的。抛开用激活函数去模拟神经元的仿生想法，激活函数的作用在数学上到底有什么意义呢？从网上资料看就是为了加入非线性因素。因为感知机其实是最原始的神经网络，同时感知机是一个判别模型，可用于分类，如果没有激活函数，我们就无法实现线性不可分问题，包括简单的异或XOR问题。如果没有激活函数，多个感知机的组合得到的仍然是一个线性分类器，仍然无法解决非线性问题。</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="148" src="https://images2.imgbox.com/c9/54/Gdmdwxkb_o.png" width="370"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="192" src="https://images2.imgbox.com/cf/8b/gK3ipv2U_o.jpg" width="381"></p> 
<p style="margin-left:0cm;">参考链接2中知乎有人的回答中提到Google的论文Batch normalization: Accelerating deep network training by reducing internal covariate shift尽可能保证每一层网络的输入具有相同的分布，解决了Sigmoid的saturate的问题，但效果仍然没有ReLU好。正好笔试过程中也遇到了Batch Normalization的问题，那么接下来就看一下它到底为何物。</p> 
<p style="margin-left:0cm;">先来说归一化，归一化是对于输入特征来说的，特征可能是多维的，同时他们之间的量纲可能不一样，就会造成不同维度的大小存在很大的差异，这对于训练来说不太友好，因为我们设置的learning rate不可能适应所有的维度，会导致训练过慢。</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="238" src="https://images2.imgbox.com/a2/29/xR1iSmhe_o.png" width="332"></p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="217" src="https://images2.imgbox.com/a5/0a/5E9fMXm0_o.png" width="336"></p> 
<p style="margin-left:0cm;">归一化不仅能加快训练速度（最快梯度下降法的求解），有时候还可以提高精度。比如KNN分类器中要求样本之间的距离，归一化使得不同维度对距离造成的影响大小相同，而不会使得结果主要取决于某一维度。</p> 
<p style="margin-left:0cm;">归一化常用方法有最大最小归一化（线性归一化），max和min值不稳定时可以取常数；z-score标准化（均值方差标准化），归一化后的数据符合标准正态分布，即进行白化whiten；函数转化（非线性归一化，函数可使用对数、正切等）。</p> 
<p style="margin-left:0cm;">其实之前提到的归一化是相对于机器学习领域说的。在深度学习中由于涉及到了多层的网络模型，归一化会显得更加重要，因为机器学习中有一个数据之间是独立同分布IID的假设（这样才能通过训练样本预测新的数据），而神经网络中由于底层参数的更新会影响之后高层的输入，这在paper中被称为Internal Covariate Shift（ICS）问题。所以我们就需要对每一层的输出都进行归一化。但是底层的输出被归一化也意味着之前学到的特征分布被破坏了，所以batch Normalization（BN）又加入了变换重构，引入了可学习参数缩放因子和平移因子。</p> 
<p style="margin-left:0cm;">这里简单介绍一下batch的概念。首先，我们的训练数据集是有限的，我们会使用这一训练数据集训练多次，相对于巩固学习，但训练太多次又会导致过拟合。每次训练后参数完成一次更新，称这一过程是一个epoch。而在每次epoch中，我们把训练数据集分成几个较小的子集，这就是batch，子集的大小是batch_size。数据集较大不利于通过训练网络，分成子集有利于加快训练。训练数据集分成子集，epoch也分成了几个iteration。根据batch_size的选择，可以把训练情况分成三种，第一种是batch取最大值，即Full Batch Learning，一次性将训练数据通过训练网络，这只适用于训练数据少的情况。第二种是另外一个极端，batch_size取1，即Online Learning在线学习/Stochastic随机学习，这种情况难以收敛，因为每次梯度方向以各自样本的梯度方向收敛。我们看到batch_size其实是在内存利用率和内存容量之间的平衡，增大了batch_size，相对于通过并行化提高了内存利用率，但占用的内存也变大。第三种是Mini-batch，在之前两个极端情况下选择一个平衡点。BN就是基于了Mini-batch SGD。BN适用于mini-batch样本量较大且与各mini-batch分布相近似的场景下（训练前需进行充分的shuffle）。不适用于动态网络结构和RNN。</p> 
<p style="margin-left:0cm;"><img alt="" class="has" height="376" src="https://images2.imgbox.com/80/4e/7WOLoS07_o.png" width="308"></p> 
<p style="margin-left:0cm;">结合前面讲到的激活函数，BN在归一化的同时还可以解决Sigmoid函数的梯度饱和的问题。因为经过BN后，通过激活函数的输入是标准正态分布，经过非线性变换时的输入大概率保持在0均值附近，反向传播时梯度值较大，从而避免了梯度消失。BN增加scale和shift的原因也可以通过对激活函数的输入的处理来说明。因为在进入激活函数之前进行了归一化，将输入集中在了Sigmoid中心附近，而Sigmoid函数虽然本身是非线性函数，但是中心附近是接近线性的，这样一来我们就失去了非线性函数的优势，而补救措施就是增加scale和shift元素。BN的细节还是要看论文《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》。</p> 
<p style="margin-left:0cm;">总结一下BN的优势：1.提升了训练速度（避免了梯度双端饱和，保证了IID）2.类似于Dropout，可以防止过拟合（加入了随机性的scale、shift？）3.对初始化的要求没有那么高（因为不必使用ReLU？即使使用ReLU也可以避免梯度为0），可以使用较大的学习率。</p> 
<p style="margin-left:0cm;">除了BN，还有其他的归一化方式。BN针对的是单一的神经元，Layer Normalization考虑一层的信息，并将该层的均值和方差作为归一化标准。weight normalization则将参数也进行了规范化。</p> 
<p style="margin-left:0cm;">Reference：</p> 
<ol><li>面面观<a href="https://blog.csdn.net/cyh_24/article/details/50593400">https://blog.csdn.net/cyh_24/article/details/50593400</a></li><li>神经网络激励函数的作用是什么？有没有形象的解释？ - 论智的回答 - 知乎<a href="https://www.zhihu.com/question/22334626/answer/465380541" rel="nofollow">https://www.zhihu.com/question/22334626/answer/465380541</a></li><li><a href="https://blog.csdn.net/yangdashi888/article/details/78015448">https://blog.csdn.net/yangdashi888/article/details/78015448</a></li><li>激活函数的作用<a href="https://blog.csdn.net/program_developer/article/details/78704224">https://blog.csdn.net/program_developer/article/details/78704224</a></li><li>DeeplearningAI实用层面<a href="https://www.cnblogs.com/cloud-ken/p/7709447.html" rel="nofollow">https://www.cnblogs.com/cloud-ken/p/7709447.html</a></li><li>归一化优点和方法和结构<a href="https://blog.csdn.net/qq_28618765/article/details/78221571">https://blog.csdn.net/qq_28618765/article/details/78221571</a></li><li>Batch_size知乎<a href="https://www.zhihu.com/question/32673260" rel="nofollow">https://www.zhihu.com/question/32673260</a></li><li>深度学习29https://blog.csdn.net/hjimce/article/details/50866313</li><li>Normalization方式https://blog.csdn.net/qq_24153697/article/details/79880202</li></ol>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6eee64c293bf266444e7f06521faed0e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">### Error updating database. Cause: com.microsoft.sqlserver.jdbc.SQLServerException: 必须声明标量变量 &#34;@P23@...</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/4e0861f65f6376c9328926c547d770aa/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">电脑显示屏的选择与测试</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>