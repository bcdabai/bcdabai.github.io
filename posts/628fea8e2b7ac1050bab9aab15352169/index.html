<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ResNet网络结构详解及代码复现 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ResNet网络结构详解及代码复现" />
<meta property="og:description" content="1. ResNet论文详解 1.1. Introduction 一般网络越深，特征就越丰富，模型效果也就越好。在深度重要的驱动下，出现了2个问题：
梯度消失和梯度爆炸：
梯度消失：误差梯度&lt;1，当网络层数增多时，最终求的梯度会以指数形式衰减梯度爆炸：误差梯度&gt;1，当网络层数增多时，最终求的梯度会以指数形式增加解决方式： Xavier 初始化、Kaiming 初始化等
Batch Normalization
退化问题：在适当深度的模型中添加更多的层会导致更高的训练误差，如下图：
在本文中，我们通过残差结构来解决退化问题。
原本是通过堆叠非线性层来适合 H ( x ) H(x) H(x)，现在是让这些非线性层来适合 F ( x ) F(x) F(x)，原始映射被表示为： H ( x ) : = F ( x ) &#43; x H(x):=F(x)&#43;x H(x):=F(x)&#43;x
H ( x ) : H(x): H(x): 原本需要学习的映射 F ( x ) : F(x): F(x): 现在需要学习的映射 x : x: x: 单位映射，可以跳过一层或多层的连接(shortcut connection) 实验表示：
极深残差网络很容易优化很容易获得网络深度带来的准确性的提高 1.2. Deep Residual Learning 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/628fea8e2b7ac1050bab9aab15352169/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-14T19:45:16+08:00" />
<meta property="article:modified_time" content="2021-05-14T19:45:16+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ResNet网络结构详解及代码复现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-github-gist">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1_ResNet_5"></a>1. ResNet论文详解</h2> 
<h3><a id="11_Introduction_6"></a>1.1. Introduction</h3> 
<p>一般网络越深，特征就越丰富，模型效果也就越好。在深度重要的驱动下，出现了2个问题：</p> 
<ol><li> <p>梯度消失和梯度爆炸：</p> 
  <ul><li>梯度消失：误差梯度&lt;1，当网络层数增多时，最终求的梯度会以指数形式衰减</li><li>梯度爆炸：误差梯度&gt;1，当网络层数增多时，最终求的梯度会以指数形式增加</li><li>解决方式： 
    <ol><li> <p>Xavier 初始化、Kaiming 初始化等</p> </li><li> <p><a href="https://blog.csdn.net/qq_37541097/article/details/104434557">Batch Normalization</a></p> </li></ol> </li></ul> </li><li> <p>退化问题：在适当深度的模型中添加更多的层会导致更高的训练误差，如下图：</p> </li></ol> 
<p><img src="https://images2.imgbox.com/05/a3/utfvWSSM_o.png" alt="在这里插入图片描述"></p> 
<p>在本文中，我们通过残差结构来解决退化问题。</p> 
<p><img src="https://images2.imgbox.com/96/2e/1MBPq4YD_o.png" alt="在这里插入图片描述"></p> 
<p>原本是通过堆叠非线性层来适合<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        H(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span>，现在是让这些非线性层来适合<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         F 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
      
        F(x) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span></span>，原始映射被表示为：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         H 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         : 
        
       
         = 
        
       
         F 
        
       
         ( 
        
       
         x 
        
       
         ) 
        
       
         + 
        
       
         x 
        
       
      
        H(x):=F(x)+x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height: 0.36687em; vertical-align: 0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></p> 
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          H 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          : 
         
        
       
         H(x): 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.08125em;">H</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span> 原本需要学习的映射</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
          ( 
         
        
          x 
         
        
          ) 
         
        
          : 
         
        
       
         F(x): 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span> 现在需要学习的映射</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
          : 
         
        
       
         x: 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">:</span></span></span></span></span> 单位映射，可以跳过一层或多层的连接(<code>shortcut connection</code>)</li></ul> 
<p>实验表示：</p> 
<ol><li>极深残差网络很容易优化</li><li>很容易获得网络深度带来的准确性的提高</li></ol> 
<p><img src="https://images2.imgbox.com/80/57/ipMJiIGd_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12_Deep_Residual_Learning_42"></a>1.2. Deep Residual Learning</h3> 
<h4><a id="121_Residual_Learning_44"></a>1.2.1 Residual Learning</h4> 
<p>退化问题表明：很难通过多个非线性层来逼近单位映射，因为如果可以的话，那么更深的模型的训练误差应该不大于更浅层的对应模型。而对应残差网络，如果单位映射为最优的话，求解器可以简单地将多个非线性层地权值置为0从而逼近单位映射。</p> 
<h4><a id="122_Identity_Mapping_by_Shortcuts_48"></a>1.2.2 Identity Mapping by Shortcuts</h4> 
<p>在本文中，残差块定义如下：</p> 
<ul><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
       
         x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
       
         F 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span></span></span></span></span>的维度一致：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
        
          F 
         
        
          ( 
         
        
          x 
         
        
          , 
         
        
          { 
         
         
         
           W 
          
         
           i 
          
         
        
          } 
         
        
          ) 
         
        
          + 
         
        
          x 
         
        
       
         y=F(x,\{W_i\})+x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">}</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span></li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          x 
         
        
       
         x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
       
         F 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span></span></span></span></span>的维度不一致：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
        
          F 
         
        
          ( 
         
        
          x 
         
        
          , 
         
        
          { 
         
         
         
           W 
          
         
           i 
          
         
        
          } 
         
        
          ) 
         
        
          + 
         
         
         
           W 
          
         
           s 
          
         
        
          x 
         
        
       
         y=F(x,\{W_i\})+W_sx 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">}</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">x</span></span></span></span></span>（<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           W 
          
         
           s 
          
         
        
       
         W_s 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>用来匹配维度）</li></ul> 
<p>残差函数<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         F 
        
       
      
        F 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span></span></span></span></span>是灵活的，主要表现层数的个数和层的类别上</p> 
<ol><li>在本文的实验中涉及的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          F 
         
        
       
         F 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">F</span></span></span></span></span>，它有两层或三层，也可以有更多层，但是如果只有一层，则就类似于线性层：<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
         
         
           W 
          
         
           1 
          
         
        
          x 
         
        
          + 
         
        
          x 
         
        
       
         y=W_1x+x 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathdefault">x</span></span></span></span></span>，我们没有观察到任何优势。</li><li>尽管上面公式表示法是全连接层，但它同样适用于卷积层</li></ol> 
<h3><a id="13_Network_Architectures_60"></a>1.3. Network Architectures</h3> 
<p><img src="https://images2.imgbox.com/ea/3d/sPwSCBrI_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/68/a0/PJASrPBD_o.png" alt="在这里插入图片描述"></p> 
<p>在一般网络结构的基础上，插入<code>shortcut connection</code>，将网络变为对应的残差网络。</p> 
<p>当输入和输出的维数相同时：对应于上图的实线<code>shortcut connection</code>，处理方式：</p> 
<ul><li>直接使用单位映射</li></ul> 
<p>当输入和输出的维数不相同时：对应于上图的虚线<code>shortcut connection</code>，处理方式：</p> 
<ul><li><code>shortcut connection</code>仍然使用单位映射，增加维数用0填充，此方法不引入额外的参数</li><li>使用1x1卷积来匹配维度，文中称为<code>projection shortcut</code></li></ul> 
<p>当跨越两种尺寸的特征图时，执行步幅为2的。有上表黄色部分可知。</p> 
<h3><a id="14_Experiments_78"></a>1.4. Experiments</h3> 
<h4><a id="141_Residual_Networks_VS_Plain_Networks_80"></a>1.4.1 Residual Networks VS. Plain Networks</h4> 
<p><img src="https://images2.imgbox.com/28/da/u1tzDA56_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d6/58/LMoCxfor_o.png" alt="在这里插入图片描述"></p> 
<p><strong>Plain Networks</strong></p> 
<p>上图表示：<code>34</code>层网络比<code>18</code>层网络具有更高的验证误差，尽管<code>18</code>层网络的解空间是<code>34</code>层网络的<code>子空间</code>，但在整个训练过程中，34层网络的训练误差都比较大，这说明了<code>退化问题</code></p> 
<p>我们认为这种优化困难不可能是梯度消失引起的，因为这些网络都使用了<code>BN</code>进行训练，保证前向传播的信号具有非零方差。我们还验证了反向传播的梯度在<code>BN</code>上表现出健康的范数。所以向前和向后的信号都不会消失。</p> 
<p><strong>Residual Networks</strong></p> 
<p>从图4和表2可知：</p> 
<ol><li><code>34</code>层的残差网络比<code>18</code>层残差网络有着相当低的训练误差，并可推广到验证数据，这说明退化问题得到很好的解决，这说明了残差网络在极深网络的有效性</li><li>通过比较<code>18</code>层网络和<code>18</code>层残差网络，发现残差网络<code>收敛的更快</code></li></ol> 
<h4><a id="142_Identity_VS_Projection_Shortcuts_100"></a>1.4.2 Identity VS. Projection Shortcuts</h4> 
<p><img src="https://images2.imgbox.com/c4/e2/u1cQHWuX_o.png" alt="在这里插入图片描述"></p> 
<p>在表3中，我们比较了3个选项（projection：在文中指利用1x1卷积来进行改变维度）</p> 
<ol><li><code>A</code>：用<code>0</code>填充以增加维度</li><li><code>B</code>：<code>projection shortcuts</code>用于增加维度，其他<code>shortcuts</code>为单位映射</li><li><code>C</code>：所有<code>shortcuts</code>均为<code>projection</code></li></ol> 
<p>由表3可知：</p> 
<ul><li><code>B</code>略优于<code>A</code>：<code>A</code>中的<code>0</code>填充的维度没有进行残差学习</li><li><code>C</code>略优于<code>B</code>：多个<code>projection</code>引入了额外的参数</li><li><code>A</code>、<code>B</code>和<code>C</code>的细小差异表明：<code>projection</code>对于解决退化问题不是必须的，因此在本文的其余部分，我们不使用<code>C</code>以减少内存/时间复杂度和模型的规模，使用<code>B</code></li></ul> 
<h4><a id="143_Deeper_Bottleneck_Architectures_117"></a>1.4.3 Deeper Bottleneck Architectures</h4> 
<p><img src="https://images2.imgbox.com/87/e6/uPl4NQz7_o.png" alt="在这里插入图片描述"></p> 
<p>1x1卷积用于升维或降维，可以减少网络的参数</p> 
<ul><li>上图左：<code>256x3x3x256+256x3x3x256=1179648</code></li><li>上图右：<code>256x1x1x64+64x3x3x64+64x1x1x256=69632</code></li></ul> 
<p>故在更深的残差网络中，使用上图右的结构，可以大大减少网络的参数，减小模型的规模</p> 
<h2><a id="2_Pytorch_128"></a>2. 基于Pytorch代码复现</h2> 
<h3><a id="21__129"></a>2.1 模型搭建</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchsummary <span class="token keyword">import</span> summary
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models


<span class="token keyword">class</span> <span class="token class-name">BasicBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    expansion <span class="token operator">=</span> <span class="token number">1</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channel<span class="token punctuation">,</span> out_channel<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        identity <span class="token operator">=</span> x
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            identity <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">+=</span> identity
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out


<span class="token keyword">class</span> <span class="token class-name">Bottleneck</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    expansion <span class="token operator">=</span> <span class="token number">4</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channel<span class="token punctuation">,</span> out_channel<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token operator">*</span>self<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token operator">*</span>self<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        identity <span class="token operator">=</span> x
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            identity <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">+=</span> identity
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out


<span class="token keyword">class</span> <span class="token class-name">ResNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> block_num<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ResNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>include_top <span class="token operator">=</span> include_top
        self<span class="token punctuation">.</span>in_channel <span class="token operator">=</span> <span class="token number">64</span>

        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> block_num<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> block_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> block_num<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> block_num<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>include_top<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span> <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>

        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> block_num<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        downsample <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> stride <span class="token operator">!=</span> <span class="token number">1</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>in_channel <span class="token operator">!=</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">:</span>
            downsample <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>channel<span class="token operator">*</span>block<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> downsample<span class="token operator">=</span>downsample<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channel <span class="token operator">=</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> block_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> channel<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>include_top<span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x


<span class="token keyword">def</span> <span class="token function">resnet34</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span> include_top<span class="token operator">=</span>include_top<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">resnet101</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span> include_top<span class="token operator">=</span>include_top<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">read_resnet34</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet34<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>
    summary<span class="token punctuation">(</span>model<span class="token punctuation">,</span> input_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">get_resnet34</span><span class="token punctuation">(</span>flag<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> flag<span class="token punctuation">:</span>
        net <span class="token operator">=</span> models<span class="token punctuation">.</span>resnet34<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        num_input <span class="token operator">=</span> net<span class="token punctuation">.</span>fc<span class="token punctuation">.</span>in_features
        net<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_input<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        net <span class="token operator">=</span> resnet34<span class="token punctuation">(</span>num_classes<span class="token punctuation">)</span>

    <span class="token keyword">return</span> net
</code></pre> 
<h3><a id="22__294"></a>2.2 训练结果如下</h3> 
<ol><li>训练数据集与验证集大小以及训练参数</li></ol> 
<pre><code>Using 3306 images for training, 364 images for validation
Using cuda GeForce RTX 2060 device for training
lr: 0.0001
batch_size: 16
</code></pre> 
<ol start="2"><li>使用自己定义的网络训练结果</li></ol> 
<pre><code>
[epoch 1/10] train_loss: 1.309 val_acc: 0.555
[epoch 2/10] train_loss: 1.146 val_acc: 0.604
[epoch 3/10] train_loss: 1.029 val_acc: 0.643
[epoch 4/10] train_loss: 0.935 val_acc: 0.695
[epoch 5/10] train_loss: 0.919 val_acc: 0.615
[epoch 6/10] train_loss: 0.860 val_acc: 0.723
[epoch 7/10] train_loss: 0.841 val_acc: 0.690
[epoch 8/10] train_loss: 0.819 val_acc: 0.725
[epoch 9/10] train_loss: 0.800 val_acc: 0.745
[epoch 10/10] train_loss: 0.783 val_acc: 0.725
Best acc: 0.745
Finished Training
Train 耗时为：281.2s
</code></pre> 
<ol start="4"><li>使用预训练模型参数训练结果</li></ol> 
<pre><code>[epoch 1/10] train_loss: 0.492 val_acc: 0.896
[epoch 2/10] train_loss: 0.327 val_acc: 0.896
[epoch 3/10] train_loss: 0.285 val_acc: 0.909
[epoch 4/10] train_loss: 0.273 val_acc: 0.904
[epoch 5/10] train_loss: 0.205 val_acc: 0.901
[epoch 6/10] train_loss: 0.245 val_acc: 0.898
[epoch 7/10] train_loss: 0.200 val_acc: 0.923
[epoch 8/10] train_loss: 0.196 val_acc: 0.923
[epoch 9/10] train_loss: 0.179 val_acc: 0.929
[epoch 10/10] train_loss: 0.173 val_acc: 0.926
Best acc: 0.929
Finished Training
Train 耗时为：281.3s
</code></pre> 
<p><a href="https://blog.csdn.net/qq_42735631/article/details/116404840">上一篇：GoogLeNet</a><br> <a href="https://blog.csdn.net/qq_42735631/article/details/116861074">下一篇：DenseNet</a><br> <a href="https://github.com/codecat0/CV/tree/main/Image_Classification">完整代码</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c7bb39fc2700303d35c47f9a3f6fe52f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Arm64内存屏障</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d1b146baef879400618056692ae26269/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">AntiAliasing学习笔记</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>