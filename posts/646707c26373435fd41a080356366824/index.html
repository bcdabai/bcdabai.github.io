<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【RBF预测】基于RBF神经网络预测模型matlab源码 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【RBF预测】基于RBF神经网络预测模型matlab源码" />
<meta property="og:description" content="一、简介 【注】蓝色字体为自己的理解部分
径向基函数神经网络的优点：逼近能力，分类能力和学习速度等方面都优于BP神经网络，结构简单、训练简洁、学习收敛速度快、能够逼近任意非线性函数，克服局部极小值问题。原因在于其参数初始化具有一定的方法，并非随机初始化。
RBF是具有单隐层的三层前向网络。第一层为输入层，由信号源节点组成。第二层为隐藏层，隐藏层节点数视所描述问题的需要而定，隐藏层中神经元的变换函数即径向基函数是对中心点径向对称且衰减的非负线性函数，该函数是局部响应函数，具体的局部响应体现在其可见层到隐藏层的变换跟其它的网络不同。以前的前向网络变换函数都是全局响应的函数。第三层为输出层，是对输入模式做出的响应。输入层仅仅起到传输信号作用，输入层和隐含层之间可以看做连接权值为1的连接，输出层与隐含层所完成的任务是不同的，因而他们的学习策略也不同。输出层是对线性权进行调整，采用的是线性优化策略，因而学习速度较快；而隐含层是对激活函数(格林函数，高斯函数，一般取后者)的参数进行调整，采用的是非线性优化策略，因而学习速度较慢。对于这句话的理解，从下面的层与层之间的变换可以发现。
RBF神经网络的基本思想：用RBF作为隐单元的“基”构成隐藏层空间，隐藏层对输入矢量进行变换，将低维的模式输入数据变换到高维空间内，使得在低维空间内的线性不可分问题在高维空间内线性可分。详细一点就是用RBF的隐单元的“基”构成隐藏层空间，这样就可以将输入矢量直接(不通过权连接)映射到隐空间。当RBF的中心点确定以后，这种映射关系也就确定 了。而隐含层空间到输出空间的映射是线性的(注意这个地方区分一下线性映射和非线性映射的关系)，即网络输出是因单元输出的线性加权和，此处的权即为网络可调参数。
下图是径向基神经元模型 《43案例分析》中介绍：径向基函数的激活函数是以输入向量和权值向量(注意此处的权值向量并非隐藏层到输出层的权值，具体看下面的径向基神经元模型结构)之间的距离||dist||作为自变量的。径向基网络的激活函数的一般表达式为。
《模式识别与智能计算》中介绍：径向基网络传递函数是以输入向量与阈值向量之间的距离|| X-Cj ||作为自变量的，其中|| X -Cj ||是通过输入向量和加权矩阵C的行向量的乘积得到。此处的C就是隐藏层各神经元的中心参数，大小为隐层神经元数目*可见层单元数。再者，每一个隐神经元中心参数C都对应一个宽度向量D，使得不同的输入信息能被不同的隐层神经元最大程度地反映出来。
得到的这个R就是隐层神经元的值。
随着权值和输入向量之间距离的减少，网络输出是递增的，当输入向量和权值向量一致时，神经元输出为1。图中的b为阈值，用于调整神经元的灵敏度。利用径向基神经元和线性神经元可以建立广义回归神经网络，此种神经网络适用于函数逼近方面的应用。径向基函数和竞争神经元可以建立概率神经网络，此种神经网络适用于解决分类问题。
RBF神经网络学习算法需要三个参数：基函数的中心，方差(宽度)以及隐含层到输出层的权值。
RBF神经网络中心选取方法：
对于RBF神经网络的学习算法，关键问题是隐藏层神经元中心参数的合理确定。常用的方法是从中心参数(或者其初始值)是从给定的训练样本集里按照某种方法直接选取，或者是采用聚类的方法确定。
①直接计算法(随机选取RBF中心)
隐含层神经元的中心是随机地在输入样本中选取，且中心固定。一旦中心固定下来，隐含层神经元的输出便是已知的，这样的神经网络的连接权就可以通过求解线性方程组来确定。适用于样本数据的分布具有明显代表性。
②自组织学习选取RBF中心法
RBF神经网络的中心可以变化，并通过自组织学习确定其位置。输出层的线性权重则是通过有监督的学习来确定的。这种方法是对神经网络资源的再分配，通过 学习，使RBF的隐含层神经元中心位于输入空间重要的区域。这种方法主要采用K-均值聚类法来选择RBF的中心，属于无监督(导师)的学习方法。
③有监督(导师)学习选取RBF中心
通过训练样本集来获得满足监督要求的网络中心和其他权重参数。常用方法是梯度下降法。
④正交最小二乘法选取RBF中心法
正交最小二乘法(Orthogoal least square)法的思想来源于线性回归模型。神经网络的输出实际上是隐含层神经元某种响应参数(回归因子)和隐含层至输出层间连接权重的线性组合。所有隐含层神经元上的回归因子构成回归向量。学习过程主要是回归向量正交化的过程。
在很多实际问题中，RBF神经网络隐含层神经元的中心并非是训练集中的某些样本点或样本的聚类中心，需要通过学习的方法获得，使所得到的中心能够更好地反应训练集数据所包含的信息。
基于高斯核的RBF神经网络拓扑结构
第一层输入层：由信号源节点构成，仅起到数据信息的传递作用，对输入信息不做任何变换
第二层隐含层：节点数视需要而定。隐含层神经元核函数(作用函数)是高斯函数，对输入信息进行空间映射的变换。
第三层输出层，对输入模式做出响应。输出层神经元的作用函数为线性函数，对隐含层神经元输出的信息进行线性加权后输出，作为整个神经网络的输出结果。
径向基网络传递函数是以输入向量与阈值向量之间的距离|| X-Cj ||作为自变量的。其中|| X-Cj ||是通过输入向量和加权矩阵C的行向量的乘积得到的。径向基神经网络传递参数可以取多种形式。常见的有：
①Gaussian函数(高斯函数)
②Reflected sigmoidal函数(反常S型函数)
③逆Multiquadric函数(逆 畸变校正函数)
较为常用的还是Gaussian函数，本文采用Gaussian函数：
当输入自变量为0时，传递函数取得最大值1,。随着权值和输入向量间的距离不断减小，网络输出是递增的。也就是说，径向基函数对输入信号在局部产生响应。函数的输入信号X靠近函数的中央范围时，隐含层节点将产生较大的输出。由此可以看出这种网络具有局部逼近能力。
当输入向量加到网络输入端时，径向基层每个神经元都会输出一个值，代表输入向量与神经元权值向量之间的接近程度。如果输入向量关于权值向量相差很多，则径向基层输出接近于0,；如果输入向量与权值向量很接近，则径向基层的输出接近于1，经过第二层(隐含层)的线性神经元，输出值就靠近第二层权值。在这个过程中，如果只有一个径向基神经元的输出为1，而其他神经元输出均为0或者接近0，那么线性神经元的输出就相当于输出为1的神经元对应的第二层(隐含层)权值的值。
RBF网络训练：
训练的目的是求两层的最终权值Cj、Dj和Wj。
训练的过程分为两步：第一步是无监督学习，训练确定输入层与隐含层间的权值Cj、Dj；第二步是有监督学习，训练确定隐含层与输出层间的权值Wj。
训练前提供输入向量X、对应的目标输出向量Y和径向基函数的宽度向量Dj。
在第 l 次输入样品(l=1,2,...,N)进行训练时，各个参数的表达及计算方法如下：
(1)确定参数
①确定输入向量X：
，n是输入层单元数
②确定输出向量Y和希望输出向量O
，q是输出层单元数
③初始化隐含层至输出层的连接权值
其中p是隐藏层单元数，q是输出层单元数。
参考中心初始化的方法给出隐藏层到输出层的权值初始化方法：
其中mink是训练集中第k个输出神经元中所有期望输出的最小值；maxk是训练集中第k个输出神经元中所有期望输出的最大值。
④初始化隐含层各神经元的中心参数。不同隐含层神经元的中心应有不同的取值，并且与中心的对应宽度能够调节，使得不同的输入信息特征能被不同的隐含层神经元最大的反映出来。在实际应用中，一个输入信息总是包含在一定的取值范围内。不失一般性，将隐含层各神经元的中心分量的初值，按从小到大等间距变化，使较弱的输入信息在较小的中心附近产生较强的响应。间距的大小可由隐藏层神经元的个数来调节。好处是能够通过试凑的方法找到较为合理的隐含层神经元数，并使中心的初始化尽量合理，不同的输入特征更为明显地在不同的中心处反映出来，体现高斯核的特点。
基于上述四项，RBF神经网络中心参数的初始值为：
(p为隐含层神经元总个数，j=1,2,...,p)
mini是训练集中第i个特征所有输入信息的最小值，maxi为训练集中第i 个特征所有输入信息的最大值。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/646707c26373435fd41a080356366824/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-29T22:42:09+08:00" />
<meta property="article:modified_time" content="2021-06-29T22:42:09+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【RBF预测】基于RBF神经网络预测模型matlab源码</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size: 16px;"> 
 <h3>一、简介</h3> 
 <p>【注】蓝色字体为自己的理解部分</p> 
 <p>     径向基函数神经网络的优点：逼近能力，分类能力和学习速度等方面都优于BP神经网络，结构简单、训练简洁、学习收敛速度快、能够逼近任意非线性函数，克服局部极小值问题。原因在于其参数初始化具有一定的方法，并非随机初始化。</p> 
 <p>        RBF是具有单隐层的三层前向网络。第一层为输入层，由信号源节点组成。第二层为隐藏层，隐藏层节点数视所描述问题的需要而定，隐藏层中神经元的变换函数即径向基函数是对中心点径向对称且衰减的非负线性函数，该函数是局部响应函数，具体的局部响应体现在其可见层到隐藏层的变换跟其它的网络不同。以前的前向网络变换函数都是全局响应的函数。第三层为输出层，是对输入模式做出的响应。输入层仅仅起到传输信号作用，输入层和隐含层之间可以看做连接权值为1的连接，输出层与隐含层所完成的任务是不同的，因而他们的学习策略也不同。输出层是对线性权进行调整，采用的是线性优化策略，因而学习速度较快；而隐含层是对激活函数(格林函数，高斯函数，一般取后者)的参数进行调整，采用的是非线性优化策略，因而学习速度较慢。对于这句话的理解，从下面的层与层之间的变换可以发现。</p> 
 <p>       RBF神经网络的基本思想：用RBF作为隐单元的“基”构成隐藏层空间，隐藏层对输入矢量进行变换，将低维的模式输入数据变换到高维空间内，使得在低维空间内的线性不可分问题在高维空间内线性可分。详细一点就是用RBF的隐单元的“基”构成隐藏层空间，这样就可以将输入矢量直接(不通过权连接)映射到隐空间。当RBF的中心点确定以后，这种映射关系也就确定 了。而隐含层空间到输出空间的映射是线性的(注意这个地方区分一下线性映射和非线性映射的关系)，即网络输出是因单元输出的线性加权和，此处的权即为网络可调参数。</p> 
 <p>下图是径向基神经元模型      </p> 
 <p><img src="https://images2.imgbox.com/42/6e/u44j1zID_o.png" alt="" style="outline: none;"></p> 
 <p> 《43案例分析》中介绍：径向基函数的激活函数是以输入向量和权值向量(注意此处的权值向量并非隐藏层到输出层的权值，具体看下面的径向基神经元模型结构)之间的距离||dist||作为自变量的。径向基网络的激活函数的一般表达式为。</p> 
 <p><img src="https://images2.imgbox.com/be/13/pccxPvQW_o.png" alt="" style="outline: none;"></p> 
 <p>《模式识别与智能计算》中介绍：径向基网络传递函数是以输入向量与阈值向量之间的距离|| X-Cj ||作为自变量的，其中|| X -Cj ||是通过输入向量和加权矩阵C的行向量的乘积得到。此处的C就是隐藏层各神经元的中心参数，大小为隐层神经元数目*可见层单元数。再者，每一个隐神经元中心参数C都对应一个宽度向量D，使得不同的输入信息能被不同的隐层神经元最大程度地反映出来。</p> 
 <p><img src="https://images2.imgbox.com/6d/5b/IBNm0XN8_o.png" alt="" style="outline: none;"></p> 
 <p>得到的这个R就是隐层神经元的值。</p> 
 <p>随着权值和输入向量之间距离的减少，网络输出是递增的，当输入向量和权值向量一致时，神经元输出为1。图中的b为阈值，用于调整神经元的灵敏度。利用径向基神经元和线性神经元可以建立广义回归神经网络，此种神经网络适用于函数逼近方面的应用。径向基函数和竞争神经元可以建立概率神经网络，此种神经网络适用于解决分类问题。</p> 
 <p><strong><em>RBF神经网络学习算法需要三个参数：基函数的中心，方差(宽度)以及隐含层到输出层的权值。</em></strong></p> 
 <p><strong>RBF神经网络中心选取方法：</strong></p> 
 <p>       对于RBF神经网络的学习算法，关键问题是隐藏层神经元中心参数的合理确定。常用的方法是从中心参数(或者其初始值)是从给定的训练样本集里按照某种方法直接选取，或者是采用聚类的方法确定。</p> 
 <p>   ①直接计算法(随机选取RBF中心)</p> 
 <p>     隐含层神经元的中心是随机地在输入样本中选取，且中心固定。一旦中心固定下来，隐含层神经元的输出便是已知的，这样的神经网络的连接权就可以通过求解线性方程组来确定。适用于样本数据的分布具有明显代表性。</p> 
 <p>   ②自组织学习选取RBF中心法</p> 
 <p>      RBF神经网络的中心可以变化，并通过自组织学习确定其位置。输出层的线性权重则是通过有监督的学习来确定的。这种方法是对神经网络资源的再分配，通过 学习，使RBF的隐含层神经元中心位于输入空间重要的区域。这种方法主要采用K-均值聚类法来选择RBF的中心，属于无监督(导师)的学习方法。</p> 
 <p>  ③有监督(导师)学习选取RBF中心</p> 
 <p>     通过训练样本集来获得满足监督要求的网络中心和其他权重参数。常用方法是梯度下降法。</p> 
 <p>  ④正交最小二乘法选取RBF中心法</p> 
 <p>     正交最小二乘法(Orthogoal least square)法的思想来源于线性回归模型。神经网络的输出实际上是隐含层神经元某种响应参数(回归因子)和隐含层至输出层间连接权重的线性组合。所有隐含层神经元上的回归因子构成回归向量。学习过程主要是回归向量正交化的过程。</p> 
 <p>     在很多实际问题中，RBF神经网络隐含层神经元的中心并非是训练集中的某些样本点或样本的聚类中心，需要通过学习的方法获得，使所得到的中心能够更好地反应训练集数据所包含的信息。</p> 
 <p><strong>基于高斯核的RBF神经网络拓扑结构</strong></p> 
 <p>      第一层输入层：由信号源节点构成，仅起到数据信息的传递作用，对输入信息不做任何变换</p> 
 <p>      第二层隐含层：节点数视需要而定。隐含层神经元核函数(作用函数)是高斯函数，对输入信息进行空间映射的变换。</p> 
 <p>      第三层输出层，对输入模式做出响应。输出层神经元的作用函数为线性函数，对隐含层神经元输出的信息进行线性加权后输出，作为整个神经网络的输出结果。</p> 
 <p><img src="https://images2.imgbox.com/d1/64/G9BQDVMY_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/16/36/I6kLkwIK_o.png" alt="" style="outline: none;"></p> 
 <p>径向基网络传递函数是以输入向量与阈值向量之间的距离|| X-Cj ||作为自变量的。其中|| X-Cj ||是通过输入向量和加权矩阵C的行向量的乘积得到的。径向基神经网络传递参数可以取多种形式。常见的有：</p> 
 <p>①Gaussian函数(高斯函数)</p> 
 <p><img src="https://images2.imgbox.com/b6/87/edbfwuFW_o.png" alt="" style="outline: none;"></p> 
 <p>②Reflected sigmoidal函数(反常S型函数)</p> 
 <p><img src="https://images2.imgbox.com/99/da/wsmVPbaw_o.png" alt="" style="outline: none;"></p> 
 <p>③逆Multiquadric函数(逆 畸变校正函数)</p> 
 <p><img src="https://images2.imgbox.com/24/4c/4qrav1Kz_o.png" alt="" style="outline: none;"></p> 
 <p>较为常用的还是Gaussian函数，本文采用Gaussian函数：<img src="https://images2.imgbox.com/f4/47/uRPFzlbH_o.png" alt="" style="outline: none;"></p> 
 <p>当输入自变量为0时，传递函数取得最大值1,。随着权值和输入向量间的距离不断减小，网络输出是递增的。也就是说，径向基函数对输入信号在局部产生响应。函数的输入信号X靠近函数的中央范围时，隐含层节点将产生较大的输出。由此可以看出这种网络具有局部逼近能力。</p> 
 <p><img src="https://images2.imgbox.com/64/83/1XgVLKdZ_o.png" alt="" style="outline: none;"></p> 
 <p>         当输入向量加到网络输入端时，径向基层每个神经元都会输出一个值，代表输入向量与神经元权值向量之间的接近程度。如果输入向量关于权值向量相差很多，则径向基层输出接近于0,；如果输入向量与权值向量很接近，则径向基层的输出接近于1，经过第二层(隐含层)的线性神经元，输出值就靠近第二层权值。在这个过程中，如果只有一个径向基神经元的输出为1，而其他神经元输出均为0或者接近0，那么线性神经元的输出就相当于输出为1的神经元对应的第二层(隐含层)权值的值。</p> 
 <p><strong>RBF网络训练：</strong></p> 
 <p>训练的目的是求两层的最终权值Cj、Dj和Wj。</p> 
 <p>训练的过程分为两步：第一步是无监督学习，训练确定输入层与隐含层间的权值Cj、Dj；第二步是有监督学习，训练确定隐含层与输出层间的权值Wj。</p> 
 <p>训练前提供输入向量X、对应的目标输出向量Y和径向基函数的宽度向量Dj。</p> 
 <p>在第 l 次输入样品(l=1,2,...,N)进行训练时，各个参数的表达及计算方法如下：</p> 
 <p>(1)确定参数</p> 
 <p>①确定输入向量X：</p> 
 <p><img src="https://images2.imgbox.com/6a/5c/0O9OsqCB_o.png" alt="" style="outline: none;">，n是输入层单元数</p> 
 <p>②确定输出向量Y和希望输出向量O</p> 
 <p><img src="https://images2.imgbox.com/44/ed/iUYB8oxM_o.png" alt="" style="outline: none;">，q是输出层单元数</p> 
 <p><img src="https://images2.imgbox.com/a6/6e/gfui9F1U_o.png" alt="" style="outline: none;"></p> 
 <p>③初始化隐含层至输出层的连接权值</p> 
 <p><img src="https://images2.imgbox.com/24/91/ZzvFVbIi_o.png" alt="" style="outline: none;"></p> 
 <p>其中p是隐藏层单元数，q是输出层单元数。</p> 
 <p>参考中心初始化的方法给出隐藏层到输出层的权值初始化方法：</p> 
 <p><img src="https://images2.imgbox.com/43/91/PGKE3fdS_o.png" alt="" style="outline: none;"></p> 
 <p>其中mink是训练集中第k个输出神经元中所有期望输出的最小值；maxk是训练集中第k个输出神经元中所有期望输出的最大值。</p> 
 <p>④初始化隐含层各神经元的中心参数<img src="https://images2.imgbox.com/07/66/QBDC9tfA_o.png" alt="" style="outline: none;">。不同隐含层神经元的中心应有不同的取值，并且与中心的对应宽度能够调节，使得不同的输入信息特征能被不同的隐含层神经元最大的反映出来。在实际应用中，一个输入信息总是包含在一定的取值范围内。不失一般性，将隐含层各神经元的中心分量的初值，按从小到大等间距变化，使较弱的输入信息在较小的中心附近产生较强的响应。间距的大小可由隐藏层神经元的个数来调节。好处是能够通过试凑的方法找到较为合理的隐含层神经元数，并使中心的初始化尽量合理，不同的输入特征更为明显地在不同的中心处反映出来，体现高斯核的特点。</p> 
 <p>基于上述四项，RBF神经网络中心参数的初始值为：</p> 
 <p><img src="https://images2.imgbox.com/10/6d/5n4nMBYt_o.png" alt="" style="outline: none;">(p为隐含层神经元总个数，j=1,2,...,p)</p> 
 <p>mini是训练集中第i个特征所有输入信息的最小值，maxi为训练集中第i 个特征所有输入信息的最大值。</p> 
 <p>⑤初始化宽度向量<img src="https://images2.imgbox.com/71/49/45kXHz30_o.png" alt="" style="outline: none;">。宽度向量影响着神经元对输入信息的作用范围：宽度越小，相应隐含层神经元作用函数的形状越窄，那么处于其他神经元中心附近的信息在该神经元出的响应就越小。计算方法：</p> 
 <p><img src="https://images2.imgbox.com/b7/85/9XcvH8Y3_o.png" alt="" style="outline: none;"></p> 
 <p>df为宽度调节系数，取值小于1，作用是使每个隐含层神经元更容易实现对局部信息的感受能力，有利于提高RBF神经网络的局部响应能力。</p> 
 <p>(2)计算隐含层第j 个神经元的输出值zj</p> 
 <p><img src="https://images2.imgbox.com/a9/b1/GlyoSZIZ_o.png" alt="" style="outline: none;"></p> 
 <p>Cj是隐含层第 j 个神经元的中心向量，由隐含层第j个神经元对应于输入层所有神经元的中心分量构成，<img src="https://images2.imgbox.com/2b/23/d3HyzfnO_o.png" alt="" style="outline: none;">；Dj为隐含层第j个神经元的宽度向量，与Cj相对应，<img src="https://images2.imgbox.com/fc/bf/oawqO9Xl_o.png" alt="" style="outline: none;">，Dj越大，隐含层对输入向量的影响范围就越大，且神经元间的平滑度也比较好；||.||为欧式范数。</p> 
 <p>(3)计算输出层神经元的输出</p> 
 <p><img src="https://images2.imgbox.com/bf/4d/QDuJAdsC_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/4f/f1/mXeKzC5C_o.png" alt="" style="outline: none;"></p> 
 <p>其中<img src="https://images2.imgbox.com/73/c2/NFWg7y7U_o.png" alt="" style="outline: none;">为输出层第k个神经元与隐含层第 j 个神经元间的调节权重。</p> 
 <p>(4)权重参数的迭代计算</p> 
 <p>RBF神经网络权重参数的训练方法在这里取为梯度下降法。中心、宽度和调节权重参数均通过学习来自适应调节到最佳值，迭代计算如下：</p> 
 <p><img src="https://images2.imgbox.com/b0/46/bxc92ZJf_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/a0/ce/ozLBVT5a_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/c3/fe/aZMLKZCe_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/e5/ed/Mxw0O6L6_o.png" alt="" style="outline: none;"> 为第k个输出神经元与第j个隐含层神经元之间在第t 次迭代计算时的调节权重。</p> 
 <p><img src="https://images2.imgbox.com/b8/d5/UqfvKsAU_o.png" alt="" style="outline: none;">为第j 个隐含层神经元对于第i个输入神经元在第t 次迭代计算时的中心分量；</p> 
 <p><img src="https://images2.imgbox.com/2b/fc/Cjn854Hh_o.png" alt="" style="outline: none;">为与中心<img src="https://images2.imgbox.com/05/8c/bqqvoD7z_o.png" alt="" style="outline: none;">对应的宽度</p> 
 <p>η为学习因子</p> 
 <p>E为RBF神经网络评价函数：</p> 
 <p><img src="https://images2.imgbox.com/82/ae/2rxhxRq2_o.png" alt="" style="outline: none;"></p> 
 <p>其中，Olk为第k 个输出神经元在第ｌ个输入样本时的期望输出值；ylk为第k个输出神经元在第l个输入样本时的网络输出值。</p> 
 <p>综上所述，给出RBF神经网络的学习算法：</p> 
 <p>① 按(1)确定参数的五个步骤对神经网络参数进行初始化，并给定η和α的取值及迭代终止精度ε 的值。</p> 
 <p>②按下式计算网络输出的均方根误差RMS 的值，若RMS≤ε ，则训练结束，否则转到第③步</p> 
 <p><img src="https://images2.imgbox.com/92/79/RB4VWDMe_o.png" alt="" style="outline: none;"></p> 
 <p>③按照(4)权重迭代计算，对调节权重，中心和宽度参数进行迭代计算。</p> 
 <p>④返回步骤②</p> 
 <h3><a href="" rel="nofollow"></a>二、源代码</h3> 
 <p>``` %% I. 清空环境变量</p> 
 <p>clear all</p> 
 <p>clc</p> 
 <p>%% II. 训练集/测试集产生</p> 
 <p>%%</p> 
 <p>% 1. 导入数据</p> 
 <p>load iris_data.mat</p> 
 <p>%%</p> 
 <p>% 2 随机产生训练集和测试集</p> 
 <p>P_train = [];</p> 
 <p>T_train = [];</p> 
 <p>P_test = [];</p> 
 <p>T_test = [];</p> 
 <p>for i = 1:3</p> 
 <p>temp_input = features((i-1)<em>50+1:i</em>50,:);</p> 
 <p>temp_output = classes((i-1)<em>50+1:i</em>50,:);</p> 
 <p>n = randperm(50);</p> 
 <p>% 训练集——120个样本</p> 
 <p>P<em>train = [P</em>train temp_input(n(1:40),:)'];</p> 
 <p>T<em>train = [T</em>train temp_output(n(1:40),:)'];</p> 
 <p>% 测试集——30个样本</p> 
 <p>P<em>test = [P</em>test temp_input(n(41:50),:)'];</p> 
 <p>T<em>test = [T</em>test temp_output(n(41:50),:)'];</p> 
 <p>end</p> 
 <p>%% III. 模型建立</p> 
 <p>result_grnn = [];</p> 
 <p>result_pnn = [];</p> 
 <p>time_grnn = [];</p> 
 <p>time_pnn = [];</p> 
 <p>for i = 1:4</p> 
 <p>for j = i:4</p> 
 <p>p<em>train = P</em>train(i:j,:);</p> 
 <p>p<em>test = P</em>test(i:j,:);</p> 
 <p>%%</p> 
 <p>% 1. GRNN创建及仿真测试</p> 
 <p>t = cputime;</p> 
 <p>% 创建网络</p> 
 <p>net<em>grnn = newgrnn(p</em>train,T_train);</p> 
 <p>% 仿真测试</p> 
 <p>t<em>sim</em>grnn = sim(net<em>grnn,p</em>test);</p> 
 <p>T<em>sim</em>grnn = round(t<em>sim</em>grnn);</p> 
 <p>t = cputime - t;</p> 
 <p>time<em>grnn = [time</em>grnn t];</p> 
 <p>result<em>grnn = [result</em>grnn T<em>sim</em>grnn'];</p> 
 <p>%%</p> 
 <p>% 2. PNN创建及仿真测试</p> 
 <p>t = cputime;</p> 
 <p>Tc<em>train = ind2vec(T</em>train);</p> 
 <p>% 创建网络</p> 
 <p>net<em>pnn = newpnn(p</em>train,Tc_train);</p> 
 <p>% 仿真测试</p> 
 <p>Tc<em>test = ind2vec(T</em>test);</p> 
 <p>t<em>sim</em>pnn = sim(net<em>pnn,p</em>test);</p> 
 <p>T<em>sim</em>pnn = vec2ind(t<em>sim</em>pnn);</p> 
 <p>t = cputime - t;</p> 
 <p>time<em>pnn = [time</em>pnn t];</p> 
 <p>result<em>pnn = [result</em>pnn T<em>sim</em>pnn'];</p> 
 <p>end</p> 
 <p>end ```</p> 
 <h3><a href="" rel="nofollow"></a>三、运行结果</h3> 
 <p><img src="https://images2.imgbox.com/6c/dd/L9tVYCSS_o.png" alt="在这里插入图片描述" style="outline: none;"></p> 
 <h3><a href="" rel="nofollow"></a><a href="" rel="nofollow"></a>四、备注</h3> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7ce4e3d54246cec149efca2ef5809b2e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【信号处理】单通道盲源分离（SSA-ICA）算法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f10252d6180c54f507e27d0949b008f2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【预测模型】BP神经网络的预测</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>