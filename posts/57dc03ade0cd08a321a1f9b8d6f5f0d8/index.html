<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Batch Normalization详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Batch Normalization详解" />
<meta property="og:description" content=" 目录
1. Batch Normalization的基本思想
2. BN层的作用 3. 训练阶段和测试阶段的BN层
4. BN、LN、IN、GN与SN
1. Batch Normalization的基本思想 BN解决的问题：深度神经网络随着网络深度加深，训练越困难，收敛越来越慢
问题出现的原因：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新——BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布
BN的基本思想：
训练过程中整体分布逐渐往非线性函数的取值区间的上下限两端靠近，导致反向传播时低层神经网络的梯度消失，训练收敛慢BN把每层神经网络任意神经元输出的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，可以得到比较大的梯度，避免梯度消失，梯度变大意味着学习收敛速度快，能大大加快训练速度 2. BN层的作用 加快网络的训练和收敛的速度 深度神经网络中，每层的数据分布不一样会导致网络非常难收敛和训练，BN将每层数据分布转化为一致的标准正态分布，更容易收敛控制梯度爆炸，防止梯度消失 梯度消失：深度神经网络中，如果网络的激活输出很大，其对应梯度会很小，甚至梯度消失，导致网络的学习速率慢，使用BN层归一化后，网络的输出不会很大，梯度就不会很小梯度爆炸：第一层偏移量的梯度=激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n，假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加，但使用BN层后权值的更新也不会很大防止过拟合 在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在一起，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝这一个方向使劲学习，一定程度上避免了过拟合Allows higher learning rates, faster convergence and networks become more robust to initialization -- Fei-Fei Li 注意⚠️：为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后？
非线性单元的输出分布形状会在训练过程中变化，归一化无法消除它的方差偏移，相反的，全连接和卷积层的输出一般是一个对称、非稀疏的分布，更加类似高斯分布，对它们进行归一化会产生更加稳定的分布 例如Relu激活函数，输入的数据是一个高斯分布，变换后数据小于0的被抑制了（分布小于0的部分直接变成0） 3. 训练阶段和测试阶段的BN层 训练阶段：
例如batchsize为32，某层的某个神经元会输出32个响应值对这32个响应求均值和标准差，再做归一化归一化的响应值乘以γ，加上β每个神经元都训练一组γ、β 测试阶段：
测试阶段均值、方差、γ、β都用训练阶段全局求出 在测试阶段Batch Normalization相当于做线性变换 注意⚠️：Dropout和BN都有防止过拟合的作用，单独使用，都带来一定的性能改进，为什么一起用反而性能下降？
原因：
当网络的状态从训练转移到测试时，Dropout转移了特定神经单元的方差。但是，在测试阶段，BN保持了它的统计方差，这是在整个学习过程中积累的。Dropout和BN中方差的不一致性(“方差偏移”)，导致推断中不稳定的数值行为，最终导致错误的预测 如下图，Dropout在训练阶段以p的概率进行失活，而在测试阶段，对每个神经元进行尺度放缩(乘p)。另一种等价的表现形式，在训练阶段乘1/p，而测试阶段不需要做任何改动，所以，训练：X=a*(1/p)*X，测试：X=X，存在方差偏移 解决方案：总体思路是降低方差偏移
在所有BN层后使用Dropout修改Dropout公式（如使用高斯Dropout）使得它对方差不是那么敏感 4. BN、LN、IN、GN与SN BN、LN、IN和GN这四个归一化的计算流程几乎一样，输入的图像shape记为[N, C, H, W]，区别在于：
BatchNorm：在batch上，对NHW做归一化，把每个通道的NHW单独拿出来归一化处理，针对每个channel都有一组γ,β，可学习的参数为2*C，当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局
LayerNorm：在通道方向上，对CHW归一化，把每个CHW单独拿出来归一化处理，不受batchsize 的影响，主要对RNN作用明显，常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理
InstanceNorm：在图像像素上，对HW做归一化，把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响，常用在风格化迁移，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理
GroupNorm：将channel分组再做归一化，把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW，GN介于LN和IN之间，LN（G=C）和IN（G=1）是GN的特例
SwitchableNorm：将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法，集万千宠爱于一身，但训练复杂 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/57dc03ade0cd08a321a1f9b8d6f5f0d8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-13T00:31:53+08:00" />
<meta property="article:modified_time" content="2023-04-13T00:31:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Batch Normalization详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1.%20Batch%20Normalization%E7%9A%84%E5%87%BA%E7%8E%B0-toc" style="margin-left:80px;"><a href="#1.%20Batch%20Normalization%E7%9A%84%E5%87%BA%E7%8E%B0" rel="nofollow">1. Batch Normalization的基本思想</a></p> 
<p id="2.%C2%A0BN%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8%C2%A0-toc" style="margin-left:80px;"><a href="#2.%C2%A0BN%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8%C2%A0" rel="nofollow">2. BN层的作用 </a></p> 
<p id="3.%20%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E5%92%8C%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E7%9A%84BN%E5%B1%82-toc" style="margin-left:80px;"><a href="#3.%20%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E5%92%8C%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E7%9A%84BN%E5%B1%82" rel="nofollow">3. 训练阶段和测试阶段的BN层</a></p> 
<p id="4.%C2%A0BN%E3%80%81LN%E3%80%81IN%E3%80%81GN%E4%B8%8ESN-toc" style="margin-left:80px;"><a href="#4.%C2%A0BN%E3%80%81LN%E3%80%81IN%E3%80%81GN%E4%B8%8ESN" rel="nofollow">4. BN、LN、IN、GN与SN</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h4 id="1.%20Batch%20Normalization%E7%9A%84%E5%87%BA%E7%8E%B0">1. Batch Normalization的基本思想</h4> 
<p><span style="color:#6eaad7;"><strong>BN解决的问题</strong></span>：深度神经网络随着网络深度加深，训练越困难，<span style="color:#e6b223;">收敛越来越慢</span></p> 
<p><span style="color:#6eaad7;"><strong>问题出现的原因</strong></span>：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的<span style="color:#ed7976;">输入数据分布发生变化</span>，通过层层叠加，高层的输入分布变化会<span style="color:#9c8ec1;">非常剧烈</span>，这就使得高层需要不断去重新适应底层的参数更新——BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的<span style="color:#ed7976;">输入保持相同分布</span></p> 
<p><img alt="" height="532" src="https://images2.imgbox.com/b4/2a/fPwiVE3H_o.png" width="1200"></p> 
<p><strong><span style="color:#6eaad7;">BN的基本思想：</span></strong></p> 
<ul><li>训练过程中整体分布逐渐往<span style="color:#faa572;">非线性函数</span>的取值区间的上下限两端靠近，导致反向传播时低层神经网络的梯度消失，训练收敛慢</li><li>BN把每层神经网络任意神经元输出的分布强行拉回到均值为0方差为1的标准正态分布，使得激活输入值落在非线性函数对输入比较敏感的区域，可以得到比较大的梯度，避免梯度消失，梯度变大意味着<span style="color:#faa572;">学习收敛速度快</span>，能大大加快训练速度</li></ul> 
<p><img alt="" height="380" src="https://images2.imgbox.com/39/9a/XrbOldOO_o.png" width="1200"></p> 
<p></p> 
<h4 id="2.%C2%A0BN%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8%C2%A0">2. BN层的作用 </h4> 
<ul><li><strong><span style="color:#b95514;">加快网络的训练和收敛的速度</span></strong> 
  <ul><li>深度神经网络中，每层的数据分布不一样会导致网络非常难收敛和训练，BN将每层数据分布转化为一致的<span style="color:#98c091;">标准正态分布</span>，更容易收敛</li></ul></li><li><strong><span style="color:#b95514;">控制梯度爆炸，防止梯度消失</span></strong> 
  <ul><li><strong>梯度消失</strong>：深度神经网络中，如果网络的激活输出很大，其对应梯度会很小，甚至梯度消失，导致网络的学习速率慢，使用BN层归一化后，网络的输出不会很大，梯度就不会很小</li><li><strong>梯度爆炸</strong>：第一层偏移量的梯度=激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n，假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加，但使用BN层后权值的更新也不会很大</li></ul></li><li><span style="color:#b95514;"><strong>防止过拟合</strong></span> 
  <ul><li>在网络的训练中，BN的使用使得一个minibatch中<span style="color:#98c091;">所有样本都被关联在一起</span>，因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝这一个方向使劲学习，一定程度上<span style="color:#faa572;">避免了过拟合</span></li></ul></li><li>Allows higher learning rates, faster convergence and networks become more robust to initialization -- Fei-Fei Li</li></ul> 
<hr> 
<p><strong><span style="color:#511b78;">注意⚠️</span></strong><span style="color:#511b78;">：为什么BN层一般用在线性层和卷积层后面，而不是放在非线性单元后？</span></p> 
<ul><li><span style="color:#e6b223;">非线性单元的输出分布形状会在训练过程中变化</span><span style="color:#494949;">，归一化无法消除它的方差偏移，相反的，全连接和卷积层的输出一般是一个对称、非稀疏的分布，更加类似高斯分布，对它们进行归一化会产生</span><span style="color:#9c8ec1;">更加稳定的分布</span><span style="color:#494949;"> </span></li><li><span style="color:#494949;">例如Relu激活函数，输入的数据是一个高斯分布，变换后数据小于0的被抑制了（分布小于0的部分直接变成0）</span></li></ul> 
<hr> 
<p><img alt="" height="1108" src="https://images2.imgbox.com/98/8d/1h7yS0Gq_o.png" width="1074"></p> 
<p></p> 
<h4 id="3.%20%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E5%92%8C%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5%E7%9A%84BN%E5%B1%82">3. 训练阶段和测试阶段的BN层</h4> 
<p><span style="color:#6eaad7;"><strong>训练阶段：</strong></span></p> 
<ul><li>例如batchsize为32，某层的某个神经元会输出32个响应值</li><li>对这32个响应求均值和标准差，再做归一化</li><li>归一化的响应值乘以γ，加上β</li><li>每个神经元都<span style="color:#ed7976;">训练</span>一组γ、β</li></ul> 
<p><span style="color:#6eaad7;"><strong>测试阶段：</strong></span></p> 
<ul><li>测试阶段<span style="color:#e6b223;">均值、方差、γ、β</span>都用<span style="color:#ed7976;">训练阶段全局</span>求出</li></ul> 
<p><img alt="" height="144" src="https://images2.imgbox.com/82/d7/MMyZbMde_o.png" width="1062"></p> 
<ul><li>在测试阶段Batch Normalization相当于做<strong><span style="color:#ed7976;">线性变换</span></strong></li></ul> 
<p><img alt="" height="98" src="https://images2.imgbox.com/81/1c/JkvWcYkI_o.png" width="992"></p> 
<hr> 
<p><strong><span style="color:#511b78;">注意⚠️</span></strong><span style="color:#511b78;">：Dropout和BN都有防止过拟合的作用，单独使用，都带来一定的性能改进，为什么一起用反而性能下降？</span></p> 
<p><strong><span style="color:#1c7892;">原因：</span></strong></p> 
<ol><li><span style="color:#494949;">当网络的状态从训练转移到测试时，Dropout转移了特定神经单元的方差。但是，在测试阶段，BN保持了它的统计方差，这是在整个学习过程中积累的。Dropout和BN中方差的不一致性(“方差偏移”)，导致推断中不稳定的数值行为，最终导致错误的预测 </span></li><li><span style="color:#494949;">如下图，Dropout在训练阶段以p的概率进行失活，而在测试阶段，对每个神经元进行尺度放缩(乘p)。另一种等价的表现形式，在训练阶段乘1/p，而测试阶段不需要做任何改动，所以，训练：X=a*(1/p)*X，测试：X=X，存在方差偏移</span></li></ol> 
<p><img alt="" height="600" src="https://images2.imgbox.com/6d/c1/bR9C9j9K_o.png" width="1200"></p> 
<p><span style="color:#1c7892;"><strong>解决方案：</strong></span>总体思路是降低方差偏移</p> 
<ol><li>在所有<span style="color:#ed7976;">BN层后</span>使用Dropout</li><li><span style="color:#ed7976;">修改Dropout公式</span>（如使用高斯Dropout）使得它对方差不是那么敏感</li></ol> 
<hr> 
<p></p> 
<h4 id="4.%C2%A0BN%E3%80%81LN%E3%80%81IN%E3%80%81GN%E4%B8%8ESN">4. BN、LN、IN、GN与SN</h4> 
<p>BN、LN、IN和GN这四个归一化的计算流程几乎一样，输入的图像shape记为[N, C, H, W]，区别在于：</p> 
<p><img alt="" height="359" src="https://images2.imgbox.com/70/4f/8dGEZSwv_o.png" width="1200"></p> 
<p><span style="color:#e6b223;"><strong>BatchNorm</strong></span>：在batch上，对NHW做归一化，把每个通道的NHW单独拿出来归一化处理，针对每个channel都有一组γ,β，可学习的参数为2*C，当<span style="color:#98c091;">batch size越小</span>，BN的<span style="color:#ed7976;">表现效果也越不好</span>，因为计算过程中所得到的均值和方差不能代表全局</p> 
<p><span style="color:#e6b223;"><strong>LayerNorm</strong></span>：在通道方向上，对CHW归一化，把每个CHW单独拿出来归一化处理，<span style="color:#98c091;">不受batchsize 的影响</span>，主要对RNN作用明显，常用在<span style="color:#ed7976;">RNN网络</span>，但如果输入的特征区别很大，那么就不建议使用它做归一化处理</p> 
<p><span style="color:#e6b223;"><strong>InstanceNorm</strong></span>：在图像像素上，对HW做归一化，把每个HW单独拿出来归一化处理，<span style="color:#98c091;">不受通道和batchsize 的影响</span>，常用在<span style="color:#ed7976;">风格化迁移</span>，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理</p> 
<p><span style="color:#e6b223;"><strong>GroupNorm</strong></span>：将channel<span style="color:#98c091;">分组再做归一化</span>，把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW，GN介于LN和IN之间，LN（G=C）和IN（G=1）是GN的特例</p> 
<p><span style="color:#e6b223;"><strong>SwitchableNorm</strong></span>：将<span style="color:#98c091;">BN、LN、IN结合</span>，赋予权重，让网络自己去学习归一化层应该使用什么方法，集万千宠爱于一身，但训练复杂 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8081c065627adfaea3c15456ae5ec1f6/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">VTP的简单应用和配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9b761442e8a90c91102118de92713b66/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">编译原理：算符优先分析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>