<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>2023文本定位模型选型调研 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="2023文本定位模型选型调研" />
<meta property="og:description" content="背景 时间点：2023年03月
场景：做一个通用型的多种证件解析服务
需求：调研一种又新又快的定位模型。要求：1）支持倾斜的文字，可以是4点定位或分割法后获取box，但不能是2点的定位；2）快速，过往的psenet需要至少0.6s，pan和db在一些场景中效果差一点但快，是否有更好平衡速度和效果的方法；3）方便改输出通道数量，这种一般是分割？；4）边缘准确；5）适用于中文大字典
方法：从3个方法研究，pp、mmocr、papers with code
调研 PP https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_ch/PP-OCRv3_introduction.md
assert alg in [ &#39;EAST&#39;, &#39;DB&#39;, &#39;SAST&#39;, &#39;Rosetta&#39;, &#39;CRNN&#39;, &#39;STARNet&#39;, &#39;RARE&#39;, &#39;SRN&#39;, &#39;CLS&#39;, &#39;PGNet&#39;, &#39;Distillation&#39;, &#39;NRTR&#39;, &#39;TableAttn&#39;, &#39;SAR&#39;, &#39;PSE&#39;, &#39;SEED&#39;, &#39;SDMGR&#39;, &#39;LayoutXLM&#39;, &#39;LayoutLM&#39;, &#39;LayoutLMv2&#39;, &#39;PREN&#39;, &#39;FCE&#39;, &#39;SVTR&#39;, &#39;ViTSTR&#39;, &#39;ABINet&#39;, &#39;DB&#43;&#43;&#39;, &#39;TableMaster&#39;, &#39;SPIN&#39;, &#39;VisionLAN&#39;, &#39;Gestalt&#39;, &#39;SLANet&#39;, &#39;RobustScanner&#39;, &#39;CT&#39;, &#39;RFL&#39;, &#39;DRRG&#39;, &#39;CAN&#39;, &#39;Telescope&#39; ] PP-OCRv3检测模块仍基于DB算法优化，而识别模块不再采用CRNN，换成了IJCAI 2022最新收录的文本识别算法SVTR
mmocr https://gitee.com/open-mmlab/mmocr#%E6%A8%A1%E5%9E%8B%E5%BA%93
最新的检测识别是dbnet&#43;&#43;、SVTR
papers with code 从ppocr和mmocr看到，2者选择的方案都是dbnet&#43;&#43;及SVTR，dbnet&#43;&#43;是2022年的算法。在paperswithcode中，不是特别高的排名，有个疑问，为啥最后会选用这个呢？出于速度考虑？？？
https://paperswithcode.com/paper/real-time-scene-text-detection-with-1
目前考虑的方法
DPText-DETR，排名最高，后续试了效果还不错，感动，哪怕是有600多切片的图片，速度也在0.1-0.2s
FAST-B-800，排名2
DBNet&#43;&#43; 商业化落地最多的
排除的方法
TextFuseNet不适合中文" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2db35fa153a146a7ba365c367ab922d8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-05T17:48:35+08:00" />
<meta property="article:modified_time" content="2023-06-05T17:48:35+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2023文本定位模型选型调研</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="kdocs-document"> 
 <h2 style="">背景</h2> 
 <p style="">时间点：2023年03月</p> 
 <p style="">场景：做一个通用型的多种证件解析服务</p> 
 <p style="">需求：调研一种又新又快的定位模型。要求：1）支持倾斜的文字，可以是4点定位或分割法后获取box，但不能是2点的定位；2）快速，过往的psenet需要至少0.6s，pan和db在一些场景中效果差一点但快，是否有更好平衡速度和效果的方法；3）方便改输出通道数量，这种一般是分割？；4）边缘准确；5）适用于中文大字典</p> 
 <p style="">方法：从3个方法研究，pp、mmocr、papers with code</p> 
 <p style=""></p> 
 <h2 style="">调研</h2> 
 <h3 style="">PP</h3> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_ch/PP-OCRv3_introduction.md" target="_blank" rel="noopener noreferrer">https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_ch/PP-OCRv3_introduction.md</a></p> 
 <pre class="kdocs-python"><code class="language-python"> assert alg in [
        'EAST', 'DB', 'SAST', 'Rosetta', 'CRNN', 'STARNet', 'RARE', 'SRN',
        'CLS', 'PGNet', 'Distillation', 'NRTR', 'TableAttn', 'SAR', 'PSE',
        'SEED', 'SDMGR', 'LayoutXLM', 'LayoutLM', 'LayoutLMv2', 'PREN', 'FCE',
        'SVTR', 'ViTSTR', 'ABINet', 'DB++', 'TableMaster', 'SPIN', 'VisionLAN',
        'Gestalt', 'SLANet', 'RobustScanner', 'CT', 'RFL', 'DRRG', 'CAN',
        'Telescope'
    ]
</code></pre> 
 <p style="">PP-OCRv3检测模块仍基于DB算法优化，而识别模块不再采用CRNN，换成了IJCAI 2022最新收录的文本识别算法<a class="kdocs-link" style="color:#0A6CFF;" href="https://arxiv.org/abs/2205.00159" rel="nofollow noopener noreferrer" target="_blank">SVTR</a></p> 
 <h3 style="">mmocr</h3> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://gitee.com/open-mmlab/mmocr#%E6%A8%A1%E5%9E%8B%E5%BA%93" rel="nofollow noopener noreferrer" target="_blank">https://gitee.com/open-mmlab/mmocr#%E6%A8%A1%E5%9E%8B%E5%BA%93</a></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:617px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:84.76499%;height:0;"> 
    <img src="https://images2.imgbox.com/83/7a/8Hl50aM5_o.png" style="margin-left:;display:block;width:617px;margin-top:-84.76499%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="">最新的检测识别是dbnet++、SVTR</p> 
 <p style=""></p> 
 <h3 style="">papers with code</h3> 
 <p style="">从ppocr和mmocr看到，2者选择的方案都是dbnet++及SVTR，dbnet++是2022年的算法。<span class="kdocs-bold" style="font-weight:bold;">在paperswithcode中，不是特别高的排名，有个疑问，为啥最后会选用这个呢？出于速度考虑？？？</span></p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://paperswithcode.com/paper/real-time-scene-text-detection-with-1" rel="nofollow noopener noreferrer" target="_blank">https://paperswithcode.com/paper/real-time-scene-text-detection-with-1</a></p> 
 <p style=""></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:1126px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:57.904083%;height:0;"> 
    <img src="https://images2.imgbox.com/50/f1/ekrXmxkz_o.png" style="margin-left:;display:block;width:1126px;margin-top:-57.904083%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <p style="">目前考虑的方法</p> 
 <p style="">DPText-DETR，排名最高，后续试了效果还不错，感动，哪怕是有600多切片的图片，速度也在0.1-0.2s</p> 
 <p style="">FAST-B-800，排名2</p> 
 <p style="">DBNet++ 商业化落地最多的</p> 
 <p style=""></p> 
 <p style="">排除的方法</p> 
 <p style="">TextFuseNet不适合中文</p> 
 <p style="">I3CL + SSL 没相关资料文章，怕有坑，本身也不是最好的2个</p> 
 <p style="">CharNet H-88 不适合中文</p> 
 <p style=""></p> 
 <h2 style="">N种排名靠前的算法</h2> 
 <p style=""></p> 
 <h3 style="">DPText-DETR 京东探索研究院</h3> 
 <p style="">时间：2022.07</p> 
 <p style="">开源：<a class="kdocs-link" style="color:#0A6CFF;" href="https://github.com/ViTAE-Transformer/DeepSolo" target="_blank" rel="noopener noreferrer">https://github.com/ViTAE-Transformer/DeepSolo</a></p> 
 <p style="">paper：DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer</p> 
 <p style="">是否支持曲线文本：是</p> 
 <p style="">速度：</p> 
 <p style="">解读：<a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/moxibingdao/article/details/128910689" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/moxibingdao/article/details/128910689</a></p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://zhuanlan.zhihu.com/p/569496186" rel="nofollow noopener noreferrer" target="_blank">https://zhuanlan.zhihu.com/p/569496186</a></p> 
 <p style="">原先算法的缺陷：</p> 
 <p style="">1）用xywh表示位置先验信息导致训练慢 </p> 
 <p style="">2）基于阅读顺序的标注方法（即文本开始循环一圈）降低了模型性能。</p> 
 <p style="">本文创新点：</p> 
 <ol start="1"><li style="margin-left:1.4em;list-style-type:decimal;text-indent:0;"><p>显式点query构建(Explicit Point Query Modeling, EPQM) 方法，用上下边界多点均匀采样得到的N点代替xywh的box，显示细化的位置先验信息有助于加速收敛。即用固定数量控制点代替检测框</p></li></ol> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:45.405407%;height:0;"> 
    <img src="https://images2.imgbox.com/7f/82/beZqFnBY_o.png" style="margin-left:;display:block;width:740px;margin-top:-45.405407%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <p style="">2）增强的因子化自注意(Enhanced Factorized Self-Attention, EFSA) 模块，挖掘同一文本实例内</p> 
 <p style="">控制点query之间的关系。引入了环形卷积与实例内自注意力并行以提供显式的环形引导，明确地模拟多边形点序列的圆形，引入更多的先验以充分挖掘实例内不同控制点query的关系。增强的实例内关系建模与实例间关系建模共同构成了EFSA模块。</p> 
 <p style="">3）不依赖文本阅读顺序的标签形式</p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:35.0%;height:0;"> 
    <img src="https://images2.imgbox.com/d6/b6/r9nYJUze_o.png" style="margin-left:;display:block;width:740px;margin-top:-35.0%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="">消融实验如下，可以看到不同新模块对速度及精度的影响</p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:717px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:83.40307%;height:0;"> 
    <img src="https://images2.imgbox.com/68/4b/2qZBuZCT_o.png" style="margin-left:;display:block;width:717px;margin-top:-83.40307%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="">基于DEtection TRansformer (DETR) 框架，detr是2020年facebook基于vision transformer做的目标检测工作，没有了anchor和box操作。架构如下图：</p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/qq_35831906/article/details/124118569" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_35831906/article/details/124118569</a></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:27.567568%;height:0;"> 
    <img src="https://images2.imgbox.com/4b/4f/6mKXD70C_o.png" style="margin-left:;display:block;width:740px;margin-top:-27.567568%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <h3 style="">FAST-B-800</h3> 
 <p style="text-align:left;">时间：2021</p> 
 <p style="text-align:left;">开源：是</p> 
 <p style="text-align:left;">paper：FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation</p> 
 <p style="text-align:left;">是否支持曲线文本：是</p> 
 <p style="text-align:left;">速度：快</p> 
 <p style="">NAS网络搜索</p> 
 <p style="">backbone+类似残差结构+预测层(head)+Text dilation</p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/qq_44498420/article/details/125593141" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_44498420/article/details/125593141</a></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:30.27027%;height:0;"> 
    <img src="https://images2.imgbox.com/29/5b/G8lV6WDk_o.png" style="margin-left:;display:block;width:740px;margin-top:-30.27027%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <p style=""></p> 
 <h3 style="">TextFuseNet</h3> 
 <p style="text-align:left;">时间：2020</p> 
 <p style="text-align:left;">开源：</p> 
 <p style="text-align:left;">paper：TextFuseNet: Scene Text Detection with Richer Fused Features</p> 
 <p style="text-align:left;">是否支持曲线文本：是</p> 
 <p style="text-align:left;">是否适合中文：否</p> 
 <p style="text-align:left;">速度：</p> 
 <p style="">解读：<a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/lz867422770/article/details/109170271" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/lz867422770/article/details/109170271</a></p> 
 <p style="">参考maskTextspotter和maskRCNN的思想，做实例分割。框架如下图，提取字符，单词和全局级别的特征，并引入多路径融合体系结构以融合它们以进行准确的文本检测。</p> 
 <p style="">maskTextspotter是单词级别的检测分割，<span class="kdocs-bold" style="font-weight:bold;">不适合中文场景。</span></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:43.37838%;height:0;"> 
    <img src="https://images2.imgbox.com/90/b3/FtDk2rU8_o.png" style="margin-left:;display:block;width:740px;margin-top:-43.37838%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <h3 style="">I3CL + SSL</h3> 
 <p style="text-align:left;">时间：2021</p> 
 <p style="text-align:left;">开源：</p> 
 <p style="text-align:left;">paper：I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection</p> 
 <p style="text-align:left;">是否支持曲线文本：</p> 
 <p style="text-align:left;">速度：</p> 
 <h3 style="">CharNet H-88</h3> 
 <p style="text-align:left;">时间：2019</p> 
 <p style="text-align:left;">开源：是</p> 
 <p style="text-align:left;">paper：Convolutional Character Networks</p> 
 <p style="text-align:left;">是否支持曲线文本：是</p> 
 <p style="text-align:left;">是否适合中文：否</p> 
 <p style="text-align:left;">速度：</p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://zhuanlan.zhihu.com/p/90683589" rel="nofollow noopener noreferrer" target="_blank">https://zhuanlan.zhihu.com/p/90683589</a></p> 
 <p style="">在字符级annotation的基础上完成了文本检测和识别的one-stage网络。又是字符级，做中文不考虑。</p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:717px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:34.309624%;height:0;"> 
    <img src="https://images2.imgbox.com/b9/8d/LsX4zw76_o.png" style="margin-left:;display:block;width:717px;margin-top:-34.309624%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <h3 style="">FAST-B-640</h3> 
 <p style="">比800弱一点</p> 
 <p style=""></p> 
 <p style=""></p> 
 <h3 style="">DBNet++</h3> 
 <p style="text-align:left;">时间：2022</p> 
 <p style="text-align:left;">开源：是</p> 
 <p style="text-align:left;">paper：</p> 
 <p style="">Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion</p> 
 <p style="text-align:left;">是否支持曲线文本：</p> 
 <p style="text-align:left;">速度：</p> 
 <p style=""><a class="kdocs-link" style="color:#0A6CFF;" href="https://blog.csdn.net/qq_14845119/article/details/127103550" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_14845119/article/details/127103550</a></p> 
 <p style="">DBNet：Real-time Scene Text Detection with Differentiable BinarizationReal-time Scene Text Detection with Differentiable Binarization</p> 
 <p style="">DBNet++：Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion</p> 
 <p style="">如下图，横轴是FPS，纵轴是F，db++牺牲了一点速度获得了性能的提升。</p> 
 <p style=""></p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:387px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:78.29457%;height:0;"> 
    <img src="https://images2.imgbox.com/48/f9/0wjNmlWG_o.png" style="margin-left:;display:block;width:387px;margin-top:-78.29457%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:78.10811%;height:0;"> 
    <img src="https://images2.imgbox.com/48/63/eRy6XLDi_o.png" style="margin-left:;display:block;width:740px;margin-top:-78.10811%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:78.10811%;height:0;"> 
    <img src="https://images2.imgbox.com/14/22/2sS2Uy98_o.png" style="margin-left:;display:block;width:740px;margin-top:-78.10811%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style=""></p> 
 <p style="">输出是概率图（probability map）和阈值图（threshold map）</p> 
 <p style="">DBNet++在DBNet的基础上增加了ASF（Adaptive Scale Fusion）模块。不同尺度的特征通过ASF模块处理，可以得到更佳的融合特征。</p> 
 <p style="">ASF模块通过引入空间attention机制，使得融合后的特征更加鲁棒。</p> 
 <p style="">其中N表示要融合的特征数，这里N=4，表示从4个不同的分支引出的特征。</p> 
 <p style="">db差异二值化的作用：每个像素都使用不同的阈值进行二值化处理。而这个不同的阈值矩阵又是网络学习得到的。为了保证整个优化过程有梯度的传递，这里又将概率图和阈值图的差传入sigmoid函数，以此来保证梯度的传递。通过梯度优化，保证了不同的图片使用不同的阈值矩阵，达到最佳的二值化效果。</p> 
 <p style="">如何处理相邻较近的文本：为了增大相邻文字之间的间距，缓解文字离得太近或者部分重叠的情况。概率图（probability map）的制作会在原始红色多边形的基础上，使用Vatti clipping算法，向内收缩D的距离。</p> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:569px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:61.68717%;height:0;"> 
    <img src="https://images2.imgbox.com/9b/8c/dtEl0T1I_o.png" style="margin-left:;display:block;width:569px;margin-top:-61.68717%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="">阈值图（threshold map）在红色多边形的基础上，分别向内收缩D距离形成蓝色多边形，向外扩张D距离形成绿色多边形。蓝色多边形和绿色多边形之间的像素形成阈值图。然后计算图内每个像素离最近的边（蓝色边，绿色边）的归一化距离，形成最终的阈值图。阈值图看起来中间像素亮，边缘像素暗。</p> 
 <p style="">后处理时，使用概率图（probability map）或者使用二值图（approximate binary map）都是可以的。两者在效果上是一样的。这样在推理过程中，就可以去掉网络中的二值化过程，直接使用概率图。这样网络中的二值化过程的loss就更像一个辅助loss，来使得网络训练的效果更好。</p> 
 <p style="">简单来说，后处理时通过概率图获取中心连通域，再用规则缩放边界。</p> 
 <p style=""></p> 
 <p style=""></p> 
 <p style=""></p> 
 <p style=""></p> 
 <h3 style="">DBNet</h3> 
 <div class="kdocs-line-container" style="display:flex;"> 
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;display:flex;width:740px;justify-content:center;align-items:center;height:auto;"> 
   <div class="kdocs-img" style="padding-top:32.7027%;height:0;"> 
    <img src="https://images2.imgbox.com/90/91/DO09Scxi_o.png" style="margin-left:;display:block;width:740px;margin-top:-32.7027%;height:auto;"> 
   </div> 
  </div> 
 </div> 
 <p style="">网络输入假设为w*h*3。网络整体结构采用FPN的设计思想，进行了5次下采样，3次上采样操作。最终的输出特征图大小为原图的1/4。网络头部部分，分别引出2个分支。一个负责预测概率图（probability map，(w/4)*(h/4)*1），代销为，另一个负责预测阈值图（threshold map，(w/4)*(h/4)*1）。概率图经过阈值图处理，进行二值化后得到二值图（approximate binary map，(w/4)*(h/4)*1）。最后经过后处理操作得到最终文字的边。</p> 
 <p style=""></p> 
 <p style=""></p> 
 <p style=""></p> 
 <p style=""></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/08c493fb56b166ac10e2179b9a323e19/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Mybatis（黑马程序员）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ddb4a992cd3c43480cebe1dbcb73360c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">centos部署openvas</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>