<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>A Diversity-Promoting Objective Function for Neural Conversation Models论文阅读笔记 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="A Diversity-Promoting Objective Function for Neural Conversation Models论文阅读笔记" />
<meta property="og:description" content="本文是李纪为的论文“A Diversity-Promoting Objective Function for Neural Conversation Models”阅读笔记。违章提出使用MMI代替原始的maximum likelihood作为目标函数，目的是使用互信息减小“I don’t Know”这类无聊响应的生成概率。一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。本文提出了两种模型（其实就是改了下目标函数，而且训练过程中仍然使用likelihood，仅在测试的时候使用新的目标函数将有意义的响应的概率变大~~），MMI-antiLM和MMI-bidi，下面分别进行介绍。
新的目标函数 在介绍模型之前先来看看新的目标函数和普通的目标函数的区别，以便清楚地明白新目标函数的作用和功能。首先看下原始的目标函数，就是在给定输入S的情况下生成T的概率，其实就是一个T中每个单词出现的条件概率的连乘。
接下来看提出的第一个目标函数MMI-antiLM，在其基础上添加了目标序列本身的概率logp(T)，p(T)就是一句话存在的概率，也就是一个模型，前面的lambda是惩罚因子，越大说明对语言模型惩罚力度越大。由于这里用的是减号，所以相当于在原本的目标上减去语言模型的概率，也就降低了“I don’t know”这类高频句子的出现概率。
然后是第二个目标函数MMI-bidi，在原始目标函数基础上添加logp(S|T)，也就是T的基础上产生S的概率，而且可以通过改变lambda的大小衡量二者的重要性。后者可以表示在响应输入模型时产生输入的概率，自然像“I don’t know”这种答案的概率会比较低，而这里使用的是相加，所以会降低这种相应的概率。接下来我们再详细介绍两个模型的细节
MMI-antiLM 如上所说，MMI-antiLM模型使用第一个目标函数，引入了logp(T)，如果lambda取值不合适可能会导致产生的响应不符合语言模型，所以在实际使用过程中会对其进行修正。由于解码过程中往往第一个单词或者前面几个单词是根据encode向量选择的，后面的单词更倾向于根据前面decode的单词和语言模型选择，而encode的信息影响较小。也就是说我们只需要对前面几个单词进行惩罚，后面的单词直接根据语言模型选择即可，这样就不会使整个句子不符合语言模型了。使用下式中的U(T)代替p(T),式中g(k)表示要惩罚的句子长度：
此外，我们还想要加入响应句子的长度这个因素，也作为模型相应的依据，所以将上面的目标函数修正为下式：
MMI-bidi MMI-bidi模型引入了p(S|T)项，这就需要先计算出完整的T序列再将其传入一个提前训练好的反向seq2seq模型中计算该项的值。但是考虑到S序列会产生无数个可能的T序列，我们不可能将每一个T都进行计算，所以这里引入beam-search只计算前200个序列T来代替。然后再计算两项和，进行得分重排。论文中也提到了这么做的缺点，比如最终的效果会依赖于选择的前N个序列的效果等等，但是实际的效果还是可以的。
实验结果 最终在Twitter和OpenSubtitle两个数据集上面进行测试，效果展示BLEU得分逗比标准的seq2seq模型要好。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/8e918083517ea04bf8fbe3a9b62e92f2/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-01-23T20:46:17+08:00" />
<meta property="article:modified_time" content="2018-01-23T20:46:17+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">A Diversity-Promoting Objective Function for Neural Conversation Models论文阅读笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文是李纪为的论文“A Diversity-Promoting Objective Function for Neural Conversation Models”阅读笔记。违章提出使用MMI代替原始的maximum likelihood作为目标函数，目的是使用互信息减小“I don’t Know”这类无聊响应的生成概率。一般的seq2seq模型，倾向于生成安全、普适的响应，因为这种响应更符合语法规则，在训练集中出现频率也较高，最终生成的概率也最大，而有意义的响应生成概率往往比他们小。通过MMI来计算输入输出之间的依赖性和相关性，可以减少模型对他们的生成概率。本文提出了两种模型（其实就是改了下目标函数，而且训练过程中仍然使用likelihood，仅在测试的时候使用新的目标函数将有意义的响应的概率变大~~），MMI-antiLM和MMI-bidi，下面分别进行介绍。</p> 
<h2 id="新的目标函数">新的目标函数</h2> 
<p>在介绍模型之前先来看看新的目标函数和普通的目标函数的区别，以便清楚地明白新目标函数的作用和功能。首先看下原始的目标函数，就是在给定输入S的情况下生成T的概率，其实就是一个T中每个单词出现的条件概率的连乘。</p> 
<p><img src="https://images2.imgbox.com/4c/bd/uJl2oyJv_o.png" alt="" title=""></p> 
<p>接下来看提出的第一个目标函数MMI-antiLM，在其基础上添加了目标序列本身的概率logp(T)，p(T)就是一句话存在的概率，也就是一个模型，前面的lambda是惩罚因子，越大说明对语言模型惩罚力度越大。由于这里用的是减号，所以相当于在原本的目标上减去语言模型的概率，也就降低了“I don’t know”这类高频句子的出现概率。</p> 
<p><img src="https://images2.imgbox.com/c0/dd/sxc9P7fz_o.png" alt="" title=""></p> 
<p>然后是第二个目标函数MMI-bidi，在原始目标函数基础上添加logp(S|T)，也就是T的基础上产生S的概率，而且可以通过改变lambda的大小衡量二者的重要性。后者可以表示在响应输入模型时产生输入的概率，自然像“I don’t know”这种答案的概率会比较低，而这里使用的是相加，所以会降低这种相应的概率。接下来我们再详细介绍两个模型的细节</p> 
<p><img src="https://images2.imgbox.com/bf/d1/eCeaUCMS_o.png" alt="" title=""></p> 
<h2 id="mmi-antilm">MMI-antiLM</h2> 
<p>如上所说，MMI-antiLM模型使用第一个目标函数，引入了logp(T)，如果lambda取值不合适可能会导致产生的响应不符合语言模型，所以在实际使用过程中会对其进行修正。由于解码过程中往往第一个单词或者前面几个单词是根据encode向量选择的，后面的单词更倾向于根据前面decode的单词和语言模型选择，而encode的信息影响较小。也就是说我们只需要对前面几个单词进行惩罚，后面的单词直接根据语言模型选择即可，这样就不会使整个句子不符合语言模型了。使用下式中的U(T)代替p(T),式中g(k)表示要惩罚的句子长度：</p> 
<p><img src="https://images2.imgbox.com/0a/9b/n2cQQPI2_o.png" alt="" title=""></p> 
<p><img src="https://images2.imgbox.com/44/1f/XxdvrSU7_o.png" alt="" title=""></p> 
<p>此外，我们还想要加入响应句子的长度这个因素，也作为模型相应的依据，所以将上面的目标函数修正为下式：</p> 
<p><img src="https://images2.imgbox.com/ce/3c/2rvIr9cq_o.png" alt="" title=""></p> 
<h2 id="mmi-bidi">MMI-bidi</h2> 
<p>MMI-bidi模型引入了p(S|T)项，这就需要先计算出完整的T序列再将其传入一个提前训练好的反向seq2seq模型中计算该项的值。但是考虑到S序列会产生无数个可能的T序列，我们不可能将每一个T都进行计算，所以这里引入beam-search只计算前200个序列T来代替。然后再计算两项和，进行得分重排。论文中也提到了这么做的缺点，比如最终的效果会依赖于选择的前N个序列的效果等等，但是实际的效果还是可以的。</p> 
<h2 id="实验结果">实验结果</h2> 
<p>最终在Twitter和OpenSubtitle两个数据集上面进行测试，效果展示BLEU得分逗比标准的seq2seq模型要好。</p> 
<p><img src="https://images2.imgbox.com/3b/c1/eCKCuSbj_o.png" alt="" title=""></p> 
<p><img src="https://images2.imgbox.com/17/fd/r4lQ6g64_o.png" alt="" title=""></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b1921c4a14869e6f27824d7ca9f7a84b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">11-STM32F1 ADC</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/ab2615c4799f4c613b74f0a4c69023a2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">数字特征值-对数字求特征值是常用的编码算法，奇偶特征是一种简单的特征值</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>