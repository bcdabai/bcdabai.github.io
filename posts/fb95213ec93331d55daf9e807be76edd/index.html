<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TensorFlow 2.0教程05：跨多个节点的分布式培训 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TensorFlow 2.0教程05：跨多个节点的分布式培训" />
<meta property="og:description" content="分布式训练允许扩大深度学习任务，因此可以学习更大的models或以更快的速度进行训练。在之前的教程中，我们讨论了如何MirroredStrategy在单个节点（物理机器）内实现多GPU训练。在本教程中，我们将解释如何在多个节点之间进行分布式训练。本教程包括：
用于多节点分布式培训的代码样板。示例代码在多台计算机上运行。 要重现本教程，请参考TensorFlow 2分布式培训 github repository。
代码样板
与单节点内的多GPU训练相似，多节点训练也使用分布式策略。在这种情况下，tf.distribute.experimental.MultiWorkerMirroredStrategy。多节点训练还需要设置TF_CONFIG环境变量。请注意，每个节点上的环境变量都会略有不同。例如，这是worker 0两节点分布式培训作业中的设置：
os.environ[&#34;TF_CONFIG&#34;] = json.dumps({ &#39;cluster&#39;: { &#39;worker&#39;: [&#34;10.1.10.58:12345&#34;, &#34;10.1.10.250:12345&#34;] }, &#39;task&#39;: {&#39;type&#39;: &#39;worker&#39;, &#39;index&#39;: 0} }) 本质上，TF_CONFIG是一个JSON字符串，代表集群并标识该机器在该集群中的角色。上面的代码设置了TF_CONFIG环境变量，也可以使用命令行导出或将其作为shell命令的前缀进行设置，例如：
TF_CONFIG=&#39;{&#34;cluster&#34;: {&#34;worker&#34;: [&#34;10.1.10.58:12345&#34;, &#34;10.1.10.250:12345&#34;]}, &#34;task&#34;: {&#34;index&#34;: 0, &#34;type&#34;: &#34;worker&#34;}}&#39; python worker.py cluster所有节点上的字段均相同。它描述了如何设置群集。在这种情况下，我们的集群只有两个工作节点，其IP:port信息列在worker阵列中。该task场从节点到不同的节点。它指定节点的类型和索引，然后将其用于从cluster字段中获取详细信息，并提供给任务管理器以划分工作。在这种情况下，此配置文件指示培训作业在工作程序0上运行，即&#34;10.1.10.58:12345&#34;
我们需要为每个节点自定义此python代码段。因此，第二个节点将具有&#39;task&#39;: {&#39;type&#39;: &#39;worker&#39;, &#39;index&#39;: 1}。
然后，我们需要创建分布式策略：
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() 注意，此行必须TF_CONFIG在定义数据管道和模型之后以及在定义数据管道和模型之前完成。否则，Collective ops must be configured at program startup将触发错误。
代码样板的最后一部分定义了策略范围内的模型：
with strategy.scope(): model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES) model.compile( optimizer=opt, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;sparse_categorical_accuracy&#39;]) model.fit(train_dataset, epochs=NUM_EPOCHS) 进行训练
要运行分布式培训，需要自定义培训脚本并将其复制到所有节点。为了更加清楚，我们可以使用前缀语法设置环境变量。各个节点的设置不同。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/fb95213ec93331d55daf9e807be76edd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-19T00:27:57+08:00" />
<meta property="article:modified_time" content="2020-08-19T00:27:57+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TensorFlow 2.0教程05：跨多个节点的分布式培训</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>分布式训练允许扩大深度学习任务，因此可以学习更大的models或以更快的速度进行训练。在之前的教程中，我们讨论了如何<code>MirroredStrategy</code>在单个节点（物理机器）内实现多GPU训练。在本教程中，我们将解释如何在多个节点之间进行分布式训练。本教程包括：</p> 
<ul><li>用于多节点分布式培训的代码样板。</li><li>示例代码在多台计算机上运行。</li></ul> 
<p>要重现本教程，请参考<a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">TensorFlow 2分布式培训</a><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training"> github repository</a>。</p> 
<p> </p> 
<p><strong>代码样板</strong></p> 
<p>与单节点内的多GPU训练相似，多节点训练也使用分布式策略。在这种情况下，<code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code>。多节点训练还需要设置<code>TF_CONFIG</code>环境变量。请注意，每个节点上的环境变量都会略有不同。例如，这是<code>worker 0</code>两节点分布式培训作业中的设置：</p> 
<p> </p> 
<pre>os.environ["TF_CONFIG"] = json.dumps({
    'cluster': {
        'worker': ["10.1.10.58:12345", "10.1.10.250:12345"]
    },
    'task': {'type': 'worker', 'index': 0}
})</pre> 
<p> </p> 
<p>本质上，TF_CONFIG是一个JSON字符串，代表集群并标识该机器在该集群中的角色。上面的代码设置了TF_CONFIG环境变量，也可以使用命令行导出或将其作为shell命令的前缀进行设置，例如：</p> 
<pre>TF_CONFIG='{"cluster": {"worker": ["10.1.10.58:12345", "10.1.10.250:12345"]}, "task": {"index": 0, "type": "worker"}}' python worker.py</pre> 
<p> </p> 
<p><code>cluster</code>所有节点上的字段均相同。它描述了如何设置群集。在这种情况下，我们的集群只有两个工作节点，其<code>IP:port</code>信息列在<code>worker</code>阵列中。该<code>task</code>场从节点到不同的节点。它指定节点的类型和索引，然后将其用于从<code>cluster</code>字段中获取详细信息，并提供给任务管理器以划分工作。在这种情况下，此配置文件指示培训作业在工作程序0上运行，即<code>"10.1.10.58:12345"</code></p> 
<p>我们需要为每个节点自定义此python代码段。因此，第二个节点将具有<code>'task': {'type': 'worker', 'index': 1}</code>。</p> 
<p>然后，我们需要创建分布式策略：</p> 
<pre>strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()</pre> 
<p> </p> 
<p>注意，此行必须<code>TF_CONFIG</code>在定义数据管道和模型之后以及在定义数据管道和模型之前完成。否则，<code>Collective ops must be configured at program startup</code>将触发错误。</p> 
<p>代码样板的最后一部分定义了策略范围内的模型：</p> 
<pre>with strategy.scope():
  model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)
  model.compile(
            optimizer=opt,
            loss='sparse_categorical_crossentropy',
            metrics=['sparse_categorical_accuracy']) 
model.fit(train_dataset,
          epochs=NUM_EPOCHS)</pre> 
<p> </p> 
<h3> </h3> 
<p><strong>进行训练</strong></p> 
<p>要运行分布式培训，需要自定义培训脚本并将其复制到所有节点。为了更加清楚，我们可以使用前缀语法设置环境变量。各个节点的设置不同。</p> 
<p>确保节点可以在不要求密码的情况下相互连接。最方便的方法是使用ssh密钥而不是密码身份验证。<a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">如何使用ssh键。</a></p> 
<p>最后，在两个节点上同时运行脚本。</p> 
<pre># On the first nodeTF_CONFIG='{"cluster": {"worker": ["10.1.10.58:12345", "10.1.10.250:12345"]}, "task": {"index": 0, "type": "worker"}}' python worker.py# On the second nodeTF_CONFIG='{"cluster": {"worker": ["10.1.10.58:12345", "10.1.10.250:12345"]}, "task": {"index": 1, "type": "worker"}}' python worker.py</pre> 
<p>训练现在分布在多个节点上。由于使用了该<code>Mirrored</code>策略，因此两个节点的输出已同步。</p> 
<p> </p> 
<p><strong>总结</strong></p> 
<p>本教程说明了如何在TensorFlow 2中进行分布式训练。关键是设置<code>TF_CONFIG</code>环境变量并使用<code>MultiWorkerMirroredStrategy</code>来限定模型定义的范围。</p> 
<p>在本教程中，我们需要在每个节点上手动运行定制的<code>TF_CONFIG</code>训练脚本。可以看到，随着节点数量的增加，设置环境变量很快变得乏味。有更高级的方法可以在大量节点上部署分布式培训作业，例如Horovod，带有TF-flow的Kubernetes，OpenMPI或使用诸如Ansible的部署脚本。要在本教程中重现结果，请参考此<a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">TensorFlow 2分布式培训教程github </a><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">repository</a>.</p> 
<p>接下来，给大家介绍一下租用GPU做实验的方法，我们是在智星云租用的GPU，使用体验很好。具体大家可以参考：智星云官网： <a href="http://www.ai-galaxy.cn/" rel="nofollow">http://www.ai-galaxy.cn/</a>，淘宝店：<a href="https://shop36573300.taobao.com/" rel="nofollow">https://shop36573300.taobao.com/</a>公众号: 智星AI</p> 
<p><img alt="" src="https://images2.imgbox.com/eb/3f/C4b69ytH_o.png" width="367"></p> 
<p> </p> 
<p>参考文献：</p> 
<p><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">https://lambdalabs.com/blog/tensorflow-2-0-tutorial-0</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">5</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">-</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">distribut</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">e</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">d-tr</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">a</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">ining-mu</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">lti</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">-</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">n</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">ode</a><a href="https://lambdalabs.com/blog/tensorflow-2-0-tutorial-05-distributed-training-multi-node/" rel="nofollow">/</a></p> 
<p><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">https://github.com/lambdal/TensorFlow2-tutorial/tree/master/0</a><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">5-distributed</a><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">-</a><a href="https://github.com/lambdal/TensorFlow2-tutorial/tree/master/05-distributed-training">training</a></p> 
<p><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">https://d</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">e</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">bi</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">a</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">n-administ</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">r</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">ation.org/artic</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">l</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">e/530/SSH_with_authentication_ke</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">y</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">_in</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">st</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">ead_</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">o</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">f_</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">p</a><a href="https://debian-administration.org/article/530/SSH_with_authentication_key_instead_of_password" rel="nofollow">assword</a>​</p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b00c436b3ea11b4ed3d3bf46557b2f32/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux Docker容器 镜像的详解与创建</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5dcfc4efbc685cbd3c9cae8b10d9c620/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">mvn编译java project: Failed to execute goal org.codehaus.mojo:exec-maven-plugin</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>