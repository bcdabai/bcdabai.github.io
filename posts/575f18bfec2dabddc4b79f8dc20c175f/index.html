<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>kafka connect，将数据批量写到hdfs完整过程 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="kafka connect，将数据批量写到hdfs完整过程" />
<meta property="og:description" content="版权声明：本文为博主原创文章，未经博主允许不得转载
本文是基于hadoop 2.7.1，以及kafka 0.11.0.0。kafka-connect是以单节点模式运行，即standalone。
一. 首先，先对kafka和kafka connect做一个简单的介绍
kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。比较直观的解释就是其有一个生产者（producer）和一个消费者（consumer）。可以将kafka想象成一个数据容器，生产者负责发送数据到这个容器中，而消费者从容器中取出数据，在将数据做处理，如存储到hdfs。
kafka connect：Kafka Connect是一种用于在Kafka和其他系统之间可扩展的、可靠的流式传输数据的工具。它使得能够快速定义将大量数据集合移入和移出Kafka的连接器变得简单。即适合批量数据导入导出操作。
二. 下面将介绍如何用kafka connect将数据写入到hdfs中。包括在这个过程中可能碰到的一些问题说明。
首先启动kafka-connect:
bin/connect-standalone.sh config/connect-standalone.properties config/connector1.properties 这个命令后面两个参数，
第一个是指定启动的模式，有分布式和单节点两种，这里是单节点。kafka自带，放于config目录下。
第二个参数指向描述connector的属性的文件，可以有多个，这里只有一个connector用来写入到hdfs。需要自己创建。
接下来看看connector1.properties的内容，
name=&#34;test&#34; #该connector的名字
#将自己按connect接口规范编写的代码打包后放在kafka/libs目录下，再根据项目结构引用对应connector
connector.class=hdfs.HdfsSinkConnector #Task是导入导出的具体实现，这里是指定多少个task来并行运行导入导出作业，由多线程实现。由于hdfs中一个文件每次只能又一个文件操作，所以这里只能是1 tasks.max=1 #指定从哪个topic读取数据，这些其实是用来在connector或者task的代码中读取的。
topics=test #指定key以那种方式转换，需和Producer发送方指定的序列化方式一致
key.converter=org.apache.kafka.connect.converters.ByteArrayConverter value.converter=org.apache.kafka.connect.json.JsonConverter #同上 hdfs.url=hdfs://127.0.0.1:9000　#hdfs的url路径，在Connector中会被读取
hdfs.path=/test/file　#hdfs文件路径，同样Connector中被读取
key.converter.schemas.enable=true　#稍后介绍，可以true也可以false，影响传输格式
value.converter.schemas.enable=true　#稍后介绍，可以true也可以false
三. 接下来看代码，connect主要是导入导出两个概念，导入是source，导出时Sink。这里只使用Sink，不过Source和Sink的实现其实基本相同。
实现Sink其实不难，实现对应的接口，即SinkConnector和SinkTask两个接口，再打包放到kafka/libs目录下即可。其中SinkConnector只有一个，而Task可以有多 先是Connector
public class HdfsSinkConnector extends SinkConnector { //这两项为配置hdfs的urlh和路径的配置项，需要在connector1.properties中指定 public static final String HDFS_URL = &#34;hdfs.url&#34;; public static final String HDFS_PATH = &#34;hdfs.path&#34;; private static final ConfigDef CONFIG_DEF = new ConfigDef() ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/575f18bfec2dabddc4b79f8dc20c175f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-03-23T15:13:00+08:00" />
<meta property="article:modified_time" content="2018-03-23T15:13:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">kafka connect，将数据批量写到hdfs完整过程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div class="content-detail markdown-body"> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">版权声明：本文为博主原创文章，未经博主允许不得转载</span></p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">本文是基于hadoop 2.7.1，以及kafka 0.11.0.0。kafka-connect是以单节点模式运行，即standalone。</span></p> 
 <p> </p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">一. 首先，先对kafka和kafka connect做一个简单的介绍</span></p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">　　<strong>kafka</strong>：Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。比较直观的解释就是其有一个生产者（producer）和一个消费者（consumer）。可以将kafka想象成一个数据容器，生产者负责发送数据到这个容器中，而消费者从容器中取出数据，在将数据做处理，如存储到hdfs。</span></p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">　　<strong>kafka connect</strong>：Kafka Connect是一种用于在Kafka和其他系统之间可扩展的、可靠的流式传输数据的工具。它使得能够快速定义将大量数据集合移入和移出Kafka的连接器变得简单。即适合批量数据导入导出操作。</span></p> 
 <p> </p> 
 <p> </p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">二. 下面将介绍如何用kafka connect将数据写入到hdfs中。包括在这个过程中可能碰到的一些问题说明。</span></p> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">首先启动kafka-connect:</span></p> 
 <pre></pre> 
 <div class="cnblogs_code"> 
  <pre>bin/connect-standalone.sh config/connect-standalone.properties config/connector1.properties</pre> 
 </div> 
 <pre><span style="font-family:'宋体';font-size:14px;"><span style="font-family:'Microsoft YaHei';">这个命令后面两个参数，</span><br><span style="font-family:'Microsoft YaHei';">　　第一个是指定启动的模式，有分布式和单节点两种，这里是单节点。kafka自带，放于config目录下。</span><br><span style="font-family:'Microsoft YaHei';">　　第二个参数指向描述connector的属性的文件，可以有多个，这里只有一个connector用来写入到hdfs。需要自己创建。</span><br><br><span style="font-family:'Microsoft YaHei';">接下来看看connector1.properties的内容，</span><br></span></pre> 
 <div class="cnblogs_code"> 
  <pre><span style="font-family:'Microsoft YaHei';font-size:14px;"><span style="color:#000000;">name="test"    #该connector的名字<br>
#</span>将自己按connect接口规范编写的代码打包后放在kafka/libs目录下，再根据项目结构引用对应connector</span><br><span style="font-family:'Microsoft YaHei';font-size:14px;">connector.class=hdfs.HdfsSinkConnector </span></pre> 
  <pre><span style="font-family:'Microsoft YaHei';font-size:14px;">#Task是导入导出的具体实现，这里是指定多少个task来并行运行导入导出作业，由多线程实现。由于hdfs中一个文件每次只能又一个文件操作，所以这里只能是1</span></pre> 
  <pre><span style="color:#000000;font-family:'Microsoft YaHei';font-size:14px;">tasks.max=1 <br></span></pre> 
  <pre><span style="font-family:'Microsoft YaHei';font-size:14px;">#指定从哪个topic读取数据，这些其实是用来在connector或者task的代码中读取的。<br>
topics=test </span></pre> 
  <pre><span style="font-family:'Microsoft YaHei';font-size:14px;">#指定key以那种方式转换，需和Producer发送方指定的序列化方式一致<br>
key.converter=org.apache.kafka.connect.converters.ByteArrayConverter </span><br><span style="font-family:'Microsoft YaHei';font-size:14px;">value.converter=org.apache.kafka.connect.json.JsonConverter     #同上</span></pre> 
  <pre><span style="color:#000000;"><span style="font-family:'Microsoft YaHei';font-size:14px;">hdfs.url=hdfs://127.0.0.1:9000　　#hdfs的url路径，在Connector中会被读取</span><br><span style="font-family:'Microsoft YaHei';font-size:14px;">hdfs.path=/test/file　　#hdfs文件路径，同样Connector中被读取<br></span><br><span style="font-family:'Microsoft YaHei';font-size:14px;">key.converter.schemas.enable=true　　#稍后介绍，可以true也可以false，影响传输格式</span><br><span style="font-family:'Microsoft YaHei';font-size:14px;">value.converter.schemas.enable=true　　#稍后介绍，可以true也可以false</span><br></span></pre> 
 </div> 
 <pre><span style="font-family:'宋体';font-size:14px;"><span style="font-family:'Microsoft YaHei';"><br><br>
三. 接下来看代码，connect主要是<strong>导入导出</strong>两个概念，<strong>导入是source，导出时Sink</strong>。这里只使用Sink，不过Source和Sink的实现其实基本相同。<br></span><span style="font-family:'Microsoft YaHei';">实现Sink其实不难，实现对应的接口，即</span></span><span style="font-family:'Microsoft YaHei';">SinkConnector和SinkTask两个接口<span style="font-size:14px;">，再打包放到kafka/libs目录下即可。其中SinkConnector只有一个，而Task可以有多</span></span></pre> 
 <pre><em><span style="font-family:'宋体';font-size:14px;"><span style="font-family:'Microsoft YaHei';">先是Connector</span><br></span></em></pre> 
 <div class="cnblogs_code"> 
  <pre><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span> HdfsSinkConnector <span style="color:#0000ff;">extends</span><span style="color:#000000;"> SinkConnector {
    </span><span style="color:#008000;">//</span><span style="color:#008000;">这两项为配置hdfs的urlh和路径的配置项，需要在connector1.properties中指定</span>
    <span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> String HDFS_URL = "hdfs.url"<span style="color:#000000;">;
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> String HDFS_PATH = "hdfs.path"<span style="color:#000000;">;
    </span><span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> ConfigDef CONFIG_DEF = <span style="color:#0000ff;">new</span><span style="color:#000000;"> ConfigDef()
            .define(HDFS_URL, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, </span>"hdfs url"<span style="color:#000000;">)
            .define(HDFS_PATH, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, </span>"hdfs path"<span style="color:#000000;">);
    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> String hdfsUrl;
    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> String hdfsPath;
    @Override
    </span><span style="color:#0000ff;">public</span><span style="color:#000000;"> String version() {
        </span><span style="color:#0000ff;">return</span><span style="color:#000000;"> AppInfoParser.getVersion();
    }<br>
//start方法会再初始的时候执行一次，这里主要用于配置
    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span> start(Map&lt;String, String&gt;<span style="color:#000000;"> props) {
        hdfsUrl </span>=<span style="color:#000000;"> props.get(HDFS_URL);
        hdfsPath </span>=<span style="color:#000000;"> props.get(HDFS_PATH);
    }
　　//这里指定了Task的类
    @Override
    </span><span style="color:#0000ff;">public</span> Class&lt;? <span style="color:#0000ff;">extends</span> Task&gt;<span style="color:#000000;"> taskClass() {
        </span><span style="color:#0000ff;">return</span> HdfsSinkTask.<span style="color:#0000ff;">class</span><span style="color:#000000;">;
    }
　　//用于配置Task的config，这些都是会在Task中用到
    @Override
    </span><span style="color:#0000ff;">public</span> List&lt;Map&lt;String, String&gt;&gt; taskConfigs(<span style="color:#0000ff;">int</span><span style="color:#000000;"> maxTasks) {
        ArrayList</span>&lt;Map&lt;String, String&gt;&gt; configs = <span style="color:#0000ff;">new</span> ArrayList&lt;&gt;<span style="color:#000000;">();
        </span><span style="color:#0000ff;">for</span> (<span style="color:#0000ff;">int</span> i = 0; i &lt; maxTasks; i++<span style="color:#000000;">) {
            Map</span>&lt;String, String&gt; config = <span style="color:#0000ff;">new</span> HashMap&lt;&gt;<span style="color:#000000;">();
            </span><span style="color:#0000ff;">if</span> (hdfsUrl != <span style="color:#0000ff;">null</span><span style="color:#000000;">)
                config.put(HDFS_URL, hdfsUrl);
            </span><span style="color:#0000ff;">if</span> (hdfsPath != <span style="color:#0000ff;">null</span><span style="color:#000000;">)
                config.put(HDFS_PATH, hdfsPath);
            configs.add(config);
        }
        </span><span style="color:#0000ff;">return</span><span style="color:#000000;"> configs;
    }
　　//关闭时的操作，一般是关闭资源。
    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> stop() {
        </span><span style="color:#008000;">//</span><span style="color:#008000;"> Nothing to do since FileStreamSinkConnector has no background monitoring.</span>
<span style="color:#000000;">    }

    @Override
    </span><span style="color:#0000ff;">public</span><span style="color:#000000;"> ConfigDef config() {
        </span><span style="color:#0000ff;">return</span><span style="color:#000000;"> CONFIG_DEF;
    }

}</span></pre> 
 </div> 
 <pre></pre> 
 <p><span style="font-family:'Microsoft YaHei';">接下来是Task</span></p> 
 <div class="cnblogs_code"> 
  <pre><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">class</span> HdfsSinkTask <span style="color:#0000ff;">extends</span><span style="color:#000000;"> SinkTask {
    </span><span style="color:#0000ff;">private</span> <span style="color:#0000ff;">static</span> <span style="color:#0000ff;">final</span> Logger log = LoggerFactory.getLogger(HdfsSinkTask.<span style="color:#0000ff;">class</span><span style="color:#000000;">);

    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> String filename;

    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span><span style="color:#000000;"> String hdfsUrl;
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">static</span><span style="color:#000000;"> String hdfsPath;
    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> Configuration conf;
    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> FSDataOutputStream os;
    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> FileSystem hdfs;


    </span><span style="color:#0000ff;">public</span><span style="color:#000000;"> HdfsSinkTask(){

    }

    @Override
    </span><span style="color:#0000ff;">public</span><span style="color:#000000;"> String version() {
        </span><span style="color:#0000ff;">return</span> <span style="color:#0000ff;">new</span><span style="color:#000000;"> HdfsSinkConnector().version();
    }
　　//Task开始会执行的代码，可能有多个Task，所以每个Task都会执行一次
    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span> start(Map&lt;String, String&gt;<span style="color:#000000;"> props) {
        hdfsUrl </span>=<span style="color:#000000;"> props.get(HdfsSinkConnector.HDFS_URL);
        hdfsPath </span>=<span style="color:#000000;"> props.get(HdfsSinkConnector.HDFS_PATH);
        System.out.println(</span>"----------------------------------- start--------------------------------"<span style="color:#000000;">);

        conf </span>= <span style="color:#0000ff;">new</span><span style="color:#000000;"> Configuration();</span>
        conf.set("fs.defaultFS"<span style="color:#000000;">, hdfsUrl);
        </span><span style="color:#008000;">//</span><span style="color:#008000;">这两个是与hdfs append相关的设置</span>
        conf.setBoolean("dfs.support.append", <span style="color:#0000ff;">true</span><span style="color:#000000;">);
        conf.set(</span>"dfs.client.block.write.replace-datanode-on-failure.policy", "NEVER"<span style="color:#000000;">);
        </span><span style="color:#0000ff;">try</span><span style="color:#000000;">{
            hdfs </span>=<span style="color:#000000;"> FileSystem.get(conf);
</span><span style="color:#008000;">//</span><span style="color:#008000;">            connector.hdfs = new Path(HDFSPATH).getFileSystem(conf);</span>
            os = hdfs.append(<span style="color:#0000ff;">new</span><span style="color:#000000;"> Path(hdfsPath));
        }</span><span style="color:#0000ff;">catch</span><span style="color:#000000;"> (IOException e){
            System.out.println(e.toString());
        }

    }
　　//核心操作，put就是将数据从kafka中取出，存放到其他地方去
    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span> put(Collection&lt;SinkRecord&gt;<span style="color:#000000;"> sinkRecords) {
        </span><span style="color:#0000ff;">for</span><span style="color:#000000;"> (SinkRecord record : sinkRecords) {
            log.trace(</span>"Writing line to {}: {}"<span style="color:#000000;">, logFilename(), record.value());
            </span><span style="color:#0000ff;">try</span><span style="color:#000000;">{
                System.out.println(</span>"write info------------------------" + record.value().toString() + "-----------------"<span style="color:#000000;">);
                os.write((record.value().toString()).getBytes(</span>"UTF-8"<span style="color:#000000;">));
                os.hsync();
            }</span><span style="color:#0000ff;">catch</span><span style="color:#000000;">(Exception e){
                System.out.print(e.toString());
            }
        }
    }

    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span> flush(Map&lt;TopicPartition, OffsetAndMetadata&gt;<span style="color:#000000;"> offsets) {
        </span><span style="color:#0000ff;">try</span><span style="color:#000000;">{
            os.hsync();
        }</span><span style="color:#0000ff;">catch</span><span style="color:#000000;"> (Exception e){
            System.out.print(e.toString());
        }

    }<br>
//同样是结束时候所执行的代码，这里用于关闭hdfs资源
    @Override
    </span><span style="color:#0000ff;">public</span> <span style="color:#0000ff;">void</span><span style="color:#000000;"> stop() {
        </span><span style="color:#0000ff;">try</span><span style="color:#000000;"> {
            os.close();
        }</span><span style="color:#0000ff;">catch</span><span style="color:#000000;">(IOException e){
            System.out.println(e.toString());
        }
    }

    </span><span style="color:#0000ff;">private</span><span style="color:#000000;"> String logFilename() {
        </span><span style="color:#0000ff;">return</span> filename == <span style="color:#0000ff;">null</span> ? "stdout"<span style="color:#000000;"> : filename;
    }


}</span></pre> 
 </div> 
 <pre><span style="font-size:14px;"><br><span style="font-family:'Microsoft YaHei';">这里重点提一下，因为在connector1.propertise中设置了key.converter=org.apache.kafka.connect.converters.ByteArrayConverter，所以不能用命令行形式的</span><br></span><span style="font-size:14px;font-family:'Microsoft YaHei';">producer发送数据，而是要用程序的方式，并且在producer总也要设置key的序列化形式为<span style="color:#ff0000;">org.apache.kafka.common.serialization.ByteArraySerializer</span>。</span></pre> 
 <pre><span style="font-family:'Microsoft YaHei';font-size:14px;">编码完成，先用idea以开发程序与依赖包分离的形式打包成jar包，然后将程序对应的jar包（一般就是“项目名.jar”）放到kafka/libs目录下面，这样就能被找到。<br><br><br></span></pre> 
 <pre><span style="font-size:14px;font-family:'Microsoft YaHei';">四. 接下来对这个过程的问题做一个汇总。</span></pre> 
 <pre><span style="font-size:14px;font-family:'Microsoft YaHei';"><strong>1.在connector1.properties中的key.converter.schemas.enable=false和value.converter.schemas.enable=false的问题。</strong><br></span></pre> 
 <pre><span style="font-size:14px;"><span style="font-family:'Microsoft YaHei';">这个选项默认在connect-standalone.properties中是true的，这个时候发送给topic的Json格式是需要使用avro格式，具体情况可以百度，这里给出一个样例。</span><span style="font-family:'宋体';"><br></span></span></pre> 
 <div class="cnblogs_code"> 
  <pre><span style="color:#000000;">{
    "schema": {
        "type": "struct",
        "fields": [{
            "type": "int32",
            "optional": true,
            "field": "c1"
        }, {
            "type": "string",
            "optional": true,
            "field": "c2"
        }, {
            "type": "int64",
            "optional": false,
            "name": "org.apache.kafka.connect.data.Timestamp",
            "version": 1,
            "field": "create_ts"
        }, {
            "type": "int64",
            "optional": false,
            "name": "org.apache.kafka.connect.data.Timestamp",
            "version": 1,
            "field": "update_ts"
        }],
        "optional": false,
        "name": "foobar"
    },
    "payload": {
        "c1": 10000,
        "c2": "bar",
        "create_ts": 1501834166000,
        "update_ts": 1501834166000
    }
}  </span></pre> 
 </div> 
 <pre></pre> 
 <p><span style="font-size:14px;font-family:'Microsoft YaHei';">主要就是schema和payload这两个，不按照这个格式会报错如下</span></p> 
 <pre></pre> 
 <div class="cnblogs_code"> 
  <pre><span style="color:#000000;">org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.

   at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:308)</span></pre> 
 </div> 
 <pre></pre> 
 <p><span style="font-family:'Microsoft YaHei';font-size:14px;">如果想发送普通的json格式而不是avro格式的话，很简单key.converter.schemas.enable和value.converter.schemas.enable设置为false就行。这样就能发送普通的json格式数据。</span></p> 
 <p><span style="font-family:'Microsoft YaHei';"><strong><span style="font-size:14px;">2.在启动的过程中出现各种各样的java.lang.ClassNotFoundException。</span></strong></span></p> 
 <p><span style="font-size:14px;font-family:'Microsoft YaHei';">在启动connector的时候，一开始总是会报各个各样的ClassNotFoundException，不是这个包就是那个包，查找问题一直说要么缺少包要么是包冲突。这个是什么原因呢？</span></p> 
 <p><span style="font-size:14px;font-family:'Microsoft YaHei';">其实归根结底还是依赖冲突的问题，因为kafka程序自定义的类加载器加载类的目录是在kafka/libs中，而写到hdfs需要hadoop的包。</span></p> 
 <p><span style="font-size:14px;font-family:'Microsoft YaHei';">我一开始的做法是将hadoop下的包路径添加到CLASSPATH中，这样子问题就来了，因为kafka和hadoop的依赖包是有冲突的，比如hadoop是guava-11.0.2.jar，而kafka是guava-20.0.jar，两个jar包版本不同，而我们是在kafka程序中调用hdfs，所以当jar包冲突时应该优先调用kafka的。但是注意kafka用的是程序自定义的类加载器，其优先级是低于CLASSPATH路径下的类的，就是说加载类时会优先加载CLASSPATH下的类。这样子就有问题了。</span></p> 
 <p><span style="font-size:14px;font-family:'Microsoft YaHei';">我的解决方案时将kafka和hadoop加载的jar包路径都添加到CLASSPATH中，并且kafka的路径写在hadoop前面，这样就可以启动connector成功。</span></p> 
 <pre><span style="font-family:'Microsoft YaHei';"><em><em><span style="font-size:14px;"> </span></em></em></span></pre> 
 <pre></pre> 
 <hr> 
 <pre></pre> 
 <pre></pre> 
 <p style="text-align:center;"><br></p> 
 <pre></pre> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d854e50f6c856ec1164d6e7f7dd8b61f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">对统计规律的理解与应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/72a3af304c735f6e71191a3bfad9af81/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Unable to parse template &#34;Class&#34; Error message: Selected class file name &#39;ProductServlet.java&#39; mappe</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>