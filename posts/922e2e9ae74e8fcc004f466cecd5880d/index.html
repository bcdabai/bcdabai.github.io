<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>php使用keras模型,浅谈keras 模型用于预测时的注意事项 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="php使用keras模型,浅谈keras 模型用于预测时的注意事项" />
<meta property="og:description" content="为什么训练误差比测试误差高很多？
一个Keras的模型有两个模式：训练模式和测试模式。一些正则机制，如Dropout，L1/L2正则项在测试模式下将不被启用。
另外，训练误差是训练数据每个batch的误差的平均。在训练过程中，每个epoch起始时的batch的误差要大一些，而后面的batch的误差要小一些。另一方面，每个epoch结束时计算的测试误差是由模型在epoch结束时的状态决定的，这时候的网络将产生较小的误差。
【Tips】可以通过定义回调函数将每个epoch的训练误差和测试误差并作图，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与Keras无关。
在keras中文文档中指出了这一误区，笔者认为产生这一问题的原因在于网络实现的机制。即dropout层有前向实现和反向实现两种方式，这就决定了概率p是在训练时候设置还是测试的时候进行设置
利用预训练的权值进行Fine tune时的注意事项：
不能把自己添加的层进行将随机初始化后直接连接到前面预训练后的网络层
in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/922e2e9ae74e8fcc004f466cecd5880d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-11T20:54:27+08:00" />
<meta property="article:modified_time" content="2021-04-11T20:54:27+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">php使用keras模型,浅谈keras 模型用于预测时的注意事项</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>为什么训练误差比测试误差高很多？</p> 
 <p>一个Keras的模型有两个模式：训练模式和测试模式。一些正则机制，如Dropout，L1/L2正则项在测试模式下将不被启用。</p> 
 <p>另外，训练误差是训练数据每个batch的误差的平均。在训练过程中，每个epoch起始时的batch的误差要大一些，而后面的batch的误差要小一些。另一方面，每个epoch结束时计算的测试误差是由模型在epoch结束时的状态决定的，这时候的网络将产生较小的误差。</p> 
 <p>【Tips】可以通过定义回调函数将每个epoch的训练误差和测试误差并作图，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与Keras无关。</p> 
 <p>在keras中文文档中指出了这一误区，笔者认为产生这一问题的原因在于网络实现的机制。即dropout层有前向实现和反向实现两种方式，这就决定了概率p是在训练时候设置还是测试的时候进行设置</p> 
 <p>利用预训练的权值进行Fine tune时的注意事项：</p> 
 <p>不能把自己添加的层进行将随机初始化后直接连接到前面预训练后的网络层</p> 
 <p>in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it.</p> 
 <p>we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features).</p> 
 <p>fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features.</p> 
 <p>补充知识：keras框架中用keras.models.Model做的时候预测数据不是标签的问题</p> 
 <p>我们发现，在用Sequential去搭建网络的时候，其中有predict和predict_classes两个预测函数，前一个是返回的精度，后面的是返回的具体标签。但是，在使用keras.models.Model去做的时候，就会发现，它只有一个predict函数，没有返回标签的predict_classes函数，所以，针对这个问题，我们将其改写。改写如下：</p> 
 <p>def my_predict_classes(predict_data):</p> 
 <p>if predict_data.shape[-1] &gt; 1:</p> 
 <p>return predict_data.argmax(axis=-1)</p> 
 <p>else:</p> 
 <p>return (predict_data &gt; 0.5).astype('int32')</p> 
 <p># 这里省略网络搭建部分。。。。</p> 
 <p>model = Model(data_input, label_output)</p> 
 <p>model.compile(loss='categorical_crossentropy',</p> 
 <p>optimizer=keras.optimizers.Nadam(lr=0.002),</p> 
 <p>metrics=['accuracy'])</p> 
 <p>model.summary()</p> 
 <p>y_predict = model.predict(X_test)</p> 
 <p>y_pre = my_predict_classes(y_predict)</p> 
 <p>这样，y_pre就是具体的标签了。</p> 
 <p>以上这篇浅谈keras 模型用于预测时的注意事项就是小编分享给大家的全部内容了，希望能给大家一个参考，也希望大家多多支持脚本之家。</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a981cce68bfd5d7a8b85afa7f110d3d9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一文带你彻底理解高性能无锁队列</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/e6a670197627f95fea2c5b6d50b6d110/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">PID参数调节经验整理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>