<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>如何用六步教会你使用python爬虫爬取数据 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="如何用六步教会你使用python爬虫爬取数据" />
<meta property="og:description" content="前言： 用python的爬虫爬取数据真的很简单，只要掌握这六步就好，也不复杂。以前还以为爬虫很难，结果一上手，从初学到把东西爬下来，一个小时都不到就解决了。
python爬出六部曲 第一步：安装requests库和BeautifulSoup库： 在程序中两个库的书写是这样的：
import` `requests``from` `bs4 ``import` `BeautifulSoup 由于我使用的是pycharm进行的python编程。所以我就讲讲在pycharm上安装这两个库的方法。在主页面文件选项下，找到设置。进一步找到项目解释器。之后在所选框中，点击软件包上的&#43;号就可以进行查询插件安装了。有过编译器插件安装的hxd估计会比较好入手。具体情况就如下图所示。
第二步：获取爬虫所需的header和cookie： 我写了一个爬取微博热搜的爬虫程序，这里就直接以它为例吧。获取header和cookie是一个爬虫程序必须的，它直接决定了爬虫程序能不能准确的找到网页位置进行爬取。
首先进入微博热搜的页面，按下F12，就会出现网页的js语言设计部分。如下图所示。找到网页上的Network部分。然后按下ctrl&#43;R刷新页面。如果，进行就有文件信息，就不用刷新了，当然刷新了也没啥问题。然后，我们浏览Name这部分，找到我们想要爬取的文件，鼠标右键，选择copy，复制下网页的URL。就如下图所示。
复制好URL后，我们就进入一个网页Convert curl commands to code。这个网页可以根据你复制的URL，自动生成header和cookie，如下图。生成的header和cookie，直接复制走就行，粘贴到程序中。
#爬虫头数据``cookies ``=` `{`` ``&#39;SINAGLOBAL&#39;``: ``&#39;6797875236621.702.1603159218040&#39;``,`` ``&#39;SUB&#39;``: ``&#39;_2AkMXbqMSf8NxqwJRmfkTzmnhboh1ygvEieKhMlLJJRMxHRl-yT9jqmg8tRB6PO6N_Rc_2FhPeZF2iThYO9DfkLUGpv4V&#39;``,`` ``&#39;SUBP&#39;``: ``&#39;0033WrSXqPxfM72-Ws9jqgMF55529P9D9Wh-nU-QNDs1Fu27p6nmwwiJ&#39;``,`` ``&#39;_s_tentry&#39;``: ``&#39;www.baidu.com&#39;``,`` ``&#39;UOR&#39;``: ``&#39;www.hfut.edu.cn,widget.weibo.com,www.baidu.com&#39;``,`` ``&#39;Apache&#39;``: ``&#39;7782025452543.054.1635925669528&#39;``,`` ``&#39;ULV&#39;``: ``&#39;1635925669554:15:1:1:7782025452543.054.1635925669528:1627316870256&#39;``,``}``headers ``=` `{`` ``&#39;Connection&#39;``: ``&#39;keep-alive&#39;``,`` ``&#39;Cache-Control&#39;``: ``&#39;max-age=0&#39;``,`` ``&#39;Upgrade-Insecure-Requests&#39;``: ``&#39;1&#39;``,`` ``&#39;User-Agent&#39;``: ``&#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36 SLBrowser/7.0.0.6241 SLBChan/25&#39;``,`` ``&#39;Accept&#39;``: ``&#39;text/html,application/xhtml&#43;xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;``,`` ``&#39;Sec-Fetch-Site&#39;``: ``&#39;cross-site&#39;``,`` ``&#39;Sec-Fetch-Mode&#39;``: ``&#39;navigate&#39;``,`` ``&#39;Sec-Fetch-User&#39;``: ``&#39;?1&#39;``,`` ``&#39;Sec-Fetch-Dest&#39;``: ``&#39;document&#39;``,`` ``&#39;Accept-Language&#39;``: ``&#39;zh-CN,zh;q=0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5ee03ee994eb49fe13abf94c9be78efc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-11T16:15:36+08:00" />
<meta property="article:modified_time" content="2023-10-11T16:15:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">如何用六步教会你使用python爬虫爬取数据</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_2"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>前言：</h4> 
<p>用python的爬虫爬取数据真的很简单，只要掌握这六步就好，也不复杂。以前还以为爬虫很难，结果一上手，从初学到把东西爬下来，一个小时都不到就解决了。</p> 
<h4><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502python_6"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>python爬出六部曲</h4> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502requestsBeautifulSoup_8"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第一步：安装requests库和BeautifulSoup库：</h5> 
<p>在程序中两个库的书写是这样的：</p> 
<pre><code>import` `requests``from` `bs4 ``import` `BeautifulSoup

</code></pre> 
<p>由于我使用的是pycharm进行的python编程。所以我就讲讲在pycharm上安装这两个库的方法。在主页面文件选项下，找到设置。进一步找到项目解释器。之后在所选框中，点击软件包上的+号就可以进行查询插件安装了。有过编译器插件安装的hxd估计会比较好入手。具体情况就如下图所示。</p> 
<p><img src="https://images2.imgbox.com/da/93/XezKhAU6_o.jpg" alt="img"></p> 
<p><img src="https://images2.imgbox.com/70/d3/PC3Ktvv5_o.jpg" alt="img"></p> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502headercookie_23"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第二步：获取爬虫所需的header和cookie：</h5> 
<p>我写了一个爬取微博热搜的爬虫程序，这里就直接以它为例吧。获取header和cookie是一个爬虫程序必须的，它直接决定了爬虫程序能不能准确的找到网页位置进行爬取。</p> 
<p>首先进入微博热搜的页面，按下F12，就会出现网页的js语言设计部分。如下图所示。找到网页上的Network部分。然后按下ctrl+R刷新页面。如果，进行就有文件信息，就不用刷新了，当然刷新了也没啥问题。然后，我们浏览Name这部分，找到我们想要爬取的文件，鼠标右键，选择copy，复制下网页的URL。就如下图所示。</p> 
<p><img src="https://images2.imgbox.com/d0/07/yt3lSC62_o.jpg" alt="img"></p> 
<p>复制好URL后，我们就进入一个网页<a href="https://curlconverter.com/" rel="nofollow">Convert curl commands to code</a>。这个网页可以根据你复制的URL，自动生成header和cookie，如下图。生成的header和cookie，直接复制走就行，粘贴到程序中。</p> 
<p><img src="https://images2.imgbox.com/49/ef/T3gjAr1c_o.jpg" alt="img"></p> 
<pre><code>#爬虫头数据``cookies ``=` `{``  ``'SINAGLOBAL'``: ``'6797875236621.702.1603159218040'``,``  ``'SUB'``: ``'_2AkMXbqMSf8NxqwJRmfkTzmnhboh1ygvEieKhMlLJJRMxHRl-yT9jqmg8tRB6PO6N_Rc_2FhPeZF2iThYO9DfkLUGpv4V'``,``  ``'SUBP'``: ``'0033WrSXqPxfM72-Ws9jqgMF55529P9D9Wh-nU-QNDs1Fu27p6nmwwiJ'``,``  ``'_s_tentry'``: ``'www.baidu.com'``,``  ``'UOR'``: ``'www.hfut.edu.cn,widget.weibo.com,www.baidu.com'``,``  ``'Apache'``: ``'7782025452543.054.1635925669528'``,``  ``'ULV'``: ``'1635925669554:15:1:1:7782025452543.054.1635925669528:1627316870256'``,``}``headers ``=` `{``  ``'Connection'``: ``'keep-alive'``,``  ``'Cache-Control'``: ``'max-age=0'``,``  ``'Upgrade-Insecure-Requests'``: ``'1'``,``  ``'User-Agent'``: ``'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36 SLBrowser/7.0.0.6241 SLBChan/25'``,``  ``'Accept'``: ``'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'``,``  ``'Sec-Fetch-Site'``: ``'cross-site'``,``  ``'Sec-Fetch-Mode'``: ``'navigate'``,``  ``'Sec-Fetch-User'``: ``'?1'``,``  ``'Sec-Fetch-Dest'``: ``'document'``,``  ``'Accept-Language'``: ``'zh-CN,zh;q=0.9'``,``}``params ``=` `(``  ``(``'cate'``, ``'realtimehot'``),``)

</code></pre> 
<p>复制到程序中就像这样。这是微博热搜的请求头。</p> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_42"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第三步：获取网页：</h5> 
<p>我们将header和cookie搞到手后，就可以将它复制到我们的程序里。之后，使用request请求，就可以获取到网页了。</p> 
<pre><code>#获取网页``response ``=` `requests.get(``'https://s.weibo.com/top/summary'``, headers``=``headers, params``=``params, cookies``=``cookies)

</code></pre> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_51"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第四步：解析网页：</h5> 
<p>这个时候，我们需要回到网页。同样按下F12，找到网页的Elements部分。用左上角的小框带箭头的标志，如下图，点击网页内容，这个时候网页就会自动在右边显示出你获取网页部分对应的代码。</p> 
<p><img src="https://images2.imgbox.com/9e/16/Gpu2I6kH_o.png" alt="img"></p> 
<p><img src="https://images2.imgbox.com/53/97/Y2OJWYXr_o.jpg" alt="img"></p> 
<p>如上图所示，我们在找到想要爬取的页面部分的网页代码后，将鼠标放置于代码上，右键，copy到selector部分。就如上图所示。</p> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_61"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第五步：分析得到的信息，简化地址：</h5> 
<p>其实刚才复制的selector就相当于网页上对应部分存放的地址。由于我们需要的是网页上的一类信息，所以我们需要对获取的地址进行分析，提取。当然，就用那个地址也不是不行，就是只能获取到你选择的网页上的那部分内容。</p> 
<pre><code>#pl_top_realtimehot &gt; table &gt; tbody &gt; tr:nth-child(1) &gt; td.td-02 &gt; a``#pl_top_realtimehot &gt; table &gt; tbody &gt; tr:nth-child(2) &gt; td.td-02 &gt; a``#pl_top_realtimehot &gt; table &gt; tbody &gt; tr:nth-child(9) &gt; td.td-02 &gt; a

</code></pre> 
<p>这是我获取的三条地址，可以发现三个地址有很多相同的地方，唯一不同的地方就是tr部分。由于tr是网页标签，后面的部分就是其补充的部分，也就是子类选择器。可以推断出，该类信息，就是存储在tr的子类中，我们直接对tr进行信息提取，就可以获取到该部分对应的所有信息。所以提炼后的地址为：</p> 
<pre><code>#pl_top_realtimehot &gt; table &gt; tbody &gt; tr &gt; td.td-02 &gt; a

</code></pre> 
<p>这个过程对js类语言有一定了解的hxd估计会更好处理。不过没有js类语言基础也没关系，主要步骤就是，保留相同的部分就行，慢慢的试，总会对的。</p> 
<h5><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_79"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>第六步：爬取内容，清洗数据</h5> 
<p>这一步完成后，我们就可以直接爬取数据了。用一个标签存储上面提炼出的像地址一样的东西。标签就会拉取到我们想获得的网页内容。</p> 
<pre><code>#爬取内容``content``=``"#pl_top_realtimehot &gt; table &gt; tbody &gt; tr &gt; td.td-02 &gt; a"

</code></pre> 
<p>之后我们就要soup和text过滤掉不必要的信息，比如js类语言，排除这类语言对于信息受众阅读的干扰。这样我们就成功的将信息，爬取下来了。</p> 
<pre><code>fo ``=` `open``(``"./微博热搜.txt"``,``'a'``,encoding``=``"utf-8"``)``a``=``soup.select(content)``for` `i ``in` `range``(``0``,``len``(a)):``  ``a[i] ``=` `a[i].text``  ``fo.write(a[i]``+``'\n'``)``fo.close()

</code></pre> 
<p>我是将数据存储到了文件夹中，所以会有wirte带来的写的操作。想把数据保存在哪里，或者想怎么用，就看读者自己了。</p> 
<h4><a id="httpsblogcsdnnetm0_59162248articledetails129156776spm1001201430015502_97"></a><a href="https://blog.csdn.net/m0_59162248/article/details/129156776?spm=1001.2014.3001.5502"></a>爬取微博热搜的代码实例以及结果展示：</h4> 
<pre><code>import` `os``import` `requests``from` `bs4 ``import` `BeautifulSoup``#爬虫头数据``cookies ``=` `{``  ``'SINAGLOBAL'``: ``'6797875236621.702.1603159218040'``,``  ``'SUB'``: ``'_2AkMXbqMSf8NxqwJRmfkTzmnhboh1ygvEieKhMlLJJRMxHRl-yT9jqmg8tRB6PO6N_Rc_2FhPeZF2iThYO9DfkLUGpv4V'``,``  ``'SUBP'``: ``'0033WrSXqPxfM72-Ws9jqgMF55529P9D9Wh-nU-QNDs1Fu27p6nmwwiJ'``,``  ``'_s_tentry'``: ``'www.baidu.com'``,``  ``'UOR'``: ``'www.hfut.edu.cn,widget.weibo.com,www.baidu.com'``,``  ``'Apache'``: ``'7782025452543.054.1635925669528'``,``  ``'ULV'``: ``'1635925669554:15:1:1:7782025452543.054.1635925669528:1627316870256'``,``}``headers ``=` `{``  ``'Connection'``: ``'keep-alive'``,``  ``'Cache-Control'``: ``'max-age=0'``,``  ``'Upgrade-Insecure-Requests'``: ``'1'``,``  ``'User-Agent'``: ``'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36 SLBrowser/7.0.0.6241 SLBChan/25'``,``  ``'Accept'``: ``'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'``,``  ``'Sec-Fetch-Site'``: ``'cross-site'``,``  ``'Sec-Fetch-Mode'``: ``'navigate'``,``  ``'Sec-Fetch-User'``: ``'?1'``,``  ``'Sec-Fetch-Dest'``: ``'document'``,``  ``'Accept-Language'``: ``'zh-CN,zh;q=0.9'``,``}``params ``=` `(``  ``(``'cate'``, ``'realtimehot'``),``)``#数据存储``fo ``=` `open``(``"./微博热搜.txt"``,``'a'``,encoding``=``"utf-8"``)``#获取网页``response ``=` `requests.get(``'https://s.weibo.com/top/summary'``, headers``=``headers, params``=``params, cookies``=``cookies)``#解析网页``response.encoding``=``'utf-8'``soup ``=` `BeautifulSoup(response.text, ``'html.parser'``)``#爬取内容``content``=``"#pl_top_realtimehot &gt; table &gt; tbody &gt; tr &gt; td.td-02 &gt; a"``#清洗数据``a``=``soup.select(content)``for` `i ``in` `range``(``0``,``len``(a)):``  ``a[i] ``=` `a[i].text``  ``fo.write(a[i]``+``'\n'``)``fo.close()

</code></pre> 
<p><img src="https://images2.imgbox.com/74/af/M77N2SEK_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="_106"></a>学习资源推荐</h3> 
<p>除了上述分享，如果你也喜欢编程，想通过学习Python获取更高薪资，这里给大家分享一份Python学习资料。</p> 
<p>这里给大家展示一下我进的最近接单的截图</p> 
<p><img src="https://images2.imgbox.com/a1/26/QgIRzreQ_o.png" alt="私单"></p> 
<p>😝朋友们如果有需要的话，可以<mark>点击下方链接领取</mark>或者<mark>V扫描下方二维码联系领取</mark>，也可以内推兼职群哦~</p> 
<blockquote> 
 <p><font color="red">🎁 CSDN大礼包，二维码失效时，点击这里领取👉：</font><a href="https://docs.qq.com/doc/DZVFMenVJWXpobVFt" rel="nofollow">【学习资料合集&amp;相关工具&amp;PyCharm永久使用版获取方式】</a><font color="#66cc66"></font></p> 
</blockquote> 
<p>学好 Python 不论是就业还是做副业赚钱都不错，但要学会 Python 还是要有一个学习规划。最后大家分享一份全套的 Python 学习资料，给那些想学习 Python 的小伙伴们一点帮助！</p> 
<img src="https://images2.imgbox.com/22/db/lLBIs93E_o.png"> 
<h4><a id="1Python_122"></a>1.Python学习路线</h4> 
<p><img src="https://images2.imgbox.com/a6/b9/gnqlhNCm_o.png" alt="image-20230619144606466"></p> 
<p><img src="https://images2.imgbox.com/23/08/0QfAZAPb_o.png" alt="python学习路线图1"></p> 
<h4><a id="2Python_128"></a>2.Python基础学习</h4> 
<h5><a id="01_130"></a>01.开发工具</h5> 
<p><img src="https://images2.imgbox.com/2d/27/nuNj211m_o.png" alt=""></p> 
<h5><a id="02_134"></a>02.学习笔记</h5> 
<p><img src="https://images2.imgbox.com/8c/c1/p7RwJVvY_o.gif" alt="在这里插入图片描述"></p> 
<h5><a id="03_138"></a>03.学习视频</h5> 
<p><img src="https://images2.imgbox.com/c3/2d/AGGBRSYc_o.gif" alt="在这里插入图片描述"></p> 
<h4><a id="3Python_142"></a>3.Python小白必备手册</h4> 
<p><img src="https://images2.imgbox.com/2e/52/dm1fbbM7_o.png" alt="图片"></p> 
<h4><a id="4_146"></a>4.数据分析全套资源</h4> 
<p><img src="https://images2.imgbox.com/69/d7/5qM1WLdU_o.gif" alt="在这里插入图片描述"></p> 
<h4><a id="5Python_150"></a>5.Python面试集锦</h4> 
<h5><a id="01_152"></a>01.面试资料</h5> 
<p><img src="https://images2.imgbox.com/af/41/s31CG3rd_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/46/75/skrBUttB_o.png" alt="在这里插入图片描述"></p> 
<h5><a id="02_158"></a>02.简历模板</h5> 
<p><img src="https://images2.imgbox.com/68/12/XBhvRegq_o.png" alt="在这里插入图片描述"></p> 
<img src="https://images2.imgbox.com/48/85/IWulhcqO_o.png"> 
<blockquote> 
 <p><font color="red">🎁 CSDN大礼包，二维码失效时，点击这里领取👉：</font><a href="https://docs.qq.com/doc/DZVFMenVJWXpobVFt" rel="nofollow">【学习资料合集&amp;相关工具&amp;PyCharm永久使用版获取方式】</a><font color="#66cc66"></font></p> 
</blockquote> 
<center> 
 <b><font color="red" size="4"> 因篇幅有限，仅展示部分资料，添加上方即可获取👆</font></b> 
</center> 
<br> 
<center> 
 <b><font color="blue" size="4"> ------ 🙇‍♂️ 本文转自网络，如有侵权，请联系删除 🙇‍♂️ ------</font></b> 
</center>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2e5faa93d9cd2c2313a760f5e86ab581/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Linux 防火墙firewall-cmd配置命令大全</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f522980e8a2532cfc5bca36a17777525/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">查看服务所在目录属于哪个文件系统，以及剩余磁盘空间</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>