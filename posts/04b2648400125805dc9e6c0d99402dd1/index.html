<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformers in Vision: A Survey论文翻译 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Transformers in Vision: A Survey论文翻译" />
<meta property="og:description" content="Transformers in Vision: A Survey 论文翻译 原文
翻译链接
摘要 摘要——Transformer模型在自然语言任务上的惊人结果引起了视觉界的兴趣，而致力于研究它们在计算机视觉问题中的应用。 这导致在许多任务上取得了令人兴奋的进展，同时在模型设计中需要最小的归纳偏差。 本次调查旨在全面概述计算机视觉学科中的Transformer模型，并假设几乎没有或没有该领域的先验背景。 我们首先介绍Transformer模型成功背后的基本概念，即自我监督和自我注意。 Transformer 架构利用自注意力机制对输入域中的远程依赖项进行编码，这使它们具有高度的表现力。 由于他们假设关于问题结构的先验知识最少，因此使用借口任务的自我监督应用于大规模（未标记）数据集上的预训练Transformer模型。 然后在下游任务上对学习的表示进行微调，由于编码特征的泛化和表达能力，通常会导致出色的性能。 我们涵盖了Transformer在视觉中的广泛应用，包括流行的识别任务（例如，图像分类、对象检测、动作识别和分割）、生成建模、多模态任务（例如，视觉问答、视觉推理和视觉基础） 、视频处理（例如，活动识别、视频预测）、低级视觉（例如，图像超分辨率、图像增强和着色）和 3D 分析（例如，点云分类和分割）。 我们比较了流行技术在架构设计和实验价值方面的各自优势和局限性。 最后，我们对开放的研究方向和未来可能的工作进行了分析。 我们希望这项努力将进一步激发社区的兴趣，以解决当前Transformer模型在计算机视觉中应用的挑战。
关键词——自注意力、Transformer、双向编码器、深度神经网络、卷积网络、自监督。
概述 Transformer模型[1]最近在广泛的语言任务中表现出模范表现，比如在文本分类、机器翻译[2]和问答系统，在这些模型中，最受欢迎的包括BERT(Bidirectional Encoder Representations from Transformers)[3]、GPT(Generative Pre-trained Transformer)v1-3[4]，[5]，[6]，RoBERTa(Robustly Optimized BERT Pre-training)[7]和T5 (Text-to-Text Transfer Transformer)[8]，随着 Transformer 模型对超大规模模型的可扩展性，其深远的影响变得更加明显。举个例子，具有 3.4 亿个参数的 BERT-large模型明显优于具有 1750 亿个参数的最新 GPT-3模型。
Transformer 网络在自然语言处理 (NLP) 领域的突破引发了计算机视觉社区的极大兴趣，已经将这些模型应用于视觉和多模态学习任务。因此，Transformer 模型已成功用于图像识别 [9]、[10]、物体检测 [11]、[12]，分割 [13]、图像超分辨率 [14]、视频理解 [15]、[16]、图像生成 [17] 和视觉问答 [18]、[19]，以及其他几个用例 [20]、[ 21]、[22]、[23]。 本次调查旨在涵盖计算机视觉领域最近的这些令人兴奋的工作，为感兴趣的读者提供全面的参考。
Transformers 的主要成功因素包括（a）自我监督和（b）自我关注。 对大规模数据集的自我监督允许在没有手动注释成本的情况下训练复杂模型，从而学习对给定数据集中存在的实体之间的有用关系进行编码的通用表示。 这是一个重要的特征，因为与其他形式的深度学习模型（例如卷积和循环神经网络 [24]、[25]、[26]）相比，自注意力假设了最小的归纳偏差。 自注意力层通过学习标记集元素（例如，语言中的单词或图像中的补丁）之间的关系来考虑给定序列中的广泛上下文。 在本次调查中，我们首先介绍了 Transformer 网络中使用的这些重要概念，然后详细说明了最近的视觉Transformer的细节。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/04b2648400125805dc9e6c0d99402dd1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-26T19:16:08+08:00" />
<meta property="article:modified_time" content="2021-07-26T19:16:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformers in Vision: A Survey论文翻译</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h2>Transformers in Vision: A Survey</h2> 
<h3>论文翻译</h3> 
<p><a href="https://export.arxiv.org/abs/2101.01169" rel="nofollow">原文</a></p> 
<p><a href="https://docs.qq.com/doc/DZHBtaXVKQklWaUJs?pub=1&amp;dver=2.1.27114423" rel="nofollow">翻译链接</a></p> 
<h2>摘要</h2> 
<p>摘要——Transformer模型在自然语言任务上的惊人结果引起了视觉界的兴趣，而致力于研究它们在计算机视觉问题中的应用。 这导致在许多任务上取得了令人兴奋的进展，同时在模型设计中需要最小的归纳偏差。 本次调查旨在全面概述计算机视觉学科中的Transformer模型，并假设几乎没有或没有该领域的先验背景。 我们首先介绍Transformer模型成功背后的基本概念，即自我监督和自我注意。 Transformer 架构利用自注意力机制对输入域中的远程依赖项进行编码，这使它们具有高度的表现力。 由于他们假设关于问题结构的先验知识最少，因此使用借口任务的自我监督应用于大规模（未标记）数据集上的预训练Transformer模型。 然后在下游任务上对学习的表示进行微调，由于编码特征的泛化和表达能力，通常会导致出色的性能。 我们涵盖了Transformer在视觉中的广泛应用，包括流行的识别任务（例如，图像分类、对象检测、动作识别和分割）、生成建模、多模态任务（例如，视觉问答、视觉推理和视觉基础） 、视频处理（例如，活动识别、视频预测）、低级视觉（例如，图像超分辨率、图像增强和着色）和 3D 分析（例如，点云分类和分割）。 我们比较了流行技术在架构设计和实验价值方面的各自优势和局限性。 最后，我们对开放的研究方向和未来可能的工作进行了分析。 我们希望这项努力将进一步激发社区的兴趣，以解决当前Transformer模型在计算机视觉中应用的挑战。</p> 
<p>关键词——自注意力、Transformer、双向编码器、深度神经网络、卷积网络、自监督。</p> 
<ol><li> <h2>概述</h2> </li></ol> 
<p>Transformer模型[1]最近在广泛的语言任务中表现出模范表现，比如在文本分类、机器翻译[2]和问答系统，在这些模型中，最受欢迎的包括BERT(Bidirectional Encoder Representations from Transformers)[3]、GPT(Generative Pre-trained Transformer)v1-3[4]，[5]，[6]，RoBERTa(Robustly Optimized BERT Pre-training)[7]和T5 (Text-to-Text Transfer Transformer)[8]，随着 Transformer 模型对超大规模模型的可扩展性，其深远的影响变得更加明显。举个例子，具有 3.4 亿个参数的 BERT-large模型明显优于具有 1750 亿个参数的最新 GPT-3模型。</p> 
<p>Transformer 网络在自然语言处理 (NLP) 领域的突破引发了计算机视觉社区的极大兴趣，已经将这些模型应用于视觉和多模态学习任务。因此，Transformer 模型已成功用于图像识别 [9]、[10]、物体检测 [11]、[12]，分割 [13]、图像超分辨率 [14]、视频理解 [15]、[16]、图像生成 [17] 和视觉问答 [18]、[19]，以及其他几个用例 [20]、[ 21]、[22]、[23]。 本次调查旨在涵盖计算机视觉领域最近的这些令人兴奋的工作，为感兴趣的读者提供全面的参考。</p> 
<p>Transformers 的主要成功因素包括（a）自我监督和（b）自我关注。 对大规模数据集的自我监督允许在没有手动注释成本的情况下训练复杂模型，从而学习对给定数据集中存在的实体之间的有用关系进行编码的通用表示。 这是一个重要的特征，因为与其他形式的深度学习模型（例如卷积和循环神经网络 [24]、[25]、[26]）相比，自注意力假设了最小的归纳偏差。 自注意力层通过学习标记集元素（例如，语言中的单词或图像中的补丁）之间的关系来考虑给定序列中的广泛上下文。 在本次调查中，我们首先介绍了 Transformer 网络中使用的这些重要概念，然后详细说明了最近的视觉Transformer的细节。</p> 
<p>本文提供了为计算机视觉应用开发的Transformer模型的整体概述以及最近竞争方法之间的系统比较。 其他文献综述主要集中在 NLP 领域 [27]、[28] 或涵盖基于注意力的通用方法 [27]、[29]。 通过关注新兴的视觉Transformer领域，我们根据自我注意的内在特征和所研究的任务全面组织了最近的方法。 我们的工作还详细阐述了在最近的工作中提出的关于传统Transformer架构的设计创新 [1]。在后面的部分，本手稿详细介绍了文献中不同设计选择所提供的关键实用优势。 该调查最终详细介绍了开放的研究问题，并对未来可能的工作进行了展望。</p> 
<p><img alt="" src="https://images2.imgbox.com/84/91/Ej1WS6wv_o.png" width="1200"></p> 
<p>过去几年BERT、Self-Attention、Transformers等关键词出现在Peer-reviewed和arXiv论文标题中的次数统计。 这些图显示了最近文献的持续增长。 我们涵盖了计算机视觉领域的这一进展。</p> 
<ol><li> <h2>基础</h2> </li></ol> 
<p>有两个关键思想有助于开发Transformer模型。 (a) 第一个是自我监督，用于在大型未标记语料库上预训练 Transformer 模型，随后使用小型标记数据集将它们微调到目标任务 [3]、[7]、[30]。 (b) 第二个关键思想是自我注意的思想，与传统的循环模型相比，它允许捕获序列元素之间的“长期”信息和依赖关系，传统的循环模型发现对这种关系进行编码具有挑战性。 下面，我们提供关于这两个想法（第 2.1、2.2 和 2.3 节）的简要教程，以及应用这些想法的开创性 Transformer 网络（第 2.4 和 2.5 节）的摘要。 这一背景将帮助我们更好地理解即将用于计算机视觉领域的基于 Transformer 的模型（第 3 节）。</p> 
<ol><li> <h3>自我监督</h3> </li></ol> 
<p>自监督学习 (SSL) 是与 Transformer 模型一起用于从大规模未标记数据集中学习的核心概念。 可以在 [31]、[32] 中找到对 SSL 的广泛调查。 正如 Y. LeCun [33] 很好地总结的那样，SSL 的基本思想是填补空白，即尝试预测图像、时间视频序列中未来或过去帧中被遮挡的数据或预测一个借口任务，例如 应用于输入的旋转量、应用于图像块的排列或灰度图像的颜色。 另一种施加自我监督约束的有效方法是通过对比学习。 在这种情况下，讨厌的转换用于创建同一图像的两种类型的修改版本，即不改变底层类语义（例如，图像风格化、裁剪）和语义改变（例如，用相同的对象替换另一个对象） 场景，或通过对图像进行较小的对抗性更改来更改类）。 随后，该模型被训练为对令人讨厌的转换保持不变，并强调对可以改变语义标签的微小变化进行建模。</p> 
<p>自监督学习提供了一种很有前途的学习范式，因为它可以从大量现成的非注释数据中进行学习。 SSL 分两个阶段执行：首先，训练模型以通过解决借口任务来学习底层数据的有意义的表示。 借口任务的伪标签是根据数据属性和任务定义自动生成的（不需要任何昂贵的手动注释）。 在第二阶段，第一阶段训练的模型使用标记数据在下游任务上进行微调。 下游任务的示例包括图像分类 [34]、对象检测 [11] 和动作识别 [16]。</p> 
<p>SSL 的核心是借口任务定义。 因此，我们可以根据它们的借口任务将现有的 SSL 方法广泛地分类为合成图像或视频的生成方法、利用图像块或视频帧之间关系的基于上下文的方法，以及利用多种数据模态的跨模态方法。 生成方法的示例包括条件生成任务，例如图像着色 [35]（模型在 RGB 图像上训练，其中输入是灰度图像，模型输出其对应的 RGB）、图像超分辨率 [36]、图像修复 [37] 和基于 GAN 的方法 [38]、[39]。 基于上下文的借口方法解决了诸如图像块上的拼图游戏 [40]、[41]、[42]、预测几何变换（如旋转）[34]、[43] 或验证视频帧的时间序列 [40]、[41]、[42] 等问题。 44]、[45]、[46]。 跨模态借口方法验证两种输入模态的对应关系，例如音频和视频 [47]、[48] 和 RGB 和流 [49]。</p> 
<ol><li> <h3>自注意力</h3> </li></ol> 
<p>自注意力机制是 Transformers 的一个组成部分，它为结构化预测任务显式地对序列的所有实体之间的交互进行建模。 基本上，自注意力层通过聚合来自完整输入序列的全局信息来更新序列的每个组件。</p> 
<p>让我们用 表示 n 个实体序列 ，其中 d 是表示每个实体的嵌入维度。 自注意力的目标是通过根据全局上下文信息对每个实体进行编码来捕获所有 n 个实体之间的交互。 这是通过定义三个可学习的权重矩阵来转换Queries 、键 和值 。 输入序列 X 首先投影到这些权重矩阵上，得到 、 和 V 。 自注意力层的输出由下式 给出，</p> 
<p><img alt="" height="126" src="https://images2.imgbox.com/53/de/B5j71Jb1_o.png" width="413"></p> 
<p>对于序列中的给定实体，self-attention 基本上计算查询与所有键的点积，然后使用 softmax 运算符对其进行归一化以获得注意力分数。 然后每个实体成为序列中所有实体的加权和，其中权重由注意力分数给出。</p> 
<p>Masked Self-Attention：标准的自注意力层关注所有实体。 对于经过训练以预测序列的下一个实体的 Transformer 模型 [1]，解码器中使用的自注意块被屏蔽以防止注意后续的未来实体。 这只是通过使用掩码 的逐元素乘法运算来完成的，其中 M 是一个上三角矩阵。 被屏蔽的自注意力定义为，</p> 
<p><img alt="" height="132" src="https://images2.imgbox.com/a3/8a/5WjIGOVH_o.png" width="385"></p> 
<p>其中 ◦ 表示 Hadamard 积。 基本上，在预测序列中的实体时，未来实体的注意力分数在掩蔽自注意力中设置为零。</p> 
<ol><li> <h3>多头自注意力</h3> </li></ol> 
<p>为了封装序列中不同位置之间的多个复杂关系，多头注意力包括多个自注意力块（原始 Transformer 模型 [1] 中的 8 个）。 每个块都有自己的一组可学习的权重矩阵 {WQi , WKi , WVi }，其中 i = 0 · · · 7。对于输入 X，多头注意中的 8 个自注意块的输出然后连接成一个单个矩阵 [Z0, Z1, · · · Z7] ∈ 并投影到权重矩阵 （见图 2）。</p> 
<p>自注意力与卷积操作的主要区别在于，权重是动态计算的，而不是像卷积那样的静态权重（对于任何输入都保持不变）。 此外，自注意力对于输入点数量的排列和变化是不变的。 因此，与需要网格结构的标准卷积相反，它可以轻松地对不规则输入进行操作。</p> 
<ol><li> <h3>Transformer模型</h3> </li></ol> 
<p>[1]中提出的transformer模型的架构如图2所示。它具有编码器-解码器结构。 编码器由六个相同的层组成，每层有两个子层：一个多头自注意力块和一个简单的位置方式全连接前馈网络。</p> 
<p>如图 2 所示，在每一层之后使用残差连接 [50] 和层归一化 [51]。请注意，与同时执行特征聚合和特征变换的常规卷积网络不同（例如，卷积层后跟非线性 )，这两个步骤在 Transformer 模型中是解耦的，即自注意力层只执行聚合，而前馈层执行转换。 与编码器类似，Transformer 模型中的解码器包含六个相同的层。 每个解码器层有三个子层，前两个（多头自注意力和前馈）类似于编码器，而第三个子层对相应编码器层的输出进行多头注意力，如图 . 2.</p> 
<p>[1] 中的原始 Transformer 模型是针对机器翻译任务进行训练的。 编码器的输入是一种语言的单词（句子）序列。 将位置编码添加到输入序列中以捕获序列中每个单词的相对位置。 位置编码与输入 d = 512 具有相同的维度，并且可以通过正弦或余弦函数学习或预定义。 作为一个自回归模型，Transformer [1] 的解码器使用先前的预测来输出序列中的 4 个下一个单词。 因此，解码器从编码器获取输入以及之前的输出，以预测翻译语言中句子的下一个单词。 为了便于残差连接，所有层的输出维度保持相同，即 d = 512,多头attention中query、key和value权重矩阵的维度设置为dq = 64； dk = 64; dv = 64。</p> 
<ol><li> <h3>双向表示</h3> </li></ol> 
<p>原始 Transformer 模型 [1] 的训练策略只能关注句子中给定单词左侧的上下文。 这是限制性的，因为对于大多数语言任务，左右两侧的上下文信息都很重要。 来自 Transformers 的双向编码器表示 (BERT) [3] 提出联合编码句子中的左右上下文，以无监督的方式学习文本数据的特征表示。 为了实现双向训练，[3] 基本上引入了两个借口任务：Masked Language Model 和 Next Sentence Prediction。然后以无监督的方式在这些借口任务上预训练的模型针对下游任务进行微调。为此，将特定任务的附加输出模块附加到预训练模型，并对完整模型进行端到端的微调。</p> 
<p>基础 BERT [3] 模型的网络架构基于 [1] 中的原始 Transformer 模型，类似于 GPT [4]。 BERT [3] 的核心贡献是借口任务定义，它以无监督的方式实现双向特征编码。 为此，BERT [3] 提出了两种策略： (1) 掩码语言模型 (MLM) - 句子中固定百分比 (15%) 的单词被随机掩码，并训练模型使用交叉算法来预测这些掩码词。 熵损失。 在预测被屏蔽的词时，该模型学习结合双向上下文。 (2) Next Sentence Prediction (NSP) 给定一对句子，该模型预测一个二元标签，即这对从原始文档中是否有效。 可以从任何单语文本语料库轻松生成用于此的训练数据。 形成一对句子 A 和 B，使得 B 是实际句子（在 A 旁边）50% 的时间，而 B 是其他 50% 时间的随机句子。 NSP 使模型能够捕获句子到句子的关系，这在许多语言建模任务中至关重要，例如问答和自然语言推理 (NLI)。</p> 
<p><img alt="" src="https://images2.imgbox.com/66/36/1hdGFwz8_o.png" width="1200"></p> 
<p>Transformer模型的架构</p> 
<ol><li> <h2>Transformer&amp;视觉自注意力</h2> </li></ol> 
<p>我们概述了图 3 中为视觉应用设计的 Transformers 中遵循的主要主题。现有框架通常应用全局或局部注意力，利用 CNN 表示或利用矩阵分解来提高设计效率并使用矢量化注意力模型。 我们在下面以特定任务的方法组的形式解释这些研究方向。</p> 
<ol><li> <h3>基于Transformer的图像识别</h3> </li></ol> 
<p>卷积运算是计算机视觉中使用的传统深度神经网络的主力，它带来了解决在例如高维度数据集ImageNet [52]上的复杂图像识别任务 的突破。 然而，卷积也有其缺点，例如，它在固定大小的窗口上运行，因此无法捕获长距离依赖关系，例如给定视频中空间和时间域中像素之间的任意关系。 此外，卷积滤波器权重在训练后保持固定，因此操作无法动态适应输入的任何变化。 在本节中，我们回顾了通过使用自注意力操作和 Transformer 网络（一种特定形式的自注意力）来缓解传统深度神经网络中上述问题的方法。 有两种主要的自注意力设计方法。 (a) 不受输入特征大小限制的全局自注意力，例如，[53] 引入了一个受非局部手段启发的层，该层将注意力应用于整个特征图，而 [54] 降低了非局部特征的计算复杂度 通过设计稀疏注意力图进行局部操作 [53]。 (b) 局部自注意力试图对给定邻域内的关系进行建模，例如，[55] 提出将注意力限制在给定像素位置周围的特定窗口内，以减少计算开销。</p> 
<p><img alt="" height="258" src="https://images2.imgbox.com/0e/1a/okGCja6w_o.png" width="420"></p> 
<p>类似地，[53] 进一步改进了局部自注意力，使其可以动态地调整其权重聚合以适应输入数据/特征的变化。</p> 
<p>最近，通过直接在图像块上使用 NLP Transformer 编码器成功地应用了全局自注意力[9]，消除了手工网络设计的需要。 Transformer 本质上需要大量数据，例如，像 ImageNet 这样的大规模数据集不足以从头开始训练视觉Transformer，因此 [10] 建议将知识从 CNN 教师提炼到学生视觉Transformer，这允许 Transformer 仅在 ImageNet 上进行训练而无需任何额外的 数据。 在这里，我们描述了基于局部/全局自注意力的不同方法的关键见解，包括专门设计用于解决图像识别任务的 Transformer。</p> 
<ol><li> <h4>非局部神经网络</h4> </li></ol> 
<p>这种方法的灵感来自于主要为图像去噪而设计的非局部均值操作 [56]。 此操作使用图像中其他像素值的加权总和修改补丁中的给定像素。 然而，它不是 5 考虑像素周围固定大小的窗口，而是根据补丁之间的相似性选择远处的像素来对滤波器响应做出贡献。 通过设计，非本地操作对图像空间中的远程依赖性进行建模。 受此启发，Wang 等人[57] 为深度神经网络提出了一种可微的非局部操作，以前馈方式捕获空间和时间上的远程依赖性。 给定一个特征图，他们提出的算子 [57] 计算​​一个位置的响应，作为特征图中所有位置特征的加权和。 通过这种方式，非局部操作能够捕获特征图中任意两个位置之间的交互，而不管它们之间的距离如何。</p> 
<p>视频分类是一个任务的例子，其中像素之间在空间和时间上都存在远程交互。 配备了模拟远程交互的能力，[57] 证明了非局部深度神经网络在 Kinetics 数据集 [58] 上更准确的视频分类的优越性。</p> 
<ol><li> <h4>交叉注意</h4> </li></ol> 
<p>尽管自注意力机制允许我们对全图像上下文信息进行建模，但它既是内存又是计算密集型过程。 如图 4(a) 所示，为了编码给定像素位置的全局上下文，非局部块 [57] 计算​​密集注意力图（绿色）。非局部块 [57] 具有 O(N2) 的高复杂度，其中 N 表示输入特征图的数量。 为了减少这种计算负担,Huang 等人[54] 提出了交叉注意模块，该模块为每个像素位置仅在交叉路径上生成稀疏注意图，如图 4（b）所示。 此外，通过反复应用交叉注意力，每个像素位置都可以从所有其他像素捕获上下文。 与 nonlocal block 相比，cross-cross 使用了 11 个更少的 GPU 内存，并且具有 O(2 p N) 的复杂度。 在包括 Cityscapes [59]、ADE20K [60]、COCO [61]、LIP [62] 和 CamVid [63] 在内的几个基准数据集上报告了语义和实例分割任务的最新结果 [54]。</p> 
<ol><li> <h4>独立的自注意力</h4> </li></ol> 
<p>如上所述，卷积层具有平移等方差性，但无法在大感受野下进行扩展，因此无法捕获远程交互 [55]。</p> 
<p><img alt="" src="https://images2.imgbox.com/a4/3e/hVDnKgl2_o.png" width="1200"></p> 
<p>另一方面，关注输入的所有空间位置的全局注意力 [1] 可能是计算密集型的，并且在下采样的小图像、图像块 [9] 或增强卷积特征空间 [64] 上是首选。 拉马钱德兰等人[55] 提出用局部自注意层代替深度神经网络中的卷积层，该层可以应用于小输入或大输入，而不会增加计算成本。 在基本层面上，提出的自注意力层 [55] 考虑给定像素周围特定窗口大小中的所有像素位置，计算这些像素的查询、键和值向量，然后聚合该窗口内的空间信息。 在投影查询和键的 softmax 分数后聚合值向量。 对所有给定像素重复此过程，并将响应连接起来以生成输出像素。 与基于卷积层的 ResNet 模型相比，具有局部自注意力层的 ResNet 模型在更少参数的基础上可以解决 ImageNet 和 COCO 目标识别任务 [55]。</p> 
<ol><li> <h4>本地关系网络</h4> </li></ol> 
<p>卷积算子的另一个缺点来自这样一个事实，即在训练后，无论视觉输入有任何变化，它都会应用固定的权重。 胡等人[53] 提出在局部窗口中自适应地组合像素。他们引入了一个新的可微层（图 5），它根据局部窗口内像素/特征之间的组合关系（相似性）调整其权重聚合。 这种自适应权重聚合将几何先验引入网络，这对识别任务很重要[53]。 卷积被认为是自上而下的算子，因为它在各个位置上保持固定，而诸如 [56] 中引入的非局部操作是一种自下而上的方法，因为它聚合了整个图像上的输入特征。 局部关系层属于自底向上方法的范畴，但它仅限于固定的窗口大小，例如 7x7 邻域。</p> 
<p><img alt="" height="276" src="https://images2.imgbox.com/11/d1/p3pDTHD5_o.png" width="343"></p> 
<ol><li> <h4>注意力增强卷积网络</h4> </li></ol> 
<p>贝洛等人[64] 探索使用 self-attention 作为卷积算子的替代方案的可能性。 他们建议在二维中使用相对位置编码 [65] 来开发一种新的自注意力机制，该机制保持平移等方差，这是处理图像的理想属性。 尽管这种自注意力作为独立的计算原语提供了有竞争力的结果，但与卷积操作结合使用时可以获得最佳性能。 大量实验表明，注意力增强可以在各种现有架构的图像分类和目标检测方面带来系统性能改进。</p> 
<ol><li> <h4>矢量化的自注意力</h4> </li></ol> 
<p>赵等人[72] 注意到传统的卷积算子联合执行特征聚合和变换（通过应用过滤器，然后通过非线性传递）。相比之下，他们建议使用 self-attention 单独执行特征聚合，然后使用元素感知器层进行转换。 为此，他们提出了两种用于特征聚合的替代策略：（a）成对自我注意和（b）补丁式自我注意。pairwise self-attention 是置换和基数不变的操作，而 patch-wise self-attention 没有这样的不变性（类似于卷积算子）。</p> 
<p>成对和逐块自注意力都被实现为向量注意力 [72]，它学习空间和通道维度的权重。 这为注意力提供了一种替代方法，该方法通常使用标量权重（通过采用点积）来执行。 成对自注意力是一个集合算子，它计算向量注意力，同时考虑特定特征与其在给定局部邻域中的邻居的关系。 相比之下，patch-wise self-attention 是卷积算子（不是集合算子）的泛化，在推导注意力向量时会查看局部邻域中的所有特征向量。 作者表明，使用相当少的参数，自注意力网络 (SAN) 可以在 ImageNet 数据集上击败 ResNet 系列的可比基线。 他们的方法的一个关键特性是它对对抗性扰动的鲁棒性 [73]、[74] 以及对数据中看不见的转换的泛化。 这种行为是由于注意力的动态特性导致对手难以计算有用的欺骗方向。</p> 
<ol><li> <h4>视觉Transformer</h4> </li></ol> 
<p>Vision Transformer (ViT) [9] 是第一个展示 Transformer 如何在大规模计算机视觉数据集上“完全”取代深度神经网络中的标准卷积的作品。 他们将原始的 Transformer 模型（与用于 NLP 任务的版本相比变化最小）应用于一系列图像“补丁”。 Transformer 模型在 Google 收集的大型专有图像数据集上进行了预训练，然后针对下游识别基准（例如 ImageNet）进行了微调。 这是重要的一步，因为在中等范围的数据集上进行预训练不会用 ViT 给出最先进的结果。</p> 
<p><img alt="" src="https://images2.imgbox.com/62/0a/jZpZNDE2_o.png" width="1045"></p> 
<p>与必须从超大规模数据集中发现此类知识规则的Transformer相比，这是因为 CNN 编码了图像域的先验知识（归纳偏差，例如平移等方差）减少了对数据的需求。 为此，使用 3 亿张图像 JFT 数据集 [75] 进行预训练，有助于将性能提升到最先进的 CNN 模型水平。</p> 
<p>值得注意的是，与同样将Transformer应用于全尺寸图像但将训练作为生成任务执行的 iGPT [17] 模型相比，ViT 使用监督分类任务预训练模型（尽管也探索了自监督变体，其结果是 性能较差）。</p> 
<p>该模型的主要架构（图 7）与语言Transformer非常相似。 代替语言嵌入的一维序列，二维图像块以矢量形式展平并作为序列馈送到Transformer。 然后使用线性层将这些矢量化补丁投影到补丁嵌入，并附加位置嵌入以编码位置信息。 重要的是，一个 [cls] 标记（代表分类）附加在Transformer输入的开头。 然后将对应于第一个位置的输出表示用作图像分类任务的全局图像表示。</p> 
<p><img alt="" src="https://images2.imgbox.com/37/6c/AV1JerxO_o.png" width="615"></p> 
<p><img alt="" src="https://images2.imgbox.com/f8/b0/e4swWXq8_o.png" width="785"></p> 
<ol><li> <h4>数据高效的图像Transformer</h4> </li></ol> 
<p>数据高效图像Transformer (DeiT) [10] 是大规模图像分类的第一个结果，不使用任何外部大规模数据集（例如 [9] 中的 JFT），与仔细相比，它展示了Transformer的潜力 调整 CNN 设计。 由于与 CNN 设计相反，Transformer 架构不假设有关图像结构的先验知识，因此通常会导致更长的训练时间，并且需要更大的数据集来训练 Transformer 模型。 然而，DeiT 展示了如何在相对较短的训练集中在中等规模的数据集（例如，120 万个示例与 ViT [9] 中使用的数亿个示例）上学习Transformer。 除了使用 CNN 中常见的增强和正则化程序外，主要贡献是一种新颖的Transformer原生蒸馏方法。</p> 
<p>蒸馏过程 [76] 使用 CNN 作为教师模型（RegNetY-16GF [77]），其输出用于训练Transformer模型。 CNN 的输出有助于 Transformer 有效地找出输入图像的有用表示。 一个蒸馏令牌附加了输入补丁嵌入和类令牌。 自注意力层对这些标记进行操作以了解它们的相互依赖性并输出学习到的类、补丁和蒸馏标记。 网络使用定义在输出类标记上的交叉熵损失和蒸馏损失进行训练，以将蒸馏标记与教师输出相匹配。 为蒸馏探索了软标签和硬标签的选择，发现硬蒸馏的性能更好。</p> 
<p>有趣的是，学习到的类和蒸馏标记没有表现出高相关性，表明它们的互补性。 学习到的表示与性能最好的 CNN 架构（例如 EfficientNet [78]）相比表现出色，并且对于许多下游识别任务也能很好地泛化。</p> 
<ol><li> <h3>基于Transformer的目标检测</h3> </li></ol> 
<p>与图像分类类似，transformer 模型应用于从骨干 CNN 模型获得的一组图像特征，以预测精确的对象边界框及其相应的类标签。 下面，第一种方法 [11] 首次使用Transformer网络攻击检测问题，第二种方法 [12] 主要将 [11] 扩展到多尺度架构</p> 
<ol><li> <h4>检测Transformer-DETR</h4> </li></ol> 
<p>DETR [11] 将对象检测视为使用变换器模型和集合损失函数的集合预测问题。 第一个贡献（transformer 模型）可以预测一组对象（一次性）并允许对它们的关系进行建模。 第二个贡献（集合损失）允许预测和真实框之间的二分匹配。 DETR 的主要优点是它消除了对手工制作的模块和操作的依赖，例如对象检测中常用的 RPN（区域提议网络）和 NMS（非最大抑制）[79]、[80]、[81] , [82], [83]。 通过这种方式，对于复杂的结构化任务（如对象检测），可以放松对先验知识和精心工程设计的依赖。</p> 
<p>给定来自 CNN 主干的空间特征图，编码器首先将空间维度展平为单个维度，如图 8 所示。这给出了一系列特征 d n，其中 d 是特征维度，n = h w h;w 是空间特征图的高度和宽度。 然后使用 [1] 中提出的多头自注意力模块对这些特征进行编码和解码。 解码阶段的主要区别在于，所有框都是并行预测的，而 [1] 使用 RNN 一个一个地预测序列元素。 由于编码器和解码器是置换不变的，位置编码被用作对象查询来生成不同的框。 DETR 获得了与流行的 Faster RCNN 模型 [79] 相当的性能，鉴于其简单的设计，这是一项令人印象深刻的壮举。</p> 
<ol><li> <h4>可变形-DETR</h4> </li></ol> 
<p>上面提到的 DETR [11] 成功地将卷积网络与 Transformers [1] 相结合，消除了手工设计的要求，并实现了端到端的可训练对象检测管道。 然而，它很难检测小物体，并且收敛速度慢，计算成本相对较高 [12]。 在使用 Transformer 进行关系建模之前，DETR 将图像映射到特征空间。 因此，自注意力的计算成本与特征图的空间大小呈二次方增长，即 O(H2W2C)，其中 H 和 W 表示特征图的高度和宽度。 这本质上限制了在 DETR 训练框架中使用多尺度分层特征 [84]，这对于检测小物体最终很重要。 此外，在训练开始时，注意力模块只是将统一注意力投射到特征图的所有位置，并需要大量训练时期来调整注意力权重以收敛到有意义的稀疏位置。 这种方法导致 DETR 的收敛速度较慢。 为了缓解上述问题，[12] 提出了一个可变形的注意力模块来处理特征图。 受可变形卷积 [85] 的启发，可变形注意力模块 [12] 只关注整个特征图中的稀疏元素集，而不管其空间大小。 这进一步允许在多尺度注意力模块的帮助下跨尺度聚合特征图，而不会显着增加计算成本。 Deformable DETR 不仅性能更好，而且其训练时间也比原始 DETR 模型 [12] 低 10。</p> 
<ol><li> <h3>基于Transformer的分割（语义分割和实力分割）</h3> </li></ol> 
<p>像将图像分割成语义标签和对象实例这样的密集预测任务需要对像素之间的 9 个丰富的交互进行建模。 在这里，我们解释了一种旨在降低自注意力复杂性的轴向自注意力操作 [86] 和一种可以分割与给定语言表达相对应的区域的跨模态方法 [13]。</p> 
<ol><li> <h4>全景分割的轴向注意力</h4> </li></ol> 
<p>全景分割 [87] 旨在通过为图像的每个像素分配语义标签和实例 id 来联合解决语义分割和实例分割的不同任务。 全局上下文可以提供有用的线索来处理如此复杂的视觉理解任务。 Self-attention 在建模远程上下文信息方面是有效的，尽管将其应用于密集预测任务（如全景分割）的大量输入代价高昂。 一个简单的解决方案是将自注意力应用于下采样输入或每个像素周围的有限区域 [55]。 即使在引入这些约束之后，self-attention 仍然具有二次复杂度，并且分别牺牲了全局上下文。</p> 
<p>为了缓解上述问题，Wang 等人[86] 提出了位置敏感的轴向注意，其中 2D 自注意机制被重新表述为两个 1D 轴向注意层，依次应用于高度轴和宽度轴（见图 9）。 轴向注意力具有很高的计算效率，并使模型能够捕获完整的图像上下文。 通过在 COCO [61]、Mapillary Vistas [88] 和 Cityscapes [59] 基准以及 ImageNet 数据集上的图像分类问题上实现全景分割任务的最先进性能，证明了轴向注意力的有效性 [52]。</p> 
<p><img alt="" src="https://images2.imgbox.com/c9/20/eSBCtIvr_o.png" width="1200"></p> 
<ol><li> <h4>CMSA：跨模态自注意力</h4> </li></ol> 
<p>跨模态自注意 (CMSA) [13] 编码语言和视觉域特征之间的远程多模态依赖关系，用于参考图像分割任务。 引用图像分割问题旨在分割图像中被语言表达式引用的实体。</p> 
<p>为此，通过将图像特征与每个词嵌入和空间坐标特征连接起来，获得一组跨模态特征。 self-attention 对这个丰富的特征进行操作，并在句子中每个单词对应的图像上生成注意力。 分割网络在多个空间级别执行自我注意，并使用门控多级融合模块通过跨多分辨率特征的信息交换来细化分割掩码。 二元 CE 损失用于训练整体模型，该模型在 UNC [89]、G-Ref [90] 和 ReferIt [91] 数据集上取得了良好的改进。</p> 
<ol><li> <h3>基于Transformer的图像生成</h3> </li></ol> 
<p>从生成建模的角度来看，图像生成任务很有趣，因为以无监督方式学习的表示可以稍后用于下游任务。 在这里，我们总结了用于图像生成 [92]、条件生成 [93] 和高分辨率图像生成 [94] 任务的基于Transformer的架构。 我们还介绍了一个结构化生成任务，其中根据房间布局填充场景对象 [20]。</p> 
<ol><li> <h4>图像GPT</h4> </li></ol> 
<p>受Transformer模型在语言领域的成功启发，图像 GPT（iGPT）[92] 证明此类模型也可用于图像生成任务，并为下游视觉任务学习强大的特征。具体来说，iGPT 在扁平图像序列（一维像素阵列）上训练 GPT v2 模型 [5]，并表明它可以在没有任何外部监督的情况下生成合理的图像输出。生成的样本描述了模型理解像素和高级属性（如对象类、纹理和比例）之间的空间关系的能力。</p> 
<p>使用 iGPT 的无监督训练机制学习的特征与其他无监督方法的竞争令人印象深刻，在 CIFAR-10/100 [95] 和 STL [96] 数据集上实现了最先进的性能，同时性能接近 SimCLR（a 对比学习方法）[97] 在 ImageNet 数据集上。 这是一个惊人的结果，因为 iGPT 架构与用于语言建模任务的架构完全相同，因此它不包含任何先前的特定领域知识。 值得注意的是，基于无监督 CNN 的竞争解决方案在架构设计、注意力机制、损失函数和正则化 [98]、[99]、[100]、[101]、[102] 的形式中广泛采用了此类先验。 然而，不利的一面是，iGPT 的计算成本很高，例如，与 MoCo [100] 相比，iGPT-L 版本的训练成本高出大约 36，后者是最先进的自监督特征学习方法。出于这个原因，训练通常仅限于 64 64 的低分辨率，而卷积架构可以有效地从高分辨率输入中学习。</p> 
<ol><li> <h4>图像Transformer</h4> </li></ol> 
<p>帕尔马等人[93] 开发了一种图像生成模型，该模型可以根据先前生成的像素顺序预测输出图像的每个像素。 他们的方法通过将图像像素分解为像素条件分布的乘积来模拟图像像素的联合分布。 先前为此任务开发的自回归模型，例如 PixelCNN [103]，受到有限的感受野的影响，这阻碍了对图像中的长期关系（例如，部分关系或遮挡）进行建模。 使用自注意力现象，Image Transformers [93] 增强了神经网络的感受野，而不会产生高计算成本（例如，与 PixelCNN [103] 的 25 像素相比，证明可以实现高达 256 像素的有效感受野） . 生成管道还在条件生成任务上进行了测试，例如图像超分辨率、图像完成和去噪。</p> 
<p>核心方法有两个主要亮点（见图 10），（a）在图像中使用 key、query 和 value 三元组的方式 10，以及（b）使用具有相对较多位置的 self-attention 作为 与语言中的句子相比（之前已经证明自我注意成功）。 对于第一部分，先前生成的像素的特征表示用于生成“值”和“键”嵌入，而当前像素的特征嵌入用作“查询”。</p> 
<p>第一层使用位置嵌入来编码位置信息。 为了解决第二个问题，局部注意力（1D 和 2D 变体）仅用于查询位置周围的局部邻域。 出于实际原因，为每个查询定义了一个固定的内存块，而不是为每个像素动态计算不同的内存邻域。 应用最大似然损失来训​​练生成模型。</p> 
<p><img alt="" src="https://images2.imgbox.com/66/59/lMJ1Amwj_o.png" width="1086"></p> 
<ol><li> <h4>高分辨率图像合成</h4> </li></ol> 
<p>当应用于高维序列时，Transformer通常会产生高计算成本。 为了克服这一限制，Esser 等人[94] 提议在Transformer旁边包括感应偏置（通常用于 CNN）以提高它们的效率。 具体来说，通过学习丰富的视觉模式字典，可以利用 CNN 结构中内置的局部连接性和空间不变性偏差。 字典是使用生成对抗方法 [104] 学习的，该方法试图对感知上的声音图像块进行编码。 然后使用Transformer来学习字典项目之间的远程交互以生成输出。 反过来，他们开发了一个条件图像生成模型，能够使用Transformer生成非常高分辨率的图像（高达百万像素范围）。 这是第一项展示了应用Transformer来生成如此高分辨率图像的工作。</p> 
<p><img alt="" src="https://images2.imgbox.com/04/06/AyaeLMf3_o.png" width="1200"></p> 
<ol><li> <h4>场景前</h4> </li></ol> 
<p>在之前关于图像生成的工作 [92]、[93]、[94] 中，图像输出通常由模型直接预测。</p> 
<p>相比之下，[20] 学习生成要放置在给定场景中的 3D 对象的参数。 具体来说，SceneFormer [20] 研究了 3D 房间布局条件场景生成任务。 鉴于空房间的形状，这种方法可以在保持真实感的同时在房间中提出新的对象配置。 值得注意的是，该模型不使用任何外观信息，仅通过使用 Transformer 中的 self-attention 对对象间关系进行建模来学习生成新场景。 类似于 Transformer 对句子的操作方式，它被应用于一系列对象以预测场景中的下一个合适的对象。 具体来说，下一个物体的大小、姿态、位置和类别是由变换器模型预测的。 一个开始标记表示推理的开始，输出标记的数量表示模型按顺序生成的对象。</p> 
<p>作者还探索了根据房间布局的文本描述生成新场景。 与外观的独立性使该方法有效，从而实现交互式场景生成。</p> 
<ol><li> <h3>低级视觉Transformer</h3> </li></ol> 
<p>在本节中，我们将解释为低级视觉任务提出的变换器模型，例如超分辨率 [14]、去噪 [17] 和图像着色 [21]。</p> 
<ol><li> <h4>超分辨率Transformer</h4> </li></ol> 
<p>图像超分辨率 (SR) 旨在从其低分辨率 (LR) 版本生成高分辨率 (HR) 图像。</p> 
<p>近年来，由于卷积神经网络 (CNN)，SR 的性能取得了重大突破。 原则上，由 CNN 生成的超分辨率图像的质量取决于优化目标的选择。</p> 
<p>一方面，基于像素损失函数（例如 L1、MSE 等）的 SR 方法 [105]、[106]、[107]、[108]、[109] 在以下方面产生了令人印象深刻的结果 图像保真度 11 个指标，例如 PSNR 和 SSIM。 然而，它们很难恢复精细的纹理细节，并且经常生成过于平滑且在感知上不太令人愉悦的图像。 另一方面，感知 SR 方法 [36]、[110]、[111]、[112]、[113]，除了每像素损失之外，还采用基于以下的对抗性损失 [104] 和感知损失 [114] 从预训练的 CNN 中提取的深层特征。 虽然这些方法生成的图像清晰、视觉上令人愉悦且在感知上似乎合理，但它们在 PSNR/SSIM 中测量的重建精度显着降低。 此外，感知 SR 算法有产生假纹理幻觉并导致伪影的倾向。 上面提到的 SR 方法遵循两个不同（但相互冲突）的研究方向：一个是最大化重建精度，另一个是最大化感知质量，但从来没有两者兼而有之。</p> 
<p>为了缓解感知再现和准确再现之间的权衡，杨等人[14] 提出了一种用于超分辨率的Transformer网络（TTSR）。</p> 
<p>在训练期间，TTSR 使用成对的 LR-HR 图像，以及与 LR 图像内容相似的参考 (Ref) 图像。 TTSR 学习在 Ref 图像中搜索相关区域并传输丰富的纹理以帮助超分辨率输入 LR 图像。 TTSR 方法的纹理变换器模块，如图 11 所示，由四个核心组件组成： (1) 可学习纹理提取器以 LR"、Ref#" 和 Ref 图像作为输入，生成纹理特征查询 (Q)、关键 (K) 和值 (V)，分别。 这里，“表示双三次上采样操作，而#”表示双三次下采样后跟上采样操作。 (2) Relevance embedding首先将Q和K展开成patch，然后计算Q中每个patch与K中每个patch的相似度，以生成硬注意力图和软注意力图。(3) Hard-attention 使用 hard attention map 将 HR 纹理特征从 V 转移到（LR 特征）Q。 (4) Soft-attention 通过使用 soft-attention map 进一步增强相关特征，同时抑制不太相关的特征。</p> 
<ol><li> <h4>用于图像增强任务的Transformer</h4> </li></ol> 
<p>为高级计算机视觉任务（例如对象检测和语义分割）开发的最先进算法通常采用在大规模数据集（例如 ImageNet）上预训练的骨干网络。 相比之下，图像去噪、超分辨率和去雨等低级视觉任务的算法直接在特定任务数据上训练，因此受到以下限制。</p> 
<p>首先，特定任务数据集中可用的图像数量很少（例如，用于图像超分辨率的常用 DIV2K 数据集仅包含 2000 张图像）。 其次，为一项图像处理任务训练的模型不能很好地适应其他相关任务。</p> 
<p>陈等人[17]提出了一种基于变换器架构的预训练模型，命名为图像处理变换器（IPT）。 它能够执行各种图像恢复任务，例如超分辨率、去噪和去雨。 如图 12 所示，IPT 的整体架构由分别处理不同任务的多头和多尾以及一个共享的编码器-解码器Transformer体组成。 由于充分利用 Transformer 需要对大规模数据进行训练，因此 Chen 等人[17] 从 ImageNet 中获取干净（真实）图像对不同任务进行基准测试并综合其降级版本。 例如，双三次插值用于生成低分辨率图像，添加加性高斯白噪声以准备噪声数据，并应用手工制作的雨条纹以获得雨图像。 总共有 1000 万张图像用于预训练 IPT 模型。 在训练期间，每个特定任务的头部将退化图像作为输入并生成视觉特征。 这些特征图被分成小块，随后在将它们馈送到Transformer编码器之前变平。 编码器的架构与原始Transformer模型 [1] 的架构相同。 编码器的输出以及任务特定的嵌入作为输入提供给Transformer解码器。 解码器输出的特征被重新整形并传递到产生恢复图像的多尾。 IPT 模型使用 L1 损失进行了优化。 实验结果表明，预训练的 IPT 模型在针对特定的低级视觉任务进行微调时，可以比最先进的方法 [108]、[115]、[116] 提供显着的性能提升 .</p> 
<p><img alt="" src="https://images2.imgbox.com/a4/a4/OcDIhGvQ_o.png" width="1200"></p> 
<ol><li> <h4>着色Transformer</h4> </li></ol> 
<p>给定灰度图像，着色试图产生相应的着色样本。 对于给定的灰度输入，这是一个一对多的任务，在彩色输出空间中存在许多可能性。 这项任务的挑战性性质需要能够生成多个彩色输出样本的概率模型。 Colorization Transformer [21] 是一种基于条件注意机制 [117] 的概率模型。 它将图像着色任务分为三个子问题（图 13），并建议通过不同的 Transformer 网络依次解决每个任务。</p> 
<p>作者首先训练一个Transformer网络将低分辨率灰度图像映射到 3 位低分辨率彩色图像。 低分辨率图像反过来允许训练更大的模型。 然后，在训练的第二阶段，另一个Transformer网络将 3 位低分辨率彩色图像上采样为 8 位 RGB 样本。 最后，训练第三级Transformer以增加由第二级Transformer产生的 8 位 RGB 样本的空间分辨率。 着色Transformer中使用的自注意力基于 [117] 中引入的行/列注意力层。 这些层捕获输入图像的每个像素之间的交互，同时计算成本较低。 row-wise attention layer 将 self-attention 应用于给定行中的所有像素，而 column-wise 12 attention layer 仅考虑图像给定列中的像素。 这项工作 [21] 是第一个成功应用经过训练的Transformer以高（256 256）分辨率对灰度图像进行着色。</p> 
<p><img alt="" src="https://images2.imgbox.com/6c/89/tW7gbtbi_o.png" width="794"></p> 
<ol><li> <h3>用于多模态任务的Transformer</h3> </li></ol> 
<p>Transformer 模型也被广泛用于视觉语言任务，例如视觉问答 (VQA) [122]、视觉常识推理 (VSR) [123]、跨模态检索 [124] 和图像字幕 [125]。 这个方向的几项工作针对大规模多模态数据集上的有效视觉语言预训练 (VLP)，以学习有效编码跨模态关系的通用表示（例如，给定图像中一个人的基础语义属性）。 然后可以将这些表示转移到下游任务，通常会获得最先进的结果。 此类模型通常应用具有多模态输入的普通多层变换器 [1]，并且不会对核心注意力块进行根本性的改变。 然而，它们的主要区别在于Transformer的配置和损耗函数（见图 14）。</p> 
<ol><li> <h4>ViLBERT：视觉和语言 BERT</h4> </li></ol> 
<p>视觉和语言 BERT 是 BERT 模型在多模态领域的第一个扩展。 目标是学习可以联合建模图像和自然语言的表示。 为此，ViLBERT 开发了一种双流架构，其中每个流都专门用于对视觉或语言输入进行建模。 两个并行流的架构都是一系列类似于 BERT 模型的Transformer块。 随后，共同注意Transformer层被应用于学习跨模态关系。 外延框架非常简单。 以标准方式 [1] 为每种模态计算查询、键和值矩阵，然后将一种模态的键值对传递给另一种模态的注意力头。</p> 
<p>ViLBERT 将 VLP 应用于在概念概念数据集上定义的一组代理任务（具有 330 万张带有弱字幕的图像），然后在 VQA 等下游任务上微调模型。 预训练阶段以自我监督的方式运行，即在没有对大规模未标记数据集进行手动标记的情况下创建借口任务。 这些借口任务包括预测文本和图像输入是否相关以及预测屏蔽图像区域和文本输入的语义（例如，类似于在 BERT 模型 [3] 中重建文本中的屏蔽词）。 通过这种方式，模型在预训练期间学习数据中的固有结构，并对跨域关联进行建模。 通过对多个任务的评估，[15] 证明双流模型比使用共享参数对语言和视觉域进行建模的单流模型性能更好 [15]。</p> 
<ol><li> <h4>LXMERT</h4> </li></ol> 
<p>与 ViLBERT [121] 类似，从 Transformers 学习跨模态编码器表示（LXMERT）[18] 也使用基于 BERT 框架的双流架构。 主要区别在于用于对视觉特征进行建模的对象关系编码器，而不是 ViLBERT 中使用的简单图像级特征。 然后使用类似于 [121] 的交叉注意块跨模态融合两个流中的信息。</p> 
<p>与 [121] 中用于 VLP 的两个 pre-texts 任务相比，LXMERT 使用了五个预训练任务，包括掩码对象和语言预测、跨模态匹配和视觉问答。 预训练模型在 VQA 任务上进行了微调，然而，预训练和微调任务之间的高度相似性引发了对学习表征对新任务的普遍性的质疑。 为此，作者对真实视觉推理 (NLVR) 任务 [126] 进行了泛化实验，展示了对新任务的显着改进。</p> 
<ol><li> <h4>VisualBERT</h4> </li></ol> 
<p>与 ViLBERT [121] 和 LXMERT [18] 等双流网络不同，VisualBERT [120] 使用单个Transformer堆栈来对两个域（图像和文本）进行建模。 文本的输入序列（例如，标题）和与对象提议相对应的视觉特征被馈送到自动发现两个域之间关系的Transformer。 值得注意的是，VisualBERT 架构有点类似于 VideoBERT [15]（在第 3.7 节中解释），但不仅专注于烹饪视频，VisualBERT 还评估各种视觉语言任务（例如，VCR、NLVR、VQA 和视觉基础） .</p> 
<p>VisualBERT 模型首先使用两个目标应用与任务无关的预训练。 第一个目标只是尝试使用图像特征和剩余的文本标记来预测丢失的文本标记。 第二个目标试图区分给定图像的真假标题。 在与任务无关的预训练之后，作者建议执行特定于任务的预训练以弥合领域差距</p> 
<ol><li> <h4>VL-BERT</h4> </li></ol> 
<p>苏等人[19] 提出了一种多模态预训练方法来学习可推广到多模态下游任务（例如视觉常识推理和视觉问答）的特征。 这项工作需要充分调整视觉和语言线索，以便学习有效的复合表示。最后，[19] 建立在 BERT 模型的基础上，并同时输入视觉和语言特征。 语言特征对应于输入句子中的标记，视觉特征对应于输入图像（通过标准 Faster R-CNN 获得）中的兴趣区域 (RoI)。 具体来说，该模型在视觉语言数据集（Conceptual Captions [127]）和仅语言数据集（例如维基百科）上进行了预训练。 损失函数与 BERT 相同，其中模型经过训练以预测被屏蔽的单词或视觉 ROI。 与 UNITER [118] 等其他工作相反，VL-BERT 声称视觉语言匹配任务在预训练期间没有用，这与后来努力的证据形成对比 [119]。 他们在几个多模态任务上的结果显示了这种预训练比仅语言预训练（例如，在 BERT 中）的好处。</p> 
<p><img alt="" src="https://images2.imgbox.com/ad/d0/uD2JYWyD_o.png" width="1200"></p> 
<ol><li> <h4>Unicoder-VL</h4> </li></ol> 
<p>视觉和语言通用编码器 (Unicoder-VL) [119] 使用大规模图像字幕数据集学习多模态表示。 语言和图像输入被馈送到单个Transformer模型（具有多个连续编码器）以学习联合嵌入。 为此，他们在预训练期间使用掩码词预测、掩码对象分类和视觉语言匹配作为自我监督任务。 值得注意的是，视觉-语言匹配仅在全局级别进行（即图像-句子对齐）。 该模型在图像文本检索、零样本学习和视觉常识推理的下游任务上进行了评估，在这些任务中，它的性能优于 ViLBERT [121] 和 VisualBERT [120] 等以前的模型。 这显示了丰富的自监督任务的重要性，并提倡使用统一的Transformer架构在通用框架中学习多模态特征表示。</p> 
<ol><li> <h4>UNITER</h4> </li></ol> 
<p>通用图像文本表示（UNITER）[118] 也是一种多模态特征学习方法，通过对四个大规模视觉语言数据集（MS-COCO [61]、Visual Genome [128]、Conceptual Captions [ 127] 和 SBU 字幕 [129]）。 学习到的表征已被证明可以很好地在下游任务上迁移，例如 VQA、多模态检索、视觉常识推理和 NLVR。 为了强调学习视觉和语言域之间的关系，他们专门设计了预训练任务来预测以其他域输入为条件的蒙面视觉或文本区域，并在全局（图像-文本）上对齐语言和视觉输入 和本地（词区域）级别。 这些任务是在 BERT 中使用的传统掩码语言建模任务之外的任务，并且明确包括细粒度的词区域对齐以及早期工作中未考虑的输入条件掩码，例如 VL-BERT [19]、VisualBERT [120]、 维尔伯特 [121] 和 Unicoder-VL [119]。 与其他方法一样，它们采用了 BERT 中提出的Transformer架构，该架构可在视觉和语言嵌入上运行。 与将独立Transformer应用于语言和视觉输入 14（如 ViLBERT [121] 和 LXMERT [18]）相比，UNITER 采用应用于文本和图像输入的单个Transformer，如 [19]、[119]、[120 ]。</p> 
<ol><li> <h4>Oscar：对象语义对齐的预训练</h4> </li></ol> 
<p>用于 VLP 的 VisualBert [120]、Uniter [118]、VL-BERT [19]、VilBERT [121]、Unicoder-VL [119] 模型连接图像和文本特征并将其留给自注意力以自动发现跨模态关系。 这会使图像中语义概念的视觉基础复杂化。</p> 
<p>为了解决这个问题，Oscar [67] 首先使用对象检测器来获取对象标签（标签），随后使用这些标签作为将相关视觉特征与语义域信息对齐的机制。 动机是文本内容通常与图像中的主要对象有关，因此通过在输入中明确添加这些图像标签，可以更好地关注视觉特征。</p> 
<p>与 BERT [3] 类似，Oscar 对 VLP 使用了 Masked Token Loss。 具体来说，文本输入和图像标签中的不同标记被随机屏蔽，模型的工作是预测丢失的标记。 这迫使它学习丢失的标记与作为视觉和语义特征给出的上下文信息之间的关系。 此外，它还使用对比损失来区分原始和嘈杂/假图像标签对。 与不使用对象标签的 VLP 方法相比，如此学习的表示在 VQA、跨模态检索、自然语言推理和图像字幕任务上进行了微调，以获得更好的性能。</p> 
<ol><li> <h4>代号化</h4> </li></ol> 
<p>Tan 和 Bansal [130] 引入了“vokens”的概念（与从句子中提取的语言标记相关的图像）。</p> 
<p>vokens（可视化标记）为语言模型提供视觉监督以学习更好的特征。 动机是人类通过将视觉信息与语义概念相关联来学习语言。 本着与其他自监督语言表征学习方法 [3]、[121] 类似的精神，它们通过定义 voken-prediction 任务的辅助任务来学习表征。</p> 
<p>由于现有数据集对有限的视觉标记进行编码，因此他们提出了一种将语言标记映射到视觉标记的 vokenization 方法。 该方法使用基于语言的检索进行此类映射，并将在小型标记数据集 (MS-COCO) 上训练的模型转移到大型数据集 (维基百科)。 此外，确保考虑句子范围的上下文以获得 tokenvoken 映射。 使用生成的令牌训练的结果模型在各种 NLP 任务集上的表现优于最先进的 BERT 模型。 从这个意义上说，所提出的模型不评估视觉任务，但是，使用视觉作为训练语言模型的有用基础线索，因此我们将其包含在多模态表示学习组中。</p> 
<ol><li> <h4>视觉和语言导航</h4> </li></ol> 
<p>此任务旨在根据视觉和语言输入预测地图上的导航计划。 基于自我注意的变换器网络早先在 [131]、[132] 中用于视觉和语言导航 (VLN)。 这些工作首先使用视觉和语言对上的自我监督对跨模态Transformer网络进行了预训练，然后是微调特定的 VLN 任务。 虽然这些作品学习了图像区域和语言之间的注意力，但 Chen 等人[70] 建议学习语言输入和空间拓扑图之间的跨模态注意力。 拓扑图将代理的环境表示为一个图，其节点表示位置，边表示它们的连接性。 给定拓扑图和自然语言输入，使用 Transformer 模型的 VLN 任务与 NLP 中的序列预测相似。 具体来说，在每个时间实例中，跨模式Transformer预测导航计划中拓扑图的单个节点。 单独的语言和地图编码首先使用单模态编码器进行处理，然后应用跨模态编码器（类似于 LXMERT [18]）来聚合跨模态的信息。 为了表示地图中的位置，学习轨迹位置编码附加了地图特征。 基于这个Transformer设置，[70] 报告了一个完整的导航系统，可以自由探索环境并智能地规划其行动。</p> 
<p><img alt="" src="https://images2.imgbox.com/9f/d7/vX3f19rS_o.png" width="1082"></p> 
<ol><li> <h3>视频理解</h3> </li></ol> 
<p>视频形式的视听数据是大量可用的。然而，流行的方法通常学习短视频（长达几秒）的表示，这使得它们只能编码短距离依赖 [1]、[26]。 远程依赖建模在各种多模态学习任务中是可取的，例如活动识别 [58]、[133]、[134]。 下面，我们将解释最近使用Transformer网络的表现力来解决这一挑战的方法。</p> 
<ol><li> <h4>VideoBERT：联合视频和语言建模</h4> </li></ol> 
<p>VideoBERT [15] 模型利用Transformer网络和自监督学习的优势来学习有效的多模态表示。 具体来说，VideoBERT 使用对蒙面视觉和语言标记的预测作为自监督学习中的借口任务。 这允许对高级语义和长期时间依赖性进行建模，这对于视频理解任务很重要。</p> 
<p>给定一段视频，他们使用现成的语音识别系统将语音转换为文本，并应用矢量量化（聚类）从预先训练的 15 个视频分类模型中获取视觉特征。 然后将 BERT 模型直接应用于这些连接的语言和视觉标记序列，以了解它们的联合分布。</p> 
<p>该模型可以使用纯文本、纯视频和视频+文本域进行训练。 由此产生的模型展示了跨模态预测的有趣功能，例如从给定的文本输入（例如，字幕或烹饪食谱）生成视频和给定视频标记的（基于视频的）未来预测。 视频+文本模型使用视觉语言对齐任务来学习跨模态关系。 这个前置文本任务的定义很简单，给定 [cls] 标记的潜在状态，任务是预测句子是否与视觉标记序列在时间上对齐。 此外，学习到的表示对于下游任务非常有用，例如动作分类、零镜头分类和视频字幕。</p> 
<ol><li> <h4>参数高效的多模态Transformer</h4> </li></ol> 
<p>李等人[71] 注意到像 VideoBERT [15] 和 ViLBERT [121] 这样的多模态表示学习方法通​​常将语言处理部分固定到预训练模型（例如，BERT [3]）以降低训练复杂性。</p> 
<p>或者，在文献中，他们首次提出在未标记视频的视听数据上学习端到端的多模态双向Transformer模型。 首先，使用 CNN 对短期（例如 1-3 秒）视频动态进行编码，然后是可以模拟长期依赖关系（例如 30 秒）的特定模态Transformer（音频/视觉）。 然后将多模态Transformer应用于特定于模态的Transformer输出，以跨视觉语言域交换信息。</p> 
<p>然而，以幼稚的形式学习这样的模型会产生巨大的内存需求。 为了降低参数复杂度，参数在每个Transformer内的各层之间共享，基于低秩近似，导致参数减少高达 80%。</p> 
<p>使用基于内容感知负采样方法的对比学习方法来训练Transformer。 具体来说，该模型使用从训练阶段学习到的 CNN 获得的特征来选择在视觉上与正实例相似的负样本。 这项工作还比较了早期工作中采用的各种融合策略，例如早期（VideoBERT [15] 和 VL-BERT [19]）、中级（ViL-BERT [121] 和 LXMERT [18]）和晚期融合机制，并展示了 认为中级融合是最优选择。 所提出的模型在 Kinetics-700 [133] 数据集上进行了预训练，然后在下游视频分类任务上进行了微调，例如 UCF101 [135] 上的短视频分类、ESC50 [136] 上的音频分类和 Charades 上的长期动作识别 [ 137] 和 Kinetics-Sounds [48] 数据集。</p> 
<ol><li> <h4>视频动作Transformer</h4> </li></ol> 
<p>吉尔德哈尔等人[16] 使用变换器架构的变体来聚合与特定人相关的视频中的上下文线索。 他们证明了这种上下文信息对动作分类和定位很有用。 最初，该模型使用 Faster-RCNN [79] 样式处理，其中主干模型生成转发到区域提议网络以获得对象提议的特征。 然后应用 RoI 池来生成特定于对象的特征。</p> 
<p>然后将多头自注意力 [1] 作为自注意力层级联应用于对象特征之上。 在每个Transformer单元中，特定的人物特征被视为“查询”（Q），而来自相邻视频剪辑的特征被用作“关键”（K）和“值”（V）。 位置信息被明确地编码在输入特征图中，K、V 和 Q 是从这些特征图中导出的，从而将位置信息合并到自注意力中。 对于给定的 400 400 64 视频剪辑，键和值张量的大小为 16 25 25 128，而查询向量为 128 维。 虽然这项工作仅使用 RGB 流，但使用额外的模式，如光流和音频信号（如在竞争视频分析工作中）将进一步增加计算复杂度。 此外，我们发现 Transformer 模型对于动作定位不是最佳的，这可能是因为它倾向于整合全局信息。 因此，一个重要的研究问题是如何在需要精确描绘的问题（例如，动作定位和分割）的全局和局部上下文之间实现正确的权衡。</p> 
<ol><li> <h4>基于骨架的动作识别</h4> </li></ol> 
<p>基于骨架表示的人体动作识别需要能够理解给定帧中身体不同关节之间以及视频不同帧之间关系的模型。 普利扎里等人[66] 提出了一个双流 Transformer 网络来模拟这种关系。 他们引入了空间自注意力 (SSA) 以用于不同身体关节之间的关系建模，而时间自注意力 (TSA) 则用于捕获远程帧间依赖关系。 他们首先使用一个小的残差网络从骨架数据中提取特征，然后使用 SSA 和 TSA 模块来处理这些特征图。 SSA 通过独立地寻找每对关节之间的相关性来模拟不同身体部位之间的关系，而 TSA 则侧重于某个关节的特征如何沿时间维度在帧之间变化。 关节可以被认为是词袋，SSA 的目的是发现周围关节之间的关系，就像 Transformer 将短语中的不同词联系起来一样。 另一方面，TSA 发现帧之间的长程关系，类似于 NLP 中短语之间的关系是如何构建的。 两个流式时空 Transformer 网络在 NTURGB+ D 60 [138] 和 NTU-RGB+D 120 [139] 数据集上取得了最先进的结果。</p> 
<p><img alt="" src="https://images2.imgbox.com/90/2d/8iJka0o6_o.png" width="804"></p> 
<ol><li> <h3>少样本学习的Transformer</h3> </li></ol> 
<p>在少样本学习设置中，在推理时提供支持集以适应一组新的类别。 Trans16 以前的模型已被用于学习此支持集 [23] 上的集到集映射或学习给定输入查询和支持集图像之间的空间关系 [22]。我们将在下面详细说明这些方法。</p> 
<ol><li> <h4>交叉Transformer</h4> </li></ol> 
<p>多尔施等人[22] 探索自我监督和Transformer架构在训练和评估阶段之间存在分布不匹配的情况下的效用。 他们特别考虑了少样本细粒度分类问题，其中模型首先在一组基类上训练，然后在评估期间，它必须使用少数标记示例（支持集）适应新类。</p> 
<p>Cross-transformer 在 Meta-dataset [140] 上进行评估，这是一个巨大的数据集，由 10 个不同的数据集（包括 ImageNet、MS-COCO 等）组成。 该数据集封装了学习者在评估期间必须适应新类别和新领域的挑战性场景。 在这种情况下，Transformer架构用于将给定的查询图像与支持集中可用的少数示例相关联。 为此，Transformer在查询和支持集图像中找到空间相似的区域，然后使用相应的特征来获得查询的类决策。 Transformer 架构中的查询来自使用查询图像获得的网格特征。 类似地，来自支持图像的网格特征用于构建键和值，这些键和值又用于派生参与输出。 除了基于对比的自我监督的训练机制之外，这种方法还可以在具有挑战性的元数据集上获得最佳性能。</p> 
<ol><li> <h4>FEAT：少镜头嵌入适应</h4> </li></ol> 
<p>叶等人[23] 建议在推理过程中使用变换器模块将在基类上学习的少样本嵌入适应少样本目标类。 这导致特定于任务的嵌入在诸如少样本分类之类的判别任务上表现更好。 虽然还评估了许多其他 set-to-set 函数，例如图卷积网络 [141]、双向 LSTMs [26] 和 DeepSets [142]，但基于变换器的映射实现了最佳性能。 这归因于Transformer更好的上下文化、任务内插和外推能力及其排列不变性，同时保持相对较低的参数复杂度。</p> 
<p>本工作中使用的Transformer架构遵循标准方法 [1]。 嵌入使用对比损失函数进行调整，以保留判别属性。</p> 
<p>由此产生的模型在归纳、转导和广义 FSL 任务上实现了强大的性能。</p> 
<ol><li> <h3>基于Transformer的聚类</h3> </li></ol> 
<p>聚类是无监督学习中的一项基本操作，旨在通过将相似的数据点分组在一起来发现数据中的结构。 它有许多应用，例如数据可视化和解释、异常检测和开放集分类。 已经为集合预测问题开发了神经网络 [142]、[143]，但是，设置点是单独处理的，这可能会丢失有关点间关系的信息。</p> 
<p>最近的工作使用在设置输入称为设置Transformer（ST）[144]，用于摊销聚类。 摊销聚类是一个具有挑战性的问题，它试图学习一个参数函数，该函数可以将一组输入点映射到它们相应的聚类中心。</p> 
<p>李等人[144] 建议使用包含多头自注意力块 [1] 的Transformer架构来学习这样的映射函数。Transformer模型在设计上是置换不变的，并允许对输入点之间的成对和高阶关系进行编码。 然而，一个完整的Transformer会导致每个自注意力层的 O(n2) 的计算成本很高，其中 n 是集合中的点数。 ST 通过使用使用低秩投影 (H 2 Rm) 的 Induced Self-Attention Block 将这个成本降低到 O(mn)，以允许在大型集合上进行操作。 该模型经过训练，可以学习使混合高斯 (MoG) 的可能性最大化的最佳参数。 因此，给定一组数据点，由 ST 估计 MoG 参数。</p> 
<p>除了摊销聚类之外，ST 还针对相关的集转换任务进行了评估，包括计算输入集中的唯一元素、集异常检测和点云分类。 最近，[145] 改进了 [144]，通过采用顺序方法来生成集群，从而允许分配给可变数量的集群。</p> 
<p><img alt="" src="https://images2.imgbox.com/44/95/gfA5niE3_o.png" width="799"></p> 
<ol><li> <h3>基于Transformer的3D分析</h3> </li></ol> 
<p>鉴于 3D 点云表示的不规则（可变点数）和置换不变性，transformer 提供了一种很好的机制来编码各个数据点之间的丰富关系 [68]、[69]、[146]。</p> 
<ol><li> <h4>点Transformer</h4> </li></ol> 
<p>赵等人[68] 研究用于 3D 点云处理的基于自注意力的变换器架构。 作为集合运算符的自注意力非常适合处理点云，这是一种 3D 数据表示，要求点数及其排列不变。 作者研究了 3D 领域中的三个问题，即对象分类、语义分割和对象部分分割。主要贡献是在 3D 点的局部邻域中应用自注意力的点变换器层。</p> 
<p>提出的点变换器层建立在矢量化自注意力网络 (SAN) [72] 上，其中注意力权重用向量表示。 此外，将位置编码添加到注意力向量和 17 个变换特征（值向量）以表示位置信息。 点变换器层夹在两个线性层之间，以创建在开发的网络架构中多次堆叠的点变换器块。 他们的设计还包括向下/向上过渡块，以减少/增加输入中的点数（以典型的编码-解码管道风格）。 由此产生的架构在 3D 分类和分割任务上提供了最先进的性能。</p> 
<p><img alt="" src="https://images2.imgbox.com/f1/1f/o1Ev5Ho5_o.png" width="794"></p> 
<ol><li> <h4>点云Transformer</h4> </li></ol> 
<p>点云变换器（PCT）[146] 也利用了变换器的排列不变性，是与 [68] 的并行工作。 然而，与[68]相比，它更直接地基于传统的Transformer架构[1]。 关键修改包括基于 3D 坐标的位置编码、偏移注意模块和对点云中的局部 3D 结构进行编码的邻居嵌入。</p> 
<p>具体来说，偏移注意力层使用逐元素减法计算自参与特征和输入特征之间的差异。 局部邻居嵌入只是在一组点而不是单个 3D 点之间找到自注意力关系。 实验报告了 ModelNet40 [147] 和 ShapeNet [148] 数据集上的 3D 形状分类、正常估计和分割任务。</p> 
<ol><li> <h4>姿势和网络重建</h4> </li></ol> 
<p>Mesh Transformer (METRO) [69] 模型针对单个 2D 图像的 3D 人体姿势和网格重建。这里的一个关键挑战是忠实地学习身体关节和网格顶点（例如，手和脚）之间的非局部相互作用。 Transformer 网络的表现力用于对网格中的顶点到顶点关系以及顶点到身体关节的关系进行联合建模。 自注意力机制可以关注网格中顶点的任意组合，从而对非局部关系进行编码。</p> 
<p>多层Transformer架构依次执行降维以将 2D 图像映射到 3D 网格。 使用每个顶点和每个身体关节的 3D 坐标 (x,y,z) 执行位置编码。 与 NLP 中的屏蔽语言建模类似，METRO 使用屏蔽顶点建模 (MVM)，它随机屏蔽一定比例的输入查询（见图 19）。 Transformer的任务是回归所有关节和顶点，这有助于编码它们之间的相互依赖关系。 METRO 在两个公开可用的数据集（Human3.6M [149] 和 3DPW [150]）上获得了人体网格重建的最新结果。 由于该方法不依赖于参数化网格模型，因此它可以很好地推广到其他重建任务，例如 3D 手部重建 [151]。总体而言，这是首次将 Transformer 用于 3D 人体重建任务，并取得了相当不错的结果。</p> 
<ol><li> <h2>未解决的问题和未来方向</h2> 
  <ol><li> <h3>高计算成本</h3> </li></ol></li></ol> 
<p>正如在 Sec 中讨论的那样。 1、Transformer模型参数复杂度高。 这导致在处理所需的计算时间和资源方面的高训练和推理成本。 例如，BERT [3] 基本模型（具有 1.09 亿个参数）的训练时间约为 1.89 peta-flop 天1，而最新的 GPT3 [6] 模型（1750 亿个参数）大约需要 3640 peta-flop 天的训练时间（ 惊人的 1925 倍增长）。 这伴随着巨大的代价，例如，根据一项估计 [157]，GPT3 训练可能花费了 OpenAI 大约 460 万美元。 此外，这些大规模模型需要积极的压缩技术（例如蒸馏），以使其推断适用于现实世界的设置。</p> 
<p>在语言领域，最近的工作重点是降低 Transformer 模型的高度复杂性（主要源于自注意力机制 [1]，其中通过考虑来自前一层的所有标记来更新标记的表示）。 例如，[158]、[159] 探索对前一层令牌的选择性或稀疏注意力，这些令牌更新每个下一层令牌。 Linformer [30] 将标准自注意力操作的复杂性从 O(n2) 降低到 O(n)（时间和内存要求）。 主要思想是表明低秩矩阵足以对自注意力机制进行建模。 改革者模型 [160] 采用局部敏感哈希（LSH）来最小化自注意力的复杂性，从 O(n2) 到 O(nlog(n))。</p> 
<p>维亚斯等人[161] 开发了一种有效的集群注意力方法来处理接近原始自我注意力的大输入序列。 他们提出了一种集群注意力方法，将查询分组到集群中，然后计算集群中心之间的注意力（而不是导致二次复杂度的所有查询之间的注意力）。 主要思想是在欧几里得空间中接近的查询应该具有相似的注意力分布。</p> 
<p>对于固定数量的集群，这种直觉有​​助于将二次复杂度降低到 O(nc) 相对于输入序列长度 n（其中 c 是集群数量）的线性复杂度。 我们将读者推荐给 [28] 以获得关于 NLP 中高效Transformer的良好文献调查</p> 
<p>与 NLP 领域类似，计算机视觉模型也受到 Transformer 模型高计算成本的影响。 例如，基于基于序列的Transformer（例如 iGPT）的图像生成器具有很高的计算成本，限制了它们对高分辨率输入的适用性。将来，探索如何将此类模型扩展到高维情况是很有趣的，例如，使用具有局部上下文建模的多尺度Transformer设计。</p> 
<p><img alt="" src="https://images2.imgbox.com/33/00/xgrzYXt7_o.png" width="767"><img alt="" src="https://images2.imgbox.com/02/40/mB88VM95_o.png" width="1200"></p> 
<ol><li> <h3>高数据成本</h3> </li></ol> 
<p>由于 Transformer 架构本身并不编码归纳偏差（先验知识）来处理视觉数据，因此它们通常需要在预训练期间使用大量训练数据来找出潜在的模态特定规则。 例如，由于池化操作或多尺度处理块，CNN 具有内置的平移不变性、权重共享和部分尺度不变性。</p> 
<p>但是，transformer 网络需要通过查看大量示例自行找出这些特定于图像的属性。 类似地，自注意力机制需要通过查看大型视频序列数据库来自动发现视频帧之间的关系。 这会导致更长的训练时间、显着增加的计算需求以及需要处理的大型数据集。 例如，ViT [9] 模型需要数亿个图像示例才能在 ImageNet 基准数据集上获得不错的性能。 以数据高效的方式学习Transformer的问题是一个开放的研究问题，最近的工作报告了令人鼓舞的解决方案（例如，DeiT [10] 使用蒸馏方法来实现数据效率）。</p> 
<ol><li> <h3>需要新颖的设计</h3> </li></ol> 
<p>我们注意到，大多数专注于视觉任务的现有工作倾向于直接将 Transformer 模型应用于计算机视觉问题。 这些包括专为图像识别 [9]、视频理解 [15] 和特别是多模态处理 [121] 设计的架构。 尽管这些简单应用的初步结果非常令人鼓舞，并激励我们进一步研究自注意力和自监督学习的优势，但当前的架构可能仍然更适合语言问题（具有序列结构），并且需要进一步的直觉来使 它们更有效地进行视觉输入。 例如，[72] 中的向量注意力在这个方向上是一项很好的工作，它试图专门为视觉输入定制自注意力操作。 有人可能会争辩说，像 Transformer 模型这样的架构应该保持通用以直接适用于跨领域，我们注意到，在自监督任务上训练此类模型的高计算和时间成本需要新颖的设计策略，以使其在视觉问题上的训练更加实惠。</p> 
<ol><li> <h3>Transformer的可解释性</h3> </li></ol> 
<p>鉴于 Transformer 架构的强大性能，解释他们的决策很有趣且至关重要，例如，通过可视化图像中给定分类决策的相关区域。 主要的挑战是，源自每一层的注意力以复杂的方式混合在后续层中，使得很难可视化输入标记对最终预测的相对贡献。 这是一个悬而未决的问题，然而，最近的一些工作 [162]、[163]、[164] 的目标是增强Transformer的可解释性，并报告了令人鼓舞的结果。 在[163]中提出了注意力推出和注意力流方法来估计准确的注意力。</p> 
<p>然而，这种方法以一种特别的方式起作用，并做出了简单的假设，例如，输入标记使用跨层的注意力权重线性组合。 谢弗等人[164] 注意到通过自我注意过程（标记之间的编码关系）或 [163] 中的重新分配直接获得的注意力分数并没有提供最佳解决方案。 作为替代方案，他们建议在Transformer网络中分配和传播相关性分数，以便相关性总和在整个网络中保持不变。 他们的设计可以处理自我注意层中经历的正面和负面归因。 提议的框架具有能够提供特定于类的可视化的额外优势。 在这个方向上的进一步进展有助于更好地理解Transformer模型，诊断决策过程中的任何错误行为和偏见。 它还可以帮助我们设计新颖的架构，帮助我们避免任何偏见。</p> 
<ol><li> <h3>硬件高效设计</h3> </li></ol> 
<p>大规模 Transformer 网络可能具有密集的功率和计算要求，阻碍了它们在边缘设备和资源受限环境（例如物联网 (IoT) 平台）上的部署。 据报道，最近的一些努力是在 FPGA 等嵌入式系统上压缩和加速 NLP 模型 [165]。 李等人[165] 使用增强的基于块循环矩阵的表示来压缩 NLP 模型，并提出了一种新的现场可编程门阵列 (FPGA) 架构设计，以有效管理资源以实现高吞吐量和低延迟。 对于 RoBERTa 模型 [7]，相对于 CPU，它们可以实现 27 倍、3 倍和 81 倍的性能提升（吞吐量以 FPS 衡量）、降低功耗和能源效率。 为了这个目标，[166] 提议使用神经架构搜索策略来设计硬件感知Transformer（HAT）[167]、[168]、[169]。 具体来说，首先对 SuperTransformer 模型进行性能近似训练，它可以在不完全训练的情况下估计模型的性能。 该模型包含搜索空间中最大可能的模型，同时在公共部分之间共享权重。 最终，考虑硬件延迟约束执行进化搜索，以找到适合目标硬件平台（例如 IoT 设备、GPU、CPU）的 SubTransformer 模型。 然而，视觉Transformer目前缺乏这样的硬件高效设计，无法在资源受限的设备中实现无缝部署。 此外，随着二氧化碳排放对环境的相关影响，进化算法的搜索成本仍然很高。</p> 
<ol><li> <h3>自我监督是答案吗？</h3> </li></ol> 
<p>在训练数据具有密集标签的情况下，需要考虑的一个有趣问题是，在小数据集上利用丰富标签的预训练过程是否会加速其学习。 这个问题已在 Virtex [170] 中进行了探讨，该模型旨在使用密集的文本注释（例如，图像标题）来学习强大的视觉表示。 由于字幕对图像中存在的对象、它们的关系、动作和属性的信息进行编码，因此它们可以提供更好的监督以学习更可概括和可转移的表示。 特别是，他们表明，用视觉主干训练的模型，然后是双向语言模型（前向和后向Transformer）[3] 来预测字幕，可以以无监督的方式在 MS-COCO 数据集上学习强大的特征。 当这些特征转移到 ImageNet 模型时，与直接在 ImageNet 数据集上学习的无监督/监督特征相比，它们表现更好或同样好。 将来，探索如何将 Transformer 模型与密集注释数据集的自监督训练一起使用，以便在推理时在新的看不见的条件下很好地转移将很有趣。</p> 
<ol><li> <h2>结论</h2> </li></ol> 
<p>注意力在提供高效准确的计算机视觉系统方面发挥了关键作用，同时提供了对深度神经网络功能的洞察。 本次调查回顾了自注意力方法，特别关注建立在自注意力原则上的Transformer和双向编码架构。 我们首先介绍与自注意力架构有关的基本概念，然后对广泛的计算机视觉应用程序的竞争方法进行深入分析。 具体来说，我们包括用于图像识别、对象检测、语义和实例分割、视频分析和分类、视觉问答、视觉常识推理、图像字幕、视觉语言导航、聚类、小样本学习、 和 3D 数据分析。 我们系统地突出了现有方法的关键优势和局限性，并特别阐述了重要的未来研究方向。 本次调查特别关注计算机视觉任务，对自我注意和基于Transformer的方法的最新进展提供了独特的视角。 我们希望这项努力将进一步激发视觉社区的兴趣，以利用Transformer模型的潜力并改善其当前的局限性，例如减少碳足迹。</p> 
<h2>参考文献</h2> 
<p><img alt="" height="733" src="https://images2.imgbox.com/fb/a5/eiekgccP_o.png" width="526"></p> 
<p></p> 
<p><img alt="" src="https://images2.imgbox.com/88/0a/rGkrlKFH_o.png" width="732"></p> 
<p><img alt="" src="https://images2.imgbox.com/a0/6e/0MpkgU9G_o.png" width="734"></p> 
<p><img alt="" src="https://images2.imgbox.com/01/95/oSUPlB4j_o.png" width="731"></p> 
<p><img alt="" src="https://images2.imgbox.com/2c/73/1paYINZ5_o.png" width="728"></p> 
<p></p> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/42fc509dda2b80e31842dfcff06944c2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">docker启动redis饮用外部配置文件失败</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/549599ecaa901a08822f0cdbdbbe12bb/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">F6-预编译&amp;编译&amp;安装</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>