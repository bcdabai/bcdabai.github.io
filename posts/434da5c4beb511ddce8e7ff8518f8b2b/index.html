<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>pytorch实现文本分类_使用变形金刚进行文本分类（Pytorch实现） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="pytorch实现文本分类_使用变形金刚进行文本分类（Pytorch实现）" />
<meta property="og:description" content="pytorch实现文本分类
‘Attention Is All You Need’
“注意力就是你所需要的” New deep learning models are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties .
新的深度学习模型被引入的速度越来越快，有时很难跟踪所有新颖性。 in this Article we will talk about Transformers with attached notebook(text classification example) are a type of neural network architecture that have been gaining popularity .
在本文中，我们将讨论带有笔记本的变压器 (文本分类示例)是一种已经越来越流行的神经网络体系结构。 In this post, we will address the following questions related to Transformers :" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/434da5c4beb511ddce8e7ff8518f8b2b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-05T08:27:04+08:00" />
<meta property="article:modified_time" content="2020-09-05T08:27:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch实现文本分类_使用变形金刚进行文本分类（Pytorch实现）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <article style="font-size: 16px;"> 
 <p>pytorch实现文本分类</p> 
 <div> 
  <section> 
   <div> 
    <div> 
     <p>‘Attention Is All You Need’</p> 
     <p> “注意力就是你所需要的” </p> 
     <p>New deep learning models are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties .</p> 
     <p> 新的深度学习模型被引入的速度越来越快，有时很难跟踪所有新颖性。 </p> 
     <p>in this Article we will talk about <strong>Transformers</strong> with attached notebook(text classification example) are a type of neural network architecture that have been gaining popularity .</p> 
     <p> 在本文中，我们将讨论带有笔记本的<strong>变压器</strong> (文本分类示例)是一种已经越来越流行的神经网络体系结构。 </p> 
     <p>In this post, we will address the following questions related to Transformers :</p> 
     <p> 在这篇文章中，我们将解决与变形金刚有关的以下问题： </p> 
     <h2> 目录 ： <span style="font-weight: bold;">(</span>Table Of Contents :<span style="font-weight: bold;">)</span></h2> 
     <ul><li><p><strong>Why do we need the Transformer ?</strong></p><p> <strong>为什么我们需要变压器？</strong> </p></li><li><p><strong>Transformer and its architecture in detail .</strong></p><p> <strong>变压器及其架构详细。</strong> </p></li><li><p><strong>Text Classification with Transformer .</strong></p><p> <strong>用变形金刚进行文本分类。</strong> </p></li><li><p><strong>useful papers to well dealing with Transformer.</strong></p><p> <strong>有用的文章，以很好地处理Transformer。</strong> </p></li></ul> 
     <h2> 我-为什么需要变压器？ <span style="font-weight: bold;">(</span>I -Why do we need the transformer ?<span style="font-weight: bold;">)</span></h2> 
     <p>Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..</p> 
     <p> 开发了变压器来解决序列转导或神经机器翻译的问题。 这意味着任何将输入序列转换为输出序列的任务。 这包括语音识别，文本到语音转换等。 </p> 
     <p>For models to perform sequence transduction, it is necessary to have some sort of memory.</p> 
     <p> 为了使模型执行序列转导，必须具有某种内存。 </p> 
     <p><strong>The limitations of Long-term dependencies :</strong></p> 
     <p> <strong>长期依赖的局限性：</strong> </p> 
     <p>Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).</p> 
     <p> Transformer是一种通过两个部分(编码器和解码器)将一个序列转换为另一个序列的体系结构，但它与先前描述/现有的序列到序列模型不同，因为它并不暗示任何递归网络(GRU，LSTM)等)。 </p> 
     <p>In the paper “Attention is All You Need” , the transformer architecture is well introduced , and like the title indicates transformer architecture uses the attention mechanism (we will make a detailed article about it later)</p> 
     <p> 在“注意就是你所需要的一切”一文中，很好地介绍了变压器体系结构，并且正如标题所示，变压器体系结构使用了注意力机制(我们将在后面进行详细的介绍) </p> 
     <p>let’s consider a language model that will predict the next word based on the previous ones !</p> 
     <p> 让我们考虑一种语言模型，该模型将根据前一个单词预测下一个单词！ </p> 
     <p>sentence : <strong>“bitcoin the best cryptocurrency”</strong></p> 
     <p> 句子： <strong>“比特币是最好的加密货币”</strong> </p> 
     <p>here we don’t need an additional context , so obvious that the next word will be “cryptocurrency” .</p> 
     <p> 这里我们不需要额外的上下文，很明显，下一个单词将是“ cryptocurrency”。 </p> 
     <p>In this case RNN’s can sove the issue and predict the answer using the past information .</p> 
     <p> 在这种情况下，RNN可以解决问题并使用过去的信息预测答案。 </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div> 
          <div style="text-align: center;"> 
           <img alt="Image for post" src="https://images2.imgbox.com/d6/97/ccHTOFhd_o.png" width="2706" height="711" style="outline: none;"> 
          </div> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>But in other cases we need more context . For example, let’s say that you are trying to predict the last word of the text: <strong>I grew up in Tunisia … I speak fluent ..</strong>. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of Tunisia, that is further back in the text.</p> 
     <p> 但是在其他情况下，我们需要更多的上下文。 例如，假设您要预测文本的最后一句话： <strong>我在突尼斯长大……我的语言说得很流利.....</strong> 最近的信息表明，下一个词可能是一种语言，但是如果我们想缩小哪种语言的范围，我们需要突尼斯的语境，这在文本中会更进一步。 </p> 
     <p>RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.</p> 
     <p> 当相关信息和需要的信息之间的差距变得很大时，RNN变得非常无效。 这是由于以下事实：信息在每个步骤中传递，并且链条越长，信息沿着链条丢失的可能性就越大。 </p> 
     <p>i recommend a nice <a href="https://towardsdatascience.com/transformers-141e32e69591" rel="noopener noopener noreferrer" target="_blank">article</a> that talk in depth about the difference between seq2seq and transformer .</p> 
     <p> 我推荐一篇不错的<a href="https://towardsdatascience.com/transformers-141e32e69591" rel="noopener noopener noreferrer" target="_blank">文章</a> ，深入探讨seq2seq和Transformer之间的区别。 </p> 
     <h2> II-变压器及其架构的详细信息： <span style="font-weight: bold;">(</span>II -Transformer and its architecture in detail :<span style="font-weight: bold;">)</span></h2> 
     <p>An image is worth thousand words, so we will start with that!</p> 
     <p> 一张图像值一千个单词，因此我们将从此开始！ </p> 
     <figure style="display:block;text-align:center;"> 
      <div> 
       <div> 
        <div> 
         <div style="text-align: center;"> 
          <img alt="Image for post" src="https://images2.imgbox.com/ec/f4/lPhFkX6S_o.png" width="700" height="851" style="outline: none;"> 
         </div> 
        </div> 
       </div> 
      </div> 
     </figure> 
     <p>The first thing that we can see is that it has a sequence-to-sequence encoder-decoder architecture.</p> 
     <p> 我们可以看到的第一件事是它具有序列到序列的编码器-解码器体系结构。 </p> 
     <p>Much of the literature on Transformers present on the Internet use this very architecture to explain Transformers.</p> 
     <p> 互联网上有关变形金刚的许多文献都使用这种架构来解释变形金刚。 </p> 
     <p>But this is not the one used in Open AI’s GPT model (or the GPT-2 model, which was just a larger version of its predecessor).</p> 
     <p> 但这不是Open AI的GPT模型(或GPT-2模型，只是其前身的较大版本)中使用的模型。 </p> 
     <p>The GPT is a 12-layer decoder only transformer with 117M parameters.</p> 
     <p> GPT是仅12层解码器的变压器，具有117M个参数。 </p> 
     <p>The Transformer has a stack of 6 Encoder and 6 Decoder, unlike Seq2Seq;</p> 
     <p> 与Seq2Seq不同，该Transformer具有6个编码器和6个解码器的堆栈； </p> 
     <p>the Encoder contains two sub-layers: multi-head self-attention layer and a fully connected feed-forward network.</p> 
     <p> 编码器包含两个子层：多头自我注意层和完全连接的前馈网络。 </p> 
     <p>The Decoder contains three sub-layers, a multi-head self-attention layer, an additional layer that performs multi-head self-attention over encoder outputs, and a fully connected feed-forward network.</p> 
     <p> 解码器包含三个子层，一个多头自我注意层，一个在编码器输出上执行多头自我注意的附加层以及一个完全连接的前馈网络。 </p> 
     <p>Each sub-layer in Encoder and Decoder has a Residual connection followed by a layer normalization.</p> 
     <p> 编码器和解码器中的每个子层都有一个残差连接，然后进行层归一化。 </p> 
     <p>All input and output tokens to Encoder/Decoder are converted to vectors using learned embeddings.</p> 
     <p> 使用学习到的嵌入，将编码器/解码器的所有输入和输出令牌转换为向量。 </p> 
     <p>These input embeddings are then passed to Positional Encoding.</p> 
     <p> 然后，将这些输入嵌入传递到位置编码。 </p> 
     <p>The Transformers architecture does not contain any recurrence or convolution and hence has no notion of word order.</p> 
     <p> 变形金刚架构不包含任何重复或卷积，因此没有词序的概念。 </p> 
     <p>All the words of the input sequence are fed to the network</p> 
     <p> 输入序列的所有单词都被馈送到网络 </p> 
     <p>with no special order or position as they all flow simultaneously through the Encoder and decoder stack.</p> 
     <p> 没有特殊的顺序或位置，因为它们都同时流经编码器和解码器堆栈。 </p> 
     <p>To understand the meaning of a sentence,</p> 
     <p> 要了解句子的含义， </p> 
     <p>it is essential to understand the position and the order of words.</p> 
     <p> 了解单词的位置和顺序至关重要。 </p> 
     <h2> III —使用Transformer(Pytorch实现)进行文本分类： <span style="font-weight: bold;">(</span>III — Text Classification using Transformer(Pytorch implementation) :<span style="font-weight: bold;">)</span></h2> 
     <p>It is too simple to use the ClassificationModel from simpletransformes :ClassificationModel(‘Architecture’, ‘model shortcut name’, use_cuda=True,num_labels=4)Architecture : Bert , Roberta , Xlnet , Xlm…shortcut name models for Roberta : roberta-base , roberta-large ….more details <a href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener nofollow noopener noreferrer" target="_blank">here</a></p> 
     <p> 使用来自simpletransformes的分类模型太简单了：ClassificationModel('Architecture'，'模型快捷方式名称'，use_cuda = True，num_labels = 4)体系结构：Bert，Roberta，Xlnet，Xlm…Roberta的快捷名称模型：roberta-base ，roberta-large…。更多详细信息<a href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener nofollow noopener noreferrer" target="_blank">在这里</a> </p> 
     <p>we create a model that classify text for 4 classes <strong>[‘art’, ‘politics’, ‘health’, ‘tourism’]</strong></p> 
     <p> 我们创建了一个模型，将文本分为4类<strong>['艺术'，'政治'，'健康'，'旅游']</strong> </p> 
     <p>we apply this model in our previous project</p> 
     <p> 我们在之前的项目中应用了此模型 </p> 
     <figure style="display:block;text-align:center;"></figure> 
     <p>and we integrate it in our flask application <a href="https://www.piecex.com/source-code/NLP-Tasks-with-Bert-Model-sentiment-extraction-text-summarisation-topic-classification-Python3-1825" rel="noopener nofollow noopener noreferrer" target="_blank">here</a> .<strong> (you can buy it for helping us to create better content and help community)</strong></p> 
     <p> 并将其集成到<a href="https://www.piecex.com/source-code/NLP-Tasks-with-Bert-Model-sentiment-extraction-text-summarisation-topic-classification-Python3-1825" rel="noopener nofollow noopener noreferrer" target="_blank">此处</a>的烧瓶应用程序中。 <strong>(您可以购买它来帮助我们创建更好的内容并帮助社区)</strong> </p> 
     <p><a href="https://github.com/NeuroData-ltd/Transformers_Tuto" target="_blank" rel="noopener nofollow noopener noreferrer">here</a> you will find a commented notebook :</p> 
     <p> <a href="https://github.com/NeuroData-ltd/Transformers_Tuto" target="_blank" rel="noopener nofollow noopener noreferrer">在这里，</a>您会找到一个有注释的笔记本： </p> 
     <ul><li>setup environment &amp; configuration<p class="nodelete"></p> 设置环境和配置 </li></ul> 
     <pre><code class="has">!pip install --upgrade transformers<br>!pip install simpletransformers<em># memory footprint support libraries/code</em><br>!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi<br>!pip install gputil<br>!pip install psutil<br>!pip install humanize importing libraries</code></pre> 
     <ul><li>Importing Libraries<p class="nodelete"></p> 导入库 </li></ul> 
     <pre><code class="has"><strong>import</strong> <strong>psutil</strong><strong>import</strong> <strong>humanize</strong><strong>import</strong> <strong>os</strong><strong>import</strong> <strong>GPUtil</strong> <strong>as</strong> <strong>GPU</strong><strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong><strong>import</strong> <strong>pandas</strong> <strong>as</strong> <strong>pd</strong><strong>from</strong> <strong>google.colab</strong> <strong>import</strong> files<strong>from</strong> <strong>tqdm</strong> <strong>import</strong> tqdm<strong>import</strong> <strong>warnings</strong><br>warnings.simplefilter('ignore')<strong>import</strong> <strong>gc</strong><strong>from</strong> <strong>scipy.special</strong> <strong>import</strong> softmax<strong>from</strong> <strong>simpletransformers.classification</strong> <strong>import</strong> ClassificationModel<strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> train_test_split, StratifiedKFold, KFold <strong>import</strong> <strong>sklearn</strong><strong>from</strong> <strong>sklearn.metrics</strong> <strong>import</strong> log_loss<strong>from</strong> <strong>sklearn.metrics</strong> <strong>import</strong> *<strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> *<strong>import</strong> <strong>re</strong><strong>import</strong> <strong>random</strong><strong>import</strong> <strong>torch</strong><br>pd.options.display.max_colwidth = 200<em>#choose the same seed to assure that our model will be roproducible</em><strong>def</strong> seed_all(seed_value):<br>    random.seed(seed_value) <em># Python</em><br>    np.random.seed(seed_value) <em># cpu vars</em><br>    torch.manual_seed(seed_value) <em># cpu  vars</em><br>    <br>    <strong>if</strong> torch.cuda.is_available(): <br>        torch.cuda.manual_seed(seed_value)<br>        torch.cuda.manual_seed_all(seed_value) <em># gpu vars</em><br>        torch.backends.cudnn.deterministic = <strong>True</strong>  <em>#needed</em><br>        torch.backends.cudnn.benchmark = <strong>False</strong><br>seed_all(2)</code></pre> 
     <ul><li>Reading Data<p class="nodelete"></p> 读取资料 </li></ul> 
     <pre><code class="has"><strong>import</strong> <strong>pandas</strong> <strong>as</strong> <strong>pd</strong><em>#We consider that our data is a csv file (2 columns : text and label)</em><em>#using pandas function (read_csv) to read the file</em><br>train=pd.read_csv()<br>feat_cols = "text"</code></pre> 
     <ul><li>Verify the topic classes in the data<p class="nodelete"></p> 验证数据中的主题类 </li></ul> 
     <pre><code class="has">train.label.unique()</code></pre> 
     <ul><li>train the model<p class="nodelete"></p> 训练模型 </li></ul> 
     <pre><code class="has">label_cols = ['art', 'politics', 'health', 'tourism']<br>train.head()<br>l=['art', 'politics', 'health', 'tourism']<em># Get the numerical ids of column label</em><br>train['label']=train.label.astype('category')<br>Y = train.label.cat.codes<br>train['label']=Y<em># Print initial shape</em><br>print(Y.shape)<strong>from</strong> <strong>keras.utils</strong> <strong>import</strong> to_categorical<em># One-hot encode the indexes</em><br>Y = to_categorical(Y)<em># Check the new shape of the variable</em><br>print(Y.shape)<em># Print the first 5 rows</em><br>print(Y[0:5])<strong>for</strong> i <strong>in</strong> range(len(l)) :     <br>     train[l[i]] = Y[:,i]<em>#using KFOLD Cross Validation is important to test our model</em>%%time<br>err=[]<br>y_pred_tot=[]<br>fold=StratifiedKFold(n_splits=5, shuffle=<strong>True</strong>, random_state=1997)<br>i=1<strong>for</strong> train_index, test_index <strong>in</strong> fold.split(train,train['label']):<br>    train1_trn, train1_val = train.iloc[train_index], train.iloc[test_index]<br>    model = ClassificationModel('roberta', 'roberta-base', use_cuda=<strong>True</strong>,num_labels=4, args={<!-- --><br>                                                                         'train_batch_size':16,<br>                                                                         'reprocess_input_data': <strong>True</strong>,<br>                                                                         'overwrite_output_dir': <strong>True</strong>,<br>                                                                         'fp16': <strong>False</strong>,<br>                                                                         'do_lower_case': <strong>False</strong>,<br>                                                                         'num_train_epochs': 4,<br>                                                                         'max_seq_length': 128,<br>                                                                         'regression': <strong>False</strong>,<br>                                                                         'manual_seed': 1997,<br>                                                                         "learning_rate":2e-5,<br>                                                                         'weight_decay':0,<br>                                                                         "save_eval_checkpoints": <strong>True</strong>,<br>                                                                         "save_model_every_epoch": <strong>False</strong>,<br>                                                                         "silent": <strong>True</strong>})<br>    model.train_model(train1_trn)<br>    raw_outputs_val = model.eval_model(train1_val)[1]<br>    raw_outputs_vals = softmax(raw_outputs_val,axis=1)<br>    print(f"Log_Loss: {log_loss(train1_val['label'], raw_outputs_vals)}")<br>    err.append(log_loss(train1_val['label'], raw_outputs_vals))</code></pre> 
     <p><strong>output :</strong></p> 
     <p> <strong>输出：</strong> </p> 
     <blockquote> 
      <p>Log_Loss: 0.35637871529928816</p> 
      <p> Log_Loss：0.35637871529928816 </p> 
      <p>CPU times: user 11min 2s, sys: 4min 21s,</p> 
      <p> CPU时间：用户11分钟2秒，系统时间：4分钟21秒， </p> 
      <p>total: 15min 23s Wall time: 16min 7s</p> 
      <p> 总计：15分23秒挂墙时间：16分7秒 </p> 
     </blockquote> 
     <p><strong>Log Loss :</strong></p> 
     <p> <strong>日志损失：</strong> </p> 
     <pre><code class="has">print("Mean LogLoss: ",np.mean(err))</code></pre> 
     <p><strong>output :</strong></p> 
     <p> <strong>输出：</strong> </p> 
     <blockquote> 
      <p>Mean LogLoss: 0.34930175561484067</p> 
      <p> 平均对数损失：0.34930175561484067 </p> 
     </blockquote> 
     <pre><code class="has">raw_outputs_vals</code></pre> 
     <p><strong>Output :</strong></p> 
     <p> <strong>输出：</strong> </p> 
     <blockquote> 
      <p>array([[9.9822301e-01, 3.4856689e-04, 3.8243082e-04, 1.0458552e-03], [9.9695909e-01, 1.1522240e-03, 5.9563853e-04, 1.2927916e-03], [9.9910539e-01, 2.3084633e-04, 2.5905663e-04, 4.0465154e-04], ..., [3.6545596e-04, 2.8826005e-04, 4.3145564e-04, 9.9891484e-01], [4.0789684e-03, 9.9224585e-01, 1.2752400e-03, 2.3997365e-03], [3.7382307e-04, 3.4797701e-04, 3.6257200e-04, 9.9891579e-01]], dtype=float32)</p> 
      <p> 数组([[9.9822301e-01，3.4856689e-04，3.8243082e-04，1.0458552e-03]，[9.9695909e-01，1.1522240e-03，5.9563853e-04，1.2927916e-03]，[9.9910539e -01，2.3084633e-04，2.5905663e-04，4.0465154e-04]，...，[3.6545596e-04，2.8826005e-04，4.3145564e-04，9.9891484e-01]，[4.0789684e-03 ，9.9224585e-01，1.2752400e-03，2.3997365e-03]，[3.7382307e-04、3.4797701e-04、3.6257200e-04、9.9891579e-01]]，dtype = float32) </p> 
     </blockquote> 
     <ul><li>test our Model<p class="nodelete"></p> 测试我们的模型 </li></ul> 
     <pre><code class="has">pred = model.predict(['i want to travel to thailand'])[1]<br>preds = softmax(pred,axis=1)<br>preds</code></pre> 
     <p><strong>output :</strong></p> 
     <p> <strong>输出：</strong> </p> 
     <blockquote> 
      <p>array([[6.0461409e-04, 3.6119239e-04, 3.3729596e-04, 9.9869716e-01]], dtype=float32)</p> 
      <p> 数组([[6.0461409e-04，3.6119239e-04，3.3729596e-04，9.9869716e-01]]，dtype = float32) </p> 
     </blockquote> 
     <blockquote> 
      <p><em>we create a function which calculate the maximum probability and detect the topic</em><em>for example if we have 0.6 politics 0.1 art 0.15 health 0.15 tourism &gt;&gt;&gt;&gt; topic = politics</em></p> 
      <p> <em>我们创建了一个计算最大概率并检测主题的函数，</em> <em>例如，如果我们有0.6政治0.1艺术0.15健康0.15旅游业&gt;&gt;&gt;&gt; topic =政治</em> </p> 
     </blockquote> 
     <pre><code class="has"><strong>def</strong> estm(raw_outputs_vals):<br> <strong>for</strong> i <strong>in</strong> range(len(raw_outputs_vals)):<br>   <strong>for</strong> j <strong>in</strong> range(4):<br>       <strong>if</strong>(max(raw_outputs_vals[i])==raw_outputs_vals[i][j]):<br>         raw_outputs_vals[i][j]=1<br>       <strong>else</strong> :<br>         raw_outputs_vals[i][j]=0<br> <strong>return</strong>(raw_outputs_vals)estm(preds)</code></pre> 
     <p><strong>output :</strong></p> 
     <p> <strong>输出：</strong> </p> 
     <blockquote> 
      <p>array([[0., 0., 0., 1.]], dtype=float32)</p> 
      <p> 数组([[0.，0.，0.，1.]]，dtype = float32) </p> 
     </blockquote> 
     <blockquote> 
      <p><em>our labels are :['art', 'politics', 'health', 'tourism']</em><em>so that's correct ;)</em></p> 
      <p> <em>我们的标签是：['艺术'，'政治'，'健康'，'旅游']，</em> <em>所以没错；)</em> </p> 
     </blockquote> 
     <p>i hope you find it useful &amp; helpful !</p> 
     <p> 我希望您觉得它有用和有用！ </p> 
     <p>Download source <a href="https://github.com/NeuroData-ltd/Transformers_Tuto/blob/master/simpletransformers-tuto.ipynb" target="_blank" rel="noopener nofollow noopener noreferrer">code</a> from our github.</p> 
     <p> 从我们的github下载源<a href="https://github.com/NeuroData-ltd/Transformers_Tuto/blob/master/simpletransformers-tuto.ipynb" target="_blank" rel="noopener nofollow noopener noreferrer">代码</a> 。 </p> 
     <h2> III-与变压器很好地打交道的有用论文： <span style="font-weight: bold;">(</span>III -useful papers to well dealing with Transformer:<span style="font-weight: bold;">)</span></h2> 
     <p>here a list of recommended papers to get in depth with transformers (mainly Bert Model) :</p> 
     <p> 这是深入了解变压器的推荐论文清单(主要是伯特模型)： </p> 
     <p>* Cross-Linguistic Syntactic Evaluation of Word Prediction Models* Emerging Cross-lingual Structure in Pretrained Language Models* Finding Universal Grammatical Relations in Multilingual BERT* On the Cross-lingual Transferability of Monolingual Representations* How multilingual is Multilingual BERT?* Is Multilingual BERT Fluent in Language Generation?* Are All Languages Created Equal in Multilingual BERT?* What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models* A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT* Cross-Lingual Ability of Multilingual BERT: An Empirical Study* Multilingual is not enough: BERT for Finnish</p> 
     <p> *单词预测模型的跨语言语法评估*预先训练的语言模型中新出现的跨语言结构*在多语言BERT中找到通用语法关系*关于单语言表示形式的跨语言可传递性*多语言BERT的多语言能力*多语言BERT会流利语言生成？*在多语言BERT中所有语言都创建相同吗？* BERT的层有何特别之处？ 仔细研究单语言和多语言模型中的NLP管道*多语言BERT中的跨语言能力和特定于语言的信息的研究*多语言BERT的跨语言能力：一项经验研究*多语言是不够的：芬兰语的BERT </p> 
     <p>Download all files from our github <a href="https://github.com/NeuroData-ltd/Transformers_Tuto" target="_blank" rel="noopener nofollow noopener noreferrer">repo</a></p> 
     <p> 从我们的github上下载的所有文件<a href="https://github.com/NeuroData-ltd/Transformers_Tuto" target="_blank" rel="noopener nofollow noopener noreferrer">回购</a> </p> 
     <h2> III-摘要： <span style="font-weight: bold;">(</span>III -Summary :<span style="font-weight: bold;">)</span></h2> 
     <p>Transformers present the next front in NLP.</p> 
     <p> 变形金刚代表了NLP的下一个前沿。 </p> 
     <p>In less than a couple of years since its introduction,</p> 
     <p> 自推出以来不到两年的时间， </p> 
     <p>this new architectural trend has surpassed the feats of RNN-based architectures.</p> 
     <p> 这种新的架构趋势已经超越了基于RNN的架构的壮举。 </p> 
     <p>This exciting pace of invention is perhaps the best part of being early to a new field like Deep Learning!</p> 
     <p> 如此激动人心的发明步伐也许是早期进入深度学习等新领域的最好部分！ </p> 
     <p>if you have any suggestions or a questions please contact NeuroData Team :</p> 
     <p> 如果您有任何建议或疑问，请联系NeuroData团队： </p> 
     <p><a href="https://www.facebook.com/NeuroData.tn" rel="noopener nofollow noopener noreferrer" target="_blank">Facebook</a></p> 
     <p> <a href="https://www.facebook.com/NeuroData.tn" rel="noopener nofollow noopener noreferrer" target="_blank">脸书</a> </p> 
     <p><a href="https://www.linkedin.com/company/30591451/" rel="noopener nofollow noopener noreferrer" target="_blank">Linkedin</a></p> 
     <p> <a href="https://www.linkedin.com/company/30591451/" rel="noopener nofollow noopener noreferrer" target="_blank">领英</a> </p> 
     <p><a href="https://www.neurodata.tn/" rel="noopener nofollow noopener noreferrer" target="_blank">Website</a></p> 
     <p> <a href="https://www.neurodata.tn/" rel="noopener nofollow noopener noreferrer" target="_blank">网站</a> </p> 
     <p><a href="https://github.com/NeuroData-ltd" target="_blank" rel="noopener nofollow noopener noreferrer">Github</a></p> 
     <p> <a href="https://github.com/NeuroData-ltd" target="_blank" rel="noopener nofollow noopener noreferrer">Github</a> </p> 
     <p>Authors :</p> 
     <p> 作者： </p> 
     <p><a href="https://www.linkedin.com/in/yassine-hamdaoui/" rel="noopener nofollow noopener noreferrer" target="_blank">Yassine</a> <a href="https://www.linkedin.com/in/yassine-hamdaoui/" rel="noopener nofollow noopener noreferrer" target="_blank">Hamdaoui</a></p> 
     <p> <a href="https://www.linkedin.com/in/yassine-hamdaoui/" rel="noopener nofollow noopener noreferrer" target="_blank">Yassine</a> <a href="https://www.linkedin.com/in/yassine-hamdaoui/" rel="noopener nofollow noopener noreferrer" target="_blank">Hamdaoui</a> </p> 
     <p>code credits goes to <a href="https://www.linkedin.com/in/med-helmi-klai-933068176/" rel="noopener nofollow noopener noreferrer" target="_blank">Med Klai Helmi</a> : Data Scientist and Zindi Mentor</p> 
     <p> 代码信用归于<a href="https://www.linkedin.com/in/med-helmi-klai-933068176/" rel="noopener nofollow noopener noreferrer" target="_blank">Med Klai</a> Helmi：数据科学家和Zindi Mentor </p> 
    </div> 
   </div> 
  </section> 
 </div> 
 <blockquote> 
  <p>翻译自: <a href="https://medium.com/swlh/text-classification-using-transformers-pytorch-implementation-5ff9f21bd106" rel="nofollow">https://medium.com/swlh/text-classification-using-transformers-pytorch-implementation-5ff9f21bd106</a></p> 
 </blockquote> 
 <p>pytorch实现文本分类</p> 
</article>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/7d97d9e613c7dad1fa2fa4ef473d3fec/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[OC RunLoop_翻译]五、配置运行循环源</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6281da3dd0bedf9f2c80a44192a49db6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java二分法查找</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>