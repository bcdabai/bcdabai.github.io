<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>qpython 教程_极简Qlearning教程（附Python源码） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="qpython 教程_极简Qlearning教程（附Python源码）" />
<meta property="og:description" content="极简Qlearning入门教程
在当前的机器学习中，主流方向为有监督学习、无监督学习以及强化学习，今天我想介绍的就是强化学习的一个小入门Qleaning算法。
回想我们小时候在妈妈的教育下进行学习，首先我们是什么都不会，但是在父母的教育下，慢慢地开始学习起来，比如看见猫，第一次见到不知道这是什么物种，于是妈妈告诉你这是猫，下次见到就知道了，这就是监督学习，当我们做出一个动作，立即得到反馈，而强化学习则不然，当我们做出动作的时候，却不会有立即的反馈，只能到结束才能知道效果，因此本人在学习的过程中，将网上的资料收集了一下，发现这篇文章是特别棒的http://mnemstudio.org/path-finding-q-learning-tutorial.htm
假设有这样的房间
这样看，我们可以将其进行建模：
这就是房间对应的图。我们首先将agent(机器人)处于任何一个位置，让他自己走动，直到走到5房间，表示成功。为了能够走出去，我们将每个节点之间设置一定的权重，能够直接到达5的边设置为100，其他不能的设置为0，这样网络的图为：
Qlearning中，最重要的就是“状态”和“动作”，状态表示处于图中的哪个节点，比如2节点，3节点等等，而动作则表示从一个节点到另一个节点的操作。
首先我们生成一个奖赏矩阵：
-1表示不可以通过
0表示可以通过
100表示直接到达终点
总结就是：R矩阵中非负的表示节点之间是可以相通的。
同时，我们创建一个Q表，表示学习到的经验，与R表同阶，初始化为0矩阵。
根据Q-learning转移方程：
S表示当前的状态
a表示当前的动作
s~表示下一个状态
a~表示下一个动作
λ为贪婪因子，0
下面就是Qlearning的学习步骤：
当Q表学习完以后，就可以根据Q表来选择路径。
看一个实际的例子
首先设定λ=0.8，R：
Q：
随机选择一个状态，比如1，查看状态1所对应的R表，也就是1可以到达3或5，随机地，我们选择5，根据转移方程：
于是，Q表为：
这样，到达目标，一次尝试结束。
接下来再选择一个随机状态，比如3，3对应的下一个状态有(1，2，4都是状态3对应的非负状态)，随机地，我们选择1，这样根据算法更新：
这样，Q表为
到达1状态以后，可以直接到达5，这样一次训练也完成了。
这样，我们将上面的解答转换为代码，采用Python编写。
import numpy as np
import random
# 建立 Q 表
q = np.zeros((6, 6))
q = np.matrix(q)
# 建立 R 表
r = np.array([[-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1]," />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/25470ac9a3836b8a9070668e232150c1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-21T22:47:20+08:00" />
<meta property="article:modified_time" content="2021-02-21T22:47:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">qpython 教程_极简Qlearning教程（附Python源码）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div style="font-size:16px;"> 
 <p>极简Qlearning入门教程</p> 
 <p>在当前的机器学习中，主流方向为有监督学习、无监督学习以及强化学习，今天我想介绍的就是强化学习的一个小入门Qleaning算法。</p> 
 <p>回想我们小时候在妈妈的教育下进行学习，首先我们是什么都不会，但是在父母的教育下，慢慢地开始学习起来，比如看见猫，第一次见到不知道这是什么物种，于是妈妈告诉你这是猫，下次见到就知道了，这就是监督学习，当我们做出一个动作，立即得到反馈，而强化学习则不然，当我们做出动作的时候，却不会有立即的反馈，只能到结束才能知道效果，因此本人在学习的过程中，将网上的资料收集了一下，发现这篇文章是特别棒的http://mnemstudio.org/path-finding-q-learning-tutorial.htm</p> 
 <p>假设有这样的房间</p> 
 <p align="center"></p> 
 <p>这样看，我们可以将其进行建模：</p> 
 <p align="center"></p> 
 <p>这就是房间对应的图。我们首先将agent(机器人)处于任何一个位置，让他自己走动，直到走到5房间，表示成功。为了能够走出去，我们将每个节点之间设置一定的权重，能够直接到达5的边设置为100，其他不能的设置为0，这样网络的图为：</p> 
 <p align="center"></p> 
 <p>Qlearning中，最重要的就是“状态”和“动作”，状态表示处于图中的哪个节点，比如2节点，3节点等等，而动作则表示从一个节点到另一个节点的操作。</p> 
 <p>首先我们生成一个奖赏矩阵：</p> 
 <p align="center">-1表示不可以通过</p> 
 <p>0表示可以通过</p> 
 <p>100表示直接到达终点</p> 
 <p>总结就是：R矩阵中非负的表示节点之间是可以相通的。</p> 
 <p>同时，我们创建一个Q表，表示学习到的经验，与R表同阶，初始化为0矩阵。</p> 
 <p>根据Q-learning转移方程：</p> 
 <p align="center">S表示当前的状态</p> 
 <p>a表示当前的动作</p> 
 <p>s~表示下一个状态</p> 
 <p>a~表示下一个动作</p> 
 <p>λ为贪婪因子，0</p> 
 <p>下面就是Qlearning的学习步骤：</p> 
 <p align="center"></p> 
 <p>当Q表学习完以后，就可以根据Q表来选择路径。</p> 
 <p>看一个实际的例子</p> 
 <p>首先设定λ=0.8，R：</p> 
 <p align="center"></p> 
 <p>Q：</p> 
 <p align="center"></p> 
 <p>随机选择一个状态，比如1，查看状态1所对应的R表，也就是1可以到达3或5，随机地，我们选择5，根据转移方程：</p> 
 <p align="center"></p> 
 <p>于是，Q表为：</p> 
 <p align="center"></p> 
 <p>这样，到达目标，一次尝试结束。</p> 
 <p align="center">接下来再选择一个随机状态，比如3，3对应的下一个状态有(1，2，4都是状态3对应的非负状态)，随机地，我们选择1，这样根据算法更新：</p> 
 <p>这样，Q表为</p> 
 <p align="center"></p> 
 <p>到达1状态以后，可以直接到达5，这样一次训练也完成了。</p> 
 <p>这样，我们将上面的解答转换为代码，采用Python编写。</p> 
 <p>import numpy as np</p> 
 <p>import random</p> 
 <p># 建立 Q 表</p> 
 <p>q = np.zeros((6, 6))</p> 
 <p>q = np.matrix(q)</p> 
 <p># 建立 R 表</p> 
 <p>r = np.array([[-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1],</p> 
 <p>[0, -1, -1, 0, -1, 100], [-1, 0, -1, -1, 0, 100]])</p> 
 <p>r = np.matrix(r)</p> 
 <p># 贪婪指数</p> 
 <p>gamma = 0.8</p> 
 <p>开始训练：</p> 
 <p>for i in range(1000):</p> 
 <p># 对每一个训练,随机选择一种状态</p> 
 <p>state = random.randint(0, 5)</p> 
 <p>while state != 5:</p> 
 <p># 选择r表中非负的值的动作</p> 
 <p>r_pos_action = []</p> 
 <p>for action in range(6):</p> 
 <p>if r[state, action] &gt;= 0:</p> 
 <p>r_pos_action.append(action)</p> 
 <p>next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]</p> 
 <p>q[state, next_state] = r[state, next_state] + gamma * q[next_state].max()</p> 
 <p>state = next_state</p> 
 <p align="center">经过训练后，Q表为</p> 
 <p>当我们的Q表训练好，就可以根据Q表来进行路径选择。</p> 
 <p>选择算法如下：</p> 
 <p align="center"></p> 
 <p>这样，对应的代码如下：</p> 
 <p>state = random.randint(0, 5)</p> 
 <p>print('机器人处于{}'.format(state))</p> 
 <p>count = 0</p> 
 <p>while state != 5:</p> 
 <p>if count &gt; 20: # 如果尝试次数大于20次，表示失败</p> 
 <p>print('fail')</p> 
 <p>break</p> 
 <p># 选择最大的q_max</p> 
 <p>q_max = q[state].max()</p> 
 <p>q_max_action = []</p> 
 <p>for action in range(6):</p> 
 <p>if q[state, action] == q_max: # 选择可行的下一个动作</p> 
 <p>q_max_action.append(action)</p> 
 <p># 随机选择一个可行的动作</p> 
 <p>next_state = q_max_action[random.randint(0, len(q_max_action) - 1)]</p> 
 <p>print("the robot goes to " + str(next_state) + '.')</p> 
 <p>state = next_state</p> 
 <p>这样，整个训练效果就是这样。，整体代码如下：</p> 
 <p>import numpy as np</p> 
 <p>import random</p> 
 <p># 建立 Q 表</p> 
 <p>q = np.zeros((6, 6))</p> 
 <p>q = np.matrix(q)</p> 
 <p># 建立 R 表</p> 
 <p>r = np.array([[-1, -1, -1, -1, 0, -1], [-1, -1, -1, 0, -1, 100], [-1, -1, -1, 0, -1, -1], [-1, 0, 0, -1, 0, -1],</p> 
 <p>[0, -1, -1, 0, -1, 100], [-1, 0, -1, -1, 0, 100]])</p> 
 <p>r = np.matrix(r)</p> 
 <p># 贪婪指数</p> 
 <p>gamma = 0.8</p> 
 <p># 训练</p> 
 <p>for i in range(1000):</p> 
 <p># 对每一个训练,随机选择一种状态</p> 
 <p>state = random.randint(0, 5)</p> 
 <p>while state != 5:</p> 
 <p># 选择r表中非负的值的动作</p> 
 <p>r_pos_action = []</p> 
 <p>for action in range(6):</p> 
 <p>if r[state, action] &gt;= 0:</p> 
 <p>r_pos_action.append(action)</p> 
 <p>next_state = r_pos_action[random.randint(0, len(r_pos_action) - 1)]</p> 
 <p>q[state, next_state] = r[state, next_state] + gamma * q[next_state].max()</p> 
 <p>state = next_state</p> 
 <p>print(q)</p> 
 <p># 验证</p> 
 <p>for i in range(10):</p> 
 <p>print("第{}次验证".format(i + 1))</p> 
 <p>state = random.randint(0, 5)</p> 
 <p>print('机器人处于{}'.format(state))</p> 
 <p>count = 0</p> 
 <p>while state != 5:</p> 
 <p>if count &gt; 20:</p> 
 <p>print('fail')</p> 
 <p>break</p> 
 <p># 选择最大的q_max</p> 
 <p>q_max = q[state].max()</p> 
 <p>q_max_action = []</p> 
 <p>for action in range(6):</p> 
 <p>if q[state, action] == q_max:</p> 
 <p>q_max_action.append(action)</p> 
 <p>next_state = q_max_action[random.randint(0, len(q_max_action) - 1)]</p> 
 <p>print("the robot goes to " + str(next_state) + '.')</p> 
 <p>state = next_state</p> 
 <p>count += 1</p> 
 <p>输出效果：</p> 
 <p align="center"></p> 
 <p>如果觉得这篇文章有用，请留言，谢谢。</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/ef4dc623fcae77cfa9f94049cce801c9/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">C语言扩展python</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/552f447afa929c7acf754efe2d70c53d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">resnet152训练_ResNet改进版来了！可训练网络超过3000层！相同深度精度更高</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>