<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>TVM代码学习 -- 代码生成流程（一） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="TVM代码学习 -- 代码生成流程（一）" />
<meta property="og:description" content="本文主要介绍TVM针对不同后端部署平台生成运行代码的流程，TVM可以通过两种方式生成代码：tvm.build 和 relay.build。两种方式流程略有不同，tvm.build 主要针对单一算子进行编译优化，relay.build 是针对整个网络计算图进行编译优化。先介绍relay.build，示例代码如下所示。
relay.build
onnx_model = onnx.load(&#39;model/mobilenetv2.onnx&#39;) mod,params = relay.frontend.from_onnx(onnx_model,shape_dict) dtype = &#39;float32&#39; with relay.build_config(opt_level=0): graph, lib, params = relay.build_module.build(mod, target, params=params) 对relay.build进行代码跟踪，首先进入tvm/python/tvm/relay/build_module.py（这里是因为在relay/init.py中将build函数直接import到relay的命名空间，因此跳过了build_module这一层），其中的build函数是build_module内的全局函数)。
def build(mod, target=None, target_host=None, params=None, mod_name=&#39;default&#39;): // ignore some code..... # If current dispatch context is fallback context (the default root context), # then load pre-tuned parameters from TopHub if isinstance(autotvm.DispatchContext.current, autotvm.FallbackContext): tophub_context = autotvm.tophub.context(list(target.values())) else: tophub_context = autotvm.util.EmptyContext() with tophub_context: bld_mod = BuildModule() graph_json, mod, params = bld_mod." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/78c3e7ad6e663f20e8cadb09e2ba06a6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-02-20T22:52:42+08:00" />
<meta property="article:modified_time" content="2021-02-20T22:52:42+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TVM代码学习 -- 代码生成流程（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文主要介绍TVM针对不同后端部署平台生成运行代码的流程，TVM可以通过两种方式生成代码：<a href="https://link.zhihu.com/?target=http://tvm.build/" rel="nofollow">tvm.build</a> 和 <a href="https://link.zhihu.com/?target=http://relay.build/" rel="nofollow">relay.build</a>。两种方式流程略有不同，tvm.build 主要针对单一算子进行编译优化，relay.build 是针对整个网络计算图进行编译优化。先介绍relay.build，示例代码如下所示。</p> 
<p><a href="https://link.zhihu.com/?target=http://relay.build/" rel="nofollow">relay.build</a></p> 
<pre><code>onnx_model = onnx.load('model/mobilenetv2.onnx')
mod,params = relay.frontend.from_onnx(onnx_model,shape_dict)

dtype = 'float32'
with relay.build_config(opt_level=0):
    graph, lib, params = relay.build_module.build(mod, target, params=params)
</code></pre> 
<p>对relay.build进行代码跟踪，首先进入tvm/python/tvm/relay/build_module.py（这里是因为在relay/<strong>init</strong>.py中将build函数直接import到relay的命名空间，因此跳过了build_module这一层），其中的build函数是build_module内的全局函数)。</p> 
<pre><code>def build(mod, target=None, target_host=None, params=None, mod_name='default'):
    // ignore some code.....

    # If current dispatch context is fallback context (the default root context),
    # then load pre-tuned parameters from TopHub
    if isinstance(autotvm.DispatchContext.current, autotvm.FallbackContext):
        tophub_context = autotvm.tophub.context(list(target.values()))
    else:
        tophub_context = autotvm.util.EmptyContext()

    with tophub_context:
        bld_mod = BuildModule()
        graph_json, mod, params = bld_mod.build(mod, target, target_host, params)
        mod = _graph_runtime_factory.GraphRuntimeFactoryModule(graph_json, mod, mod_name, params)
        return mod
</code></pre> 
<p>首先是寻找AutoTVM是否有预先tune好的参数记录，然后构造tophub_context，在该域内创建BuildModule实例并调用build方法。BuildModule类实现在build_module.py中，部分代码如下。</p> 
<pre><code>class BuildModule(object):
    """Build an IR module to run on TVM graph runtime. This class is used
    to expose the `RelayBuildModule` APIs implemented in C++.
    """
    def __init__(self):
        self.mod = _build_module._BuildModule()
        self._get_graph_json = self.mod["get_graph_json"]
        self._get_module = self.mod["get_module"]
        self._build = self.mod["build"]
        self._optimize = self.mod["optimize"]
        self._set_params_func = self.mod["set_params"]
        self._get_params_func = self.mod["get_params"]

    def build(self, mod, target=None, target_host=None, params=None):
        target = _update_target(target)

        # Setup the params.
        if params:
            self._set_params(params)
        # Build the IR module
        self._build(mod, target, target_host)
        # Get artifacts
        graph_json = self.get_json()
        mod = self.get_module()
        params = self.get_params()

        return graph_json, mod, params
</code></pre> 
<p>而_build_module._BuildModule()又通过FFI在python/tvm/relay/_build_module.py中与C++函数建立联系。</p> 
<pre><code>"""The interface for building Relay functions exposed from C++."""
import tvm._ffi

tvm._ffi._init_api("relay.build_module", __name__)
</code></pre> 
<p>对应的C++函数实现在tvm/src/relay/backend/http://build_module.cc中。</p> 
<pre><code>runtime::Module RelayBuildCreate() {
  auto exec = make_object&lt;RelayBuildModule&gt;();
  return runtime::Module(exec);
}

TVM_REGISTER_GLOBAL("relay.build_module._BuildModule").set_body([](TVMArgs args, TVMRetValue* rv) {
  *rv = RelayBuildCreate();
});
</code></pre> 
<p>也就是注册一个RelayBuildModule供调用，可以在RelayBuildModule类中看build函数实现，也在build_module.cc文件中。</p> 
<pre><code>class RelayBuildModule : public runtime::ModuleNode {
public:
PackedFunc GetFunction(const std::string&amp; name, const ObjectPtr&lt;Object&gt;&amp; sptr_to_self) final {
   // ignore some code ....
    } else if (name == "build") {
      return PackedFunc([sptr_to_self, this](TVMArgs args, TVMRetValue* rv) {
        CHECK_EQ(args.num_args, 3);
        this-&gt;Build(args[0], args[1], args[2]);
      });
   // ignore some code ....
 }
</code></pre> 
<p>TVM对build函数做了一次封装返回一个PackedFunc，即RelayBuildModule类成员函数Build：this-&gt;Build(…)。</p> 
<pre><code>/*!
 1. \brief Build relay IRModule for graph runtime
 2.  3. \param mod Relay IRModule
 4. \param target Target device
 5. \param target_host Host target device
   */
  void Build(IRModule mod, const TargetsMap&amp; targets, const tvm::Target&amp; target_host) {
    targets_ = targets;
    target_host_ = target_host;
    BuildRelay(mod, params_);
    // Clear compile engine so that tuning schedules can be changed between runs. See issue #6096.
    CompileEngine::Global()-&gt;Clear();
  }
</code></pre> 
<p>会进一步调用成员函数BuildRelay。</p> 
<pre><code>void BuildRelay(IRModule relay_module,
                  const std::unordered_map&lt;std::string, tvm::runtime::NDArray&gt;&amp; params) {
    // Relay IRModule -&gt; IRModule optimizations.
    relay_module = Optimize(relay_module, targets_, params);
    // Get the updated function.
    auto func = Downcast&lt;Function&gt;(relay_module-&gt;Lookup("main"));

    // Generate code for the updated function.
    graph_codegen_ = std::unique_ptr&lt;GraphCodegen&gt;(new GraphCodegen());
    graph_codegen_-&gt;Init(nullptr, targets_);
    graph_codegen_-&gt;Codegen(func);

    ret_.graph_json = graph_codegen_-&gt;GetJSON();
    ret_.params = graph_codegen_-&gt;GetParams();

    auto lowered_funcs = graph_codegen_-&gt;GetIRModule();

    // When there is no lowered_funcs due to reasons such as optimization.
    if (lowered_funcs.size() == 0) {
      // skip some code ......
      }
    } else {
      ret_.mod = tvm::build(lowered_funcs, target_host_);
    }

    Array&lt;tvm::runtime::Module&gt; ext_mods = graph_codegen_-&gt;GetExternalModules();
    // TODO(zhiics) We should be able to completely switch to MetadataModule no
    // matter whether there are external modules or not.
    if (!ext_mods.empty()) {
      ret_.mod = tvm::codegen::CreateMetadataModule(ret_.params, ret_.mod, ext_mods);
    }
  }
</code></pre> 
<p>经过多番跳转，终于到达build的核心模块，再来看TVM逐步做的工作：<br> 1.<strong>计算图优化：relay_module =Optimize(relay_module, targets_, params)</strong><br> 2.<strong>计算图生成</strong><br> 3.<strong>后端运行代码生成</strong></p> 
<p><strong>计算图优化</strong><br> 调用的是成员函数Optimize，对计算图做设备无关的优化。</p> 
<pre><code>IRModule Optimize(IRModule relay_module, const TargetsMap&amp; targets,
                    const std::unordered_map&lt;std::string, runtime::NDArray&gt;&amp; params) {
    // skip some code ....

    Array&lt;Pass&gt; pass_seqs;
    Array&lt;runtime::String&gt; entry_functions{"main"};
    pass_seqs.push_back(transform::RemoveUnusedFunctions(entry_functions));
    pass_seqs.push_back(transform::ToBasicBlockNormalForm());

    // Run all dialect legalization passes.
    pass_seqs.push_back(relay::qnn::transform::Legalize());

    // Legalize pass is restricted to homogeneous execution for now.
    if (targets.size() == 1) {
      pass_seqs.push_back(transform::Legalize());
    }

    pass_seqs.push_back(transform::SimplifyInference());
    // skip some code ....
    pass_seqs.push_back(transform::EliminateCommonSubexpr(fskip));
    pass_seqs.push_back(transform::SimplifyExpr());
    pass_seqs.push_back(transform::CombineParallelConv2D(3));
    pass_seqs.push_back(transform::CombineParallelDense(3));
    pass_seqs.push_back(transform::CombineParallelBatchMatmul(3));
    pass_seqs.push_back(transform::FoldConstant());
    pass_seqs.push_back(transform::FoldScaleAxis());
    pass_seqs.push_back(transform::CanonicalizeCast());
    pass_seqs.push_back(transform::CanonicalizeOps());

    // Alter layout transformation is only applied to homogeneous execution yet.
    if (targets.size() == 1) {
      pass_seqs.push_back(transform::AlterOpLayout());
    }

    // Fast math optimizations.
    pass_seqs.push_back(transform::FastMath());
    pass_seqs.push_back(transform::FoldConstant());

    // Create a sequential pass and perform optimizations.
    transform::Pass seq = transform::Sequential(pass_seqs);
    if (targets.size() == 1) {
      const auto&amp; it = targets.begin();
      With&lt;Target&gt; tctx((*it).second);
      relay_module = seq(relay_module);
    } else {
      relay_module = seq(relay_module);
    }

    // Handle heterogeneous compilation.
    transform::PassContext pass_ctx = PassContext::Current();
    if (targets_.size() &gt; 1) {
      Optional&lt;Integer&gt; opt_fallback_dev =
          pass_ctx-&gt;GetConfig("relay.fallback_device_type", Integer(static_cast&lt;int&gt;(kDLCPU)));
      auto fallback_dev = opt_fallback_dev.value();
      CHECK_GT(fallback_dev-&gt;value, 0U);
      relay_module = RunDeviceAnnotationPass(relay_module, fallback_dev-&gt;value);
    }

    // Fuse the operations if it is needed.
    relay_module = transform::FuseOps()(relay_module);
    relay_module = transform::InferType()(relay_module);
    // Inline the functions that have been lifted by the module scope.
    
    relay_module = transform::Inline()(relay_module);
    CHECK(relay_module.defined());

    return relay_module;
  }
</code></pre> 
<p>定义了Array pass_seqs，运行不同的Pass操作，对计算图进行编译优化。</p> 
<p><strong>计算图生成</strong><br> 基于GraphCodegen类实现，对应BuildRelay函数代码为</p> 
<pre><code>// Generate code for the updated function.
    graph_codegen_ = std::unique_ptr&lt;GraphCodegen&gt;(new GraphCodegen());
    graph_codegen_-&gt;Init(nullptr, targets_);
    graph_codegen_-&gt;Codegen(func);

    ret_.graph_json = graph_codegen_-&gt;GetJSON();
    ret_.params = graph_codegen_-&gt;GetParams();

    auto lowered_funcs = graph_codegen_-&gt;GetIRModule();
</code></pre> 
<p>GraphCodegen类具体实现在tvm/src/relay/backend/http://build_module.cc中。</p> 
<pre><code>struct GraphCodegen {
 public:
  GraphCodegen() {
    auto pf = GetPackedFunc("relay.build_module._GraphRuntimeCodegen");
    mod = (*pf)();
  }
  ~GraphCodegen() {}

  void Init(runtime::Module* m, TargetsMap targets) { CallFunc("init", m, targets); }

  void Codegen(const Function&amp; func) { CallFunc("codegen", func); }

  std::string GetJSON() { return CallFunc&lt;std::string&gt;("get_graph_json", nullptr); }

  Array&lt;tvm::runtime::Module&gt; GetExternalModules() {
    return CallFunc&lt;Array&lt;tvm::runtime::Module&gt;&gt;("get_external_modules", nullptr);
  }

  // skip some code ...
 protected:
  tvm::runtime::Module mod;
 // skip some code ...
};
</code></pre> 
<p>然后实际调用的是tvm/src/relay/backend/http://graph_runtime_codegen.cc中注册的relay.build_module._GraphRuntimeCodegen方法。</p> 
<pre><code>runtime::Module CreateGraphCodegenMod() {
  auto ptr = make_object&lt;GraphRuntimeCodegenModule&gt;();
  return runtime::Module(ptr);
}

TVM_REGISTER_GLOBAL("relay.build_module._GraphRuntimeCodegen")
    .set_body([](TVMArgs args, TVMRetValue* rv) { *rv = CreateGraphCodegenMod(); });
</code></pre> 
<p>在GraphRuntimeCodegenModule类中有codegen方法，该方法实际调用的是GraphRuntimeCodegen类的Codegen方法最终生成计算图LoweredOutput Functions。</p> 
<pre><code>class GraphRuntimeCodegen : public backend::MemoizedExprTranslator&lt;std::vector&lt;GraphNodeRef&gt;&gt; {
 public:
  GraphRuntimeCodegen(runtime::Module* mod, const TargetsMap&amp; targets) : mod_(mod) {
    compile_engine_ = CompileEngine::Global();
    targets_ = targets;
  }

  LoweredOutput Codegen(relay::Function func) {
    auto pf = GetPackedFunc("relay.backend.GraphPlanMemory");
    storage_device_map_ = (*pf)(func);
    // First we convert all the parameters into input nodes.
    for (auto param : func-&gt;params) {
      auto node_ptr = GraphInputNode::make_node_ptr(param-&gt;name_hint(), GraphAttrs());
      var_map_[param.get()] = AddNode(node_ptr, param);
    }
    heads_ = VisitExpr(func-&gt;body);
    std::ostringstream os;
    dmlc::JSONWriter writer(&amp;os);
    GetJSON(&amp;writer);
    LoweredOutput ret;
    ret.graph_json = os.str();
    ret.params = params_;

    for (auto&amp; kv : lowered_funcs_) {
      if (ret.lowered_funcs.count(kv.first) == 0) {
        ret.lowered_funcs.Set(kv.first, IRModule());
      }
      auto&amp; mod = ret.lowered_funcs[kv.first];
      mod-&gt;Update(kv.second);
      ret.lowered_funcs.Set(kv.first, mod);
    }
    ret.external_mods = compile_engine_-&gt;LowerExternalFunctions();
    return ret;
  }
  // skip some code....
}
</code></pre> 
<p><strong>后端代码生成</strong><br> Relay得到lower后的函数，最后一步则是交给tvm::build做代码生成，跳转到tvm/src/driver/driver_api.cc中的build函数（注意这里重载了几个版本），然后跳转到核心build，注意这里的build函数支持异构编译，只要在inputs划分好不同硬件设施即可。</p> 
<pre><code>// Build for heterogeneous execution.
runtime::Module build(const Map&lt;Target, IRModule&gt;&amp; inputs, const Target&amp; target_host) {
  auto pass_ctx = transform::PassContext::Current();

  std::vector&lt;runtime::Module&gt; device_modules;
  Target target_host_val = target_host;
  // skip some code ...

  IRModule mhost_all = IRModule(Map&lt;GlobalVar, BaseFunc&gt;());

  for (const auto&amp; it : inputs) {
    auto pair = SplitDevHostFuncs(it.second, it.first, target_host_val, pass_ctx);
    auto&amp; mhost = pair.first;
    auto&amp; mdevice = pair.second;

    mhost_all-&gt;Update(mhost);
    if (mdevice-&gt;functions.size() != 0) {
      device_modules.push_back(codegen::Build(mdevice, it.first));
    }
  }

  runtime::Module mhost = codegen::Build(mhost_all, target_host_val);
  // Import all modules
  for (const auto&amp; it : device_modules) {
    if (it.operator-&gt;()) {
      mhost.Import(it);
    }
  }
  return mhost;
}
</code></pre> 
<p>当中最最核心的则是codegen::Build，最后跳转过去就开始调用代码生成模块（tvm/src/target/codegen.cc）。会根据后端设备名称调用已经注册的方法，</p> 
<pre><code>runtime::Module Build(IRModule mod, Target target) {
  // skip some code.....
  std::string build_f_name;
  if (target-&gt;kind-&gt;name == "micro_dev") {
    build_f_name = "target.build.c";
  } else {
    build_f_name = "target.build." + target-&gt;kind-&gt;name;
  }
  // the build function.
  const PackedFunc* bf = runtime::Registry::Get(build_f_name);
  CHECK(bf != nullptr) &lt;&lt; build_f_name &lt;&lt; " is not enabled";
  return (*bf)(mod, target);
}
</code></pre> 
<p>以LLVM为例，target.build.llvm 会在tvm/src/target/llvm/llvm_module.cc注册，然后调用同个文件中的LLVMModuleNode-&gt;Init。这时会跳转到tvm/src/target/llvm/codegen_llvm.cc中的CodeGenLLVM类进行代码生成。<br> <img src="https://images2.imgbox.com/e9/86/ICqKx4bZ_o.png" alt="在这里插入图片描述"></p> 
<p>至此就完成了relay.build过程，生成后端可运行的代码。</p> 
<p>以上只是粗略的熟悉下relay编译流程，其中有些代码没有细看，写的比较粗糙，后续有时间还需要再花时间梳理。</p> 
<p><strong>Reference</strong><br> <a href="https://link.zhihu.com/?target=https://chhzh123.github.io/blogs/2020-03-26-tvm-flow/" rel="nofollow">https://link.zhihu.com/?target=https%3A//chhzh123.github.io/blogs/2020-03-26-tvm-flow/</a><br> <a href="https://link.zhihu.com/?target=https://chhzh123.github.io/blogs/2020-03-19-tvm-te/" rel="nofollow">https://link.zhihu.com/?target=https%3A//chhzh123.github.io/blogs/2020-03-19-tvm-te/</a><br> <a href="https://link.zhihu.com/?target=https://chhzh123.github.io/blogs/2020-04-02-relay-ir-pass/" rel="nofollow">https://link.zhihu.com/?target=https%3A//chhzh123.github.io/blogs/2020-04-02-relay-ir-pass/</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/6eb69a2df660b96e808c700c292587f2/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Ubuntu:使用cd命令后直接自动ls</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/a826d24809ea20ec9643632fc3f950d9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Ubuntu20.04 关于搜狗输入法无法安装的问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>