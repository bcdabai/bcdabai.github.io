<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习激活函数 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习激活函数" />
<meta property="og:description" content="激活函数 激活函数是人工神经网络中的一个重要组成部分。它们用于向神经网络中添加非线性因素，使得网络能够解决复杂问题，如图像识别、语言处理等。激活函数的作用是决定一个神经元是否应该被激活，也就是说，它帮助决定神经元的输出是什么。
一些常见的激活函数包括：
Sigmoid函数：将输入压缩到0和1之间，常用于二分类问题。
Tanh函数：也称为双曲正切函数，将输入压缩到-1和1之间，形状类似于Sigmoid，但由于输出范围更广，通常比Sigmoid更好。
ReLU函数：线性整流函数（Rectified Linear Unit），如果输入是正数则直接输出该数，如果是负数则输出0。这个函数在实践中非常流行，因为它能加快训练过程并提高性能。
Leaky ReLU：是ReLU的变体，它允许小的负值输出，避免了ReLU的一些问题，如神经元“死亡”。
Softmax函数：通常用于多分类问题的输出层，可以将输入转化为概率分布。
激活函数的选择取决于具体任务和网络的结构。每种激活函数都有其优势和适用场景。
当然可以。在PyTorch中，使用GPU加速和实现不同的激活函数是相对简单的。以下是一些常用激活函数的示例，以及如何使用PyTorch将计算移到GPU上。
首先，确保你的系统中安装了PyTorch，并且你的机器配备了NVIDIA GPU以及相应的CUDA支持。这是使用GPU加速的前提。
下面是一些示例代码，展示如何在PyTorch中使用Sigmoid、Tanh、ReLU和Softmax激活函数，并将计算移至GPU上：
import torch import torch.nn as nn # 检查CUDA（GPU加速）是否可用 device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) # 创建一个简单的张量（Tensor） tensor = torch.randn(10, 10).to(device) # 将张量移至GPU上 # Sigmoid激活函数 sigmoid = nn.Sigmoid() output_sigmoid = sigmoid(tensor) # Tanh激活函数 tanh = nn.Tanh() output_tanh = tanh(tensor) # ReLU激活函数 relu = nn.ReLU() output_relu = relu(tensor) # Softmax激活函数，适用于多分类问题 softmax = nn.Softmax(dim=1) # dim指定要计算Softmax的维度 output_softmax = softmax(tensor) # 打印输出，确认它们是在GPU上计算的 print(output_sigmoid) print(output_tanh) print(output_relu) print(output_softmax) 这段代码首先检查是否可以使用GPU。如果可以，它会将一个随机生成的张量移至GPU上，并对该张量应用不同的激活函数。需要注意的是，对于Softmax函数，你需要指定dim参数，这是因为Softmax通常用于多维数据，如多分类问题的输出层。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/9c2aa68e547b6b9e93130841f8484a99/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-11T22:46:36+08:00" />
<meta property="article:modified_time" content="2024-01-11T22:46:36+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习激活函数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p><img src="https://images2.imgbox.com/92/d7/DweLPxVH_o.png" alt="（）"></p> 
<h3><a id="_1"></a>激活函数</h3> 
<p>激活函数是人工神经网络中的一个重要组成部分。它们用于向神经网络中添加非线性因素，使得网络能够解决复杂问题，如图像识别、语言处理等。激活函数的作用是决定一个神经元是否应该被激活，也就是说，它帮助决定神经元的输出是什么。</p> 
<p>一些常见的激活函数包括：</p> 
<ol><li> <p><strong>Sigmoid函数</strong>：将输入压缩到0和1之间，常用于二分类问题。</p> </li><li> <p><strong>Tanh函数</strong>：也称为双曲正切函数，将输入压缩到-1和1之间，形状类似于Sigmoid，但由于输出范围更广，通常比Sigmoid更好。</p> </li><li> <p><strong>ReLU函数</strong>：线性整流函数（Rectified Linear Unit），如果输入是正数则直接输出该数，如果是负数则输出0。这个函数在实践中非常流行，因为它能加快训练过程并提高性能。</p> </li><li> <p><strong>Leaky ReLU</strong>：是ReLU的变体，它允许小的负值输出，避免了ReLU的一些问题，如神经元“死亡”。</p> </li><li> <p><strong>Softmax函数</strong>：通常用于多分类问题的输出层，可以将输入转化为概率分布。</p> </li></ol> 
<p>激活函数的选择取决于具体任务和网络的结构。每种激活函数都有其优势和适用场景。<br> 当然可以。在PyTorch中，使用GPU加速和实现不同的激活函数是相对简单的。以下是一些常用激活函数的示例，以及如何使用PyTorch将计算移到GPU上。</p> 
<p>首先，确保你的系统中安装了PyTorch，并且你的机器配备了NVIDIA GPU以及相应的CUDA支持。这是使用GPU加速的前提。</p> 
<p>下面是一些示例代码，展示如何在PyTorch中使用Sigmoid、Tanh、ReLU和Softmax激活函数，并将计算移至GPU上：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 检查CUDA（GPU加速）是否可用</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

<span class="token comment"># 创建一个简单的张量（Tensor）</span>
tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment"># 将张量移至GPU上</span>

<span class="token comment"># Sigmoid激活函数</span>
sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
output_sigmoid <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token comment"># Tanh激活函数</span>
tanh <span class="token operator">=</span> nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span>
output_tanh <span class="token operator">=</span> tanh<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token comment"># ReLU激活函数</span>
relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
output_relu <span class="token operator">=</span> relu<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token comment"># Softmax激活函数，适用于多分类问题</span>
softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># dim指定要计算Softmax的维度</span>
output_softmax <span class="token operator">=</span> softmax<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token comment"># 打印输出，确认它们是在GPU上计算的</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_sigmoid<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_tanh<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_relu<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output_softmax<span class="token punctuation">)</span>
</code></pre> 
<p>这段代码首先检查是否可以使用GPU。如果可以，它会将一个随机生成的张量移至GPU上，并对该张量应用不同的激活函数。需要注意的是，对于Softmax函数，你需要指定<code>dim</code>参数，这是因为Softmax通常用于多维数据，如多分类问题的输出层。</p> 
<p>运行这段代码前，请确保你的环境支持CUDA。如果你的机器不支持GPU，这段代码仍然可以在CPU上运行。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c0f35f5ecaa6dd53a952750ddf874d53/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Inveta数字孪生基座</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/df1100c45468b389486b29c20994efe0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【VerilogA】4位DAC的编写与测试</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>