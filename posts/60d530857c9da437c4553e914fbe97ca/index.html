<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>文本生成（附代码视频） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="文本生成（附代码视频）" />
<meta property="og:description" content="文本生成技术是自然语言处理领域的另一重要技术。应用者可以利用既定信息与文本生成模型生成满足特定目标的文本序列。文本生成模型的应用场景丰富如生成式阅读理解、人机对话或者智能写作等。当前深度学习的发展也推动了该项技术的进步，越来越多高可用的文本生成模型诞生，促进行业效率，服务智能化社会。
10.1 文本生成的发展现状 文本生成的技术路线发展与其他NLP技术路线类似，均是从简单的规则逐步发展至大型深度神经网络的过程。当然，文本生成显然难于其他NLP技术，因为文本生成技术的预测目标并不在既定的文本中，其需要根据既定文本去生成符合目标的文本，而阅读理解、命名实体识别等技术则是通过抽取既定文本里的相应文本段来达成预测的目标。
10.1.1 文本生成模板 文本生成技术的发展同样离不开简单规则的约束。本文所说的规则就是预先定义好的模板，每一个模板对应一种应用场景。根据应用场景的特性，我们将需要生成的通用性语句事先规范化（模板），而后再利用NLP技术去抽取相应的非通用语句对模板进行插空，从而完成文本生成任务。如图10-1所示，图中标红的字体就是模板，黑色字体则可通过计算涨跌来填充模板中的空缺，从而完成股市新闻的签发。
图10-1文本生成模板 当然，图10-1只是一个简单的模板呈现，要生成符合复杂应用场景的文字模板，这需要考虑模板中的内容、文本结构、句子语法与阅读流畅度等，这就需要大量的专业人员去维护每一套模板的产生。
10.1.2 变分自编码器 变分自编码器（VAE）是自编码器中的一种。常见的自编码器如图10-2所示，最简单的自编码器只有三层结构，中间的隐藏层才是我们所需要关注的地方，以隐藏层为界限，左边为编码器（encoder）， 右边为解码器（decoder），所以在训练过程中，输入才能在经过编码后再解码，还原成原来的模样。
对传统机器学习有所了解的读者应该都知道主成分分析（PCA），它就是用来对数据进行降维。假如我们通过一组数据训练出了我们的自编码器，然后我们拆掉自编码器的解码器（decoder），就可以用剩下的编码器（encoder）与隐藏层来表征我们的数据了。隐藏层的神经元数目远低于输入层，那么就相当于我们用更少的特征（神经元）去表征我们的输入数据，从而达到数据降维压缩的目的。
当然，自编码器学习到的特征表征不仅仅可以用来数据降维，也可以将特征表征接入一个简单的分类器，将抽象的特征用来文本分类。同样地，我们可以利用自编码器所得到的特征（这里也称隐变量）并对编码器与解码器进行改造，从而实现文本生成技术。
图10-2自编码器网络结构 10.1.3 序列到序列技术 虽然序列到序列（Seq2Seq）技术与变分自编码器在文本生成都利用了编码器与解码器，但两者仍然存在些许不同，变分自编码的文本生成技术在预测过程中会从隐变量的分布中进行采样，在这种方法下，对于同一条文本输入，模型能够得到不一样的文本输出。而Seq2Seq文本生成模型则能保证在预测过程中的文本输入与输出是确定的。因此，当前采用Seq2Seq结构的文本生成模型更为主流，Seq2Seq的优点是能够处理变长文本，常见的Seq2Seq结构首先利用编码器将输入序列映射成固定的中间序列h4，而后解码器再对中间序列h4进行解码，如图10-3所示。
图10-3 常见的Seq2Seq结构 然而，图10-3的Seq2Seq结构存在一定缺陷，因为编码器将文本统一映射成了固定的中间序列，这让文本中每一个词语在固定的中间序列的信息（贡献量）是一致的。显然，一句话的中心往往有文本中的几个词来表征，故而固定的中间序列信息对后续的解码产生了一定影响。这里就诞生了基于注意力机制的Seq2Seq模型，如图10-4所示。注意力机制下的Seq2Seq模型的输入中间序列不是固定的，而是经过编码器转换的中间语义C。
图10-4注意力模型（解码器部分） 而这些输入C也各不相同，每一个C都是由权重w和译码器的隐藏层输出h加权组成，如图10-5所示。在解码器decoder部分，中间语义C1，C2，C3之间的权值表征是不同的，这也就是我们所说的注意力机制。
图10-5中间语义转换示意图 换言之，随着训练过程的进行，重点一直在变化，而这些变化则由上图的权重w去表示，当训练停止时，权重值也就确定下来了，此时的权重值是最拟合当前训练数据的。例如C1的重点在“中”这个字，那么中间语义可以表示为C1=0.6h1 &#43; 0.2h2 &#43; 0.1h3 &#43; 0.1h4 (权值可以看成概率，所有权值值加起来为1)。因此中间语义的转换公式如式（10-1）所示。其中，n为输入序列的长度。
此时，我们唯一要解决的是，如何去求中间语义C的权值w表征。这就涉及到注意力模型的编码器部分，如图10-6所示。F函数和softmax函数可以理解为我们要计算当前的hi与全部h（包括hi）之间的差别，从而计算出在i时刻下，每一个h对应的权值（即概率）。换言之，大家可以将下图看成分类问题，与hi越相近的，输出的概率也就越大。
图10-6注意力模型（编码器部分） 10.2 基于预训练模型的文本生成模型 Seq2Seq模型需要预训练模型的语义表征作为输入，而当前类BERT预训练模型发展迅猛。Li等人[1]提出的UniLM模型则是将Seq2Seq与BERT模型结合起来。UniLM能够在不改变以往BERT模型微调的方式进行文本生成任务，使得BERT模型在自然语言生成（NLG）任务与自然语言理解（NLU）任务中实现统一。
为了统一NLG与NLU两种任务，UniLM模型使用了三种目标函数进行预训练：双向语言模型，单向语言模型和序列到序列语言模型，模型框架如图10-7所示。三种不同目标函数的语言模型共享同一个Transformer网络，而这三种目标函数则通过自注意力掩盖矩阵（self-attention mask）来实现。
1. 双向语言模型：与BERT模型一致，在预测被掩蔽的字符令牌时，可以观察到所有的字符令牌。
2. 单向语言模型：从左到右掩盖策略与从右到左掩盖策略。从左到右方向的掩盖策略是通过被掩盖的字符令牌（token）的左侧文本来预测被掩盖的字符令牌。从右到左掩盖策略则与前者相反。
3. 序列到序列语言模型：如果被掩蔽字符令牌在第一个文本序列中，那么仅可以使用第一个文本序列中所有字符令牌，不能使用第二个文本序列的任何信息；如果被掩蔽字符令牌在第二个文本序列中，那么使用一个文本序列中所有字符令牌和第二个文本序列中被掩蔽字符令牌的左侧文本序列来预测被掩蔽字符令牌。
在模型预训练过程中，三分之一的数据用于双向语言模型，三分之一的数据用于序列到序列语言模型优化，三分之一的数据用于单向语言模型。实验证明，三种不同的掩盖策略能够让模型在预训练过程学习NLU与NLG任务的信息。UniLM模型的微调与BERT模型的微调没有区别。我们可以使用BERT模型代码直接加载开源出来的UniLM模型进行微调。
图10-7 UniLM 模型结构图 10.3 文本生成任务实践 笔者在10.2节中介绍的UniLM模型需要利用大量的无监督语料数据进行预训练，最终得到可用的模型。受限于预训练所需的硬件设备，本文实验将对利用UniLM模型的序列到序列掩盖策略与预训练模型进行结合，直接构造下游任务进行微调，这样可以省去了预训练的过程，而且我们还能沿用文本分类的代码框架结构，达到代码复用且文本生成性能优良的效果。
预训练模型如BERT模型本身就经过了大量无监督语料的预训练，其已经具备很好的语义表征能力，在NLU任务上表现出色。为了在NLG任务上表现优良，文本生成模型可以利用预训练模型强大的表征能力，再配以相应的下游结构进行微调，就能学习到NLG任务的信息，无需耗费过高的时空成本进行预训练学习。
UniLM模型应用的三种预训练掩盖策略中，序列到序列的掩盖策略对文本生成任务起到较大的作用。在其他参数与正常使用预训练模型一致的前提下，序列到序列的掩盖策略只需要修改输入到预训练模型的自注意力掩盖矩阵（self-attention mask）就能直接使用预训练模型的权重，修改成本低，但却能有效地保证文本生成任务的准确性。
文本生成任务可以简单理解为模型根据输入句子A的语义信息，输出符合既定目标的句子B。序列到序列的网络结构是通过句子A的语义信息，逐字迭代出句子B的每一个字。因此，通过修改自注意力掩盖矩阵，我们首先保证句子A中的每一个字符令牌能够互相观看到句子A中的所有字符令牌，这样能满足自注意力机制（self-attention）。其次，在自注意力掩盖矩阵中，保证句子B的每一个字符令牌只能从左往右地被查看，这样就能保证序列到序列的策略能够有效。
由图10-8可知，“[CLS]你是哪国人[SEP]”可以看成句子A，“中国人[SEP]”可以看成句子B。句子A对应的掩盖矩阵是全1矩阵，也就是句子A中的每一个字符令牌在训练过程中都能互相看到对方的信息，而句子B则对应一个下三角为1的矩阵，这样保证句子B从左往右的每一个字符令牌在训练过程中只能看到左边序列的信息，也就是“国”字符令牌只能看到它之前的字符令牌但看不到“人”与“[SEP]”字符令牌，从而保证模型拥有序列到序列的能力。其中，自注意力矩阵中的0代表模型无需关注的信息，在计算自注意力（self-attention）的时候会被自动忽略，这也包含用来保证模型输入长度一致的补充字符令牌“[PAD]”，其在自注意力掩盖矩阵中同样用0表示。
另外，我们也可以从图10-8中看到行为“[CLS]你是哪国人[SEP]中国人”，列为“你是哪国人[SEP]中国人[SEP]”，掩盖矩阵同时也通过字符令牌的错位来保证模型有用序列到序列的能力，因此这两句话分别构成了文本生成模型的输入与输出，模型结构如图10-9所示。
图10-8序列到序列的自注意力掩盖矩阵 图10-9文本生成模型结构图 因此，在构建完自注意力矩阵之后，我们将该矩阵与其他参数输入到预训练模型中，获得预训练模型的语义表征，并将这个语义表征喂入序列到序列下游结构中进行迭代训练，从而得到一个文本生成模型。
10.3.1 数据介绍 本文实验采用阿里天池的中医药文本生成数据集，每一条数据由一个文本段（text）与多组问答对（Q&amp;A）组成。文本生成的任务是根据文本段（text）与答案（A），生成对应的问题（Q）。根据本节所介绍的模型原理，我们可以将输入数据构造成“[CLS] text [SEP] A [SEP] Q”，输出数据构造成“text [SEP] A [SEP] Q [SEP]”，通过预训练模型与序列到序列模型构造的模型进行逐字递归预测，从而完成文本生成任务。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/60d530857c9da437c4553e914fbe97ca/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-08T09:01:00+08:00" />
<meta property="article:modified_time" content="2021-06-08T09:01:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">文本生成（附代码视频）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p>文本生成技术是自然语言处理领域的另一重要技术。应用者可以利用既定信息与文本生成模型生成满足特定目标的文本序列。文本生成模型的应用场景丰富如生成式阅读理解、人机对话或者智能写作等。当前深度学习的发展也推动了该项技术的进步，越来越多高可用的文本生成模型诞生，促进行业效率，服务智能化社会。</p> 
 <h3>10.1 文本生成的发展现状</h3> 
 <p>文本生成的技术路线发展与其他NLP技术路线类似，均是从简单的规则逐步发展至大型深度神经网络的过程。当然，文本生成显然难于其他NLP技术，因为文本生成技术的预测目标并不在既定的文本中，其需要根据既定文本去生成符合目标的文本，而阅读理解、命名实体识别等技术则是通过抽取既定文本里的相应文本段来达成预测的目标。</p> 
 <h4>10.1.1 文本生成模板</h4> 
 <p><br>文本生成技术的发展同样离不开简单规则的约束。本文所说的规则就是预先定义好的模板，每一个模板对应一种应用场景。根据应用场景的特性，我们将需要生成的通用性语句事先规范化（模板），而后再利用NLP技术去抽取相应的非通用语句对模板进行插空，从而完成文本生成任务。如图10-1所示，图中标红的字体就是模板，黑色字体则可通过计算涨跌来填充模板中的空缺，从而完成股市新闻的签发。</p> 
 <img src="https://images2.imgbox.com/d8/21/3XrVLSuX_o.png" width="560"> 
 <figcaption>
   图10-1文本生成模板 
 </figcaption> 
 <p>当然，图10-1只是一个简单的模板呈现，要生成符合复杂应用场景的文字模板，这需要考虑模板中的内容、文本结构、句子语法与阅读流畅度等，这就需要大量的专业人员去维护每一套模板的产生。</p> 
 <h4>10.1.2 变分自编码器</h4> 
 <p>变分自编码器（VAE）是自编码器中的一种。常见的自编码器如图10-2所示，最简单的自编码器只有三层结构，中间的隐藏层才是我们所需要关注的地方，以隐藏层为界限，左边为编码器（encoder）， 右边为解码器（decoder），所以在训练过程中，输入才能在经过编码后再解码，还原成原来的模样。</p> 
 <p>对传统机器学习有所了解的读者应该都知道主成分分析（PCA），它就是用来对数据进行降维。假如我们通过一组数据训练出了我们的自编码器，然后我们拆掉自编码器的解码器（decoder），就可以用剩下的编码器（encoder）与隐藏层来表征我们的数据了。隐藏层的神经元数目远低于输入层，那么就相当于我们用更少的特征（神经元）去表征我们的输入数据，从而达到数据降维压缩的目的。</p> 
 <p>当然，自编码器学习到的特征表征不仅仅可以用来数据降维，也可以将特征表征接入一个简单的分类器，将抽象的特征用来文本分类。同样地，我们可以利用自编码器所得到的特征（这里也称隐变量）并对编码器与解码器进行改造，从而实现文本生成技术。</p> 
 <img src="https://images2.imgbox.com/fe/fb/pWyRtpQ3_o.png" width="501"> 
 <figcaption>
   图10-2自编码器网络结构 
 </figcaption> 
 <h4>10.1.3 序列到序列技术</h4> 
 <p>虽然序列到序列（Seq2Seq）技术与变分自编码器在文本生成都利用了编码器与解码器，但两者仍然存在些许不同，变分自编码的文本生成技术在预测过程中会从隐变量的分布中进行采样，在这种方法下，对于同一条文本输入，模型能够得到不一样的文本输出。而Seq2Seq文本生成模型则能保证在预测过程中的文本输入与输出是确定的。因此，当前采用Seq2Seq结构的文本生成模型更为主流，Seq2Seq的优点是能够处理变长文本，常见的Seq2Seq结构首先利用编码器将输入序列映射成固定的中间序列h4，而后解码器再对中间序列h4进行解码，如图10-3所示。</p> 
 <img src="https://images2.imgbox.com/1c/e5/CtguSbpZ_o.png" width="523"> 
 <figcaption>
   图10-3 常见的Seq2Seq结构 
 </figcaption> 
 <p>然而，图10-3的Seq2Seq结构存在一定缺陷，因为编码器将文本统一映射成了固定的中间序列，这让文本中每一个词语在固定的中间序列的信息（贡献量）是一致的。显然，一句话的中心往往有文本中的几个词来表征，故而固定的中间序列信息对后续的解码产生了一定影响。这里就诞生了基于注意力机制的Seq2Seq模型，如图10-4所示。注意力机制下的Seq2Seq模型的输入中间序列不是固定的，而是经过编码器转换的中间语义C。</p> 
 <img src="https://images2.imgbox.com/8c/70/DJsSFB7C_o.png" width="359"> 
 <figcaption>
   图10-4注意力模型（解码器部分） 
 </figcaption> 
 <p>而这些输入C也各不相同，每一个C都是由权重w和译码器的隐藏层输出h加权组成，如图10-5所示。在解码器decoder部分，中间语义C1，C2，C3之间的权值表征是不同的，这也就是我们所说的注意力机制。</p> 
 <img src="https://images2.imgbox.com/d7/3b/W1bMkjV2_o.png" width="554"> 
 <figcaption>
   图10-5中间语义转换示意图 
 </figcaption> 
 <p>换言之，随着训练过程的进行，重点一直在变化，而这些变化则由上图的权重w去表示，当训练停止时，权重值也就确定下来了，此时的权重值是最拟合当前训练数据的。例如C1的重点在“中”这个字，那么中间语义可以表示为C1=0.6h1 + 0.2h2 + 0.1h3 + 0.1h4 (权值可以看成概率，所有权值值加起来为1)。因此中间语义的转换公式如式（10-1）所示。其中，n为输入序列的长度。</p> 
 <img src="https://images2.imgbox.com/de/78/okHeELBT_o.png" width="1082"> 
 <p>此时，我们唯一要解决的是，如何去求中间语义C的权值w表征。这就涉及到注意力模型的编码器部分，如图10-6所示。F函数和softmax函数可以理解为我们要计算当前的hi与全部h（包括hi）之间的差别，从而计算出在i时刻下，每一个h对应的权值（即概率）。换言之，大家可以将下图看成分类问题，与hi越相近的，输出的概率也就越大。</p> 
 <img src="https://images2.imgbox.com/b6/6b/utHtyPS0_o.png" width="342"> 
 <figcaption>
   图10-6注意力模型（编码器部分） 
 </figcaption> 
 <h3>10.2 基于预训练模型的文本生成模型</h3> 
 <p>Seq2Seq模型需要预训练模型的语义表征作为输入，而当前类BERT预训练模型发展迅猛。Li等人[1]提出的UniLM模型则是将Seq2Seq与BERT模型结合起来。UniLM能够在不改变以往BERT模型微调的方式进行文本生成任务，使得BERT模型在自然语言生成（NLG）任务与自然语言理解（NLU）任务中实现统一。</p> 
 <p>为了统一NLG与NLU两种任务，UniLM模型使用了三种目标函数进行预训练：双向语言模型，单向语言模型和序列到序列语言模型，模型框架如图10-7所示。三种不同目标函数的语言模型共享同一个Transformer网络，而这三种目标函数则通过自注意力掩盖矩阵（self-attention mask）来实现。</p> 
 <p>1. 双向语言模型：与BERT模型一致，在预测被掩蔽的字符令牌时，可以观察到所有的字符令牌。</p> 
 <p>2. 单向语言模型：从左到右掩盖策略与从右到左掩盖策略。从左到右方向的掩盖策略是通过被掩盖的字符令牌（token）的左侧文本来预测被掩盖的字符令牌。从右到左掩盖策略则与前者相反。</p> 
 <p>3. 序列到序列语言模型：如果被掩蔽字符令牌在第一个文本序列中，那么仅可以使用第一个文本序列中所有字符令牌，不能使用第二个文本序列的任何信息；如果被掩蔽字符令牌在第二个文本序列中，那么使用一个文本序列中所有字符令牌和第二个文本序列中被掩蔽字符令牌的左侧文本序列来预测被掩蔽字符令牌。</p> 
 <p>在模型预训练过程中，三分之一的数据用于双向语言模型，三分之一的数据用于序列到序列语言模型优化，三分之一的数据用于单向语言模型。实验证明，三种不同的掩盖策略能够让模型在预训练过程学习NLU与NLG任务的信息。UniLM模型的微调与BERT模型的微调没有区别。我们可以使用BERT模型代码直接加载开源出来的UniLM模型进行微调。</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/e4/8c/A2LudwYP_o.png"></p> 
 <figcaption>
   图10-7 UniLM 模型结构图 
 </figcaption> 
 <h3>10.3 文本生成任务实践</h3> 
 <p>笔者在10.2节中介绍的UniLM模型需要利用大量的无监督语料数据进行预训练，最终得到可用的模型。受限于预训练所需的硬件设备，本文实验将对利用UniLM模型的序列到序列掩盖策略与预训练模型进行结合，直接构造下游任务进行微调，这样可以省去了预训练的过程，而且我们还能沿用文本分类的代码框架结构，达到代码复用且文本生成性能优良的效果。</p> 
 <p>预训练模型如BERT模型本身就经过了大量无监督语料的预训练，其已经具备很好的语义表征能力，在NLU任务上表现出色。为了在NLG任务上表现优良，文本生成模型可以利用预训练模型强大的表征能力，再配以相应的下游结构进行微调，就能学习到NLG任务的信息，无需耗费过高的时空成本进行预训练学习。</p> 
 <p>UniLM模型应用的三种预训练掩盖策略中，序列到序列的掩盖策略对文本生成任务起到较大的作用。在其他参数与正常使用预训练模型一致的前提下，序列到序列的掩盖策略只需要修改输入到预训练模型的自注意力掩盖矩阵（self-attention mask）就能直接使用预训练模型的权重，修改成本低，但却能有效地保证文本生成任务的准确性。</p> 
 <p>文本生成任务可以简单理解为模型根据输入句子A的语义信息，输出符合既定目标的句子B。序列到序列的网络结构是通过句子A的语义信息，逐字迭代出句子B的每一个字。因此，通过修改自注意力掩盖矩阵，我们首先保证句子A中的每一个字符令牌能够互相观看到句子A中的所有字符令牌，这样能满足自注意力机制（self-attention）。其次，在自注意力掩盖矩阵中，保证句子B的每一个字符令牌只能从左往右地被查看，这样就能保证序列到序列的策略能够有效。</p> 
 <p>由图10-8可知，“[CLS]你是哪国人[SEP]”可以看成句子A，“中国人[SEP]”可以看成句子B。句子A对应的掩盖矩阵是全1矩阵，也就是句子A中的每一个字符令牌在训练过程中都能互相看到对方的信息，而句子B则对应一个下三角为1的矩阵，这样保证句子B从左往右的每一个字符令牌在训练过程中只能看到左边序列的信息，也就是“国”字符令牌只能看到它之前的字符令牌但看不到“人”与“[SEP]”字符令牌，从而保证模型拥有序列到序列的能力。其中，自注意力矩阵中的0代表模型无需关注的信息，在计算自注意力（self-attention）的时候会被自动忽略，这也包含用来保证模型输入长度一致的补充字符令牌“[PAD]”，其在自注意力掩盖矩阵中同样用0表示。</p> 
 <p>另外，我们也可以从图10-8中看到行为“[CLS]你是哪国人[SEP]中国人”，列为“你是哪国人[SEP]中国人[SEP]”，掩盖矩阵同时也通过字符令牌的错位来保证模型有用序列到序列的能力，因此这两句话分别构成了文本生成模型的输入与输出，模型结构如图10-9所示。</p> 
 <img src="https://images2.imgbox.com/a1/92/dFAHdumZ_o.png" width="560"> 
 <figcaption>
   图10-8序列到序列的自注意力掩盖矩阵 
 </figcaption> 
 <img src="https://images2.imgbox.com/3a/d3/5PBJ1dpj_o.png" width="448"> 
 <figcaption>
   图10-9文本生成模型结构图 
 </figcaption> 
 <p>因此，在构建完自注意力矩阵之后，我们将该矩阵与其他参数输入到预训练模型中，获得预训练模型的语义表征，并将这个语义表征喂入序列到序列下游结构中进行迭代训练，从而得到一个文本生成模型。</p> 
 <h4>10.3.1 数据介绍</h4> 
 <p>本文实验采用阿里天池的中医药文本生成数据集，每一条数据由一个文本段（text）与多组问答对（Q&amp;A）组成。文本生成的任务是根据文本段（text）与答案（A），生成对应的问题（Q）。根据本节所介绍的模型原理，我们可以将输入数据构造成“[CLS] text [SEP] A [SEP] Q”，输出数据构造成“text [SEP] A [SEP] Q [SEP]”，通过预训练模型与序列到序列模型构造的模型进行逐字递归预测，从而完成文本生成任务。</p> 
 <h4>10.3.2 评估指标</h4> 
 <p>Rouge(Recall-Oriented Understudy for Gisting Evaluation)：Rouge是一组评估自动文摘以及机器翻译的指标。它通过对模型生成的预测文本段与真实文本段进行比较计算，得出相应的分值，用以衡量自动生成的摘要或翻译与参考摘要之间的“相似度”，从而评估当前文本生成模型的性能。本次实验采用的Rouge-L评估指标中的L为最长公共子序列（LCS，longest common subsequence），公式如（10-1）~（10-3）所示。其中，LCS(X,Y)代表两个文本段的最长公共子序列，m与n分别代表真实文本段与预测文本段的长度（即所含词的个数）</p> 
 <p><img src="https://images2.imgbox.com/64/4e/Ijgjsw5k_o.png" width="1088"></p> 
 <h4>10.3.3集束搜索（Beam Search）<br></h4> 
 <p>模型的预测与验证一样，都需要使用集束搜索（Beam Search）算法对模型的输出概率进行逐字递归解码预测。我们可以将这个过程理解为时序过程，也就是每生成一个字符令牌就需要当前所得到的文本段输入到模型中进行预测，这样每一个生成的字符令牌都与前者有关系。</p> 
 <p>在集束搜索之前，有人使用贪心算法进行解码，如图10-10所示。这个算法的好处是将模型解码的指数级别复杂度降到了线性级别复杂度，因为每一次预测字符令牌，只需要取最大的概率作为当前的最优解，但所得到的序列无法保证是最优解。</p> 
 <p>而集束搜索则是贪心算法的改进，也就是让贪心算法每一次预测不一定取最优秀的那个解，可以设置超参数保留多几个解，并累加每一次预测的每一个解的得分（概率值），最终根据解的得分返回当前分数最高的序列作为最终序列。如图10-11所示，我们可以设置每次预测保留2个最优解。在规定步数的情况下，实验每次对两路分支的所有候选对象进行比较，输出两个最优解，最终在达到规定的步数或者遇到结束标志符“[SEP]”时结束解码，并输出当前分数最高的序列。</p> 
 <img src="https://images2.imgbox.com/65/a3/80uogD42_o.png" width="375"> 
 <figcaption>
   图10-10贪心解码 
 </figcaption> 
 <img src="https://images2.imgbox.com/02/50/QTtsy7x9_o.png" width="560"> 
 <figcaption>
   图10-11集束搜索解码示意图 
 </figcaption> 
 <h3>10.4 总结</h3> 
 <p>本文系统地介绍了文本生成技术的发展现状以及基于文本分类的代码框架针兼容了文本生成实验。然而，本文的介绍只是起到了抛砖引玉的作用，当前文本生成技术仍然有很大的提升空间：</p> 
 <p>1. 规则角度。先预测关键词，然后根据生成的关键词去补全整个句子；</p> 
 <p>2. 数据角度：收集更多的高质量语料，或者利用回译等手段做数据增强等。</p> 
 <p>3. 模型的角度：改造seq2seq模型结构，以促使多样化表达等。</p> 
 <p>4. 损失函数角度：采用了最大互信息作为目标损失函数等。</p> 
 <p>5. 解码算法角度：对Beam Search算法每个时间步的条件概率添加多样性约束等。</p> 
 <p>文本生成技术未来的发展方向应该聚焦于生成可控、质量优良、语义一致、句式通顺等方向。因此，文本生成技术的发展仍然需要更加长足的研究与探索。</p> 
 <h3>参考文献</h3> 
 <p>[1]Unified Language Model Pre-training for Natural Language Understanding and Generation</p> 
 <p>【NLP学习课程】</p> 
 <p>1.七个国家级竞赛获奖代码<br>2.一套代码，七个项目，学习成本低。<br>3.课程内容：预训练、文本分类、智能问答、命名实体识别、文本生成、模型蒸馏与减枝、loss应用。<br>4.详情私聊加【wēi-xìn】：xilan912427166</p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/244b09dac5dcf57813e19d2d318d8482/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一些Ubuntu系统问题的解决汇总（持续更新）</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b997bd3913054a0e95e0a5e5b94991b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">html 数组动态添加元素,js如何动态添加数组？</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>