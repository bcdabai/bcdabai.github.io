<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【文献阅读笔记】之基于注意力机制的深度学习路面裂缝检测 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【文献阅读笔记】之基于注意力机制的深度学习路面裂缝检测" />
<meta property="og:description" content="中文，计算机辅助设计与图形学学报，第 32 卷 第 8 期，2020 年 8 月。
DOI: 10.3724/SP.J.1089.2020.18059
摘要： 为实现自动准确地检测路面裂缝, 提升路面裂缝检测效果, 提出了一种基于注意力机制的裂缝检测网络 (attention-based crack networks, ACNet). 该网络采用编码器-解码器网络构架, 编码器采用 ResNet34 为骨干网, 提取路面裂缝特征; 在编码器和解码器间加入基于注意力机制的特征模块(attention-based feature module, AFM), 以利用全局信息和增加对检测不同尺度裂缝的鲁棒性, 更好地提取裂缝特征和定位裂缝位置; 在解码阶段也引入注意力机制, 设计了基于注意力机制的解码模块(attention-based decoder module, ADM), 实现对裂缝的准确定位. 在公共裂缝 数据集 CFD 和 CRACK500 上, 与U-Net 等其他 8 种方法进行了比较, 结果表明, ACNet 裂缝检测效果更理想, 在主观视觉上, 裂缝定位更准确, 细节更丰富; 在实验指标 F1和重合率上, 检测结果都有明显提升, 说明了该网络的有效性。
研究背景 基于图像的路面裂缝检测, 可以看成是从图像中分割出裂缝区域,图像分割时, 一般以图像(单通道或多通道)为输入, 输出是带有标签的分类后的图像, 而图像的大小保持不变。
传统方法 基于直方图估计、梯度方向直方图、局部二值模式、Gabor 滤波和多特征融合。
局限性：对输入图像的质量要求较高, 往往在外界环境发生变化、光线不均和有噪声干扰时, 裂缝检测的性能将受到极大影响。
基于深度学习的方法 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/4c8d6dad6794c01f8b030afbab90e90d/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-12T20:47:12+08:00" />
<meta property="article:modified_time" content="2022-04-12T20:47:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【文献阅读笔记】之基于注意力机制的深度学习路面裂缝检测</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>中文，计算机辅助设计与图形学学报，第 32 卷 第 8 期，2020 年 8 月。</strong></p> 
<p><strong>DOI: 10.3724/SP.J.1089.2020.18059</strong></p> 
<hr> 
<h2>摘要：</h2> 
<p>        为实现自动准确地检测路面裂缝, 提升路面裂缝检测效果, 提出了一种<strong>基于注意力机制的裂缝检测网络</strong> (attention-based crack networks, ACNet). 该网络采用<strong>编码器-解码器网络构架</strong>, 编码器采用 <strong>ResNet34 为骨干网</strong>, 提取路面裂缝特征; 在<strong>编码器和解码器间加入基于注意力机制的特征模块</strong>(attention-based feature module, AFM), 以利用全局信息和增加对检测不同尺度裂缝的鲁棒性, 更好地提取裂缝特征和定位裂缝位置; 在<strong>解码阶段也引入注意力机制, 设计了基于注意力机制的解码模块</strong>(attention-based decoder module, ADM), 实现对裂缝的准确定位. 在公共裂缝 数据集 CFD 和 CRACK500 上, 与U-Net 等其他 8 种方法进行了比较, 结果表明, ACNet 裂缝检测效果更理想, 在主观视觉上, 裂缝定位更准确, 细节更丰富; 在实验指标 F1和重合率上, 检测结果都有明显提升, 说明了该网络的有效性。</p> 
<hr> 
<h2>研究背景</h2> 
<p>        基于图像的路面裂缝检测, 可以看成是从图像中分割出裂缝区域,图像分割时, 一般以图像(单通道或多通道)为输入, 输出是带有标签的分类后的图像, 而图像的大小保持不变。</p> 
<h3><strong>传统方法</strong></h3> 
<p>        <strong>基于直方图估计、梯度方向直方图、局部二值模式、Gabor 滤波和多特征融合。</strong></p> 
<p>        局限性：对输入图像的质量要求较高, 往往在外界环境发生变化、光线不均和有噪声干扰时, 裂缝检测的性能将受到极大影响。</p> 
<h3><strong>基于深度学习的方法</strong></h3> 
<p>        <strong>1.以像素为单位, 利用周围的像素对该像素进行分类。基于卷积神经网络框架, 以每个像素点得到一个相应的图像块, 输入网络训练, 图像的分割被看成像素点的二分类问题。</strong></p> 
<p>        第一种方法像是以每一个像素点作为一张图片的图像分类问题，得到的结果只有两种：是裂缝和不是裂缝。问题在于，对于一张图片而言，若某一像素点是我们需要检测的目标，那么该像素点周围大概率也是我们需要检测的目标（即相邻像素点得到的图像块相似度高），不仅计算量大，且高度冗余。</p> 
<p>       <strong> 2.以图像为整体进行分割, 一般采用 FCN 或 U-Net 网络框架， 即采用编码器-解码器结构。encoder通过下采样进行特征提取, 包括底层、中层和高层特征；decoder通过上采样将特征还原到原图像的尺寸, 从而得到最终的分割结果。</strong></p> 
<p>        这种方法为端到端的图像分割方法，是目前深度学习中常用的一种方法。在此基础上，有许多改进的模块。例如：</p> 
<p><strong>        金字塔池化模块</strong>（Pyramid Pooling Module）它能够聚合不同区域的上下文信息,从而提高获取全局信息的能力。该模块的具体实现过程为，先对feature map 进行下采样，得到不同尺度的输出，再通过1*1卷积改变通道，再分别对不同尺寸的feature map进行上采样，最后将上采样结果concat。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/32/60/IIDyYZlb_o.png"></p> 
<p style="text-align:center;">图1.PSPNet网络结构</p> 
<p>       <strong> 空洞空间卷积池化金字塔</strong>（Atrous Spatial Pyramid Pooling）对所给定的输入以不同采样率的空洞卷积并行采样。与PPM相比，区别在于PPM对feature map使用了4次池化操作。ASPP采用了4次卷积操作（包括1次1*1卷积和3次3*3卷积）和一次池化操作。</p> 
<p>ASPP模块详细信息可见我的这篇博客：<a class="link-info" href="https://blog.csdn.net/qq_44785998/article/details/123776818?spm=1001.2014.3001.5502" title="ASPP模块">ASPP模块</a></p> 
<p> <img alt="" height="293" src="https://images2.imgbox.com/67/d7/YM4h8P3U_o.png" width="1190"></p> 
<p style="text-align:center;"> 图2.ASPP模块</p> 
<p><strong>        空洞卷积</strong>（dilated convolution）空洞卷积/膨胀卷积，是在标准的 convolution map 里注入空洞，以此来扩大感受野(<span style="color:#fe2c24;">越大的感受野包含越多的上下文关系</span>)。膨胀卷积与普通的卷积相比：除了卷积核的大小以外，还有一个扩张率(dilation rate)参数，主要用来表示膨胀的大小。在普通卷积中，可以认为它的dilated rate = 1。</p> 
<p>空洞卷积的卷积核尺寸计算公式为：</p> 
<p style="text-align:center;"><img alt="k+(k-1)*(d-1)" class="mathcode" src="https://images2.imgbox.com/00/68/M60OYy0u_o.png"></p> 
<p>其中 k：原卷积核的尺寸</p> 
<p>        d：自己设置的dilation rate</p> 
<p style="text-align:center;"> <img alt="" height="257" src="https://images2.imgbox.com/8a/3d/hS32H5sP_o.png" width="300"></p> 
<p style="text-align:center;">图3.空洞卷积</p> 
<p><strong>        空间金字塔池化</strong>（Spatial Pyramid Pooling）引出的目的是为了解决输入图片大小不一的情况（因为存在全连接层，所以对输入的尺寸有要求，否则矩阵维度不匹配）。具体过程：下图将feature map根据不同的刻度进行划分（4*4，2*2，1*1）最后总共可以得到16+4+1=21个块，我们即将从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。</p> 
<p>        第一张图片，我们把一张完整的图片，分成了16个块，也就是每个块的大小就是(w/4,h/4);</p> 
<p>        第二张图片，划分了4个块，每个块的大小就是(w/2,h/2);</p> 
<p>        第三张图片，把一整张图片作为了一个块，也就是块的大小为(w,h)。</p> 
<p>        空间金字塔最大池化的过程，其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出神经元。最后把一张任意大小的图片转换成了一个固定大小的21维特征。</p> 
<p>     <strong>   上面的三种不同刻度的划分，每一种刻度我们称之为：金字塔的一层，每一个图片块大小我们称之为：windows size。</strong></p> 
<p style="text-align:center;"></p> 
<p style="text-align:center;"><img alt="" height="337" src="https://images2.imgbox.com/3e/f7/XpKUmouK_o.png" width="500"></p> 
<p style="text-align:center;"> 图4.SPP模块</p> 
<p>        这些模块存在的问题：SPP 采用池化操作会造成局部信息的损失, 而采用 ASPP, 结果容易出现棋盘伪影，可以参照这篇文献：<a class="link-info" href="https://distill.pub/2016/deconv-checkerboard/" rel="nofollow" title="棋盘伪影">棋盘伪影</a>（大致意思为：若kernel_size / stride 不是整数，使用反卷积容易出现不均匀重叠(uneven overlap)的问题，虽然把最后一层的stride设为1可以减少失真的问题，但是效果并不明显。解决办法有两个：1、设置kernel_size可以被stride整除 2、先调整图像的大小（最近邻插值或者双线性插值），然后再执行卷积操作。）</p> 
<hr> 
<h3>作者提出的网络</h3> 
<p>    <strong>    </strong>本文提出一种基于注意力机制的路面裂缝检测网络. 该网络基于编码器-解码器网络框架, 采用 ResNet34作为编码器, 提取裂缝特征; 在编码器和解码器间增加一个基于注意力机制的特征模块, 该模块利用分层的卷积组模块提取裂缝的更高层特征, 再用融合后的更高层特征指引编码器的高层特征, 从而得到最终的高层特征; 在解码器阶段, 不是简单地将高低层特征融合, 而是利用基于注意力机制的解码模块(attention-based decoder module, ADM)融合高低层特征后, 再提取特征图的空间域和通道域注意力特征, 根据注意力特征来恢复裂缝细节, 以提升路面裂缝检测的性能。</p> 
<h4>        理论基础：</h4> 
<p><strong>        1.残差网络：</strong>残差网络的目的是为了解决深度神经网络因存在梯度消失和梯度爆炸而难以训练的问题。其中, X 表示输入, F(X) 表示残差映射。 H (X) = F(X) + X. 当 F(X)=0 时, 就变成一个恒等映射 H(X)= X。</p> 
<p>        对残差的理解：对于输入的X，经过恒等映射后得到H(X) = X，若我们在X之后加了卷积层（加深了网络的层次），卷积层的输出为F(X)，这时残差块的输出为 H (X) = F(X) + X。可以把残差映射（F(X)）看作是辅助函数，因为不确定加了之后会对网络有什么样的影响，但经验所得，随着网络层次的加深，一般神经网络的预测效果会越好，但是会产生梯度消失或者梯度爆炸。这时就体现出了残差块的作用：在加深网络层次的同时，避免梯度消失或者梯度爆炸的情况。</p> 
<p class="img-center"><img alt="" height="273" src="https://images2.imgbox.com/ff/ed/hN1iwvjU_o.png" width="400"></p> 
<p style="text-align:center;"> 图5.残差块</p> 
<p><strong>        2.注意力机制</strong>：深度学习中的注意力机制则是去除冗杂的信息, 选择对当前目标更重要的信息，它可以让网络自适应的去注意我们需要的信息。当前, 注意力机制在语义和图像的分割与理解等方面均有应用。</p> 
<p>        注意力机制分为  <strong>空间注意力机制 </strong> 和  <strong>通道注意力机制 </strong> 。典型的有：</p> 
<p>        <strong><span style="color:#fe2c24;">SEnet（通道注意力机制）：</span></strong></p> 
<p>                1、对输入图片进行<strong>全局平均池化</strong>，输出结果为1*1*C(C是输入通道)</p> 
<p>                2、进行两次全连接，第一次<strong>全连接的神经元个数较少</strong>，第二次<strong>神经元个数等于C</strong></p> 
<p><strong>                </strong>3、取sigmoid将全连接后的值固定在0-1之间，此时我们得到了<strong>输入特征层每一个通道的权值</strong></p> 
<p><strong>                </strong>4、将权值系数乘上原输入特征层。</p> 
<p class="img-center"><img alt="" height="264" src="https://images2.imgbox.com/80/d9/972EyFjV_o.png" width="450"></p> 
<p style="text-align:center;">图6. SE block结构</p> 
<p><span style="color:#fe2c24;"><strong>        CBAM（通道注意力机制+空间注意力机制）：</strong></span></p> 
<p>                先说通道注意力部分：</p> 
<p>                1、输入图片分别进行<strong>全局平均池化</strong>和<strong>全局最大池化</strong>，得到两个1*1*C(C是输入通道)的输出</p> 
<p>                2、这两个输出经过共享的全连接层，全连接层包括有两层，第一次<strong>全连接的神经元个数较少</strong>，第二次<strong>神经元个数等于C</strong></p> 
<p><strong>                </strong>3、将这两个经过全连接层的结果相加，并取sigmoid，得到了<strong>输入特征层每一个通道的权值</strong>（0-1之间）</p> 
<p><strong>              </strong>  4、将权值系数乘上原输入特征层。</p> 
<p><img alt="" height="282" src="https://images2.imgbox.com/ad/4a/kdZRne04_o.png" width="1173"></p> 
<p style="text-align:center;">图7.CBAM的通道注意力部分</p> 
<p>                再说空间注意力部分</p> 
<p>                1、将输入的<strong>每一个特征点在通道上取最大值和平均值（得到结果h*w*2）</strong></p> 
<p><strong>        </strong>        2、将这两个结果进行一个<strong>concat</strong>，再通过一个<strong>卷积层</strong>，得到结果h*w*1，对结果取<strong>sigmoid</strong>，此时我们获得了<strong>输入特征层每一个特征点的权值</strong>（0-1之间）</p> 
<p>                3、在获得这个权值后，我们将<strong>这个权值乘上原输入特征层即可。</strong></p> 
<p><img alt="" height="250" src="https://images2.imgbox.com/5b/d2/DoubM9Kl_o.png" width="760"></p> 
<p style="text-align:center;"> 图8.CBAM的空间注意力部分</p> 
<p><span style="color:#fe2c24;"><strong>        ECAnet（通道注意力机制）：</strong></span></p> 
<p><span style="color:#fe2c24;"><strong>      </strong></span><span style="color:#0d0016;">  ECANet可以看作是SENet的改进版，ECAnet的作者认为，</span><strong>捕获所有通道的依赖关系是低效并且是不必要的，</strong>没有必要对所有输入信息都采用全连接，而是将全连接层改为了1D卷积，其他过程同SENet。   </p> 
<p class="img-center"><img alt="" height="280" src="https://images2.imgbox.com/14/32/bB6ZvlLh_o.png" width="509"></p> 
<p style="text-align:center;">图9.ECA net结构</p> 
<p><strong>注意力机制是一个即插即用的模块，理论上可以放在任何一个特征层后面</strong>，可以放在主干网络，也可以放在加强特征提取网络。一般在<strong>主干网络提取出有效特征层</strong>后我们会增加一个注意力机制。</p> 
<h4>        网络模型</h4> 
<p>     <strong>   1.编码器模块</strong></p> 
<p><strong>        </strong>基于 ResNet34 构成, 包括一<strong>个输入卷积层</strong>,<strong> 4 个残差卷积模块 ResConv1~ResConv4</strong>. 输入图像尺寸统一为 <strong>512×512×3,</strong> 输入卷积层采用 <strong>7×7×64</strong> 卷积, <strong>步长为 2</strong>; ResConv1~ResConv4 都采用了<strong> 3×3 </strong>的卷积核, 通道数分别为<strong> 64, 128, 258 </strong>和 <strong>512</strong>. 每经过一个残差卷积模块, 输出特征图尺寸缩小为输入特征图的<strong> 1/2</strong> , 通道数<strong>增加一倍</strong>. 经过编码器后得到的特征图尺寸为<strong> 16×16×512</strong>.</p> 
<p>        <strong>2.AFM</strong></p> 
<p><strong>        </strong>本文在编码器获得高层特征之后不再采用池化操作, 而是通过分层的连续卷积操作来提取更高层特征, 并将各层的特征进行相加融合, 然后与进行了 1×1 卷积操作的高层特征相乘, 构成 AFM 中的注意力模块。</p> 
<p>        输入是编码器输出的高层特征(尺寸为 16×16×512), 该特征被同时输入到上中下3条不同支路: <strong>上层支路对输入先进行全局池化</strong>(global pooling, GP)操作, <strong>再进行 1×1 卷积</strong>操作 Conv1×1; <strong>中层支路直接对输入进行 1×1 卷积操作</strong> Conv1×1; 下层支路是注意力模块的核心, 首先通过分层的卷积组(convolutional group, ConvG) 模块提取更高层特征, 上一层 ConvG 的输出作为下一层 ConvG 的输入, 共分 3 层. <strong>ConvG 采用不同的卷积核对输入特征图分别进行卷积操作, 再将结果相加融合输出</strong>, 其结构如图 b 所示. <strong>ConvG1 采用 7×7 和 3×3 步长均为 2 的 2 种卷积核</strong>,<strong> ConvG2 采用 5×5 和 3×3 步长均为 2 的 2 种卷积核</strong>, <strong>ConvG3 只采用了一种 3×3 步长为 2 的卷积核</strong>. 前 2 层卷积组模块的输出除了送入下一层卷积组模块之外, 还会再分别进行一次卷积核分别为 7×7 和 5×5 卷 积操作 Conv7×7 和 Conv5×5, 第 3 层输出需再进行 一次卷积核为 3×3 的卷积操作 Conv3×3. 下层支路 各卷积操作的输出在尺度统一后进行相加融合, 再与中层支路输出进行相乘, 最后与上层支路输 出相加, 从而得到 AFM 的输出。</p> 
<p class="img-center"><img alt="" height="379" src="https://images2.imgbox.com/44/c0/xVzbNm0R_o.png" width="500"></p> 
<p class="img-center"><img alt="" height="182" src="https://images2.imgbox.com/a8/d8/qqFEKsHZ_o.png" width="300"></p> 
<p>         <strong>3.ADM</strong></p> 
<p>        ADM 分为 2 部分, 左边部分将低层特征和高层特征相加融合; 右边部分对融合后的特征进行 基于注意力机制的特征提取, 包括通道注意力模块(channel attention module, CAM)和空间注意力 模块(spatial attention module, SAM) 2 个子模块,</p> 
<p>        1、CAM 实现过程: 首先, 对输入特征分别进行平均池化(average pooling, AvgP)和最大池化(max pooling, MaxP)操作; 然后, 将池化结果分别通过 MLP; 接着, 将 2 部分结果相加; 最后, 与输入的特征相乘融合得到通道注意力特征.</p> 
<p>        2、SAM 实现过程: 首先, 对输入的特征分别进行 AvgP 和 MaxP 操作; 然后, 将 2 种池化结果组 合后进行 5×5 卷积操作 Conv; 最后, 再与输入特征相乘融合</p> 
<p>        CAM 和 SAM 得到的特征相加融合后成为 ADM 的输出. ADM 通过对低高层特征融合后, 再 进行基于注意力机制的特征生成, 可以有效地恢 复裂缝的细节信息.</p> 
<p class="img-center"><img alt="" height="214" src="https://images2.imgbox.com/70/96/zXCEoFcC_o.png" width="500"></p> 
<p style="text-align:center;">图10.ADM结构 </p> 
<p style="text-align:center;"><img alt="" height="399" src="https://images2.imgbox.com/7a/95/RN2VenNe_o.png" width="500"></p> 
<p style="text-align:center;">图11.CAM结构和SAM结构</p> 
<hr> 
<h2>实验</h2> 
<p>        数据集采用：<strong>CFD</strong>（在原裂缝图像的基础上通过添 加随机高斯噪声、颜色变换以及随机裁剪和旋转又生成了 118 幅裂缝图像及其对应的 GT） 和<strong> CRACK500</strong>（将每 幅图像分割成不重叠的 16 个子图像, 保留其中裂 缝像素数大于1000 的图像, 从而得到的裂缝数据 集中训练集有 1896 幅裂缝图像,）</p> 
<p><strong>     <span style="color:#fe2c24;">   网络模型的训练、验证和测试的输入图像像素大小统一调整为512×512×3。</span></strong></p> 
<p>        在训练过程中, <strong>batchsize 设置为 2</strong>, <strong>shuffle 设置 为 True,</strong> <strong>epoch设置为 200</strong>, 初始学习率为 0.000 2, 优化器选用 Adam, 损失函数采用 BCE_loss 函数. 训练过程中, 当 10 轮中训练集的损失函数值都比当前最小损失函数值大时, 对学习率进行更新, 将学习率减半. 如果在 20 轮中训练集或验证集的损失函数都比当前最小损失函数值大, 或者学习率小于 5×1e-7, 则训练提前结束。</p> 
<p>        以下是作者的实验数据：</p> 
<p class="img-center"><img alt="" height="296" src="https://images2.imgbox.com/c7/79/kuMePp3G_o.png" width="500"></p> 
<p class="img-center"><img alt="" height="186" src="https://images2.imgbox.com/b1/36/xDgBGLVY_o.png" width="500"></p> 
<p class="img-center"><img alt="" height="346" src="https://images2.imgbox.com/84/cd/MA5Pn0j5_o.png" width="500"></p> 
<p></p> 
<p><span style="color:#fe2c24;"><strong> 除了与常见的深度学习网络模型对比之外，作者还与以下几个方法做了对比：</strong></span></p> 
<p><strong>1、ACNet </strong> </p> 
<p><strong>2、Canny  </strong></p> 
<p><strong>3、TuFF</strong></p> 
<p><strong>4、文献：基于RPCA和视觉显著性的风机叶片表面缺陷检测  </strong></p> 
<p> 对比数据：</p> 
<p class="img-center"><img alt="" height="304" src="https://images2.imgbox.com/58/e3/IfcdOFXy_o.png" width="500"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c1a141a43b41d1dd8a615f5aaca74a4b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">STM32之音乐播放器</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c3ae5917206c5ffa8b5d9b8bbb33d8c0/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">go tool link: fork/exec /usr/local/go/pkg/tool/darwin_amd64/link: no such file or directory</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>