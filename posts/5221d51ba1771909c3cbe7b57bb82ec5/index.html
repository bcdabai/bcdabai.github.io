<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>《Graph neural networks A review of methods and applications》翻译 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="《Graph neural networks A review of methods and applications》翻译" />
<meta property="og:description" content="1.介绍
论文研究了gnn中的其他计算模块，如跳过连接和池化操作。
Skip Connections（或 Shortcut
Connections），跳过连接，会跳过神经网络中的某些层，并将一层的输出作为下一层的输入。
gnn分为四类:循环图神经网络、卷积图神经网络、图自编码器和时空图神经网络。
论文讨论了异构图、动态图和组合优化的gnn
论文的贡献是:提供了对现有图神经网络模型的详细回顾。提出了一个通用的设计管道，并讨论了每个模块的变体。本文还介绍了GNN模型的理论研究和实证分析。我们对应用程序进行系统分类，将应用程序分为结构化场景和非结构化场景。我们针对每个场景给出了几个主要的应用程序及其相应的方法。我们提出四个有待进一步研究的问题。我们对每个问题进行了深入的分析，并提出了未来的研究方向。
2.GNN模型设计的一般流程：
该流程包括四个步骤:(1)查找图的结构，(2)确定图的类型和比例，(3)设计损失函数，(4)使用计算模块构建模型。
2.1查找图结构
必须找出应用程序中的图结构。通常有两种场景:结构化场景和非结构化场景。在结构型场景中，图形结构在分子、物理系统、知识图等应用中是明确的。在非结构化场景中，图是隐式的，因此我们必须首先从任务中构建图，例如为文本构建一个完全连接的“单词”图，或为图像构建一个场景图。在我们得到这个图之后，后面的设计过程试图在这个特定的图上找到一个最优的GNN模型。
2.2确定图的类型和比例
在应用程序中获得图形后，我们必须找出图形类型及其比例。具有复杂类型的图可以提供关于节点及其连接的更多信息。图表通常被分类为:
有向图/无向图：有向图中的边都是从一个节点指向另一个节点，这比无向图提供了更多的信息。无向图中的每条边也可以看作是两条有向边。
同构图/异构图：同构图中的节点和边具有相同的类型，异构图中的节点和边具有不同的类型。节点和边的类型在异构图中起着重要的作用，需要进一步考虑。
静态图/动态图：当图的输入特征或拓扑随时间变化时，将图视为动态图。在动态图中应该仔细考虑时间信息。
注意，这些类别是正交的，这意味着这些类型可以组合，例如，可以处理动态有向异构图。还有一些其他的图类型是为不同的任务设计的，比如超图和符号图。我们不会在这里列举所有类型，但最重要的想法是考虑这些图表提供的附加信息。一旦我们指定了图形类型，在设计过程中应该进一步考虑这些图形类型提供的附加信息。在图的尺度上，对于小图和“大”图并没有明确的分类标准。这个标准仍然随着计算设备(例如gpu的速度和内存)的发展而变化。在本文中，当一个图(空间复杂度isOðn2Þ)的邻接矩阵或图拉普拉斯算子不能被设备存储和处理时，我们将该图视为一个大规模图，然后考虑一些采样方法。
2.3设计损失函数
在这一步中，我们需要根据我们的任务类型和训练设置来设计损失函数。对于图学习任务，通常有三种任务:
节点层数：任务以节点为中心，包括节点分类、节点回归、节点聚类等。节点分类试图将节点分类为几个类，节点回归预测每个节点的连续值。节点聚类的目的是将节点划分为几个不相交的组，相似的节点应该在同一组中。
边层数：任务是边缘分类和链接预测，这需要模型对边缘类型进行分类或预测给定的两个节点之间是否存在一条边
图层数：任务包括图分类、图回归和图匹配，所有这些都需要模型来学习图的表示。
从监督的角度，我们也可以将图学习任务分为三种不同的训练设置:
监督环境：为训练提供标记数据
半监督环境：为训练提供了少量的标记节点和大量的未标记节点。在测试阶段，转导设置要求模型预测给定的未标记节点的标签，而归纳设置提供来自相同分布的新的未标记节点来推断。大多数节点和边缘分类任务是半监督的。最近，wang和Leskovec(2020)以及rossi等人(2018)提出了一种混合的转导-诱导方案，寻求通向混合环境的新途径。
非监督环境：只提供未标记的数据供模型查找模式。节点聚类是典型的无监督学习任务。
通过任务类型和训练设置，我们可以为任务设计一个特定的损失函数。例如，对于节点级半监督分类任务，交叉熵损失函数可以用于训练集中标记的节点。
2.4使用计算模块构建模型
最后，我们可以开始使用计算模块构建模型。一些常用的计算模块有:
传播模块：传播模块用于在节点之间传播信息，从而聚合的信息既可以捕获特征信息，也可以捕获拓扑信息。在传播模块中，通常使用卷积算子和递归算子来聚合邻居的信息，而跳过连接操作则用来从节点的历史表示中收集信息，从而缓解过平滑问题
采样模块：当图较大时，通常需要采样模块对图进行传播。采样模块通常与传播模块结合使用。
池化模块：当我们需要高级子图或图的表示时，需要池模块从节点中提取信息
利用这些计算模块，通常将它们结合起来建立一个典型的GNN模型。图的中间部分给出了GNN模型的典型结构。其中利用卷积算子、递归算子、采样模块和跳跃连接在每一层传播信息，然后加入池化模块提取高层信息。这些层通常是堆叠起来以获得更好的表示。注意，这种架构可以推广大多数GNN模型，但也有例外，例如，NDCN
(zan和Wang,
2020)结合了常微分方程系统(ode)和GNN。它可以被看作是一个连续时间的GNN模型，在连续时间内集成GNN层，而不通过一个离散的层数传播。图中显示了通用设计管道的说明。
3.计算模块的实例化
在本节中，我们将介绍三个计算模块的现有实例化:传播模块、采样模块、和池化模块。我们分别在3.1节、3.2节和3.3节中介绍了传播模块的三个子组件:卷积算子、递归算子和跳跃连接。然后在3.4节和3.5节中介绍了采样模块和池模块。GNN模型的一般设计管道和计算模块的概述如图所示
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Vga46pqA-1638176337186)(media/cf6c624f2cbfe45a80388f36ee8c3c71.png)]
GNN模型的一般设计管道
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-XogT6oK8-1638176337190)(media/20fbc7992e7d7d79bcd5dacff7fe7029.png)]
计算模块的概述
3.1传播模块-卷积算子
本节介绍的卷积算子是GNN模型中最常用的传播算子。卷积算子的主要思想是将卷积从其他域推广到图域。这方面的进展通常分为光谱方法和空间方法。
3.1.1谱方法
谱方法使用图的谱表示。这些方法在理论上基于图信号处理(Shuman et al.，
2013)，并在谱域定义卷积算子。在谱方法中，首先通过图的傅里叶变换f将图信号变换到谱域，然后进行卷积运算。卷积后，结果信号用反图傅里叶变换f
’ 1进行变换。这些转换定义为:
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jFmEvjzr-1638176337192)(media/50892b9a0d95ed715a1abd8babf20312.png)]
这里是归一化图拉普拉斯特征向量的矩阵：（其中为N维单位向量）D是度矩阵，是图的邻接矩阵，归一化图拉普拉斯算子是实对称正半正定的，所以它可以分解为
Λ是特征值的对角矩阵，基于卷积定理(Mallat, 1999)，将卷积运算定义为:
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-wT4gajlR-1638176337195)(media/ee7393a65381a21fa2bcf3337f0b1de2.png)]
其中为光谱域的滤波器，如果我们简化过滤器通过使用一个可学习的对角矩阵，然后我们有了光谱方法的基本函数：
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jPAFuAf5-1638176337197)(media/aee2a34825acc14d08f5c04807d632dc.png)]
接着介绍了几种设计不同滤波器的典型光谱方法 。
光谱网络：光谱网络使用可学习对角矩阵作为滤波器，这是=diag(w),
是作为参数，然而，这种操作的计算效率很低，而且过滤器是非空间本地化的。Henaff等人(2015)试图通过引入光滑系数参数化来实现光谱滤波器的空间局部化。
ChebNet：Hammond等人(2011)认为可以通过切比雪夫多项式上升到排序，Defferrard等(2016)基于这一理论提出了ChebNet。因此，这个运算可以写成：
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2R6aldaZ-1638176337199)(media/a97e0d69d79f7231fef813fba3b8d51f.png)]
[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ONTYySV9-1638176337202)(media/20b5204605289825fadacc359eff1884.png)]表示L的最大特征值。的取值范围是[-1,1],
是切比雪夫系数的向量。
切比雪夫多项式定义为当=1和=x
由于在拉普拉斯变换中是一个k阶多项式，因此可以看出操作风险是局部化的。Defferrard等人(2016)使用这种k" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5221d51ba1771909c3cbe7b57bb82ec5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-29T17:03:20+08:00" />
<meta property="article:modified_time" content="2021-11-29T17:03:20+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《Graph neural networks A review of methods and applications》翻译</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>1.介绍</p> 
<p>论文研究了gnn中的其他计算模块，如跳过连接和池化操作。</p> 
<p><em>Skip Connections</em>（或 Shortcut<br> Connections），跳过连接，会跳过神经网络中的某些层，并将一层的输出作为下一层的输入。</p> 
<p>gnn分为四类:循环图神经网络、卷积图神经网络、图自编码器和时空图神经网络。</p> 
<p>论文讨论了异构图、动态图和组合优化的gnn</p> 
<p>论文的贡献是:提供了对现有图神经网络模型的详细回顾。提出了一个通用的设计管道，并讨论了每个模块的变体。本文还介绍了GNN模型的理论研究和实证分析。我们对应用程序进行系统分类，将应用程序分为结构化场景和非结构化场景。我们针对每个场景给出了几个主要的应用程序及其相应的方法。我们提出四个有待进一步研究的问题。我们对每个问题进行了深入的分析，并提出了未来的研究方向。</p> 
<p>2.GNN模型设计的一般流程：</p> 
<p>该流程包括四个步骤:(1)查找图的结构，(2)确定图的类型和比例，(3)设计损失函数，(4)使用计算模块构建模型。</p> 
<p>2.1查找图结构</p> 
<p>必须找出应用程序中的图结构。通常有两种场景:结构化场景和非结构化场景。在结构型场景中，图形结构在分子、物理系统、知识图等应用中是明确的。在非结构化场景中，图是隐式的，因此我们必须首先从任务中构建图，例如为文本构建一个完全连接的“单词”图，或为图像构建一个场景图。在我们得到这个图之后，后面的设计过程试图在这个特定的图上找到一个最优的GNN模型。</p> 
<p>2.2确定图的类型和比例</p> 
<p>在应用程序中获得图形后，我们必须找出图形类型及其比例。具有复杂类型的图可以提供关于节点及其连接的更多信息。图表通常被分类为:</p> 
<p>有向图/无向图：有向图中的边都是从一个节点指向另一个节点，这比无向图提供了更多的信息。无向图中的每条边也可以看作是两条有向边。</p> 
<p>同构图/异构图：同构图中的节点和边具有相同的类型，异构图中的节点和边具有不同的类型。节点和边的类型在异构图中起着重要的作用，需要进一步考虑。</p> 
<p>静态图/动态图：当图的输入特征或拓扑随时间变化时，将图视为动态图。在动态图中应该仔细考虑时间信息。</p> 
<p>注意，这些类别是正交的，这意味着这些类型可以组合，例如，可以处理动态有向异构图。还有一些其他的图类型是为不同的任务设计的，比如超图和符号图。我们不会在这里列举所有类型，但最重要的想法是考虑这些图表提供的附加信息。一旦我们指定了图形类型，在设计过程中应该进一步考虑这些图形类型提供的附加信息。在图的尺度上，对于小图和“大”图并没有明确的分类标准。这个标准仍然随着计算设备(例如gpu的速度和内存)的发展而变化。在本文中，当一个图(空间复杂度isOðn2Þ)的邻接矩阵或图拉普拉斯算子不能被设备存储和处理时，我们将该图视为一个大规模图，然后考虑一些采样方法。</p> 
<p>2.3设计损失函数</p> 
<p>在这一步中，我们需要根据我们的任务类型和训练设置来设计损失函数。对于图学习任务，通常有三种任务:</p> 
<p>节点层数：任务以节点为中心，包括节点分类、节点回归、节点聚类等。节点分类试图将节点分类为几个类，节点回归预测每个节点的连续值。节点聚类的目的是将节点划分为几个不相交的组，相似的节点应该在同一组中。</p> 
<p>边层数：任务是边缘分类和链接预测，这需要模型对边缘类型进行分类或预测给定的两个节点之间是否存在一条边</p> 
<p>图层数：任务包括图分类、图回归和图匹配，所有这些都需要模型来学习图的表示。</p> 
<p>从监督的角度，我们也可以将图学习任务分为三种不同的训练设置:</p> 
<p>监督环境：为训练提供标记数据</p> 
<p>半监督环境：为训练提供了少量的标记节点和大量的未标记节点。在测试阶段，转导设置要求模型预测给定的未标记节点的标签，而归纳设置提供来自相同分布的新的未标记节点来推断。大多数节点和边缘分类任务是半监督的。最近，wang和Leskovec(2020)以及rossi等人(2018)提出了一种混合的转导-诱导方案，寻求通向混合环境的新途径。</p> 
<p>非监督环境：只提供未标记的数据供模型查找模式。节点聚类是典型的无监督学习任务。</p> 
<p>通过任务类型和训练设置，我们可以为任务设计一个特定的损失函数。例如，对于节点级半监督分类任务，交叉熵损失函数可以用于训练集中标记的节点。</p> 
<p>2.4使用计算模块构建模型</p> 
<p>最后，我们可以开始使用计算模块构建模型。一些常用的计算模块有:</p> 
<p>传播模块：传播模块用于在节点之间传播信息，从而聚合的信息既可以捕获特征信息，也可以捕获拓扑信息。在传播模块中，通常使用卷积算子和递归算子来聚合邻居的信息，而跳过连接操作则用来从节点的历史表示中收集信息，从而缓解过平滑问题</p> 
<p>采样模块：当图较大时，通常需要采样模块对图进行传播。采样模块通常与传播模块结合使用。</p> 
<p>池化模块：当我们需要高级子图或图的表示时，需要池模块从节点中提取信息</p> 
<p>利用这些计算模块，通常将它们结合起来建立一个典型的GNN模型。图的中间部分给出了GNN模型的典型结构。其中利用卷积算子、递归算子、采样模块和跳跃连接在每一层传播信息，然后加入池化模块提取高层信息。这些层通常是堆叠起来以获得更好的表示。注意，这种架构可以推广大多数GNN模型，但也有例外，例如，NDCN<br> (zan和Wang,<br> 2020)结合了常微分方程系统(ode)和GNN。它可以被看作是一个连续时间的GNN模型，在连续时间内集成GNN层，而不通过一个离散的层数传播。图中显示了通用设计管道的说明。</p> 
<p>3.计算模块的实例化</p> 
<p>在本节中，我们将介绍三个计算模块的现有实例化:传播模块、采样模块、和池化模块。我们分别在3.1节、3.2节和3.3节中介绍了传播模块的三个子组件:卷积算子、递归算子和跳跃连接。然后在3.4节和3.5节中介绍了采样模块和池模块。GNN模型的一般设计管道和计算模块的概述如图所示</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Vga46pqA-1638176337186)(media/cf6c624f2cbfe45a80388f36ee8c3c71.png)]</p> 
<p>GNN模型的一般设计管道</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-XogT6oK8-1638176337190)(media/20fbc7992e7d7d79bcd5dacff7fe7029.png)]</p> 
<p>计算模块的概述</p> 
<p>3.1传播模块-卷积算子</p> 
<p>本节介绍的卷积算子是GNN模型中最常用的传播算子。卷积算子的主要思想是将卷积从其他域推广到图域。这方面的进展通常分为光谱方法和空间方法。</p> 
<p>3.1.1谱方法</p> 
<p>谱方法使用图的谱表示。这些方法在理论上基于图信号处理(Shuman et al.，<br> 2013)，并在谱域定义卷积算子。在谱方法中，首先通过图的傅里叶变换f将图信号变换到谱域，然后进行卷积运算。卷积后，结果信号用反图傅里叶变换f<br> ’ 1进行变换。这些转换定义为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jFmEvjzr-1638176337192)(media/50892b9a0d95ed715a1abd8babf20312.png)]</p> 
<p>这里是归一化图拉普拉斯特征向量的矩阵：（其中为N维单位向量）D是度矩阵，是图的邻接矩阵，归一化图拉普拉斯算子是实对称正半正定的，所以它可以分解为<br> Λ是特征值的对角矩阵，基于卷积定理(Mallat, 1999)，将卷积运算定义为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-wT4gajlR-1638176337195)(media/ee7393a65381a21fa2bcf3337f0b1de2.png)]</p> 
<p>其中为光谱域的滤波器，如果我们简化过滤器通过使用一个可学习的对角矩阵，然后我们有了光谱方法的基本函数：</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jPAFuAf5-1638176337197)(media/aee2a34825acc14d08f5c04807d632dc.png)]</p> 
<p>接着介绍了几种设计不同滤波器的典型光谱方法 。</p> 
<p>光谱网络：光谱网络使用可学习对角矩阵作为滤波器，这是=diag(w),</p> 
<p>是作为参数，然而，这种操作的计算效率很低，而且过滤器是非空间本地化的。Henaff等人(2015)试图通过引入光滑系数参数化来实现光谱滤波器的空间局部化。</p> 
<p>ChebNet：Hammond等人(2011)认为可以通过切比雪夫多项式上升到排序，Defferrard等(2016)基于这一理论提出了ChebNet。因此，这个运算可以写成：</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2R6aldaZ-1638176337199)(media/a97e0d69d79f7231fef813fba3b8d51f.png)]</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ONTYySV9-1638176337202)(media/20b5204605289825fadacc359eff1884.png)]表示L的最大特征值。的取值范围是[-1,1],</p> 
<p>是切比雪夫系数的向量。</p> 
<p>切比雪夫多项式定义为当=1和=x</p> 
<p>由于在拉普拉斯变换中是一个k阶多项式，因此可以看出操作风险是局部化的。Defferrard等人(2016)使用这种k<br> - localized来定义卷积神经网络，它可以消除计算拉普拉斯特征向量的需要。</p> 
<p>Kipf简化了Eq中的卷积运算，当K=1来缓解过拟合的问题，他们进一步假设，并将方程简化成</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-w3Abxdzj-1638176337204)(media/36c1b2bb4f4cc945feee9c72764d3984.png)]</p> 
<p>有两个自由参数w0和w1。使用参数约束w=w0=-w1下，我们可以得到以下表达式:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-R8rt0GZj-1638176337207)(media/e1921297b1eb2220887c0c8f02679be5.png)]</p> 
<p>GCN进一步引入了正则化技巧来解决Eq中的爆炸/消失梯度问题[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-O3AMfrjE-1638176337210)(media/275c13aae694bde7cbfd368b30aaae0c.png)]有[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pQ5KmVka-1638176337212)(media/29fa95e19d8c42f9ce7579af01d0c0b3.png)]最后，GCN的紧凑型定义为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-lhoS1ijG-1638176337214)(media/fe9b015c3222a8fa0a94f4f99fa42796.png)]</p> 
<p>其中[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-PxPzpIV7-1638176337217)(media/a78072d662537ff495070257570fd89d.png)]是输入矩阵，[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-3Oiq7WUw-1638176337219)(media/4b7f33ac2dccd2b743db17012fcd81de.png)]是参数，并且</p> 
<p>是卷积矩阵，F和F’分别是输入和输出的尺寸，注意GCN也可以被视为一种空间方法，将在后面讨论。</p> 
<p>AGCN：所有这些模型都使用原始的图结构来表示节点之间的关系。然而，不同节点之间可能存在隐式关系。自适应图卷积网络(Adaptive<br> Graph Convolution Network, AGCN)被提出学习底层关系(Li et al.，<br> 2018a)。AGCN学习一个“残差”图拉普拉斯矩阵，并将其添加到原始拉普拉斯矩阵中。结果表明，该方法在几种图结构数据集上是有效的。</p> 
<p>DGCN：双重图卷积网络(DGCN) (Zhuang and Ma,<br> 2018)被提出来联合考虑图的局部一致性和全局一致性。该算法使用两个卷积网络捕获局部和全局一致性，并采用无监督损失对它们进行集成。第一个卷积网络与式(7)相同，第二个网络用正的PPMI<br> (point - twise mutual information)矩阵替换邻接矩阵:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-cgnuBypT-1638176337222)(media/c1f40af14b2bedc92ce5b8bc81c0d775.png)]</p> 
<p>其中Ap为PPMI矩阵，Dp为Ap的对角度矩阵</p> 
<p>GWNN：图小波神经网络(GWNN) (Xu et al.，<br> 2019a)采用图小波变换代替图傅立叶变换。它具有以下几个优点:(1)无需矩阵分解即可快速得到图小波;(2)图小波是稀疏的和局域的，因此结果更好，更容易解释。在半监督节点分类任务中，GWNN优于几种谱方法。</p> 
<p>ACGN和DGCN试图从增广图拉普拉斯的角度改进了光谱方法，而GWNN代替了傅里叶变换。总之，光谱方法很好地基于理论，最近也提出了一些理论分析(见第7.1.1节)。然而，在上述几乎所有的光谱方法中，学习的滤波器依赖于图结构。也就是说，过滤器不能应用于具有不同结构的图，而那些模型只能在图任务的“转换”设置下应用。</p> 
<p>3.1.2基本空间的方法</p> 
<p>空间方法基于图拓扑直接在图上定义卷积。空间方法的主要挑战是定义不同大小的邻域的卷积操作，并保持cnn的局部不变性。</p> 
<p>Neural FPs：Neural FPs对不同程度的节点使用不同的权重矩阵:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-fB7zdkKV-1638176337223)(media/70398a434f41aa574c840ef04fda413c.png)]</p> 
<p>其中 作为节点权重矩阵。该方法的主要缺点是不能应用于具有更多节点度的大规模图</p> 
<p>DCNN：扩散卷积神经网络(DCNN) (Atwood and Towsley,<br> 2016)使用过渡矩阵为节点定义邻域。对于节点分类，图中每个节点的扩散表示可以表示为:<br> [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-onFzo47A-1638176337225)(media/82e19667e506605b941682aa515691f4.png)]</p> 
<p>是输入特征矩阵（F是维数）P*是一个N*K*N的张量，其中包含矩阵P的幂级数{P，P^2,…，P^k}并且P是图邻接矩阵A的度归一化转换矩阵，每个实体被转换为扩散卷积表示，即K*F矩阵定义为???</p> 
<p>PATCHY-SAN: PATCHY-SAN模型(Niepert et al.，<br> 2016)精确地提取并规范化每个节点的邻域。在传统的卷积运算中，归一化邻域作为接收场。</p> 
<p>LGCN: 可学习图卷积网络(LGCN) (Gao et al.，<br> 2018a)也利用cnn作为聚合器。它对节点的邻域矩阵进行最大池化，得到top-k个特征元素，然后应用一维CNN计算隐藏表示</p> 
<p>可学习图卷积网络(LGCN) (Gao et al.，<br> 2018a)也利用cnn作为聚合器。它对节点的邻域矩阵进行最大池化，得到top-k个特征元素，然后应用一维CNN计算隐藏表示</p> 
<p>GraphSAGE: GraphSAGE<br> (Hamilton等人，2017a)是一个通用的归纳框架，通过从节点的局部邻域取样和聚合特征来生成嵌入:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-3WGtuMLd-1638176337228)(media/9de2ceb873c5689b90efadebdf1ef581.png)]</p> 
<p>GraphSAGE不使用完整的领域集，而是统一采样固定大小的邻居集来聚合信息。是聚合函数，GraphSAGE提出了三种聚合器:均值聚合器、LSTM聚合器和池化聚合器。带有平均聚合器的GraphSAGE可以看作是GCN的归纳版本，而LSTM聚合器不是置换不变的，它需要指定节点的顺序。</p> 
<p>3.1.3引起空间的方法</p> 
<p>注意机制已成功应用于许多序列任务，如机器翻译(Bahdanau et al.， 2015;Gehring et<br> al.， 2017;Vaswani et al.， 2017)，机器阅读(Cheng et al.，<br> 2016)等。还有几个模型试图在图上推广注意操作符(Velickovic et al.，<br> 2018;张等，2018c)。与前面提到的算子相比，基于注意的算子对邻居分配了不同的权重，从而可以缓解噪声，取得更好的效果。</p> 
<p>GAT：图表注意网络(GAT) (Velickovic et al.，<br> 2018)将注意机制整合到传播步骤中。它通过关注它的邻居来计算每个节点的隐藏状态，遵循自我注意策略。节点的隐藏状态可以通过以下方法获得:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ntdPMxF1-1638176337230)(media/a208e740e985dd1561ddfe078916baad.png)]</p> 
<p>其中W是权矩阵与应用于每个节点的线性变换相关联。<br> a是单层MLP的权向量，此外，GAT利用了vaswani等人(2017)使用的多头注意来稳定学习过程。它使用独立的注意力头矩阵来计算隐藏状态，然后连接它们的特征(或计算平均值)，导致以下两种情况输出的表示。</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ZqxQJVic-1638176337232)(media/d6db65e9ae4db4ff73fa5e928ae643d6.png)]</p> 
<p>这里是由第k次注意头计算的归一化注意系数。该注意结构具有以下几个特点:(1)节点-邻居对的计算是并行的，因此操作是高效的;(2)通过对相邻节点指定任意权重，可以应用于不同度的图节点;(3)易于应用于归纳学习问题。</p> 
<p>GaAN：门控式注意网络(GaAN) (Zhang et al.，<br> 2018c)也使用了多头注意机制。但是，它使用一种自我注意机制从不同的头部收集信息，取代GAT的平均操作。</p> 
<p>3.1.4空间方法的一般框架</p> 
<p>除了空间方法的不同变体外，还提出了几种通用框架，旨在将不同的模型整合到一个单一的框架中。Monti等人(2017)提出了混合模型网络(MoNet)，它是定义在图或流形上的几种方法的一般空间框架。Gilmer<br> et al.(2017)提出了消息传递神经网络(MPNN)，它使用消息传递函数统一多种变体。Wang<br> et al. (2018a)提出了非局部神经网络(non-local neural network,<br> NLNN)，它结合了几种“自我注意”式的方法(Hoshen, 2017;Vaswani et al.，<br> 2017;Velickovic et al.，<br> 2018)。Battaglia等(2018)提出了图网络(GN)。它定义了一个更通用的框架，用于学习节点级、边级和图级表示</p> 
<p>MoNet: 混合模型网络(MoNet)<br> (Monti等人，2017)是一个空间框架，试图统一非欧几里德域的模型，包括流形和gnn的cnn。在流形上的Geodesic<br> CNN (GCNN) (Masci等人，2015年)和各向异性CNN (ACNN)<br> (Boscaini等人，2016年)或图上的GCN (Kipf和Welling, 2017年)和DCNN<br> (Atwood和Towsley,<br> 2016年)可以作为莫奈的具体实例。在莫奈的作品中，流形上的每一点或图上的每一个顶点(用v表示)都被认为是伪坐标系的原点。</p> 
<p>与伪坐标uðv;uÞ相关联的neighborsu2Nvare。给定在图(或流形上的点)的顶点上定义的两个函数f,g，在MoNet中卷积算子定义为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-14hU3x1X-1638176337235)(media/0d9051d4b1b82268ef442d2e0cb6131c.png)]</p> 
<p>给定w1(u),w2(u)….wj(u)函数根据伪坐标为邻居分配权重，因此为邻居函数的聚合值，通过定义不同的u和w，MoNet可以实例化几种方法。对于GCN，函数f,g将节点映射到他们的特征。[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ElA7Nt1h-1638176337237)(media/624b8fee5a26a853bfe893abb3cc3a26.png)]</p> 
<p>MPNN：消息传递神经网络(MPNN) (Gilmer et al.，<br> 2017)提取了几个经典模型的一般特征。该模型包含两个阶段:消息传递阶段和读出阶段。</p> 
<p>在消息传递阶段，模型首先使用消息函数，Mt来聚合消息。从邻居并且使用更新函数来更新隐藏隐藏状态</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-IlEfQiUV-1638176337239)(media/b635551cc67be73516dfef84080b37ea.png)]</p> 
<p>给定表示无向边(v,u)的特征，读出阶段使用读出函数R计算出整个图的特征向量:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-tVDU7EFG-1638176337241)(media/7c53877ccfd67b89ac44f1a360f40400.png)]</p> 
<p>其中T表示总时间步长。消息函数Mt，顶点更新函数Ut和读取函数R可能有不同的设置，因此，MPNN框架可以通过不同的函数设置实例化几个不同的模型。不同模型的具体设置可以在(Gilmer<br> et al.， 2017)中找到。</p> 
<p>MLNN：非局部神经网络(NLNN)推广并扩展了计算机视觉中经典的非局部平均操作(Buades et<br> al.，<br> 2005)。非局部运算将一个位置上的隐藏状态计算为所有可能位置上特征的加权和。可能的位置可以是在空间，时间或时空中。因此，NLNN可以被看作是不同“自我关注”风格方法的统一(Hoshen,<br> 2017;Vaswani等人，2017;Velickovic等人，2018)。</p> 
<p>非局部神经网络(NLNN)推广并扩展了计算机视觉中经典的非局部平均操作(Buades et al.，<br> 2005)。非局部运算将一个位置上的隐藏状态计算为所有可能位置上特征的加权和。可能的位置可以是在空间，时间或时空中。因此，NLNN可以被看作是不同“自我关注”风格方法的统一(Hoshen,<br> 2017;Vaswani等人，2017;Velickovic等人，2018)。</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-OhF26T1v-1638176337243)(media/c51e520012a57ce7fd1e02181ad51387.png)]</p> 
<p>其中u是所有可能位置的索引，计算v和u之间的范围表示他们之间的关系。表示一个转换，[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-atPUXa5B-1638176337245)(media/43acf8f928660bab2882ca0e0368cd96.png)]是一个归一化因子。不同的NLNN变体可以通过不同的设置来定义，更多的细节可以在原始论文中找到(Buades<br> et al.， 2005)。</p> 
<p>Graph NetWork ：图网络(GN) (Battaglia et al.，<br> 2018)通过学习节点级、边级和图级表示，与其他框架相比，是一个更通用的框架。它可以统一许多变种，如MPNN,<br> NLNN，交互网络(Battaglia等人，2016;Watters等人，2017)，神经物理引擎(Chang等人，2017)，CommNet<br> (Sukhbaatar Ferguset al.， 2016)， structure2vec<br> (Dai等人，2016;Khalil等人，2017)，</p> 
<p>GN的核心计算单元称为GN块。一个GN块定义了三个更新函数和三个聚合函数</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ItBRcGht-1638176337247)(media/78c61e6a869c51ebd87e4ba676724349.png)]</p> 
<p>给定是接收节点，是边k的发送节点，。和分别是边向量和节点向量的堆叠矩阵。通过接收节点v收集边向量。u是图形表示的全局属性。φ和ρ函数可以有各种设置，而ρ函数必须不受输入顺序的影响，并且参数长度应该是可变的。</p> 
<p>3.2传播模块-循环算子</p> 
<p>循环法是这一研究领域的先驱。递归运算符和卷积运算符之间的主要区别是，卷积运算符中的层使用不同的权重，而在卷积运算符中的层使用不同的权重。递归算子的不同变体：</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-th926UaG-1638176337248)(media/1ff4572f31ac1dd0373da0a385b536ae.png)]</p> 
<p>递归运算符具有相同的权值。早期基于递归神经网络的方法专注于处理有向无环图(Sperduti和Starita,<br> 1997;Frasconi等人，1998;Micheli等人，2004;Hammer等人，2004)。随后，图神经网络(GNN)的概念在(Scarselli<br> et al.， 2009;Gori et al.，<br> 2005)中首次提出，扩展了现有的神经网络来处理更多的图类型。为了与通用名称区分，本文将该模型命名为gnnn。我们首先介绍了gnn及其后续的变种，它们需要隐藏状态的收敛，然后我们讨论了基于门机制的方法。</p> 
<p>3.2.1基于融合的方法</p> 
<p>在图中，每个节点都是由其特征和相关节点自然定义的。GNN的目标是学习嵌入[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-zhm1H0yi-1638176337251)(media/cc3b62bc98aafa8fb0c6f4c5bffd2089.png)]的状态，其中包含每个节点的邻居和自身的信息。节点的状态嵌入和维向量可以用来产生预测节点标签的分布等输出。则hvandovare的计算步骤定义为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-r0wTCEzd-1638176337253)(media/c3ca274f598fb0ae70c4598a860a1380.png)]</p> 
<p>其中[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-jaiwJ2sI-1638176337254)(media/86d591dbd0e6eb288b9174c4fbc36c7e.png)]分别是v的特征，它的边的特征，v的邻域节点的状态和特征。这里有一个参数函数，叫做局部跃迁函数。它在所有节点之间共享，并根据输入的邻域更新节点状态。G是描述如何产生输出的局部输出函数。注意，f和g都可以解释为前馈神经网络。设H、0、X和X分别是由所有状态、所有输出、所有特征和所有节点特征叠加而成的矩阵。然后我们得到一个紧凑形式为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-2LY8mzS8-1638176337257)(media/0d6eee22404f43d9581e4c87a0abe5ef.png)]</p> 
<p>其中，F全局转换函数和全局输出函数g分别是图中所有节点和的堆叠版本。H的值是Eq.(20)的不动点，并且是唯一定义的，假设fis收缩映射。根据Banach不动点定理(Khamsi<br> and Kirk, 2011)的建议，gnnus采用以下经典迭代格式来计算状态:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-IiTc1Byi-1638176337259)(media/2d1fd02c49faa85321a058401e03cb32.png)]</p> 
<p>其中表示h的第t次迭代。动力系统Eq.(21)以指数速度收敛于任意初值的解。虽然实验结果表明GNN是一种强大的结构数据建模架构，但仍然存在一些局限性:</p> 
<p>gnn要求是一个限制模型能力的收缩映射。迭代更新节点的隐状态到定点是低效的。如果我们专注于节点的表示而不是图，那么就不适合使用不动点，因为在不动点中表示的分布在值上更平滑，对于区分每个节点来说信息更少。</p> 
<p>GraphESN: 图回声状态网络(Graph echo state network, GraphESN) (Gallicchio and<br> Micheli, 2010)在图上概括了回声状态网络(回声状态网络，ESN) (Jaeger,<br> 2001)。它使用固定的压缩编码函数，只训练一个读出函数。储层动力学的收缩性保证了收敛性。因此，GraphESN比GNN更高效</p> 
<p>SEE: 随机稳态嵌入(SSE) (Dai et al.，<br> 2018a)也被提出来提高GNN的效率。SSE提出了一个包含两个步骤的学习框架。在更新步骤中，通过参数化算子更新每个节点的嵌入，并将这些嵌入投影到稳态约束空间以满足稳态条件。</p> 
<p>LP-GNN: 随机稳态嵌入(SSE) (Dai et al.，<br> 2018a)也被提出来提高GNN的效率。SSE提出了一个包含两个步骤的学习框架。在更新步骤中，通过参数化算子更新每个节点的嵌入，并将这些嵌入投影到稳态约束空间以满足稳态条件。</p> 
<p>3.2.2基于门的方法</p> 
<p>有几个研究试图在传播步骤中使用门机制，如GRU (Cho et al.， 2014)或LSTM<br> (Hochreiter and Schmidhuber,<br> 1997)，以减少GNN的计算限制，改善信息在图结构中的长期传播。他们在没有收敛性保证的情况下运行固定数量的训练步骤。</p> 
<p>GGNN：门控图神经网络(GGNN) (Li et al.，<br> 2016)的提出是为了缓解GNN的局限性。它释放了函数作为收缩映射的需求，并在传播步骤中使用了Gate循环单位(GRU)。它还使用反向时间传播(BPTT)来计算梯度。GGNN的计算步骤见表2</p> 
<p>节点v优先聚合来自邻居的消息。然后类似于gru的更新函数合并来自其他节点和前一个时间步的信息，以更新每个节点的隐藏状态。[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-aolsnFRl-1638176337262)(media/00587612b7d34972324e3df993d91717.png)]收集节点v的邻居信息。然而z和r是更新和重置门。通过基于树或图的传播过程，lstm也以类似于GRU的方式使用。</p> 
<p>树LSTM：Tai通过基于树或图的传播过程，lstm也以类似于GRU的方式使用。</p> 
<p>N-ary<br> Tree-LSTM。它们也是我们之前提到的基于递归神经网络模型的扩展。树是图的一种特殊情况，Tree-<br> lstm中的每个节点都从它的子节点收集信息。</p> 
<p>与传统LSTM中的单一遗忘门不同，tree<br> -LSTM单元为每个子节点包含一个忘记门。子和树lstm的计算步骤如表2所示。</p> 
<p>图LSTM。两种类型的tree - lstm可以很容易地适应图。在(Zayats和Ostendorf,<br> 2018)中的图结构LSTM是一个例子，然后是树LSTM应用于图。然而，这是一个简化的版本，因为图中的每个节点最多有2条传入边(来自其父节点和同级节点)。Peng等(2017)提出了另一种基于关系抽取任务的图LSTM的变体。(Peng<br> et al.， 2017)中的图的边有不同的标签，因此Peng et<br> al.(2017)利用不同的权重矩阵来表示不同的标签。InTable 2,mðv;kÞdenotes<br> nodevandk之间的边标签。Liang等人(2016)提出了一个Graph<br> LSTM网络来解决语义对象解析任务。该算法采用置信驱动方案自适应地选择起始节点，确定节点更新顺序。它遵循将现有lstm一般化为图结构数据的相同思想，但具有特定的更新顺序，而上面提到的方法与节点的顺序无关。S-LSTM。Zhang等(2018d)提出了句子LSTM<br> (S-LSTM)来改进文本编码。它将文本转换为图，并利用graph<br> LSTM来学习表示。S-LSTM在许多自然语言处理问题中表现出较强的表示能力。</p> 
<p>3.3 传播模块-跳过连接</p> 
<p>许多应用程序将图神经网络层展开或堆叠，目的是为了获得更好的结果，因为更多的层(即eklayers)使每个节点从邻居处聚合更多的信息。然而，在许多实验中发现，更深层次的模型并不能提高性能，甚至可能表现得更差。这主要是因为更多的层也可以从指数增长的扩展邻居成员传播噪声信息。它还会导致过度平滑问题，因为当模型深入时，在聚合操作之后，节点往往具有类似的表示。因此，许多方法试图添加“跳过连接”，使GNN模型更深入。在本小节中，我们将介绍三种跳跃连接的实例化。公路之下。Rahimi等人(2018年)提出了一个高速公路GCN，它使用类似于高速公路网络的分层门(Zilly等人，2016年)。一个层的输出和它的输入加上门控权重:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-q3o7IYgn-1638176337265)(media/9d139204264fdcc272a6dd7d10e3d7cb.png)]</p> 
<p>通过添加高速公路闸门，在(Rahimi et al.，<br> 2018)中讨论的特定问题中，性能在4层达到峰值。柱状网络(CLN) (Pham et al.，<br> 2017)也利用了公路网络。但是它有不同的函数来计算门权值。</p> 
<p>JKN。Xu et<br> al.(2018)研究了邻域聚集方案的性质和局限性。他们提出了跳跃知识网络(JKN)，它可以学习自适应和结构感知的表示。JKN从最后一层的每个节点的所有中间表示(即“跳转”到最后一层)中选择，这使得模型根据需要适应每个节点的有效邻域大小。Xu<br> et al.(2018)使用了三种方法:连接、最大池和lstm<br> -注意在实验中聚集信息。JKN在社会、生物信息学和引文网络的实验中表现良好。它还可以与GCN、GraphSAGE和GAT等模型相结合，以提高性能。</p> 
<p>DeepGCNs。Li等人(2019a)借鉴了ResNet (He et al.， 2016a,2016b)和DenseNet (Huang<br> et al.，<br> 2017)的想法。ResGCN和DenseGCN通过结合残余连接和密集连接来解决消失梯度和过平滑问题。具体来说，ResGCN和DenseGCN中节点的隐藏状态可以计算为:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-25HuluXN-1638176337267)(media/aa91cf16adc42d9ac0dd58fe2ef54a86.png)]</p> 
<p>在点云语义分割任务上进行了DeepGCNs实验，在56层模型下取得了最好的结果。</p> 
<p>3.4采样模块</p> 
<p>GNN模型将来自前一层邻居的每个节点的消息聚合在一起。直观地说，如果我们跟踪多个GNN层，支持邻居的大小将随着深度呈指数增长。缓解这种“邻居爆炸”问题，一个有效而有效的方法是抽样。另外，当我们处理大图时，我们不能总是存储和处理每个节点的所有邻域信息，因此需要采样模块进行传播。在本节中，我们将介绍三种图抽样模块:节点抽样、层抽样和子图抽样。</p> 
<p>3.4.1节点采样</p> 
<p>减少邻近节点大小的一种直接方法是从每个节点的邻域中选择一个子集。GraphSAGE<br> (Hamilton et al.，<br> 2017a)样本固定了少量的邻居，确保每个节点的邻域大小为2到50。为了减少抽样方差，Chen等人(2018a)利用节点的历史激活作为控制变量，为GCN引入了基于控制变量的随机逼近算法。这种方法限制了接收域在1-hop邻居，并使用历史隐藏状态作为一个负担得起的近似。PinSage<br> (Ying et al.，<br> 2018a)提出了基于重要性的抽样方法。该方法通过模拟从目标节点开始的随机行走，选择标准化访问次数最高的T节点。</p> 
<p>3.4.2 分层抽样</p> 
<p>分层抽样不是对每个节点的邻居进行抽样，而是在每一层中保留一小组节点进行聚集，以控制扩展因子。FastGCN<br> (Chen et al.，<br> 2018b)直接对每一层的接收场进行采样。它使用重要性抽样，其中重要节点更有可能被抽样。与上述固定采样方法相比，Huang等人(2018)引入了一个参数化和可训练的采样器，以前一层为条件进行分层采样。此外，该自适应采样器可以在优化采样重要性的同时降低方差。LADIES<br> (Zou等人，2019)打算通过从节点的邻居的并集生成样本来缓解分层采样中的稀疏性问题。</p> 
<p>3.4.3子图抽样</p> 
<p>而不是采样节点和边建立在整个图，一个根本不同的方法是采样多个子图，并限制在这些子图的邻域搜索。ClusterGCN<br> (Chiang et al.， 2019)通过图聚类算法对子图进行采样，而GraphSAINT (Zeng et al.，<br> 2020)直接对节点或边进行采样，生成子图。</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-rAS0JTCz-1638176337273)(media/9cbc0d4a360ba601ae8ad1996187a460.png)]</p> 
<p>3.5池模块</p> 
<p>在计算机视觉领域，卷积层之后通常会有一个池化层，以获得更多的通用特性。复杂而大规模的图通常具有丰富的层次结构，这些层次结构对于节点级和图级分类任务非常重要。与这些池层类似，很多工作都关注在图上设计分层池层。在本节中，我们将介绍两种池模块:直接池模块和分层池模块。</p> 
<p>3.5.1直接池模块</p> 
<p>直接池模块直接从具有不同节点选择策略的节点学习图级表示。在某些变体中，这些模块也称为读出函数。简单的节点池。几个模型使用了简单的节点池方法。在这些模型中，节点的最大/平均/和/注意操作应用于节点特征得到全局图表示。</p> 
<p>Set2set：MPNN使用Set2set方法(Vinyals et al.，<br> 2015a)作为读出函数，得到图的表示。Set2set是为了处理无序集[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-pHbWiAz8-1638176337275)(media/e02fadeffab0dcd205cf769114e20b02.png)]而设计的，并使用基于lstm的方法在预定义的步骤数之后生成顺序不变表示。</p> 
<p>SortPooling (Zhang et al.，<br> 2018e)首先根据节点的结构角色对节点嵌入进行排序，然后将排序后的嵌入送入cnn中得到表示</p> 
<p>3.5.2分层池模块</p> 
<p>前面提到的方法直接从节点学习图表示，它们不研究图结构的层次属性。接下来，我们将讨论遵循分层池模式和按层学习图表示的方法。</p> 
<p>图粗化：<br> 早期的方法通常是基于图的粗化算法。首先采用光谱聚类算法，但由于特征分解步骤的问题，其效率较低。Graclus<br> (Dhillon et al.，<br> 2007)提供了一种更快的节点聚类方式，它被应用为一个池化模块。例如，ChebNet和MoNet使用Graclus合并节点对，并进一步添加额外的节点，以确保池化过程形成平衡二叉树。</p> 
<p>ECC：Edge-Conditioned Convolution (ECC) (Simonovsky and Komodakis,<br> 2017)设计了递归降采样操作的池化模块。下采样方法是根据拉普拉斯矩阵的最大特征向量的符号将图分成两个分量。</p> 
<p>DiffPool：DiffPool (Ying et al.，<br> 2018b)使用可学习的分层聚类模块，通过在每一层训练一个赋值矩阵:<br> [外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-X4uAbF2P-1638176337279)(media/b46d13bbe734a03690fee9c2d30cdca1.png)]</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-WWOJY64u-1638176337282)(media/f1825f3b94acdf9a68157a8729f23d65.png)]</p> 
<p>Ht是节点在t层特征矩阵，At是t层的粗化邻接矩阵，St表示一个节点在t层可以被一个粗化节点在t+1层分配的可能性</p> 
<p>gPool：gPool (Gao and Ji,<br> 2019)使用一个项目向量来学习每个节点的投影得分，并选择得分最高的k的节点。与DiffPool相比，它在每一层使用向量而不是矩阵，从而降低了存储复杂度。但投影过程没有考虑图的结构。</p> 
<p>EigenPooling.：EigenPooling (Ma et al.，<br> 2019a)设计的目的是联合使用节点特征和局部结构。该方法利用局部图傅里叶变换提取子图信息，存在图特征分解效率不高的问题。</p> 
<p>SAGPool：SAGPool (Lee et al.，<br> 2019)也被提出使用特征和拓扑共同学习图表示。它采用基于自我注意的方法，具有合理的时间和空间复杂性。</p> 
<p>4.考虑图像类型和缩放的变量</p> 
<p>在上面几节中，我们假设图形是最简单的格式。然而，现实世界中的许多图表是复杂的。在本小节中，我们将介绍试图解决复杂图类型挑战的方法。这些变体的概述如图4所示</p> 
<p>4.1有向图</p> 
<p>第一种类型是有向图。有向边通常比无向边包含更多的信息。例如，在一个知识图中，头部实体是尾部实体的父类，边缘方向提供了关于部分顺序的信息。与简单地在卷积算子中采用非对称邻接矩阵不同，我们可以对边的正方向和反方向进行不同的建模。</p> 
<p>DGP (Kampffmeyer et al.， 2019)使用了两种权重矩阵Wp和Wc用于正向和反向卷积</p> 
<p>4.2异构体</p> 
<p>图的第二种变体是异构图，其中节点和边是多类型或多模态的。更具体地说在异构体[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-IyUdhO32-1638176337285)(media/0ad0ffe3874d51de7b4a0e84bb8c2ee1.png)]中，每个节点vi都与类型[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-AY9LptnD-1638176337292)(media/ec4e927309ecdad851b38432cecc792b.png)]有关联，每个边ej与类型[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-xw3pMnSN-1638176337294)(media/604450ea4b439b088be7bfa018346200.png)]有关联。</p> 
<p>4.2.1基于元路径的推荐的方法</p> 
<p>处理这种图类型的大多数方法都利用了路径的概念。元路径是一种路径方案，它决定了路径各位置的节点类型，如[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-0MzF1LRS-1638176337296)(media/17d1e096c559ba95c6d193e5f4593d61.png)]，其中L为元路径的长度。在训练过程中，元路径被实例化为节点序列。通过连接元路径实例的两个端点节点，元路径可以获取两个可能不直接连接的节点的相似性。因此，可以将一个异构图简化为几个同构图，并在这些同构图上应用图学习算法。在早期的工作中，研究了基于元路径的相似性搜索(Sun<br> et al.， 2011)。近年来，越来越多的GNN模型利用元路径。HAN (Wang et al.，<br> 2019a)首先对每个元路径下的基于元路径的邻居进行图注意，然后在所有元路径方案下对节点的输出嵌入使用语义注意来生成节点的最终表示。MAGNN<br> (Fu et al.，<br> 2020)提出考虑元路径中的中间节点。它首先使用神经模块聚集沿元路径的信息，然后在与节点相关的不同元路径实例上执行注意，最后在不同元路径方案上执行注意。GTN<br> (Yun et al.，<br> 2019)提出了一种新的图转换层，它在学习节点表示的同时识别未连接节点之间的新连接。学习到的新连接可以连接彼此相距几跳但紧密相关的节点，这些节点起到元路径的作用。</p> 
<p>4.2.2基于边缘的方法</p> 
<p>也有不使用元路径的作品。这些工作通常使用不同的函数在采样，聚集等方面的不同种类的邻居和边。HetGNN通过在采样、特征编码和聚合步骤中以不同的方式直接对待不同类型的邻居，解决了这一挑战。HGT<br> (Hu等，2020a)定义元关系为两个相邻节点的类型及其链接[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-0uGfkih5-1638176337298)(media/7a07086be7f4b9a8ba79283e8d185ad7.png)]它将不同的注意力权重矩阵分配给不同的元关系，使模型能够考虑类型信息。</p> 
<p>4.2.3关系图的方法</p> 
<p>一些图的边缘可能包含比类型更多的信息，或者类型的数量可能太大，给应用基于元路径或元关系的方法带来困难。我们将这类图称为关系图(Schlichtkrull<br> et al.， 2018)，为了处理关系图，G2S (Beck et<br> al.，2018)将原始图转换为二部图，其中原始边也成为节点，将一条原始边分裂为两条新边，即在边缘节点和开始/结束节点之间有两条新边。在此转换之后，使用门控图神经网络和递归神经网络将具有边缘信息的图转换为句子。GGNN的聚合函数以节点的隐藏表示和关系作为输入。另一种方法R-GCN<br> (Schlichtkrull et al.，<br> 2018)不需要对原始图形格式进行转换。它为不同类型的边的传播分配了不同的权矩阵。然而，当关系的数量非常多时，模型中的参数数量就会激增。为此，引入了基-对角线分解和块对角线分解两种正则化方法来减少关系建模参数的数量。通过基础分解，每个Wr的定义如下：</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-kzkyaT6c-1638176337301)(media/877ed7bffa9d91ed63d3667f4323f973.png)]</p> 
<p>这里每一个Wr是基变换的线性组合。</p> 
<p>4.2.4多重图的方法</p> 
<p>在更复杂的场景中，图中的一对节点可以与不同类型的多条边关联。通过在不同类型的边下查看，图可以形成多个层，其中每一层代表一种关系。因此，多重图也可以称为多视图图(multi-view<br> graph，多维图)。例如，在YouTube中，两个用户之间可能有三种不同的关系:分享、订阅、评论。边类型并不是相互独立的，因此简单地将图分割成具有一种边类型的子图可能不是一个最佳解决方案。mGCN<br> (Ma等人，2019b)介绍了GNN每一层节点的一般表示和特定尺寸表示。使用不同的投影矩阵从一般表示中投影特定于维度的表示，然后聚合以形成下一层的一般表示</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-AgAChXFg-1638176337305)(media/b961782d50a0d65421bf1765780395db.png)]</p> 
<p>4.3动态图</p> 
<p>图的另一种变体是动态图，其中图的结构，例如边和节点的存在，会随着时间不断变化。为了将图结构数据与时间序列数据一起建模，DCRNN<br> (Li et al.， 2018b)和STGCN (Yu et al.，<br> 2018)首先通过gnn收集空间信息，然后将输出输入序列到序列模型(sequence- To<br> -sequence model)或rnn等序列模型。不同的是，Structural-RNN (Jain et al.，<br> 2016)和ST-GCN (Yan et al.，<br> 2018)同时收集空间和时间信息。他们使用时间连接扩展静态图结构，因此他们可以在扩展图上应用传统的gnn。同样，DGNN<br> (Manessi et al.，<br> 2020)将GCN中每个节点的输出嵌入到单独的lstm中。lstm的权值在每个节点之间共享。另一方面，EvolveGCN<br> (Pareja et al.，<br> 2020)认为直接建模节点表示的动态将阻碍模型在节点集不断变化的图上的性能。因此，它没有将节点特征作为RNN的输入，而是将GCN的权值输入到RNN，以捕获图交互的内在动态。最近的一项调查(Huang<br> et al.，<br> 2020)根据链接持续时间将动态网络划分为若干类，并将现有模型按照其专业化程度进行分类。建立了动态图模型的通用框架，并将现有模型纳入该框架。</p> 
<p>4.4其他图类型</p> 
<p>对于图的其他变体，如超图和签名图，也有一些模型被提出来解决这些挑战。</p> 
<p>4.4.1超图</p> 
<p>超图可以用[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-3ZiESNdp-1638176337307)(media/ed2523289402803022c884a567199e13.png)]表示，其中边e连接两个或多个顶点，并赋值一个权重w。超图的邻接矩阵可以表示成</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-QTawBb9L-1638176337309)(media/8d11934370e932326afd214cef325139.png)]</p> 
<p>HGNN (Feng et al.， 2019)提出超图卷积来处理这些节点之间的高阶交互:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-Cs5u2S2h-1638176337312)(media/c93bc3ff3230e8ec92d5c304d18dda11.png)]</p> 
<p>其中Dv;We;De;X是节点度矩阵、边权矩阵、边度矩阵和节点特征矩阵。</p> 
<p>W是可学习参数。这个公式是用截断的切比雪夫多项式逼近超图拉普拉斯方程而得到的。</p> 
<p>4.4.2符号图</p> 
<p>有符号图是有符号边的图，即一条边可以是正的也可以是负的。SGCN (Derr et al.，<br> 2018)并没有简单地将负边视为缺失边或另一种类型的边，而是利用平衡理论捕获正边和负边之间的相互作用。从直觉上来说，平衡理论认为，朋友的朋友(正面优势)也是我的朋友，敌人的敌人(负面优势)也是我的朋友。这为SGCN模型的正负边相互作用提供了理论基础。</p> 
<p>4.5大图</p> 
<p>正如我们在第 3.4<br> 节中提到的，采样算子通常用于处理大规模图。除了采样技术，还有缩放问题的其他方法。利用近似个性化<br> PageRank，</p> 
<p>避免计算高阶传播矩阵。罗西等人。 (2020) 提出一种预计算图的方法</p> 
<p>用于有效训练和推理的不同大小的卷积滤波器。基于 PageRank 的模型将多个 GCN<br> 层压缩为一个传播层来缓解“邻居爆炸”问题，因此是</p> 
<p>高度可扩展和高效的。</p> 
<p>5.不同训练设置的变体</p> 
<p>在本节中，我们将介绍不同训练设置的变体。对于监督和半监督设置，提供标签，使损失功能很容易设计这些标签的样品。对于无监督设置，没有标记的样本，因此损失函数应该依赖于图本身提供的信息，例如输入特征或图拓扑。在本节中，我们主要介绍非监督训练的变体，它们通常基于自动编码器或对比学习的思想。我们提到的方法的概述如图所示。5.</p> 
<p>5.1图自动编码</p> 
<p>在无监督图表示学习中，自动编码器(AE)已成为扩展到图域的趋势。Graph Auto-Encoder<br> (GAE) (Kipf和Welling,<br> 2016)首先使用GCNs对图中的节点进行编码。然后用一个简单的解码器重建邻接矩阵，根据原邻接矩阵与重建矩阵的相似度计算损失:</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-BJrpOctP-1638176337315)(media/233b139f629f7a2400d3855a84bcbf4a.png)]</p> 
<p>Kipf和Welling(2016)也以变分的方式训练GAE模型，该模型被命名为变分图自动编码器(VGAE)。</p> 
<p>对抗正则化图自编码(ARGA)<br> (Pan等人，2018)采用生成式对抗网络(GANs)来正则化基于gcn的图自编码，它可以学习更健壮的节点表示。</p> 
<p>Wang et al. (2017)，Park et al.(2019)试图重建特征矩阵，而不是恢复邻接矩阵。MGAE<br> (Wang et al.，<br> 2017)利用边缘去噪自编码器获得鲁棒的节点表示。为了构建对称图形自动编码器，GALA<br> (Park et al.， 2019)提出了拉普拉斯锐化(Laplacian<br> sharpening，拉普拉斯平滑的逆操作)来解码隐藏状态。该机制缓解了GNN训练中的过度平滑问题。与上述不同的是，AGE<br> (Cui et al.，<br> 2020)认为恢复损失与下游任务不兼容。因此，他们将自适应学习应用于两两节点相似度的测量，并在节点聚类和链接预测方面实现了最先进的性能。</p> 
<p>5.2对比学习</p> 
<p>除了图形自动编码器，对比学习为无监督图表示学习开辟了另一种途径。Deep Graph<br> Infomax (DGI)<br> (Velickovic等人，2019)最大化了节点表示和图表示之间的相互信息。Infograph (Sun et<br> al.，<br> 2020)的目标是通过图层表示和不同尺度(包括节点、边和三角形)的子结构层表示之间的相互信息最大化来学习图表示。多视图(Hassani和Khasahmadi,<br> 2020)对比了一阶邻接矩阵和图扩散表示，在多个图学习任务上实现了最先进的性能。</p> 
<p>6.一个GNN的设计实例</p> 
<p>在本节中，我们给出一个现有的GNN模型来说明设计过程。以异构图预训练任务为例，我们使用GPT-GNN<br> (Hu et al.， 2020b)作为模型来说明设计过程。</p> 
<ol><li> <p>找到图结构。本文主要研究了学术知识图谱和推荐系统的应用。在学术知识图谱中，图谱结构是明确的。在推荐系统中，用户、条目和评论可以看作节点，它们之间的交互可以看作边，所以图结构也易于构建。</p> </li><li> <p>指定图形类型和比例。任务集中在异构图上，因此节点和边的类型应该被考虑并纳入最终的模型中。由于学术图和推荐图包含数百万个节点，因此模型需要进一步考虑效率问题。总之，该模型应该关注大规模异构图。</p> </li><li> <p>设计功能损失。由于(Hu et al.，<br> 2020b)中的下游任务都是节点级任务(如学术图中的Paper-Field预测)，因此模型在预训练步骤中需要学习节点表示。在预训练步骤中，没有可用的标记数据，所以一个自监督的图生成任务是设计来学习节点的嵌入。在微调步骤中，根据每个任务的训练数据对模型进行微调，从而应用每个任务的监督损失。</p> </li><li> <p>使用计算模块建立模型。最后利用计算模块建立模型。对于传播模块，作者使用了我们之前提到的卷积算子HGT<br> (Hu等人，2020a)。HGT将节点和边的类型合并到模型的传播步骤中，并在体系结构中添加了跳过连接。采样模块采用了专门设计的采样方法HGSampling<br> (Hu et al.， 2020a)，这是LADIES的异构版本(Zou et al.，<br> 2019)。由于模型关注的是学习节点表示，因此不需要池化模块。HGT层被多层堆叠，以学习更好的节点嵌入。</p> </li></ol> 
<p>7.GNN的分析</p> 
<p>7.1理论方面</p> 
<p>在这一节中，我们从各个角度总结了关于图神经网络的理论基础和解释的文献。</p> 
<p>7.1.1图像信号处理</p> 
<p>从频谱的角度来看，GCNs在频谱域对输入特征进行卷积运算，这在理论上是进行图信号处理的。</p> 
<p>从图信号处理的角度分析gnn的研究已有很多。Li等人(2018c)首先解决了图神经网络中的图卷积问题，实际上是拉普拉斯平滑，它对特征矩阵进行平滑，使附近的节点具有类似的隐藏表示。拉普拉斯平滑反映了同质性假设，即相邻节点应该是相似的。拉普拉斯矩阵作为输入特征的低通滤波器。SGC<br> (Wu et al.，<br> 2019b)进一步消除了权重矩阵和层之间的非线性，表明低通滤波器是gnn工作的原因</p> 
<p>根据低通滤波的思想，Zhang等人(2019c)，Cui等人(2020)，NT和Maehara (NT和Maehara,<br> 2019)，Chen等人(2020b)分析了不同的滤波器并提供了新的见解。为实现对所有特征值的低通滤波，AGC<br> (Zhang et al.， 2019c)根据频率响应函数设计了一个graphfilterI’12l。AGE (Cui et<br> al.， 2020)进一步证明i ’ 1λ maxl滤波器可以得到更好的结果，其中λ<br> maxl为拉普拉斯矩阵的最大特征值。尽管有线性滤波器，GraphHeat<br> (Xu等人，2019a)利用热核获得更好的低通特性。</p> 
<p>图卷积主要是对输入特征进行去噪处理，模型的性能很大程度上取决于特征矩阵中噪声的大小。为了缓解过度平滑问题，Chen等人(2020b)提出了两个度量指标来衡量节点表示的平滑性和GNN模型的过度平滑性。作者认为，信息噪声比是超平滑的关键因素。</p> 
<p>7.1.2概括</p> 
<p>近年来，gnn的泛化能力也受到了人们的关注。Scarselli等人(2018)证明了有限类gnn的vc<br> -维。Garg et<br> al.(2020)基于Rademacher边界进一步给出了更严格的神经网络泛化边界。Verma和Zhang(2019)分析了不同卷积滤波器的单层gnn的稳定性和泛化特性。作者认为，gnn的稳定性取决于滤波器的最大特征值。Knyazev<br> et<br> al.(2019)关注gnn中注意机制的泛化能力。他们的结论表明，注意有助于gnn概括为更大的和有噪声的图。</p> 
<p>7.1.3表现度</p> 
<p>关于gnn的表达率，Xu等人(2019b)和Morris等人(2019)表明GCNs和GraphSAGE的区别性低于Weisfeiler-Leman<br> (WL)检验，一种图同构检验算法。Xu等人(2019a)也提出了GINs用于更有表现力的gnn。超越WL测试，Barcel!o等人(2019)讨论了gnn是否可表示FOC2，一个片段一阶逻辑。作者发现现有的gnn很难符合这种逻辑。对于学习图的拓扑结构，Garg等人(2020)证明局部依赖的GNN变体不能学习全局图的属性，包括直径、最大/最小循环或基序</p> 
<p>Loukas(2020)和dehmamy等人(2019)认为，现有作品只考虑gnn具有无限层和单元时的表现力。他们的工作是研究有限深度和宽度的gnn的表示能力。ooono和Suzuki(2020)讨论了随着模型的深化，gnn的渐近行为，并将它们建模为动态系统</p> 
<p>7.1.4不变性</p> 
<p>由于图中没有节点顺序，因此gnn的输出嵌入应该是排列不变的或与输入特征等变的。Maron等人(2019a)刻画了排列不变或等变线性层来构建不变gnn。Maron等(2019b)进一步证明了通过高阶张量化可以得到通用不变量gnn的结果。Keriven<br> Peyr<br> !E(2019)提供了一个替代证明，并将该结论推广到等变情况。Chen等人(2019)建立了排列不变性和图同构检验之间的联系。为了证明它们的等价性，Chen等人(2019)利用sigma代数来描述gnn的表达性。</p> 
<p>7.1.5可转移性</p> 
<p>gnn的一个确定性特征是参数化与图相结合，这表明在性能保证的情况下跨图传输的能力(所谓的可转移性)。Levie等人(2019)研究了光谱图滤波器的可转移性，表明此类滤波器能够在同一域的图上转移。Ruiz等人(2020)分析了GNN在石墨烯上的行为。Graphon指的是图序列的极限，它也可以看作是稠密图的生成器。作者的结论是，gnn可以在确定地从同一图中以不同大小获得的图之间转移。</p> 
<p>7.1.6标签效率</p> 
<p>gnn的(半)监督学习需要大量的标记数据才能达到令人满意的性能。本文从主动学习的角度对如何提高标签效率进行了研究，即主动选择信息节点由oracle进行标签，以训练gnn。Cai<br> et al.(2017)、Gao et al. (2018b)、Hu et al.<br> (2020c)等研究表明，通过选择高程度节点和不确定节点等信息节点，可以显著提高标签效率</p> 
<p>7.2经验方面</p> 
<p>除了理论分析外，还需要对gnn进行实证研究，以便更好地进行比较和评价。在这里，我们包括几个实证研究的GNN评价和基准。</p> 
<p>7.2.1评估</p> 
<p>评估机器学习模型是研究的一个重要步骤。多年来，人们对实验再现性和可复制性提出了担忧。GNN模型是否有效，在何种程度上有效?模型的哪些部分对最终性能有贡献?为了研究这些基本问题，迫切需要对公平评价策略进行研究。</p> 
<p>在半监督节点分类任务中，Shchur等人(2018a)探索了GNN模型在相同的训练策略和超参数调优下的表现。他们的研究得出结论，不同的数据集可以拆分导致不同的模型排名。此外，在适当的设置下，简单模型可以优于复杂模型。Errica等人(2020)回顾了几个图表分类模型，并指出它们的比较不恰当。基于严格的评估，结构信息在图分类中没有得到充分的利用。You<br> et<br> al.(2020)讨论了GNN模型的架构设计，如层数和聚合功能。通过大量的实验，这项工作为各种任务的GNN指定提供了全面的指导方针</p> 
<p>7.2.2基准</p> 
<p>高质量和大规模的基准数据集，如ImageNet在机器学习研究中具有重要意义。然而在图学习中，广泛采用的基准是有问题的。例如，大多数节点分类数据集只包含3000到20000个节点，这与真实世界的图相比很小。此外，各个研究的实验方案不统一，这对文献是有害的。为了缓解这一问题，Dwivedi等人(2020)和Hu等人(2020d)为图学习提供了可扩展且可靠的基准。Dwivedi<br> et al.(2020)在多个领域和任务中构建中等规模的基准数据集，而OGB (Hu et al.，<br> 2020d)提供大规模数据集。此外，这两款作品都评估了当前的GNN模型，并提供排行榜供进一步比较。</p> 
<p>8.应用</p> 
<p>图神经网络已经在有监督、半监督、无监督和强化学习设置的广泛领域中进行了探索。在本节中，我们通常将应用程序分组到两个场景中:(1)结构化场景，其中数据具有显式的关系结构。这些场景，一方面来自于科学研究，如图挖掘，建模物理系统和化学系统。另一方面，它们来自于工业应用，如知识图、交通网络和推荐系统。(2)关系结构隐含或不存在的非结构化场景。这些场景通常包括图像(计算机视觉)和文本(自然语言处理)，这是人工智能研究最活跃的两个分支。这些应用程序的一个简单示例是inFig。6.<br> 注意，我们只列出了几个有代表性的应用程序，而没有提供完整的列表。应用程序的摘要可以在表3中找到。</p> 
<p>8.1结构化场景</p> 
<p>在接下来的小节中，我们将介绍gnn在结构化场景中的应用，在结构化场景中，数据自然地在图结构中执行</p> 
<p>8.1.1图挖掘</p> 
<p>第一个应用是解决图挖掘中的基本任务。通常，图挖掘算法用于识别下游任务的有用结构。传统的图挖掘挑战包括频繁子图挖掘、图匹配、图分类、图聚类等。虽然有了深度学习，一些下游任务无需图挖掘作为中间步骤就可以直接解决，但其基本挑战值得从gnn的角度进行研究。</p> 
<p>图匹配：第一个挑战是图匹配。传统的图匹配方法计算复杂度高。gnn的出现使研究人员能够利用神经网络捕捉图的结构，从而为这个问题提供了另一种解决方案。Riba<br> et al.(2018)提出了一个siamese<br> MPNN模型来学习图的编辑距离。siamese框架是两个具有相同结构和权值共享的并行mpnn，其训练目标是将编辑距离较小的一对图嵌入到较近的潜在空间中。Li<br> et al.<br> (2019b)设计了类似的方法，同时在更真实的场景中进行实验，如相似搜索在控制流图中</p> 
<p>图聚类：图聚类是根据图的结构和/或节点属性将图的顶点分组成簇。在节点表示学习方面开展了各种工作(Zhang<br> et al.， 2019c)，节点表示可以传递到传统的聚类算法。除了学习节点嵌入，图池(Ying<br> et al.，<br> 2018b)可以看作是一种聚类。最近，Tsitsulin等人(2020)直接针对聚类任务。他们研究了一种好的图聚类方法的可取之处，并提出了优化谱模块化的方法，这是一个非常有用的图聚类指标。</p> 
<p>8.1.2物理特性</p> 
<p>模拟真实世界的物理系统是理解人类智能最基本的方面之一。物理系统可以建模为系统中的对象和对象之间成对的交互。物理系统的仿真需要模型学习系统的规律，并对系统的下一个状态进行预测。通过将对象建模为节点，将成对交互建模为边，系统可以简化为图形。例如，在粒子系统中，粒子可以通过多种相互作用进行相互作用，包括碰撞(Hoshen,<br> 2017)、弹簧连接、电磁力(Kipf et al.，<br> 2018)等，其中粒子被视为节点，相互作用被视为边缘。另一个例子是机器人系统，它是由由关节连接的多个身体(如手臂、腿)组成的。体和关节可以分别看作节点和边缘。模型需要根据系统的当前状态和物理原理推断出物体的下一个状态。在图神经网络出现之前，works使用可用的神经块处理系统的图表示。交互网络(Battaglia<br> et al.， 2016)利用MLP编码图的关联矩阵。CommNet (Sukhbaatar Ferguset al.，<br> 2016)使用节点先前表示和所有节点先前表示的平均值来执行节点更新。VAIN (Hoshen,<br> 2017)进一步介绍了注意机制。VIN (Watters et al.， 2017)结合了cnn、rnn和IN<br> (Battaglia et al.，<br> 2016)。gnn的出现让我们能够以一种简化但有效的方式对物体、关系和物理进行基于gnn的推理。NRI<br> (Kipf et al.，<br> 2018)以物体轨迹为输入，推导出显式交互图，同时学习动态模型。从以前的轨迹中学习交互图，并通过解码交互图生成轨迹预测。Sanchez等(2018)提出了一种基于图网络的模型，对机器人系统的身体和关节形成的图进行编码。他们将遗传算法与强化学习相结合，进一步学习了系统的稳定控制策略。</p> 
<p>8.1.3化学和生物学</p> 
<p>分子指纹：分子指纹作为一种编码分子结构的方式。最简单的指纹可以是一个热点向量，其中每个数字表示某个特定子结构的存在或不存在。这些指纹可以用于分子搜索，这是计算机辅助药物设计的核心步骤。传统的分子指纹是手工制作和固定的(例如，一个热向量)。然而，分子可以很自然地看作图形，原子是节点，化学键是边。因此，将gnn应用于分子图谱，可以获得更好的指纹图谱。Duvenaud等人(2015)提出了神经图谱(neural<br> graphfingerprint)，该算法通过GCNs计算子结构特征向量并求和得到整体表示。Kearnes等人(2016)明确模拟原子和原子对独立强调原子相互作用。它引入了边缘表示，而不是聚合函数。</p> 
<p>化学反应预测：化学反应产物预测是有机化学中的一个基本问题。图形变换政策网络(Do et<br> al.， 2019)对输入分子进行编码，生成一个包含节点对预测网络和政策网络的中间图</p> 
<p>蛋白质界面预测：蛋白质通过界面相互作用，界面是由每个参与蛋白质的氨基酸残基组成的。蛋白质界面预测任务是确定特定残基是否构成蛋白质的一部分。一般来说，单个残基的预测依赖于相邻的其他残基。通过让残基为节点，蛋白质可以表示为图，这可以利用基于gnn的机器学习算法。Fout等人(2017)提出了一种基于gcn的方法来学习配体和受体蛋白残留表示，并将它们合并以进行两两分类。MR-GNN<br> (Xu et al.，<br> 2019d)引入了一种多分辨率方法来提取和总结局部和全局特征，以更好地预测。</p> 
<p>生物医学工程：Rhee等(2018)利用蛋白质-蛋白质相互作用网络(Protein-Protein<br> Interaction<br> Network)，利用图卷积和关系网络进行乳腺癌亚型分类。Zitnik等人(2018)也提出了一种基于gcn的多药副作用预测模型。他们的工作是建立药物和蛋白质相互作用网络的模型，并分别处理不同类型的边缘。</p> 
<p>8.1.4知识网络</p> 
<p>知识图(KG)表示现实世界的实体和实体对之间的关系事实的集合。它在问答、信息检索、知识引导生成等方面有广泛的应用。KGs的任务包括学习包含丰富语义的实体和关系的低维嵌入，预测实体之间的缺失链接，以及对知识图进行多跳推理。有一条研究线将图视为三元组的集合，并提出各种损失函数来区分正确的三元组和错误的三元组(Bordes<br> et al.，<br> 2013)。另一条线利用了KG的图形特性，并为各种任务使用基于gnn的方法。当把KG看作一个图时，可以看作是一个异构图。然而，与其他异构图(如社交网络)不同，逻辑关系比纯图结构更重要。R-GCN<br> (Schlichtkrull et al.，<br> 2018)是第一个将gnn用于知识图嵌入的著作。为了处理各种关系，R-GCN在消息传递步骤中提出了特定于关系的转换。结构感知卷积网络(Shang<br> et al.，<br> 2019)结合了一个GCN编码器和一个CNN解码器，以更好地表示知识。一个更具挑战性的设置是针对非知识库实体的知识库完成。OOKB实体在训练集中是看不见的，但是直接连接到训练集中观察到的实体。OOKB实体的嵌入可以从观察到的实体进行聚合。Hamaguchi<br> et<br> al.(2017)使用gnn来解决该问题，无论在标准KBC设置还是OOKB设置中都取得了令人满意的性能。除了知识图表示学习外，Wang等(2018b)利用GCN解决了跨语言知识图对齐问题。该模型将不同语言的实体嵌入到一个统一的嵌入空间中，并根据嵌入相似度对其进行对齐。为了对大规模异构知识图进行对齐，OAG<br> (Zhang et al.，<br> 2019d)使用图注意网络对各种类型的实体进行建模。Xu等(2019c)将表示实体的子图作为其周围的子图，将实体对齐问题转化为图匹配问题，然后通过图匹配网络解决。</p> 
<p>8.1.5生成模型</p> 
<p>现实世界图形的生成模型因其重要的应用而受到重视，包括建模社会互动、发现新的化学结构和构建知识图。由于深度学习方法对图的隐式分布具有强大的学习能力，神经图的研究出现了激增最近生成模型</p> 
<p>NetGAN (Shchur et al.，<br> 2018b)是最早建立神经图生成模型的工作之一，该模型通过随机漫步生成图。它将图生成问题转化为步行生成问题，以特定图的随机步行为输入，利用GAN架构训练步行生成模型。生成的图虽然保留了原始图的重要拓扑性质，但在生成过程中节点数不会发生变化，与原始图相同。GraphRNN<br> (You et al.，<br> 2018b)通过逐步生成每个节点的邻接向量，成功生成一个图的邻接矩阵，可以输出节点数量不同的网络。Li<br> et al.<br> (2018d)提出了一个顺序生成边和节点的模型，利用图神经网络提取当前图的隐藏状态，用于在顺序生成过程中决定下一步的行动。GraphAF<br> (Shi et al.，<br> 2020)也将图生成作为一个顺序决策过程。它结合了基于流的生成和自进位模型。在分子生成方面，在每一步生成后，它也利用现有的化学规则对生成的分子进行有效性检查。</p> 
<p>其他工作不是按顺序生成图，而是一次性生成图的邻接矩阵。MolGAN (De Cao and Kipf,<br> 2018)利用排列不变鉴别器来解决邻接矩阵中的节点变分问题。此外，它还应用了一个基于rl的奖励网络来优化所期望的化学性质。此外，Ma等人(2018)提出了约束变分自动编码器，以确保生成的图的语义有效性。而且，GCPN<br> (You等人，2018a)通过强化学习整合了特定领域的规则。GNF (Liu et al.，<br> 2019)将归一化流适应于图形数据。归一化流是一种生成模型，利用可逆映射将观测数据转换为潜在向量空间。利用逆矩阵将潜在矢量转换回观测数据作为生成过程。GNF结合了归一化流和排列不变图的自编码器，以图结构化数据作为输入，并在测试时生成新的图。石墨(Grover<br> et al.，<br> 2019)将GNN集成到变分自编码器中，将图结构和特征编码为潜在变量。更具体地说，它使用各向同性高斯作为潜变量，然后使用迭代细化策略对潜变量进行解码。</p> 
<p>8.1.6组合优化</p> 
<p>图上的组合优化问题是一组np困难问题，引起了各领域科学家的广泛关注。一些具体的问题如旅行商问题(TSP)和最小生成树问题(MST)都有不同的启发式解。近年来，利用深度神经网络来解决此类问题已成为研究的热点，其中一些解决方案由于图神经网络具有图的结构特点，进一步利用图神经网络来解决此类问题。Bello<br> et<br> al.(2017)首先提出了一种深度学习方法来解决TSP问题。他们的方法包括两个部分:参数化奖励的指针网络(Vinyals<br> et al.， 2015b)和培训的政策梯度(Sutton and Barto,<br> 2018)模块。这项工作已被证明与传统方法具有可比性。然而，指针网络是为文本等顺序数据设计的，而顺序不变编码器更适合这种工作。Khalil<br> et al. (2017)，Kool et<br> al.(2019)通过加入图神经网络对上述方法进行了改进。前一项工作首先从structure2vec中获取节点嵌入(Dai等人，2016)，然后将它们放入q学习模块中进行决策。后者建立了一个基于注意力的编解码系统。通过将强化学习模块替换为基于注意力的解码器，提高了训练效率。这些工作取得了比以往算法更好的性能，证明了图神经网络的表示能力。更一般地说，Gasse等人(2019)代表了组合的状态并利用GCN对其进行编码。对于特定的组合优化问题，Nowak等(2018)关注于二次分配问题，即测量两个图的相似度。基于GNN的模型独立学习每个图的节点嵌入，并使用注意机制进行匹配。这种方法提供了有趣的良好性能，即使在标准的放松技术似乎受到影响的情况下。Zheng等(2020a)使用生成图神经网络来建模dag<br> -结构学习问题，这也是一个组合优化和np困难问题。NeuroSAT (Selsam et al.，<br> 2019)学习了一个信息传递神经网络来分类SAT问题的可满足性。这证明了学习到的模型可以推广到SAT的新分布以及其他可以转化为SAT的问题。不同于以往试图设计特定的GNN来解决组合问题，Sato等(2019)在这些问题上提供了GNN模型的理论分析。它建立了gnn和分布式局部算法之间的联系，分布式局部算法是一组解决这些问题的经典图上算法。此外，它论证了最强大的GNN能够达到的最优解的最优逼近比。同时也证明了现有的大多数GNN模型都不能超过这个上界。此外，它还对节点特征进行着色，以提高近似率。</p> 
<p>8.1.7交通网络</p> 
<p>流量状态预测是一项具有挑战性的任务，因为流量网络是动态的，具有复杂的依赖性。Cui等人(2018b)结合gnn和lstm来捕获空间和时间依赖。STGCN<br> (Yu et al.，<br> 2018)利用空间和时间卷积层构造ST-Conv块，并采用带有瓶颈策略的剩余连接。Zheng et<br> al. (2020b)，Guo et al.(2019)都加入了注意机制，以更好地模拟时空相关性。</p> 
<p>8.1.8推荐系统</p> 
<p>用户-物品交互预测是推荐中的经典问题之一。通过将交互过程建模为图形，可以在该区域利用gnn。GC-MC<br> (van den Berg et al.，<br> 2017)首先将GCN应用于用户-物品评级图中来学习用户和物品嵌入。为了在网络规模的场景中有效地采用gnn,<br> PinSage (Ying et al.，<br> 2018a)对二部图采用加权采样策略构建计算图，以减少重复计算。社交推荐试图结合用户的社交网络来提高推荐性能。GraphRec<br> (Fan等人，2019年)从物品和用户两方面学习用户嵌入。Wu等(2019c)超越了静态的社会效应。他们试图模拟同质性，并通过双重注意影响效应。</p> 
<p>8.1.9结构场景中的其他应用</p> 
<p>由于图结构数据的普遍性，gnn已经被应用到比我们前面介绍的更广泛的任务中。我们非常简单地列出了更多的场景。在金融市场中，gnn用于建模不同股票之间的相互作用，预测股票的未来趋势(Matsunaga<br> et al.， 2019;Yang et al.， 2019;Chen et al.， 2018c;Li et al.，<br> 2020)。Kim等人(2019)也通过将其定义为一个图分类问题来预测市场指数的运动。在SDN<br> (Software-Defined Networks)中，gnn用于优化路由性能(Rusek et al.，<br> 2019)。在抽象意义表示(Abstract Meaning Representation,<br> AMR)图到文本生成任务中，Song等人(2018a)、Beck等人(2018)使用gnn对抽象意义的图表示进行编码。</p> 
<p>8.2非结构场景</p> 
<p>在本节中，我们将讨论非结构化场景中的应用程序。一般来说，将gnn应用于非结构化网络有两种方法</p> 
<p>场景:(1)结合其他领域的结构化信息来提高性能，例如利用知识图信息来缓解图像任务中的零射问题;(2)推断或假设任务中的关系结构，然后应用模型解决在图上定义的问题，如(Zhang<br> et al.，<br> 2018d)中将文本模型化为图的方法。常见的非结构场景包括图像、文本和编程源代码(Allamanis等人，2018;Li等人，2016)。然而，我们只对前两个场景做了详细的介绍。</p> 
<p>8.2.1图像</p> 
<p>小样本图像分类：图像分类是计算机视觉领域中一个非常基础和重要的任务，备受关注，拥有ImageNet等著名数据集(Russakovsky<br> et al.，<br> 2015)。近年来，零拍和少拍学习在图像分类领域越来越受到人们的重视。InN-shot学习，为了对某些类中的测试数据样本进行预测，在训练集中提供相同类中的onlyNtraining样本。因此，少射学习限制为小，零射要求为0。模型必须学会从有限的训练数据中进行归纳，以便对测试数据做出新的预测。另一方面，图神经网络可以帮助图像分类系统在这些具有挑战性的场景。首先，知识图可以作为额外信息来指导零镜头识别分类(Wang<br> et al.， 2018d;Kampffmeyer et al.，<br> 2019)。Wang等人(2018d)使视觉分类器不仅从视觉输入中学习，还从类别名称的单词嵌入以及它们与其他类别的关系中学习。开发了一个知识图来帮助连接相关的类别，他们使用一个6层的GCN对知识图进行编码。由于超平滑效应发生在图卷积结构变得深的时候，(Wang<br> et al.，<br> 2018d)中使用的6层GCN将洗掉表示中很多有用的信息。为了解决平滑问题，Kampffmeyer等(2019)使用了一个具有更大邻域的单层GCN，该GCN同时包含图中的单跳节点和多跳节点。在已有分类器的基础上，建立零射击分类器是有效的。由于大多数知识图具有较大的推理能力，Marino等(2017)根据目标检测结果选择一些相关实体构建子图，并将GGNN应用于提取的图中进行预测。此外，Lee<br> et al.<br> (2018b)也利用了类别之间的知识图。进一步定义了类别之间的三种关系:上下级关系、正相关关系和负相关关系，并直接在图中传播关系标签的置信度。除了知识图外，数据集中图像之间的相似性也有助于少镜头学习(Garcia<br> and Bruna,<br> 2018)。Garcia和Bruna(2018)基于相似性构建加权的全连通图像网络，在图中进行消息传递，实现少镜头识别。</p> 
<p>视觉推理：计算机视觉系统通常需要结合空间和语义信息来进行推理。因此，为推理任务生成图表是很自然的。一个典型的视觉推理任务是视觉问答(VQA)。在这个任务中，模型需要在给出问题的文本描述的情况下回答关于图像的问题。通常，答案在于图像中物体之间的空间关系。Teney<br> et<br> al.(2017)构建了一个图像场景图和一个问题句法图。然后他们应用GGNN训练嵌入以预测最终答案。尽管对象之间存在空间联系，Norcliffebrown等人(2018)还是基于问题构建了关系图。Wang<br> et al. (2018c)、Narasimhan et<br> al.(2018)利用知识图可以进行更精细的关系探索和更可解释的推理过程。视觉推理的其他应用包括目标检测、交互检测和区域分类。在目标检测中(Hu<br> et al.， 2018;Gu et al.， 2018)， gnn用于计算RoI特征。在交互检测(Qi et al.，<br> 2018;Jain et al.， 2016)，</p> 
<p>GNN是人与对象之间的消息传递工具。在区域分类中(Chen et al.， 2018d)，<br> gnn对连接区域和类的图进行推理。</p> 
<p>语义分割：语义分割是图像理解的关键步骤。这里的任务是为图像中的每一个像素分配一个唯一的标签(或类别)，这可以被认为是一个密集分类问题。但是图像中的区域往往不是网格状的，需要非局部的信息，这导致了传统CNN的失败。一些作品利用图结构数据来处理它。Liang等人(2016)利用Graph-LSTM对长期依赖和空间连接进行建模，以基于距离的超像素图的形式构建图，并利用LSTM对邻域信息进行全局传播。后续工作从编码层次信息的角度对其进行了改进(Liang<br> et al.，<br> 2017)。此外，3D语义分割(RGBD语义分割)和点云分类利用了更多的几何信息，因此很难用2D<br> CNN进行建模。Qi等(2017b)构建k近邻(KNN)图，并使用3D<br> GNN作为传播模型。在展开几个步骤后，预测模型将每个节点的隐藏状态作为输入，并预测其语义标签。Landrieu和Simonovsky(2018)针对点云分类任务中点太多的问题，通过构建超点图并对其生成嵌入来解决大规模三维点云分割问题。Landrieu和Simonovsky(2018)利用GGNN和图卷积对超级节点进行分类。Wang<br> et al.<br> (2018e)提出通过边建模点的相互作用。它们通过输入末端节点的坐标来计算边缘表示向量。然后通过边缘聚合更新节点嵌入。</p> 
<p>8.2.2文本</p> 
<p>图神经网络可以应用于多种基于文本的任务。它既可以应用于句子级任务(如文本分类)，也可以应用于单词级任务(如序列标记)。下面我们列出了几个主要的文本应用。</p> 
<p>文本分类：文本分类是自然语言处理中的一个重要而经典的问题。传统的文本分类使用词袋特征。然而，将文本表示为单词图可以进一步捕获非连续单词和长距离单词之间的语义(Peng<br> et al.， 2018)。Peng et al.(2018)使用基于graph-<br> cnn的深度学习模型首先将文本转换为单词图，然后在(Niepert et al.，<br> 2016)中使用图卷积操作对单词图进行卷积。Zhang等(2018d)提出了句子LSTM对文本进行编码。他们将整个句子视为一个单一的状态，该状态由单个单词的次状态和整个句子级别的状态组成。他们使用全局句子级表示来完成分类任务。这些方法要么以单词节点图的形式查看文档或句子。Yao等(2019)将文档和单词作为节点构建语料库图，利用Text<br> GCN学习单词和文档的嵌入。情感分类也可以看作是一个文本分类问题，Tree-LSTM方法由Tai<br> et al.， 2015提出。</p> 
<p>序列标签：</p> 
<p>给定一个观察到的变量序列(如单词)，序列标记是为每个变量分配一个分类标签。典型的任务包括POS-tagging，即根据词性对句子中的单词进行标记;以及命名实体识别(Named<br> Entity Recognition,<br> NER)，即预测句子中的每个单词是否属于命名实体的一部分。如果我们把序列中的每个变量看作一个节点，把依赖项看作边，我们就可以利用gnn的隐藏状态来完成任务。Zhang等(2018d)利用句子LSTM对序列进行标注。他们进行了pos标记和NER任务的实验，并取得了良好的表现。语义角色标注是序列标注的另一项任务。Marcheggiani和Titov(2017)提出了一个句法GCN来解决这个问题。句法GCN操作于带标记边的直图上，是GCN的一种特殊变体(Kipf和Welling,<br> 2017)。它整合了边缘门，让模型调节个人的贡献</p> 
<p>依赖边缘。利用句法依存树上的句法GCNs作为句子编码器来学习句子中单词的潜在特征表示</p> 
<p>神经网络机器翻译：</p> 
<p>神经机器翻译(neural machine translation,<br> NMT)是利用神经网络将文本从源语言自动翻译到目标语言。它通常被认为是一个序列对序列的任务。Transformer<br> (Vaswani et al.，<br> 2017)引入了注意机制，并取代了最常用的循环或卷积层。事实上，Transformer假定单词之间是完全连接的图结构。其他的图结构可以用gnn来探索。GNN的一个流行应用是将语法或语义信息整合到NMT任务中。Bastings等人(2017)在句法感知的NMT任务上使用句法GCN。Marcheggiani等人(2018)使用句法GCN整合了源句谓语-argument结构的信息(即语义-角色表示)，并比较了只整合句法信息、只整合语义信息以及两者都整合的结果。Beck等(2018)在句法感知的NMT中利用GGNN。他们通过将边转换为额外的节点，将语法依赖图转换为名为Levi图(Levi,<br> 1942)的新结构，这样边标签就可以表示为嵌入。</p> 
<p>关系抽取：提取文本中实体之间的语义关系有助于扩展现有知识库。传统的方法是使用cnn或rnn学习实体的特征，并预测一对实体的关系类型。一种更复杂的方法是利用句子的依赖结构。可以构建一个文档图，其中节点表示单词，边表示各种依赖关系，如邻接关系、句法依赖关系和语篇关系。Zhang等人(2018f)提出了一种针对关系提取的图卷积网络扩展，并将剪枝策略应用于输入树。跨句n元关系抽取检测多句实体之间的关系。Peng等(2017)通过在文档图上应用图LSTMs，探索了一个跨句关系提取的通用框架。Song等人(2018b)也使用了图形状态LSTM模型，并通过允许更多的并行来加快计算速度。</p> 
<p>事件提取：</p> 
<p>事件提取是识别文本中特定类型事件实例的一项重要信息提取任务。这总是通过识别事件触发器并预测每个触发器的参数来实现的。Nguyen和Grishman(2018)研究了基于依赖树的卷积神经网络(确切地说就是句法GCN)来执行事件检测。Liu等(2018)提出了一种联合多事件提取(joint<br> Multiple Events Extraction,<br> JMEE)框架，通过引入句法捷径弧来增强信息流到基于注意的图卷积网络中，对图信息建模，从而联合提取多个事件触发器和参数。</p> 
<p>事实验证：</p> 
<p>事实验证是一项要求模型提取证据来验证给定主张的任务。然而，有些主张需要基于多个证据进行推理。提出了基于gnn的方法，如GEAR<br> (Zhou et al.， 2019)和KGAT (Liu et al.，<br> 2020)，基于全连通证据图进行证据聚合和推理。Zhong等(2020)利用语义角色标注信息构建了一个句子内图，并取得了很好的效果。</p> 
<p>文本的其他应用：</p> 
<p>gnn还可以应用于文本上的许多其他任务。例如，gnn也用于问题回答和阅读理解(Song et<br> al.， 2018c;De Cao et al.， 2019;Qiu et al.， 2019;Tu et al.， 2019;Ding et<br> al.， 2019)。另一个重要的方向是关系推理，提出了关系网络(Santoro et al.，<br> 2017)、交互网络(Battaglia et al.， 2016)和递归关系网络(Palm et al.，<br> 2018)来解决基于文本的关系推理任务。</p> 
<p>9.开放的问题</p> 
<p>尽管gnn在不同的领域取得了巨大的成功，但它确实是值得注意的是，GNN模型并不足以为任何条件下的任何图提供令人满意的解。在本节中，我们列出了一些有待进一步研究的问题。</p> 
<p>鲁棒性：作为一类基于神经网络的模型，gnn也容易受到对抗性攻击。相对于只针对特征的图像或文本的对抗性攻击，针对图的攻击进一步考虑了结构信息。已经提出了一些工作来攻击现有的图模型(Zügner<br> et al.， 2018;Dai et al.， 2018b)，并提出了更鲁棒的模型来防御(Zhu et al.，<br> 2019)。我们参考(Sun et al.， 2018)进行全面回顾</p> 
<p>可解释性：可解释性也是神经模型的一个重要研究方向。但gnn也是黑匣子，缺乏解释。仅提出了几种方法(Ying<br> et al.， 2019;Baldassarre和Azizpour,<br> 2019)来生成GNN模型的实例级解释。将GNN模型应用于具有可信解释的真实应用程序是很重要的。与CV和NLP领域一样，图的可解释性也是一个重要的研究方向</p> 
<p>图预训练：基于神经网络的模型需要大量的标记数据，而获取大量的人标记数据的成本很高。提出了自监督的方法来指导模型从网站或知识库中容易获取的无标签数据中学习。这些方法以预训练的思想在CV和NLP领域取得了巨大的成功(Krizhevsky<br> et al.， 2012;Devlin et al.， 2019)。最近，有一些研究专注于图的预训练(Qiu et<br> al.， 2020;Hu et al.， 2020b,2020e;Zhang et al.，<br> 2020)，但他们的问题设置不同，关注的方面也不同。该领域还存在许多有待研究的问题，如预训练任务的设计、现有GNN模型在学习结构或特征信息方面的有效性等。</p> 
<p>复杂的图形结构：在现实应用中，图结构是灵活而复杂的。各种工作被提出处理复杂的图结构，如我们之前讨论过的动态图或异构图。随着互联网上社交网络的快速发展，必然会出现更多的问题、挑战和应用场景，需要更强大的模型。</p> 
<p>10.结论</p> 
<p>在过去的几年里，图神经网络已经成为图领域机器学习任务中强大而实用的工具。这一进展归功于表达能力、模型灵活性和训练算法的进步。在这项研究中，我们对图神经网络进行了全面的回顾。对于GNN模型，我们根据计算模块、图类型和训练类型介绍了它的变种。此外，我们还总结了几个总体框架，并介绍了一些理论分析。在应用程序分类方面，我们将GNN应用程序分为结构化场景、非结构化场景和其他场景，并对每个场景中的应用程序进行详细的回顾。最后，我们提出了图神经网络面临的主要挑战和未来的研究方向，包括鲁棒性、可解释性、预训练和复杂结构建模。<br> 2019)。我们参考(Sun et al.， 2018)进行全面回顾</p> 
<p>可解释性：可解释性也是神经模型的一个重要研究方向。但gnn也是黑匣子，缺乏解释。仅提出了几种方法(Ying<br> et al.， 2019;Baldassarre和Azizpour,<br> 2019)来生成GNN模型的实例级解释。将GNN模型应用于具有可信解释的真实应用程序是很重要的。与CV和NLP领域一样，图的可解释性也是一个重要的研究方向</p> 
<p>图预训练：基于神经网络的模型需要大量的标记数据，而获取大量的人标记数据的成本很高。提出了自监督的方法来指导模型从网站或知识库中容易获取的无标签数据中学习。这些方法以预训练的思想在CV和NLP领域取得了巨大的成功(Krizhevsky<br> et al.， 2012;Devlin et al.， 2019)。最近，有一些研究专注于图的预训练(Qiu et<br> al.， 2020;Hu et al.， 2020b,2020e;Zhang et al.，<br> 2020)，但他们的问题设置不同，关注的方面也不同。该领域还存在许多有待研究的问题，如预训练任务的设计、现有GNN模型在学习结构或特征信息方面的有效性等。</p> 
<p>复杂的图形结构：在现实应用中，图结构是灵活而复杂的。各种工作被提出处理复杂的图结构，如我们之前讨论过的动态图或异构图。随着互联网上社交网络的快速发展，必然会出现更多的问题、挑战和应用场景，需要更强大的模型。</p> 
<p>10.结论</p> 
<p>在过去的几年里，图神经网络已经成为图领域机器学习任务中强大而实用的工具。这一进展归功于表达能力、模型灵活性和训练算法的进步。在这项研究中，我们对图神经网络进行了全面的回顾。对于GNN模型，我们根据计算模块、图类型和训练类型介绍了它的变种。此外，我们还总结了几个总体框架，并介绍了一些理论分析。在应用程序分类方面，我们将GNN应用程序分为结构化场景、非结构化场景和其他场景，并对每个场景中的应用程序进行详细的回顾。最后，我们提出了图神经网络面临的主要挑战和未来的研究方向，包括鲁棒性、可解释性、预训练和复杂结构建模。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/1ac63f9f835aac5f6fda9922718c3786/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Could not find androidx.compose:compose-compiler:xxx</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/835e9b17adf31dce60358ed4e517ac33/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">《动手学深度学习》(tensorflow版)中“d2lzh_tensorflow2“包的安装</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>