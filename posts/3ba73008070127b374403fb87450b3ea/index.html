<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>llava1.5模型安装、预测、训练详细教程 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="llava1.5模型安装、预测、训练详细教程" />
<meta property="og:description" content="引言 本博客介绍LLava1.5多模态大模型的安装教程、训练教程、预测教程，也会涉及到hugging face使用与wandb使用。
源码链接:点击这里
demo链接:点击这里
论文链接:点击这里
一、系统环境 ubuntu 20.04
gpu: 2*3090
cuda:11.6
二、LLava环境安装 1、代码下载 git clone https://github.com/haotian-liu/LLaVA.git cd LLaVA 2、虚拟环境构建 conda create -n llava python=3.10 -y conda activate llava pip install --upgrade pip # enable PEP 660 support 3、模型预测安装 pip install -e . 4、模型训练环境安装 pip install -e &#34;.[train]&#34; pip install flash-attn --no-build-isolation # 可能安装失败 5、flash-attn离线环境安装 根据对应环境格式下载相应flash-attn，
flash-attn下载链接点击这里
实际为whl的离线文件，在使用pip install *.whl 即可
三、LLava推理运行 1、启动网页预测(类似服务端与客户端) Launch a controller
python -m llava.serve.controller --host 0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/3ba73008070127b374403fb87450b3ea/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-11-02T21:29:59+08:00" />
<meta property="article:modified_time" content="2023-11-02T21:29:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">llava1.5模型安装、预测、训练详细教程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="_0"></a>引言</h2> 
<p>本博客介绍LLava1.5多模态大模型的安装教程、训练教程、预测教程，也会涉及到hugging face使用与wandb使用。</p> 
<p>源码链接:<a href="https://github.com/haotian-liu/LLaVA">点击这里</a></p> 
<p>demo链接:<a href="https://llava.hliu.cc/" rel="nofollow">点击这里</a></p> 
<p>论文链接:<a href="https://arxiv.org/abs/2310.03744" rel="nofollow">点击这里</a></p> 
<h2><a id="_13"></a>一、系统环境</h2> 
<p>ubuntu 20.04<br> gpu: 2*3090<br> cuda:11.6</p> 
<h2><a id="LLava_19"></a>二、LLava环境安装</h2> 
<h3><a id="1_20"></a>1、代码下载</h3> 
<pre><code class="prism language-c">git clone https<span class="token operator">:</span><span class="token comment">//github.com/haotian-liu/LLaVA.git</span>
cd LLaVA
</code></pre> 
<h3><a id="2_25"></a>2、虚拟环境构建</h3> 
<pre><code class="prism language-c">conda create <span class="token operator">-</span>n llava python<span class="token operator">=</span><span class="token number">3.10</span> <span class="token operator">-</span>y
conda activate llava
pip install <span class="token operator">--</span>upgrade pip  # enable PEP <span class="token number">660</span> support
</code></pre> 
<h3><a id="3_31"></a>3、模型预测安装</h3> 
<pre><code class="prism language-c">pip install <span class="token operator">-</span>e <span class="token punctuation">.</span>
</code></pre> 
<h3><a id="4_35"></a>4、模型训练环境安装</h3> 
<pre><code class="prism language-c">pip install <span class="token operator">-</span>e <span class="token string">".[train]"</span>
pip install flash<span class="token operator">-</span>attn <span class="token operator">--</span>no<span class="token operator">-</span>build<span class="token operator">-</span>isolation  # 可能安装失败
</code></pre> 
<h3><a id="5flashattn_40"></a>5、flash-attn离线环境安装</h3> 
<p>根据对应环境格式下载相应flash-attn，<br> flash-attn下载链接<a href="https://github.com/Dao-AILab/flash-attention/releases">点击这里</a><br> 实际为whl的离线文件，在使用pip install *.whl 即可</p> 
<h2><a id="LLava_45"></a>三、LLava推理运行</h2> 
<h3><a id="1_46"></a>1、启动网页预测(类似服务端与客户端)</h3> 
<p>Launch a controller</p> 
<pre><code class="prism language-Shell">python -m llava.serve.controller --host 0.0.0.0 --port 10000
</code></pre> 
<p>Launch a gradio web server.</p> 
<pre><code class="prism language-Shell">python -m llava.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload
</code></pre> 
<p><code>注:host 0.0.0.0表示自动填充本机ip，网页将其替换本机ip即可，web server启动后会有网页ip，若在其它电脑将其0.0.0.0替换运行服务器的ip即可。</code></p> 
<h3><a id="2_59"></a>2、推理权重下载</h3> 
<p>我们使用llava-v1.5-7b模型做推理。</p> 
<h4><a id="llavav157b_61"></a>llava-v1.5-7b权重下载</h4> 
<p>权重下载地址:<a href="https://huggingface.co/liuhaotian/llava-v1.5-7b/tree/main" rel="nofollow">点击这里</a></p> 
<p>权重下载，需要使用hugging face才能下载，自己注册账号即可，文件格式如下：<br> <img src="https://images2.imgbox.com/20/71/r62eK0vG_o.png" alt="在这里插入图片描述"></p> 
<p>将图示文件全部下载，内有一个config.json文件，该文件很重要，部分内容如下：</p> 
<p><img src="https://images2.imgbox.com/5e/e8/XaGFskH1_o.png" alt="在这里插入图片描述"></p> 
<p>该文件可看出视觉编码也缺少相应权重，需下载如下内容，并将其路径修改本地权重保存文件。</p> 
<h4><a id="clipvitlargepatch14336_76"></a>clip-vit-large-patch14-336权重下载</h4> 
<p>权重下载地址:<a href="https://huggingface.co/openai/clip-vit-large-patch14-336/tree/main" rel="nofollow">点击这里</a><br> 该文件可通过点击链接连接，也可在hugging face自行搜索。<br> <img src="https://images2.imgbox.com/39/76/lKwRZk0A_o.png" alt="在这里插入图片描述"></p> 
<p>同理，也是全部下载，放到一个文件夹中。</p> 
<h3><a id="3_84"></a>3、启动预测模型</h3> 
<p>若已完成权重下载，便可执行以下模型启动命令，而–model-path后面需跟模型权重路径文件，若联网能范文hugging face便可无需修改直接使用官方给定命令。</p> 
<pre><code class="prism language-shell">python <span class="token parameter variable">-m</span> llava.serve.model_worker <span class="token parameter variable">--host</span> <span class="token number">0.0</span>.0.0 <span class="token parameter variable">--controller</span> http://localhost:10000 <span class="token parameter variable">--port</span> <span class="token number">40000</span> <span class="token parameter variable">--worker</span> http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b

</code></pre> 
<p>–load-4bit: 该指令也可加上，使用4bit模型推理</p> 
<h2><a id="LLavalora_95"></a>四、LLava的lora训练</h2> 
<h3><a id="1_97"></a>1、权重下载</h3> 
<p>根据下图的finetune_lora.sh文件指定权重在hugging face下载即可，如下图。</p> 
<p><img src="https://images2.imgbox.com/bd/9a/rx09cxEj_o.png" alt="在这里插入图片描述"></p> 
<p>下载好对应权重，即可修改路径路径，如下：<br> <img src="https://images2.imgbox.com/6f/7a/D0dFVlee_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="2_106"></a>2、数据准备</h3> 
<p>使用官网也行，若不想下载太多，使用下面代码准备部分也行，如下：</p> 
<pre><code class="prism language-c">import json

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token char">'__main__'</span><span class="token operator">:</span>
    json_root<span class="token operator">=</span>r<span class="token char">'*\llava_v1_5_mix665k.json'</span>
    with <span class="token function">open</span><span class="token punctuation">(</span>json_root<span class="token punctuation">,</span> <span class="token char">'r'</span><span class="token punctuation">)</span> as f<span class="token operator">:</span>
        json_info <span class="token operator">=</span> json<span class="token punctuation">.</span><span class="token function">load</span><span class="token punctuation">(</span>f<span class="token punctuation">)</span>
    save_info<span class="token operator">=</span>json_info<span class="token punctuation">[</span><span class="token operator">:</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token operator">+</span>json_info<span class="token punctuation">[</span><span class="token number">480000</span><span class="token operator">:</span><span class="token number">481000</span><span class="token punctuation">]</span><span class="token operator">+</span>json_info<span class="token punctuation">[</span><span class="token number">620000</span><span class="token operator">:</span><span class="token number">621000</span><span class="token punctuation">]</span>
    with <span class="token function">open</span><span class="token punctuation">(</span><span class="token char">'info.json'</span><span class="token punctuation">,</span> <span class="token char">'w'</span><span class="token punctuation">)</span> as fp<span class="token operator">:</span>
        json<span class="token punctuation">.</span><span class="token function">dump</span><span class="token punctuation">(</span>save_info<span class="token punctuation">,</span> fp<span class="token punctuation">,</span> indent<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>

</code></pre> 
<p>数据图如下：<br> <img src="https://images2.imgbox.com/36/2a/DnuyKx1I_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="3_123"></a>3、训练命令</h3> 
<p>我是将其移动到LLAVA文件内，直接执行此命令：</p> 
<pre><code class="prism language-shell">finetune_lora.sh
</code></pre> 
<h3><a id="4_128"></a>4、报错处理</h3> 
<p>模型有可能报数据错误，此时不用担心，该问题是数据的问题，如下：<br> <img src="https://images2.imgbox.com/c3/da/sDsLxPXX_o.png" alt="在这里插入图片描述"></p> 
<p>直接修改上面代码如下：</p> 
<pre><code class="prism language-c">def <span class="token function">get_modality_length_grouped_indices</span><span class="token punctuation">(</span>lengths<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> generator<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token operator">:</span>
    <span class="token macro property"><span class="token directive-hash">#</span> <span class="token expression">We need to use torch <span class="token keyword">for</span> the random part as a distributed sampler will set the random seed <span class="token keyword">for</span> torch<span class="token punctuation">.</span></span></span>
    assert <span class="token function">all</span><span class="token punctuation">(</span>l <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">for</span> l in lengths<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"Should not have zero length."</span>
    mm_indices<span class="token punctuation">,</span> mm_lengths <span class="token operator">=</span> <span class="token function">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> l<span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> l in <span class="token function">enumerate</span><span class="token punctuation">(</span>lengths<span class="token punctuation">)</span> <span class="token keyword">if</span> l <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">lang</span><span class="token expression">_indices<span class="token punctuation">,</span> lang_lengths <span class="token operator">=</span> <span class="token function">zip</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> <span class="token operator">-</span>l<span class="token punctuation">)</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> l in <span class="token function">enumerate</span><span class="token punctuation">(</span>lengths<span class="token punctuation">)</span> <span class="token keyword">if</span> l <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span></span>

    lang_indices<span class="token punctuation">,</span> lang_lengths<span class="token operator">=</span>mm_indices<span class="token punctuation">,</span> mm_lengths

    assert <span class="token function">len</span><span class="token punctuation">(</span>mm_indices<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"Should have at least one multimodal sample."</span>
    assert <span class="token function">len</span><span class="token punctuation">(</span>lang_indices<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">"Should have at least one language sample."</span>

</code></pre> 
<p>也有可能有其它数据问题，是可能无eval数据，添加以下红色框即可，如下:</p> 
<p><img src="https://images2.imgbox.com/28/a9/ITv1RQER_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="5_153"></a>5、训练效果</h3> 
<p>若完成以上方式，使用训练命令，可实现如下训练效果：<br> <img src="https://images2.imgbox.com/93/0b/nFPwBX8k_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="6wandb_160"></a>6、训练使用wandb</h3> 
<p>llava训练自带wandb方式显示化查看，训练代码会自动提醒你，你只需注册，然后将其key复制，即可实现，其效果如下：<br> <img src="https://images2.imgbox.com/7a/70/BgQ9cOLi_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/e9/b5/4y8HuLPo_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_170"></a>总结</h2> 
<p>以上便llava所有运行过程，愿踩过的坑对你有帮助。最后，我额外说下，我使用4090显卡搭建，跑测试问题不大，大概16g左右吧，跑训练一张24G卡有些够呛。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8c7525b4cf628397e2cea2950ee66975/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Win10系统盘迁移--不借助第三方工具</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b6d92877ff52dfd08cdfefcfe5d98686/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">图像特征Vol.1：计算机视觉特征度量|第二弹：【统计区域度量】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>