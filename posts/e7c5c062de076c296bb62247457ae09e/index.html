<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>使用sklearn中的神经网络模块MLPClassifier处理分类问题 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="使用sklearn中的神经网络模块MLPClassifier处理分类问题" />
<meta property="og:description" content="MLPClassifier（多层感知器分类器） 一.首先简单使用sklearn中的neural_network,实例1:
#coding=utf-8 &#39;&#39;&#39; Created on 2017-12-7 &#39;&#39;&#39; from sklearn.neural_network import MLPClassifier X = [[0., 0.], [1., 1.]] y = [0, 1] mlp = MLPClassifier(solver=&#39;lbfgs&#39;, alpha=1e-5,hidden_layer_sizes=(5, 5), random_state=1) mlp.fit(X, y) print mlp.n_layers_ print mlp.n_iter_ print mlp.loss_ print mlp.out_activation_ # 输出分别为 # 4 # 15 # 0.000141233472537 # logistic 二.MNIST数据集的下载 MNIST是一些手写数字的图片，通过http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz下载数据集。 三.使用神经网络训练MNIST数据集并实现分类，实例2
#coding=utf-8 &#39;&#39;&#39; Created on 2017-12-6 &#39;&#39;&#39; from sklearn.neural_network import MLPClassifier from sklearn.datasets import fetch_mldata import numpy as np import pickle import gzip # 加载数据 # mnist = fetch_mldata(&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e7c5c062de076c296bb62247457ae09e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-12-07T17:12:50+08:00" />
<meta property="article:modified_time" content="2017-12-07T17:12:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">使用sklearn中的神经网络模块MLPClassifier处理分类问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>MLPClassifier（多层感知器分类器） <br> 一.首先简单使用sklearn中的neural_network,实例1:</p> 
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#coding=utf-8</span>
<span class="hljs-string">'''
Created on 2017-12-7

'''</span>
<span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier
X = [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>], [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]]
y = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
mlp = MLPClassifier(solver=<span class="hljs-string">'lbfgs'</span>, alpha=<span class="hljs-number">1e-5</span>,hidden_layer_sizes=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), random_state=<span class="hljs-number">1</span>)
mlp.fit(X, y)                         
<span class="hljs-keyword">print</span> mlp.n_layers_
<span class="hljs-keyword">print</span> mlp.n_iter_
<span class="hljs-keyword">print</span> mlp.loss_
<span class="hljs-keyword">print</span> mlp.out_activation_
<span class="hljs-comment"># 输出分别为</span>
<span class="hljs-comment"># 4</span>
<span class="hljs-comment"># 15</span>
<span class="hljs-comment"># 0.000141233472537</span>
<span class="hljs-comment"># logistic</span></code></pre> 
<p>二.MNIST数据集的下载 <br> MNIST是一些手写数字的图片，通过<a target="_blank" href="http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz" rel="nofollow noopener noreferrer">http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</a>下载数据集。 <br> 三.使用神经网络训练MNIST数据集并实现分类，实例2</p> 
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#coding=utf-8</span>
<span class="hljs-string">'''
Created on 2017-12-6

'''</span>

<span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_mldata
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> gzip

<span class="hljs-comment"># 加载数据</span>
<span class="hljs-comment"># mnist = fetch_mldata("MNIST original")</span>
<span class="hljs-keyword">with</span> gzip.open(<span class="hljs-string">"D:\\xxx\\mnist.pkl.gz"</span>) <span class="hljs-keyword">as</span> fp:
    training_data,valid_data,test_data = pickle.load(fp)
x_training_data,y_training_data = training_data
x_valid_data,y_valid_data = valid_data
x_test_data,y_test_data = test_data
classes = np.unique(y_test_data)

<span class="hljs-comment"># 将验证集和训练集合并</span>
x_training_data_final = np.vstack((x_training_data,x_valid_data))
y_training_data_final = np.append(y_training_data,y_valid_data)


<span class="hljs-comment"># 设置神经网络模型参数</span>
<span class="hljs-comment"># mlp = MLPClassifier(solver='lbfgs', activation='relu',alpha=1e-4,hidden_layer_sizes=(50,50), random_state=1,max_iter=10,verbose=10,learning_rate_init=.1)</span>
<span class="hljs-comment"># 使用solver='lbfgs',准确率为79%，比较适合小(少于几千)数据集来说，且使用的是全训练集训练，比较消耗内存</span>
<span class="hljs-comment"># mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(50,50), random_state=1,max_iter=10,verbose=10,learning_rate_init=.1)</span>
<span class="hljs-comment"># 使用solver='adam'，准确率只有67%</span>
mlp = MLPClassifier(solver=<span class="hljs-string">'sgd'</span>, activation=<span class="hljs-string">'relu'</span>,alpha=<span class="hljs-number">1e-4</span>,hidden_layer_sizes=(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>), random_state=<span class="hljs-number">1</span>,max_iter=<span class="hljs-number">10</span>,verbose=<span class="hljs-number">10</span>,learning_rate_init=<span class="hljs-number">.1</span>)
<span class="hljs-comment"># 使用solver='sgd'，准确率为98%，且每次训练都会分batch，消耗更小的内存</span>

<span class="hljs-comment"># 训练模型</span>
mlp.fit(x_training_data_final, y_training_data_final) 

<span class="hljs-comment"># 查看模型结果</span>
<span class="hljs-keyword">print</span> mlp.score(x_test_data,y_test_data)
<span class="hljs-keyword">print</span> mlp.n_layers_
<span class="hljs-keyword">print</span> mlp.n_iter_
<span class="hljs-keyword">print</span> mlp.loss_
<span class="hljs-keyword">print</span> mlp.out_activation_
<span class="hljs-comment"># Iteration 1, loss = 0.31452262</span>
<span class="hljs-comment"># Iteration 2, loss = 0.13094946</span>
<span class="hljs-comment"># Iteration 3, loss = 0.09715855</span>
<span class="hljs-comment"># Iteration 4, loss = 0.08033498</span>
<span class="hljs-comment"># Iteration 5, loss = 0.06761733</span>
<span class="hljs-comment"># Iteration 6, loss = 0.06085069</span>
<span class="hljs-comment"># Iteration 7, loss = 0.05485305</span>
<span class="hljs-comment"># Iteration 8, loss = 0.04950742</span>
<span class="hljs-comment"># Iteration 9, loss = 0.04468061</span>
<span class="hljs-comment"># Iteration 10, loss = 0.04156696</span>
<span class="hljs-comment"># D:\Python27\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.</span>
<span class="hljs-comment">#   % (), ConvergenceWarning)</span>
<span class="hljs-comment"># 0.9726</span>
<span class="hljs-comment"># 4</span>
<span class="hljs-comment"># 10</span>
<span class="hljs-comment"># 0.0415669639552</span>
<span class="hljs-comment"># softmax</span>
</code></pre> 
<p>四.分析简单实例1和实例2 <br> 对于神经网络的输出层（即mlp.out_activation_,且mlp.out_activation_不可设置），激活函数的选取还是有一定的原则的： <br> 1) 如果是两类判别，输出层只有一个神经元，那么选logistic，即实例1mlp.out_activation_输出； <br> 2) 如果是n类判别，输出层有n个神经元，那么选softmax即实例2，输出0-9多分类； <br> 3) 如果是回归，那么选线性。 <br> 这些选择并不是为了提高性能，而只是让输出的范围合理。 <br> 对于判别问题，输出是概率，所以必须在0到1之间，多类判别时还需要加起来等于1； <br> 对于回归问题，一般没有理由要求输出在0到1之间。 <br> 五.fit和partial_fit进行训练的区别 <br> 可以看出即使训练的顺序和迭代的次数一样但准确率仍然有区别。</p> 
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#coding=utf-8</span>
<span class="hljs-string">'''
Created on 2017-12-6

'''</span>

<span class="hljs-keyword">from</span> sklearn.neural_network <span class="hljs-keyword">import</span> MLPClassifier
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> fetch_mldata
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> gzip

<span class="hljs-comment"># 加载数据</span>
<span class="hljs-comment"># mnist = fetch_mldata("MNIST original")</span>
<span class="hljs-keyword">with</span> gzip.open(<span class="hljs-string">"D:\\xxx\\mnist.pkl.gz"</span>) <span class="hljs-keyword">as</span> fp:
    training_data,valid_data,test_data = pickle.load(fp)
x_training_data,y_training_data = training_data
x_valid_data,y_valid_data = valid_data
x_test_data,y_test_data = test_data
classes = np.unique(y_test_data)

<span class="hljs-comment"># 将验证集和训练集合并</span>
x_training_data_final = np.vstack((x_training_data,x_valid_data))
y_training_data_final = np.append(y_training_data,y_valid_data)

<span class="hljs-comment"># 将数据切割成batch</span>
x_training_data_final1 = x_training_data_final[:<span class="hljs-number">30000</span>]
y_training_data_final1 = y_training_data_final[:<span class="hljs-number">30000</span>]
x_training_data_final2 = x_training_data_final[<span class="hljs-number">30000</span>:]
y_training_data_final2 = y_training_data_final[<span class="hljs-number">30000</span>:]


<span class="hljs-comment"># 设置神经网络模型参数</span>
mlp = MLPClassifier(solver=<span class="hljs-string">'sgd'</span>, activation=<span class="hljs-string">'relu'</span>,alpha=<span class="hljs-number">1e-4</span>,hidden_layer_sizes=(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>), random_state=<span class="hljs-number">1</span>,max_iter=<span class="hljs-number">10</span>,verbose=<span class="hljs-number">10</span>,learning_rate_init=<span class="hljs-number">.1</span>)

<span class="hljs-comment"># 训练模型</span>
<span class="hljs-comment"># mlp.fit(x_training_data_final, y_training_data_final) </span>
<span class="hljs-comment"># # Iteration 1, loss = 0.31452262</span>
<span class="hljs-comment"># # Iteration 2, loss = 0.13094946</span>
<span class="hljs-comment"># # Iteration 3, loss = 0.09715855</span>
<span class="hljs-comment"># # Iteration 4, loss = 0.08033498</span>
<span class="hljs-comment"># # Iteration 5, loss = 0.06761733</span>
<span class="hljs-comment"># # Iteration 6, loss = 0.06085069</span>
<span class="hljs-comment"># # Iteration 7, loss = 0.05485305</span>
<span class="hljs-comment"># # Iteration 8, loss = 0.04950742</span>
<span class="hljs-comment"># # Iteration 9, loss = 0.04468061</span>
<span class="hljs-comment"># # Iteration 10, loss = 0.04156696</span>
<span class="hljs-comment"># # 0.9726</span>
<span class="hljs-comment"># # 4</span>
<span class="hljs-comment"># # 10</span>
<span class="hljs-comment"># # 0.0415669639552</span>
<span class="hljs-comment"># # softmax</span>


<span class="hljs-comment"># #partial_fit只会进行一次迭代，需要循环进行迭代max_iter=10</span>
<span class="hljs-comment"># for i in range(10):</span>
<span class="hljs-comment">#     #第一次调用需要加classes</span>
<span class="hljs-comment">#     mlp.partial_fit(x_training_data_final1,y_training_data_final1,classes)</span>
<span class="hljs-comment">#     mlp.partial_fit(x_training_data_final2,y_training_data_final2)</span>
<span class="hljs-comment"># # Iteration 1, loss = 0.45255654</span>
<span class="hljs-comment"># # Iteration 2, loss = 0.18496152</span>
<span class="hljs-comment"># # Iteration 3, loss = 0.13822314</span>
<span class="hljs-comment"># # Iteration 4, loss = 0.12282689</span>
<span class="hljs-comment"># # Iteration 5, loss = 0.10101545</span>
<span class="hljs-comment"># # Iteration 6, loss = 0.09582430</span>
<span class="hljs-comment"># # Iteration 7, loss = 0.07989131</span>
<span class="hljs-comment"># # Iteration 8, loss = 0.07949180</span>
<span class="hljs-comment"># # Iteration 9, loss = 0.06864537</span>
<span class="hljs-comment"># # Iteration 10, loss = 0.06737931</span>
<span class="hljs-comment"># # Iteration 11, loss = 0.05911539</span>
<span class="hljs-comment"># # Iteration 12, loss = 0.06098091</span>
<span class="hljs-comment"># # Iteration 13, loss = 0.05412258</span>
<span class="hljs-comment"># # Iteration 14, loss = 0.05293271</span>
<span class="hljs-comment"># # Iteration 15, loss = 0.04915052</span>
<span class="hljs-comment"># # Iteration 16, loss = 0.04907510</span>
<span class="hljs-comment"># # Iteration 17, loss = 0.04609325</span>
<span class="hljs-comment"># # Iteration 18, loss = 0.04990013</span>
<span class="hljs-comment"># # Iteration 19, loss = 0.04260426</span>
<span class="hljs-comment"># # Iteration 20, loss = 0.04509487</span>
<span class="hljs-comment"># # 0.9664</span>
<span class="hljs-comment"># # 4</span>
<span class="hljs-comment"># # 20</span>
<span class="hljs-comment"># # 0.0450948664394</span>
<span class="hljs-comment"># # softmax</span>

<span class="hljs-comment"># 查看模型结果</span>
<span class="hljs-keyword">print</span> mlp.score(x_test_data,y_test_data)
<span class="hljs-keyword">print</span> mlp.n_layers_
<span class="hljs-keyword">print</span> mlp.n_iter_
<span class="hljs-keyword">print</span> mlp.loss_
<span class="hljs-keyword">print</span> mlp.out_activation_</code></pre> 
<p>六.neural_network中MLPClassifier各个参数 <br> 例如:mlp = MLPClassifier(solver=’sgd’, activation=’relu’,alpha=1e-4,hidden_layer_sizes=(50,50), random_state=1,max_iter=10,verbose=10,learning_rate_init=.1)</p> 
<p>参数说明: <br> 1. hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。 <br> 2. activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu <br> - identity：f(x) = x <br> - logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)). <br> - tanh：f(x) = tanh(x). <br> - relu：f(x) = max(0, x) <br> 3. solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 <br> - lbfgs：quasi-Newton方法的优化器 <br> - sgd：随机梯度下降 <br> - adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 <br> 注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。 <br> 4. alpha :float,可选的，默认0.0001,正则化项参数 <br> 5. batch_size : int , 可选的，默认’auto’,随机优化的minibatches的大小batch_size=min(200,n_samples)，如果solver是’lbfgs’，分类器将不使用minibatch <br> 6. learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant <br> - ‘constant’: 有’learning_rate_init’给定的恒定学习率 <br> - ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t) <br> - ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. <br> 7. power_t: double, 可选, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。 <br> 8. max_iter: int，可选，默认200，最大迭代次数。 <br> 9. random_state:int 或RandomState，可选，默认None，随机数生成器的状态或种子。 <br> 10. shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 <br> 11. tol：float, 可选，默认1e-4，优化的容忍度 <br> 12. learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。 <br> 14. verbose : bool, 可选, 默认False,是否将过程打印到stdout <br> 15. warm_start : bool, 可选, 默认False,当设置成True，使用之前的解决方法作为初始拟合，否则释放之前的解决方法。 <br> 16. momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用. <br> 17. nesterovs_momentum : boolean, 默认True, Whether to use Nesterov’s momentum. 只有solver=’sgd’并且momentum &gt; 0使用. <br> 18. early_stopping : bool, 默认False,只有solver=’sgd’或者’adam’时有效,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善，低于tol时终止训练。 <br> 19. validation_fraction : float, 可选, 默认 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用 <br> 20. beta_1 : float, 可选, 默认0.9，只有solver=’adam’时使用，估计一阶矩向量的指数衰减速率，[0,1)之间 <br> 21. beta_2 : float, 可选, 默认0.999,只有solver=’adam’时使用估计二阶矩向量的指数衰减速率[0,1)之间 <br> 22. epsilon : float, 可选, 默认1e-8,只有solver=’adam’时使用数值稳定值。 <br> 属性说明： <br> - classes_:每个输出的类标签 <br> - loss_:损失函数计算出来的当前损失值 <br> - coefs_:列表中的第i个元素表示i层的权重矩阵 <br> - intercepts_:列表中第i个元素代表i+1层的偏差向量 <br> - n_iter_ ：迭代次数 <br> - n_layers_:层数 <br> - n_outputs_:输出的个数 <br> - out_activation_:输出激活函数的名称。 </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/340e71917985187d9574b691edf10027/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">BeanUtils中copyProperties的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/3c0ce7d1af905e0954edc49fc3e66b2a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">部署.net core到CentOS系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>