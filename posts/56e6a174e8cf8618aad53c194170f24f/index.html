<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>机器学习周报第28周 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="机器学习周报第28周" />
<meta property="og:description" content="目录 摘要Abstract一、文献阅读1.题目：2.摘要3.问题描述4.过去方案5.论文方案6.论文模型7.相关代码 摘要 本周阅读了一篇混沌时间序列预测的论文，论文模型主要使用的是时间卷积网络（Temporal Convolutional Network，TCN）、LSTM以及GRU。在数据集方面除了使用现实的时间序列数据外，还通过若干混沌系统生成了一些混沌的时间序列数据，这些数据没有现实方面的意义，但可以用来证明论文模型的实用性。因为混沌时间序列在现实世界普遍存在，例如水质，股票，天气等，所以论文模型也有运用于预测的潜力。
Abstract This week, We read a paper on chaotic time series prediction. The paper primarily utilized models such as Temporal Convolutional Network (TCN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). In terms of the dataset, in addition to using real-world time series data, the paper also generated chaotic time series data through several chaotic systems. Although these chaotic data lack real-world significance, they serve to demonstrate the practicality of the proposed models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/56e6a174e8cf8618aad53c194170f24f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-14T16:43:08+08:00" />
<meta property="article:modified_time" content="2024-01-14T16:43:08+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习周报第28周</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">摘要</a></li><li><a href="#Abstract_3" rel="nofollow">Abstract</a></li><li><a href="#_5" rel="nofollow">一、文献阅读</a></li><li><ul><li><a href="#1_6" rel="nofollow">1.题目：</a></li><li><a href="#2_8" rel="nofollow">2.摘要</a></li><li><a href="#3_10" rel="nofollow">3.问题描述</a></li><li><a href="#4_12" rel="nofollow">4.过去方案</a></li><li><a href="#5_14" rel="nofollow">5.论文方案</a></li><li><a href="#6_30" rel="nofollow">6.论文模型</a></li><li><a href="#7_33" rel="nofollow">7.相关代码</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>摘要</h2> 
<p>本周阅读了一篇混沌时间序列预测的论文，论文模型主要使用的是时间卷积网络（Temporal Convolutional Network，TCN）、LSTM以及GRU。在数据集方面除了使用现实的时间序列数据外，还通过若干混沌系统生成了一些混沌的时间序列数据，这些数据没有现实方面的意义，但可以用来证明论文模型的实用性。因为混沌时间序列在现实世界普遍存在，例如水质，股票，天气等，所以论文模型也有运用于预测的潜力。</p> 
<h2><a id="Abstract_3"></a>Abstract</h2> 
<p>This week, We read a paper on chaotic time series prediction. The paper primarily utilized models such as Temporal Convolutional Network (TCN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). In terms of the dataset, in addition to using real-world time series data, the paper also generated chaotic time series data through several chaotic systems. Although these chaotic data lack real-world significance, they serve to demonstrate the practicality of the proposed models. Given the prevalence of chaotic time series in the real world, such as in water quality, stock markets, and weather patterns, the models presented in the paper hold potential for applications in water quality prediction.</p> 
<h2><a id="_5"></a>一、文献阅读</h2> 
<h3><a id="1_6"></a>1.题目：</h3> 
<p>Temporal Convolutional Networks with RNN approach for chaotic time series prediction</p> 
<h3><a id="2_8"></a>2.摘要</h3> 
<p>混沌时间序列（Chaotic Time Series）的预测构成了科学与工程领域的许多系统，近年来成为研究者关注的焦点。混沌时间序列预测是使用先前观测到的数据对具有已知初始条件的非线性混沌系统进行未来预测。混沌时间序列预测可以应用于天气预报、金融和股票市场等许多领域。许多学科致力于解决时间序列预测问题，从提前几天预测天气事件到交易者预测股票的未来。在最近的研究中，已经知道到混合深度神经网络方法在解决时间序列预测问题方面具有更好的性能，并且因为从解决此类问题的多种方法的优势中受益而使得其越来越受欢迎。本文提出了一种用于混沌时间序列预测的混合深度神经网络架构。所使用的混合方法包括时间卷积网络，用于从输入和递归神经网络层中提取低级特征，例如长短期记忆和门控递归单元，以捕获时间信息。</p> 
<h3><a id="3_10"></a>3.问题描述</h3> 
<p>自然界中许多随时间变化的系统的数学模型具有动态系统属性，这些系统会产生混沌数据并对其进行操作。虽然混沌还没有一个普遍接受的定义，但它是以非线性系统的形式出现的，产生对初始条件敏感的非周期性输出。混沌信号是具有连续功率谱的类噪声信号。这些信号可以在自然界中找到，例如流体流动、心跳不规则、天气和气候数据，以及许多具有现实生活模式的系统，例如股票市场和道路交通。随着混沌时间序列预测研究的成功率的提高，对于这些难以预测的过程的未来情况，将有可能进行预防和准备。目前，研究人员对混沌脑电图(EEG)、载波振动、臭氧浓度、太阳活动、风速等包含混沌时间序列的系统进行未来预测，并根据这些预测提供必要的系统控制。混沌时间序列预测已成为最近研究人员最感兴趣的时间序列问题之一。其原因是自然界中的许多系统都表现出动态行为，并且随着混沌时间序列的预测成为可能，自然界中的许多现象将变得可预测。</p> 
<h3><a id="4_12"></a>4.过去方案</h3> 
<p>早期的预测研究中使用的传统自回归和移动平均预测方法也被用于时间序列，但由于其线性性质，它们的预测性能较低。因此，人工神经网络（ANN）和机器学习（ML）算法在2000年已经取代了时间序列预测问题的统计方法，因为它们在处理非线性方面取得了很高的成功。通过将ANN、ML与统计方法相结合，相比较单个预测算法往往能得到更高的精度。自2010年以来，深度神经网络（DNN）的使用已开始在研究中广泛使用。DNN广泛用于时间序列预测问题，可分为四种主要网络架构：Elman循环神经网络 （ERNN）、卷积神经网络（CNN）和时间卷积网络（TCN）。通过对真实世界工程测试用例的实验研究，观察到与19种不同的方法相比，ERNN架构给出了更准确的预测。除了在时间序列预测问题中使用 ERNN之外，LSTM是另一种递归神经网络架构，包含可以存储过去输入信息的门，近年来也在该领域得到了广泛的应用。CNN结构通常用于二维数据处理，在解决分类问题中占有重要地位，近年来也被用于时间序列数据的预测，并且已经观察到CNN结构在时间序列预测问题中取得了高性能的结果。</p> 
<h3><a id="5_14"></a>5.论文方案</h3> 
<p>该文提出了一种由不同神经网络层组合而成的混合模型，即时间卷积神经网络（TCN）和循环神经网络（RNN）。主要由TCN 、LSTM和GRU层组成的RNN架构的TCN来解决混沌时间序列预测问题。TCN有助于提取时间序列中短时间内发生的变化特征，而LSTM揭示了长时间内发生的变化特征，GRU具有有效的非线性拟合能力。通过这种方式，可以对分布在宽频谱上的所有特征进行建模。在所使用的模型中，TCN提取时间序列的一维空间特征，并将这些特征向量提供给RNN。将两个网络一起训练，从而获得两轮特征提取和两轮数据杂质过滤。由于TCN已经大大减少了数据杂质，因此随后的RNN阶段可以更有效地工作并更好地提取序列特征。<br> 通常认为，卷积神经网络非常适合用于图像处理任务，但实际上它也适用于时间序列预测任务。卷积操作有一维卷积和二维卷积两种方式，当卷积神经网络用于处理时间序列时，通常是对时间序列进行一维卷积。所谓的一维卷积就是从上至下、依据样本的顺序对矩阵进行扫描点积并生成一个新序列的计算方式。 与图像处理的二维卷积一样，可以创建多个卷积核对同一组数据进行多次扫描，使得算法以不同的角度来解读同样的数据。<br> <img src="https://images2.imgbox.com/0d/25/MVBgBTYp_o.png" alt="在这里插入图片描述"><br> 而且还可以在卷积的基础上再做卷积，通过多层卷积来放大感受野，使得最深层次的输出结果能考虑到更大范围的时间序列数据。例如下图中，第一层卷积的一次点积只考虑了原始时间序列的三行数据，而第二层卷积的一次点积则可以考虑原始时间序列的五行数据，更多层卷积下去考虑的也更多。<br> <img src="https://images2.imgbox.com/87/fe/XOicaMIz_o.png" alt="在这里插入图片描述"><br> 上述只是一维卷积的基本原理，在TCN中最为核心的是被称之为膨胀因果卷积的计算结构，它由膨胀卷积和因果卷积两种卷积构成。<br> 在处理时间序列数据时，特别是在预测未来的时刻，我们不希望未来的信息在当前时刻被考虑。比如说在下图t=1时刻进行的卷积实际上是参考了未来t=2和t=3时刻的数据，也就是说我们在用未来的数据在进行当前时刻的预测，这显然和我们期望的不一样。<br> <img src="https://images2.imgbox.com/1d/4c/LzdGIhLW_o.png" alt="在这里插入图片描述"><br> 因此就引入了因果卷积的概念。因果卷积保证了在任何时间点t，输出只依赖于时间点t及其之前的输入，而不依赖t之后的输入。因果卷积可以通过对输入数据进行适当的“填充"来实现。具体地说，假设我们有一个一维的输入序列和一个大小为k的卷积核，为了实现因果卷积，我们可以在序列的开始处填充k-1个零，然后进行标准的卷积操作。这样，卷积的输出在任何时间点t都只会依赖于时间点t及其之前的输入，如下图所示：<br> <img src="https://images2.imgbox.com/62/b0/yFCu5Phc_o.png" alt="在这里插入图片描述"><br> 膨胀卷积是TCN中的关键组件，它可以通过对卷积核填上“空洞"的方式来放大卷积层的感受野。填补空洞的方式是卷及操作中常见的方式，这种方式无需增加模型参数或计算成本，就可以轻松放大感受野。在标准的卷积中，卷积核的元素是连续的，一次覆盖输入数据的连续部分。而在膨胀卷积中，卷积核的元素之间存在间隔，这些间隔使得卷积核可以覆盖更广的范围。如下图所示，当我们使用膨胀指数为1时，就是在原始卷积核的每行中填补一行0。<br> <img src="https://images2.imgbox.com/64/19/OJv9TOED_o.png" alt="在这里插入图片描述"><br> 虽然添0会让卷积核在一次点积运算中损失一部分时间序列的信息，但是也让卷积核看到了“更远的过去”，在很大程度上放大了感受野的范围，而且这种放大不会增加额外的训练参数。值得注意的是，就算某些数据在这一次的卷积中未被捕获，在上一次或者下一次的卷积中还是会被考虑到的，所有这种损失是可以接受的。<br> <img src="https://images2.imgbox.com/24/c2/jXK3SKZL_o.png" alt="在这里插入图片描述"><br> 很显然，如果我们使用更大的膨胀指数，那感受野就可以被放得更大。在TCN当中，原作者建议的膨胀指数是第一个卷积层使用1，第二个卷积层使用2，第三个卷积层使用4，这种结构可以使网络的顶层捕捉到非常长的时间依赖关系，而底层则可以捕捉到更短的依赖关系。本篇论文也是使用的这样的卷积指数。</p> 
<h3><a id="6_30"></a>6.论文模型</h3> 
<p>本文提出了一种将时间卷积神经网络与递归神经网络相结合的时间序列预测方法。当研究时间序列预测文献时，值得注意的是，TCN架构在解决大多数预测问题时比LSTM和GRU表现更好，并且大多数研究都研究了多少层堆叠的TCN层能给出最好的结果。由于TCN本身的结构，随着层数的增加，卷积层的计算负荷增加，模型并不总是给出更成功的结果。在本研究提出的混合结构中，TCN被用作第一个DNN层，在广泛的感受野中提取低级特征。特征的时间信息，即TCN层的输出，在下一步与RNN层一起处理，以充分利用混沌时间序列。本文提出了两种不同的混合方法。如下图所示，第一种方法是在TCN层输出后使用LSTM和Dense Layer进行预测，第二种方法是在TCN层输出后使用GRU和Dense Layer进行预测。在本研究范围内，使用TCN层将数据转换为模式，然后使用RNN层将模式传输到具有记忆门的未来时间，从而提高了时间序列的预测性能。<br> <img src="https://images2.imgbox.com/64/ee/gNt0N5v9_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="7_33"></a>7.相关代码</h3> 
<p>TCN模型定义：</p> 
<pre><code class="prism language-c">import torch
import torch<span class="token punctuation">.</span>nn as nn
from torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils import weight_norm
 
 
class <span class="token function">Chomp1d</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>
    def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> chomp_size<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token function">super</span><span class="token punctuation">(</span>Chomp1d<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>chomp_size <span class="token operator">=</span> chomp_size
 
    def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        裁剪模块，裁剪多出来的padding
        <span class="token string">""</span>"
        <span class="token keyword">return</span> x<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token punctuation">,</span> <span class="token operator">:</span><span class="token operator">-</span>self<span class="token punctuation">.</span>chomp_size<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">contiguous</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 
 
class <span class="token function">TemporalBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>
    def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_inputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        相当于一个Residual block
        <span class="token operator">:</span>param n_inputs<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 输入通道数
        <span class="token operator">:</span>param n_outputs<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 输出通道数
        <span class="token operator">:</span>param kernel_size<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 卷积核尺寸
        <span class="token operator">:</span>param stride<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 步长，一般为<span class="token number">1</span>
        <span class="token operator">:</span>param dilation<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 膨胀系数
        <span class="token operator">:</span>param padding<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 填充系数
        <span class="token operator">:</span>param dropout<span class="token operator">:</span> <span class="token keyword">float</span><span class="token punctuation">,</span> dropout比率
        <span class="token string">""</span>"
        <span class="token function">super</span><span class="token punctuation">(</span>TemporalBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> <span class="token function">weight_norm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span><span class="token function">Conv1d</span><span class="token punctuation">(</span>n_inputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                                           stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
        # 经过conv1，输出的size其实是<span class="token punctuation">(</span>Batch<span class="token punctuation">,</span> input_channel<span class="token punctuation">,</span> seq_len <span class="token operator">+</span> padding<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>chomp1 <span class="token operator">=</span> <span class="token function">Chomp1d</span><span class="token punctuation">(</span>padding<span class="token punctuation">)</span>  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len
        self<span class="token punctuation">.</span>relu1 <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout1 <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Dropout</span><span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> <span class="token function">weight_norm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span><span class="token function">Conv1d</span><span class="token punctuation">(</span>n_outputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                                           stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>chomp2 <span class="token operator">=</span> <span class="token function">Chomp1d</span><span class="token punctuation">(</span>padding<span class="token punctuation">)</span>  #  裁剪掉多出来的padding部分，维持输出时间步为seq_len
        self<span class="token punctuation">.</span>relu2 <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout2 <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Dropout</span><span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
 
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Sequential</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>chomp1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>relu1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dropout1<span class="token punctuation">,</span>
                                 self<span class="token punctuation">.</span>conv2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>chomp2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>relu2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dropout2<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Conv1d</span><span class="token punctuation">(</span>n_inputs<span class="token punctuation">,</span> n_outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">if</span> n_inputs <span class="token operator">!=</span> n_outputs <span class="token keyword">else</span> None
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">ReLU</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span><span class="token function">init_weights</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 
    def <span class="token function">init_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        参数初始化
        <span class="token operator">:</span><span class="token keyword">return</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        self<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token function">normal_</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token function">normal_</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample is not None<span class="token operator">:</span>
            self<span class="token punctuation">.</span>downsample<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token function">normal_</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span>
 
    def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        <span class="token operator">:</span>param x<span class="token operator">:</span> size <span class="token function">of</span> <span class="token punctuation">(</span>Batch<span class="token punctuation">,</span> input_channel<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>
        <span class="token operator">:</span><span class="token keyword">return</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        out <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">net</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        res <span class="token operator">=</span> x <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample is None <span class="token keyword">else</span> self<span class="token punctuation">.</span><span class="token function">downsample</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span><span class="token function">relu</span><span class="token punctuation">(</span>out <span class="token operator">+</span> res<span class="token punctuation">)</span>
 
 
class <span class="token function">TemporalConvNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>
    def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        TCN，目前paper给出的TCN结构很好的支持每个时刻为一个数的情况，即sequence结构，
        对于每个时刻为一个向量这种一维结构，勉强可以把向量拆成若干该时刻的输入通道，
        对于每个时刻为一个矩阵或更高维图像的情况，就不太好办。
        <span class="token operator">:</span>param num_inputs<span class="token operator">:</span> <span class="token keyword">int</span>， 输入通道数
        <span class="token operator">:</span>param num_channels<span class="token operator">:</span> list，每层的hidden_channel数，例如<span class="token punctuation">[</span><span class="token number">25</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">,</span><span class="token number">25</span><span class="token punctuation">]</span>表示有<span class="token number">4</span>个隐层，每层hidden_channel数为<span class="token number">25</span>
        <span class="token operator">:</span>param kernel_size<span class="token operator">:</span> <span class="token keyword">int</span><span class="token punctuation">,</span> 卷积核尺寸
        <span class="token operator">:</span>param dropout<span class="token operator">:</span> <span class="token keyword">float</span><span class="token punctuation">,</span> drop_out比率
        <span class="token string">""</span>"
        <span class="token function">super</span><span class="token punctuation">(</span>TemporalConvNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        num_levels <span class="token operator">=</span> <span class="token function">len</span><span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i in <span class="token function">range</span><span class="token punctuation">(</span>num_levels<span class="token punctuation">)</span><span class="token operator">:</span>
            dilation_size <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span><span class="token operator">*</span> i   # 膨胀系数：<span class="token number">1</span>，<span class="token number">2</span>，<span class="token number">4</span>，<span class="token number">8</span>……
            in_channels <span class="token operator">=</span> num_inputs <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> num_channels<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  # 确定每一层的输入通道数
            out_channels <span class="token operator">=</span> num_channels<span class="token punctuation">[</span>i<span class="token punctuation">]</span>  # 确定每一层的输出通道数
            layers <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token function">TemporalBlock</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation_size<span class="token punctuation">,</span>
                                     padding<span class="token operator">=</span><span class="token punctuation">(</span>kernel_size<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> dilation_size<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span><span class="token punctuation">]</span>
 
        self<span class="token punctuation">.</span>network <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Sequential</span><span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>
 
    def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token operator">:</span>
        <span class="token string">""</span>"
        输入x的结构不同于RNN，一般RNN的size为<span class="token punctuation">(</span>Batch<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> channels<span class="token punctuation">)</span>或者<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> Batch<span class="token punctuation">,</span> channels<span class="token punctuation">)</span>，
        这里把seq_len放在channels后面，把所有时间步的数据拼起来，当做Conv1d的输入尺寸，实现卷积跨时间步的操作，
        很巧妙的设计。
        
        <span class="token operator">:</span>param x<span class="token operator">:</span> size <span class="token function">of</span> <span class="token punctuation">(</span>Batch<span class="token punctuation">,</span> input_channel<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>
        <span class="token operator">:</span><span class="token keyword">return</span><span class="token operator">:</span> size <span class="token function">of</span> <span class="token punctuation">(</span>Batch<span class="token punctuation">,</span> output_channel<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span>
        <span class="token string">""</span>"
        <span class="token keyword">return</span> self<span class="token punctuation">.</span><span class="token function">network</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<p>论文参数下的Lorenz系统：</p> 
<pre><code class="prism language-c">import numpy as np
import matplotlib<span class="token punctuation">.</span>pyplot as plt
from mpl_toolkits<span class="token punctuation">.</span>mplot3d import Axes3D
 
def <span class="token function">lorenz_system</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">,</span> sigma<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> rho<span class="token operator">=</span><span class="token number">28</span><span class="token punctuation">,</span> beta<span class="token operator">=</span><span class="token number">8</span><span class="token operator">/</span><span class="token number">3</span><span class="token punctuation">,</span> initial_state<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token operator">:</span>
    dt <span class="token operator">=</span> <span class="token number">0.01</span>
    <span class="token keyword">if</span> initial_state is None<span class="token operator">:</span>
        state <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token operator">:</span>
        state <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>initial_state<span class="token punctuation">)</span>
    trajectory <span class="token operator">=</span> <span class="token punctuation">[</span>state<span class="token punctuation">]</span>
 
    <span class="token keyword">for</span> _ in <span class="token function">range</span><span class="token punctuation">(</span>num_steps<span class="token punctuation">)</span><span class="token operator">:</span>
        dx <span class="token operator">=</span> sigma <span class="token operator">*</span> <span class="token punctuation">(</span>state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        dy <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>rho <span class="token operator">-</span> state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        dz <span class="token operator">=</span> state<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> state<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> beta <span class="token operator">*</span> state<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
 
        state <span class="token operator">=</span> state <span class="token operator">+</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span><span class="token punctuation">[</span>dx<span class="token punctuation">,</span> dy<span class="token punctuation">,</span> dz<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> dt
        trajectory<span class="token punctuation">.</span><span class="token function">append</span><span class="token punctuation">(</span>state<span class="token punctuation">.</span><span class="token function">copy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 
    <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token function">array</span><span class="token punctuation">(</span>trajectory<span class="token punctuation">)</span>
 
# 生成 Lorenz 系统轨迹
num_steps <span class="token operator">=</span> <span class="token number">10000</span>
initial_state <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
trajectory <span class="token operator">=</span> <span class="token function">lorenz_system</span><span class="token punctuation">(</span>num_steps<span class="token operator">=</span>num_steps<span class="token punctuation">,</span> initial_state<span class="token operator">=</span>initial_state<span class="token punctuation">)</span>
 
# 绘制 Lorenz 系统轨迹
fig <span class="token operator">=</span> plt<span class="token punctuation">.</span><span class="token function">figure</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
ax <span class="token operator">=</span> fig<span class="token punctuation">.</span><span class="token function">add_subplot</span><span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token char">'3d'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span><span class="token function">plot</span><span class="token punctuation">(</span>trajectory<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> trajectory<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> trajectory<span class="token punctuation">[</span><span class="token operator">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lw<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span><span class="token function">set_xlabel</span><span class="token punctuation">(</span><span class="token char">'X'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span><span class="token function">set_ylabel</span><span class="token punctuation">(</span><span class="token char">'Y'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span><span class="token function">set_zlabel</span><span class="token punctuation">(</span><span class="token char">'Z'</span><span class="token punctuation">)</span>
ax<span class="token punctuation">.</span><span class="token function">set_title</span><span class="token punctuation">(</span>'Lorenz System <span class="token operator">-</span> Chaotic Trajectory'<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span><span class="token function">show</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/40143c3b5e31eb0a5a4c9bf2826ed935/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Tomcat Notes: URL Mapping</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/85a0d029f769c837ee3a8ab175c64bec/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">中职组安全-win20230217-环境-解析</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>