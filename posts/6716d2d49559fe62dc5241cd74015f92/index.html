<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>聚类算法——python实现SOM算法 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="聚类算法——python实现SOM算法" />
<meta property="og:description" content="算法简介 SOM网络是一种竞争学习型的无监督神经网络，将高维空间中相似的样本点映射到网络输出层中的邻近神经元。
训练过程简述：在接收到训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元。然后最佳匹配单元及其邻近的神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。
网络结构：输入层和输出层（或竞争层），如下图所示。 输入层：假设一个输入样本为X=[x1,x2,x3,…,xn]，是一个n维向量，则输入层神经元个数为n个。 输出层（竞争层）：通常输出层的神经元以矩阵方式排列在二维空间中，每个神经元都有一个权值向量。 假设输出层有m个神经元，则有m个权值向量，Wi = [wi1,wi2,....,win], 1&lt;=i&lt;=m。 算法流程： 1. 初始化:权值使用较小的随机值进行初始化，并对输入向量和权值做归一化处理 X’ = X/||X|| ω’i= ωi/||ωi||， 1&lt;=i&lt;=m ||X||和||ωi||分别为输入的样本向量和权值向量的欧几里得范数。 2.将样本输入网络:样本与权值向量做点积，点积值最大的输出神经元赢得竞争， （或者计算样本与权值向量的欧几里得距离，距离最小的神经元赢得竞争）记为获胜神经元。 3.更新权值:对获胜的神经元拓扑邻域内的神经元进行更新,并对学习后的权值重新归一化。 ω(t&#43;1)= ω(t)&#43; η(t，n) * (x-ω(t)) η(t，n):η为学习率是关于训练时间t和与获胜神经元的拓扑距离n的函数。 η(t，n)=η(t)e^(-n) η(t)的几种函数图像如下图所示。 4.更新学习速率η及拓扑邻域N,N随时间增大距离变小，如下图所示。 5.判断是否收敛。如果学习率η&lt;=ηmin或达到预设的迭代次数，结束算法。 python代码实现SOM import numpy as np import pylab as pl class SOM(object): def __init__(self, X, output, iteration, batch_size): &#34;&#34;&#34; :param X: 形状是N*D， 输入样本有N个,每个D维 :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵 :param iteration:迭代次数 :param batch_size:每次迭代时的样本数量 初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维 &#34;&#34;&#34; self.X = X self.output = output self." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6716d2d49559fe62dc5241cd74015f92/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-05-18T10:24:50+08:00" />
<meta property="article:modified_time" content="2017-05-18T10:24:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">聚类算法——python实现SOM算法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3 id="算法简介">算法简介</h3> 
<p>SOM网络是一种竞争学习型的无监督神经网络，将高维空间中相似的样本点映射到网络输出层中的邻近神经元。</p> 
<p>训练过程简述：在接收到训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元。然后最佳匹配单元及其邻近的神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小。这个过程不断迭代，直至收敛。</p> 
<pre><code>网络结构：输入层和输出层（或竞争层），如下图所示。
输入层：假设一个输入样本为X=[x1,x2,x3,…,xn]，是一个n维向量，则输入层神经元个数为n个。
输出层（竞争层）：通常输出层的神经元以矩阵方式排列在二维空间中，每个神经元都有一个权值向量。
假设输出层有m个神经元，则有m个权值向量，Wi = [wi1,wi2,....,win], 1&lt;=i&lt;=m。
</code></pre> 
<p><img src="https://images2.imgbox.com/18/be/q9ebiIt4_o.png" alt="这里写图片描述" title=""></p> 
<pre><code>算法流程：
1. 初始化:权值使用较小的随机值进行初始化，并对输入向量和权值做归一化处理 
          X’ = X/||X|| 
          ω’i= ωi/||ωi||， 1&lt;=i&lt;=m 
          ||X||和||ωi||分别为输入的样本向量和权值向量的欧几里得范数。
2.将样本输入网络:样本与权值向量做点积，点积值最大的输出神经元赢得竞争，
（或者计算样本与权值向量的欧几里得距离，距离最小的神经元赢得竞争）记为获胜神经元。
3.更新权值:对获胜的神经元拓扑邻域内的神经元进行更新,并对学习后的权值重新归一化。 
        ω(t+1)= ω(t)+ η(t，n) * (x-ω(t))
        η(t，n):η为学习率是关于训练时间t和与获胜神经元的拓扑距离n的函数。
        η(t，n)=η(t)e^(-n)
        η(t)的几种函数图像如下图所示。

4.更新学习速率η及拓扑邻域N,N随时间增大距离变小，如下图所示。
5.判断是否收敛。如果学习率η&lt;=ηmin或达到预设的迭代次数，结束算法。
</code></pre> 
<p><img src="https://images2.imgbox.com/e5/63/8wdH25Lu_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/26/2c/iJwlTUTo_o.png" alt="这里写图片描述" title=""></p> 
<h3 id="python代码实现som">python代码实现SOM</h3> 
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pylab <span class="hljs-keyword">as</span> pl

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SOM</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, X, output, iteration, batch_size)</span>:</span>
        <span class="hljs-string">"""
        :param X:  形状是N*D， 输入样本有N个,每个D维
        :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵
        :param iteration:迭代次数
        :param batch_size:每次迭代时的样本数量
        初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维
        """</span>
        self.X = X
        self.output = output
        self.iteration = iteration
        self.batch_size = batch_size
        self.W = np.random.rand(X.shape[<span class="hljs-number">1</span>], output[<span class="hljs-number">0</span>] * output[<span class="hljs-number">1</span>])
        <span class="hljs-keyword">print</span> (self.W.shape)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">GetN</span><span class="hljs-params">(self, t)</span>:</span>
        <span class="hljs-string">"""
        :param t:时间t, 这里用迭代次数来表示时间
        :return: 返回一个整数，表示拓扑距离，时间越大，拓扑邻域越小
        """</span>
        a = min(self.output)
        <span class="hljs-keyword">return</span> int(a-float(a)*t/self.iteration)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Geteta</span><span class="hljs-params">(self, t, n)</span>:</span>
        <span class="hljs-string">"""
        :param t: 时间t, 这里用迭代次数来表示时间
        :param n: 拓扑距离
        :return: 返回学习率，
        """</span>
        <span class="hljs-keyword">return</span> np.power(np.e, -n)/(t+<span class="hljs-number">2</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">updata_W</span><span class="hljs-params">(self, X, t, winner)</span>:</span>
        N = self.GetN(t)
        <span class="hljs-keyword">for</span> x, i <span class="hljs-keyword">in</span> enumerate(winner):
            to_update = self.getneighbor(i[<span class="hljs-number">0</span>], N)
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(N+<span class="hljs-number">1</span>):
                e = self.Geteta(t, j)
                <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> to_update[j]:
                    self.W[:, w] = np.add(self.W[:,w], e*(X[x,:] - self.W[:,w]))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getneighbor</span><span class="hljs-params">(self, index, N)</span>:</span>
        <span class="hljs-string">"""
        :param index:获胜神经元的下标
        :param N: 邻域半径
        :return ans: 返回一个集合列表，分别是不同邻域半径内需要更新的神经元坐标
        """</span>
        a, b = self.output
        length = a*b
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">distence</span><span class="hljs-params">(index1, index2)</span>:</span>
            i1_a, i1_b = index1 // a, index1 % b
            i2_a, i2_b = index2 // a, index2 % b
            <span class="hljs-keyword">return</span> np.abs(i1_a - i2_a), np.abs(i1_b - i2_b)

        ans = [set() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(N+<span class="hljs-number">1</span>)]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(length):
            dist_a, dist_b = distence(i, index)
            <span class="hljs-keyword">if</span> dist_a &lt;= N <span class="hljs-keyword">and</span> dist_b &lt;= N: ans[max(dist_a, dist_b)].add(i)
        <span class="hljs-keyword">return</span> ans




    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">"""
        train_Y:训练样本与形状为batch_size*(n*m)
        winner:一个一维向量，batch_size个获胜神经元的下标
        :return:返回值是调整后的W
        """</span>
        count = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> self.iteration &gt; count:
            train_X = self.X[np.random.choice(self.X.shape[<span class="hljs-number">0</span>], self.batch_size)]
            normal_W(self.W)
            normal_X(train_X)
            train_Y = train_X.dot(self.W)
            winner = np.argmax(train_Y, axis=<span class="hljs-number">1</span>).tolist()
            self.updata_W(train_X, count, winner)
            count += <span class="hljs-number">1</span>
        <span class="hljs-keyword">return</span> self.W

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_result</span><span class="hljs-params">(self)</span>:</span>
        normal_X(self.X)
        train_Y = self.X.dot(self.W)
        winner = np.argmax(train_Y, axis=<span class="hljs-number">1</span>).tolist()
        <span class="hljs-keyword">print</span> (winner)
        <span class="hljs-keyword">return</span> winner

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normal_X</span><span class="hljs-params">(X)</span>:</span>
    <span class="hljs-string">"""
    :param X:二维矩阵，N*D，N个D维的数据
    :return: 将X归一化的结果
    """</span>
    N, D = X.shape
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(N):
        temp = np.sum(np.multiply(X[i], X[i]))
        X[i] /= np.sqrt(temp)
    <span class="hljs-keyword">return</span> X
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">normal_W</span><span class="hljs-params">(W)</span>:</span>
    <span class="hljs-string">"""
    :param W:二维矩阵，D*(n*m)，D个n*m维的数据
    :return: 将W归一化的结果
    """</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(W.shape[<span class="hljs-number">1</span>]):
        temp = np.sum(np.multiply(W[:,i], W[:,i]))
        W[:, i] /= np.sqrt(temp)
    <span class="hljs-keyword">return</span> W

<span class="hljs-comment">#画图</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw</span><span class="hljs-params">(C)</span>:</span>
    colValue = [<span class="hljs-string">'r'</span>, <span class="hljs-string">'y'</span>, <span class="hljs-string">'g'</span>, <span class="hljs-string">'b'</span>, <span class="hljs-string">'c'</span>, <span class="hljs-string">'k'</span>, <span class="hljs-string">'m'</span>]
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(C)):
        coo_X = []    <span class="hljs-comment">#x坐标列表</span>
        coo_Y = []    <span class="hljs-comment">#y坐标列表</span>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(C[i])):
            coo_X.append(C[i][j][<span class="hljs-number">0</span>])
            coo_Y.append(C[i][j][<span class="hljs-number">1</span>])
        pl.scatter(coo_X, coo_Y, marker=<span class="hljs-string">'x'</span>, color=colValue[i%len(colValue)], label=i)

    pl.legend(loc=<span class="hljs-string">'upper right'</span>)
    pl.show()

<span class="hljs-comment">#数据集：每三个是一组分别是西瓜的编号，密度，含糖量</span>
data = <span class="hljs-string">"""
1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,
6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,
11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,
16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,
21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,
26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459"""</span>

a = data.split(<span class="hljs-string">','</span>)
dataset = np.mat([[float(a[i]), float(a[i+<span class="hljs-number">1</span>])] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, len(a)-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)])
dataset_old = dataset.copy()

som = SOM(dataset, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), <span class="hljs-number">1</span>, <span class="hljs-number">30</span>)
som.train()
res = som.train_result()
classify = {}
<span class="hljs-keyword">for</span> i, win <span class="hljs-keyword">in</span> enumerate(res):
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> classify.get(win[<span class="hljs-number">0</span>]):
        classify.setdefault(win[<span class="hljs-number">0</span>], [i])
    <span class="hljs-keyword">else</span>:
        classify[win[<span class="hljs-number">0</span>]].append(i)
C = []<span class="hljs-comment">#未归一化的数据分类结果</span>
D = []<span class="hljs-comment">#归一化的数据分类结果</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> classify.values():
    C.append(dataset_old[i].tolist())
    D.append(dataset[i].tolist())
draw(C)
draw(D)</code></pre> 
<p>由于数据比较少，就直接用的训练集做测试了，运行结果图如下，分别是对未归一化的数据和归一化的数据进行的展示。 <br> <img src="https://images2.imgbox.com/fc/f1/DoK73dBi_o.png" alt="这里写图片描述" title=""> <br> <img src="https://images2.imgbox.com/17/12/zNwe1Bsc_o.png" alt="这里写图片描述" title=""></p> 
<p>参考内容：1.《机器学习》周志华 <br> 2.<a href="https://wenku.baidu.com/view/74927ae8aeaad1f346933f42.html?qq-pf-to=pcqq.c2c" rel="nofollow noopener noreferrer" target="_blank">https://wenku.baidu.com/view/74927ae8aeaad1f346933f42.html?qq-pf-to=pcqq.c2c</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/25423efebaea8b30249ff96f91796f16/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">java 汉字与UTF-8十六进制编码 间相互转换方法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dec72a6bf7179fd6008121a61fca96ae/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">UNITY 学习笔记（三）——UGUI使用</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>