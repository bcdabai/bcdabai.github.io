<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Python 爬虫实战之爬淘宝商品并做数据分析 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Python 爬虫实战之爬淘宝商品并做数据分析" />
<meta property="og:description" content="前言 是这样的，之前接了一个金主的单子，他想在淘宝开个小鱼零食的网店，想对目前这个市场上的商品做一些分析，本来手动去做统计和分析也是可以的，这些信息都是对外展示的，只是手动比较麻烦，所以想托我去帮个忙。
一、 项目要求： 具体的要求如下：
1.在淘宝搜索“小鱼零食”，想知道前10页搜索结果的所有商品的销量和金额，按照他划定好的价格区间来统计数量，给我划分了如下的一张价格区间表：
2.这10页搜索结果中，商家都是分布在全国的哪些位置？
3.这10页的商品下面，用户评论最多的是什么？
4.从这些搜索结果中，找出销量最多的10家店铺名字和店铺链接。
从这些要求来看，其实这些需求也不难实现，我们先来看一下项目的效果。
二、效果预览 获取到数据之后做了下分析，最终做成了柱状图，鼠标移动可以看出具体的商品数量。
在10~30元之间的商品最多，越往后越少，看来大多数的产品都是定位为低端市场。
然后我们再来看一下全国商家的分布情况：
可以看出，商家分布大多都是在沿海和长江中下游附近，其中以沿海地区最为密集。
然后再来看一下用户都在商品下面评论了一些什么：
字最大的就表示出现次数最多，口感味道、包装品质、商品分量和保质期是用户评价最多的几个方面，那么在产品包装的时候可以从这几个方面去做针对性阐述，解决大多数人比较关心的问题。
最后就是销量前10的店铺和链接了。
在拿到数据并做了分析之后，我也在想，如果这个东西是我来做的话，我能不能看出来什么东西？或许可以从价格上找到切入点，或许可以从产品地理位置打个差异化，又或许可以以用户为中心，由外而内地做营销。
越往深想，越觉得有门道，算了，对于小鱼零食这一块我是外行，不多想了。
三、爬虫源码 由于源码分了几个源文件，还是比较长的，所以这里就不跟大家一一讲解了，懂爬虫的人看几遍就看懂了，不懂爬虫的说再多也是云里雾里，等以后学会了爬虫再来看就懂了。
测试淘宝爬虫数据 apikey secret
import csvimport osimport timeimport wordcloudfrom selenium import webdriverfrom selenium.webdriver.common.by import By def tongji(): prices = [] with open(&#39;前十页销量和金额.csv&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;, newline=&#39;&#39;) as f: fieldnames = [&#39;价格&#39;, &#39;销量&#39;, &#39;店铺位置&#39;] reader = csv.DictReader(f, fieldnames=fieldnames) for index, i in enumerate(reader): if index != 0: price = float(i[&#39;价格&#39;].replace(&#39;¥&#39;, &#39;&#39;)) prices." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0e8ceb64810bc9c195a36777687536c3/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-21T15:10:40+08:00" />
<meta property="article:modified_time" content="2023-09-21T15:10:40+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 爬虫实战之爬淘宝商品并做数据分析</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h4><strong>前言</strong></h4> 
<p>是这样的，之前接了一个金主的单子，他想在淘宝开个小鱼零食的网店，想对目前这个市场上的商品做一些分析，本来手动去做统计和分析也是可以的，这些信息都是对外展示的，只是手动比较麻烦，所以想托我去帮个忙。</p> 
<p></p> 
<p></p> 
<p class="img-center"><img alt="图片" height="850" src="https://images2.imgbox.com/24/f8/1njtpwrm_o.png" width="1080"></p> 
<h3><strong>一、 项目要求：</strong></h3> 
<p>具体的要求如下：</p> 
<p>1.在淘宝搜索“小鱼零食”，想知道前10页搜索结果的所有商品的销量和金额，按照他划定好的价格区间来统计数量，给我划分了如下的一张价格区间表：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="295" src="https://images2.imgbox.com/a2/59/SIWkNrlD_o.png" width="365"></p> 
<p>2.这10页搜索结果中，商家都是分布在全国的哪些位置？</p> 
<p>3.这10页的商品下面，用户评论最多的是什么？</p> 
<p>4.从这些搜索结果中，找出销量最多的10家店铺名字和店铺链接。</p> 
<p>从这些要求来看，其实这些需求也不难实现，我们先来看一下项目的效果。</p> 
<h3><strong>二、效果预览</strong></h3> 
<p>获取到数据之后做了下分析，最终做成了柱状图，鼠标移动可以看出具体的商品数量。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="428" src="https://images2.imgbox.com/65/9f/IIOX5H9i_o.gif" width="844"></p> 
<p>在10~30元之间的商品最多，越往后越少，看来大多数的产品都是定位为低端市场。</p> 
<p>然后我们再来看一下全国商家的分布情况：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="836" src="https://images2.imgbox.com/b7/e4/CI8uO93b_o.gif" width="1040"></p> 
<p>可以看出，商家分布大多都是在沿海和长江中下游附近，其中以沿海地区最为密集。</p> 
<p>然后再来看一下用户都在商品下面评论了一些什么：</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="560" src="https://images2.imgbox.com/69/ec/lCnqzKrC_o.png" width="800"></p> 
<p>字最大的就表示出现次数最多，口感味道、包装品质、商品分量和保质期是用户评价最多的几个方面，那么在产品包装的时候可以从这几个方面去做针对性阐述，解决大多数人比较关心的问题。</p> 
<p>最后就是销量前10的店铺和链接了。</p> 
<p></p> 
<p class="img-center"><img alt="图片" height="743" src="https://images2.imgbox.com/10/9f/PREWC68q_o.png" width="649"></p> 
<p>在拿到数据并做了分析之后，我也在想，如果这个东西是我来做的话，我能不能看出来什么东西？或许可以从价格上找到切入点，或许可以从产品地理位置打个差异化，又或许可以以用户为中心，由外而内地做营销。</p> 
<p>越往深想，越觉得有门道，算了，对于小鱼零食这一块我是外行，不多想了。</p> 
<h3><strong>三、爬虫源码</strong></h3> 
<p>由于源码分了几个源文件，还是比较长的，所以这里就不跟大家一一讲解了，懂爬虫的人看几遍就看懂了，不懂爬虫的说再多也是云里雾里，等以后学会了爬虫再来看就懂了。</p> 
<p><a class="link-info" href="https://o0b.cn/jennif" rel="nofollow" title="测试淘宝爬虫数据 apikey secret">测试淘宝爬虫数据 apikey secret</a></p> 
<pre><code>import csv</code><code>import os</code><code>import time</code><code>import wordcloud</code><code>from selenium import webdriver</code><code>from selenium.webdriver.common.by import By</code>

<code>def tongji():</code><code>    prices = []</code><code>    with open('前十页销量和金额.csv', 'r', encoding='utf-8', newline='') as f:</code><code>        fieldnames = ['价格', '销量', '店铺位置']</code><code>        reader = csv.DictReader(f, fieldnames=fieldnames)</code><code>        for index, i in enumerate(reader):</code><code>            if index != 0:</code><code>                price = float(i['价格'].replace('¥', ''))</code><code>                prices.append(price)</code><code>    DATAS = {'&lt;10': 0, '10~30': 0, '30~50': 0,</code><code>             '50~70': 0, '70~90': 0, '90~110': 0,</code><code>             '110~130': 0, '130~150': 0, '150~170': 0, '170~200': 0, }</code><code>    for price in prices:</code><code>        if price &lt; 10:</code><code>            DATAS['&lt;10'] += 1</code><code>        elif 10 &lt;= price &lt; 30:</code><code>            DATAS['10~30'] += 1</code><code>        elif 30 &lt;= price &lt; 50:</code><code>            DATAS['30~50'] += 1</code><code>        elif 50 &lt;= price &lt; 70:</code><code>            DATAS['50~70'] += 1</code><code>        elif 70 &lt;= price &lt; 90:</code><code>            DATAS['70~90'] += 1</code><code>        elif 90 &lt;= price &lt; 110:</code><code>            DATAS['90~110'] += 1</code><code>        elif 110 &lt;= price &lt; 130:</code><code>            DATAS['110~130'] += 1</code><code>        elif 130 &lt;= price &lt; 150:</code><code>            DATAS['130~150'] += 1</code><code>        elif 150 &lt;= price &lt; 170:</code><code>            DATAS['150~170'] += 1</code><code>        elif 170 &lt;= price &lt; 200:</code><code>            DATAS['170~200'] += 1</code>
<code>    for k, v in DATAS.items():</code><code>        print(k, ':', v)</code>

<code>def get_the_top_10(url):</code><code>    top_ten = []</code><code>    # 获取代理</code><code>    ip = zhima1()[2][random.randint(0, 399)]</code><code>    # 运行quicker动作（可以不用管）</code><code>    os.system('"C:\Program Files\Quicker\QuickerStarter.exe" runaction:5e3abcd2-9271-47b6-8eaf-3e7c8f4935d8')</code><code>    options = webdriver.ChromeOptions()</code><code>    # 远程调试Chrome</code><code>    options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')</code><code>    options.add_argument(f'--proxy-server={ip}')</code><code>    driver = webdriver.Chrome(options=options)</code><code>    # 隐式等待</code><code>    driver.implicitly_wait(3)</code><code>    # 打开网页</code><code>    driver.get(url)</code><code>    # 点击部分文字包含'销量'的网页元素</code><code>    driver.find_element(By.PARTIAL_LINK_TEXT, '销量').click()</code><code>    time.sleep(1)</code><code>    # 页面滑动到最下方</code><code>    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>    time.sleep(1)</code><code>    # 查找元素</code><code>    element = driver.find_element(By.ID, 'mainsrp-itemlist').find_element(By.XPATH, './/div[@class="items"]')</code><code>    items = element.find_elements(By.XPATH, './/div[@data-category="auctions"]')</code><code>    for index, item in enumerate(items):</code><code>        if index == 10:</code><code>            break</code><code>        # 查找元素</code><code>        price = item.find_element(By.XPATH, './div[2]/div[1]/div[contains(@class,"price")]').text</code><code>        paid_num_data = item.find_element(By.XPATH, './div[2]/div[1]/div[@class="deal-cnt"]').text</code><code>        store_location = item.find_element(By.XPATH, './div[2]/div[3]/div[@class="location"]').text</code><code>        store_href = item.find_element(By.XPATH, './div[2]/div[@class="row row-2 title"]/a').get_attribute(</code><code>            'href').strip()</code><code>        # 将数据添加到字典</code><code>        top_ten.append(</code><code>            {'价格': price,</code><code>             '销量': paid_num_data,</code><code>             '店铺位置': store_location,</code><code>             '店铺链接': store_href</code><code>             })</code>
<code>    for i in top_ten:</code><code>        print(i)</code>

<code>def get_top_10_comments(url):</code><code>    with open('排名前十评价.txt', 'w+', encoding='utf-8') as f:</code><code>        pass</code><code>    # ip = ipidea()[1]</code><code>    os.system('"C:\Program Files\Quicker\QuickerStarter.exe" runaction:5e3abcd2-9271-47b6-8eaf-3e7c8f4935d8')</code><code>    options = webdriver.ChromeOptions()</code><code>    options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')</code><code>    # options.add_argument(f'--proxy-server={ip}')</code><code>    driver = webdriver.Chrome(options=options)</code><code>    driver.implicitly_wait(3)</code><code>    driver.get(url)</code><code>    driver.find_element(By.PARTIAL_LINK_TEXT, '销量').click()</code><code>    time.sleep(1)</code><code>    element = driver.find_element(By.ID, 'mainsrp-itemlist').find_element(By.XPATH, './/div[@class="items"]')</code><code>    items = element.find_elements(By.XPATH, './/div[@data-category="auctions"]')</code><code>    original_handle = driver.current_window_handle</code><code>    item_hrefs = []</code><code>    # 先获取前十的链接</code><code>    for index, item in enumerate(items):</code><code>        if index == 10:</code><code>            break</code><code>        item_hrefs.append(</code><code>            item.find_element(By.XPATH, './/div[2]/div[@class="row row-2 title"]/a').get_attribute('href').strip())</code><code>    # 爬取前十每个商品评价</code><code>    for item_href in item_hrefs:</code><code>        # 打开新标签</code><code>        # item_href = 'https://item.taobao.com/item.htm?id=523351391646&amp;ns=1&amp;abbucket=11#detail'</code><code>        driver.execute_script(f'window.open("{item_href}")')</code><code>        # 切换过去</code><code>        handles = driver.window_handles</code><code>        driver.switch_to.window(handles[-1])</code>
<code>        # 页面向下滑动一部分，直到让评价那两个字显示出来</code><code>        try:</code><code>            driver.find_element(By.PARTIAL_LINK_TEXT, '评价').click()</code><code>        except Exception as e1:</code><code>            try:</code><code>                x = driver.find_element(By.PARTIAL_LINK_TEXT, '评价').location_once_scrolled_into_view</code><code>                driver.find_element(By.PARTIAL_LINK_TEXT, '评价').click()</code><code>            except Exception as e2:</code><code>                try:</code><code>                    # 先向下滑动100，放置评价2个字没显示在屏幕内</code><code>                    driver.execute_script('var q=document.documentElement.scrollTop=100')</code><code>                    x = driver.find_element(By.PARTIAL_LINK_TEXT, '评价').location_once_scrolled_into_view</code><code>                except Exception as e3:</code><code>                    driver.find_element(By.XPATH, '/html/body/div[6]/div/div[3]/div[2]/div/div[2]/ul/li[2]/a').click()</code><code>        time.sleep(1)</code><code>        try:</code><code>            trs = driver.find_elements(By.XPATH, '//div[@class="rate-grid"]/table/tbody/tr')</code><code>            for index, tr in enumerate(trs):</code><code>                if index == 0:</code><code>                    comments = tr.find_element(By.XPATH, './td[1]/div[1]/div/div').text.strip()</code><code>                else:</code><code>                    try:</code><code>                        comments = tr.find_element(By.XPATH,</code><code>                                                   './td[1]/div[1]/div[@class="tm-rate-fulltxt"]').text.strip()</code><code>                    except Exception as e:</code><code>                        comments = tr.find_element(By.XPATH,</code><code>                                                   './td[1]/div[1]/div[@class="tm-rate-content"]/div[@class="tm-rate-fulltxt"]').text.strip()</code><code>                with open('排名前十评价.txt', 'a+', encoding='utf-8') as f:</code><code>                    f.write(comments + '\n')</code><code>                    print(comments)</code><code>        except Exception as e:</code><code>            lis = driver.find_elements(By.XPATH, '//div[@class="J_KgRate_MainReviews"]/div[@class="tb-revbd"]/ul/li')</code><code>            for li in lis:</code><code>                comments = li.find_element(By.XPATH, './div[2]/div/div[1]').text.strip()</code><code>                with open('排名前十评价.txt', 'a+', encoding='utf-8') as f:</code><code>                    f.write(comments + '\n')</code><code>                    print(comments)</code>

<code>def get_top_10_comments_wordcloud():</code><code>    file = '排名前十评价.txt'</code><code>    f = open(file, encoding='utf-8')</code><code>    txt = f.read()</code><code>    f.close()</code>
<code>    w = wordcloud.WordCloud(width=1000,</code><code>                            height=700,</code><code>                            background_color='white',</code><code>                            font_path='msyh.ttc')</code><code>    # 创建词云对象，并设置生成图片的属性</code>
<code>    w.generate(txt)</code><code>    name = file.replace('.txt', '')</code><code>    w.to_file(name + '词云.png')</code><code>    os.startfile(name + '词云.png')</code>

<code>def get_10_pages_datas():</code><code>    with open('前十页销量和金额.csv', 'w+', encoding='utf-8', newline='') as f:</code><code>        f.write('\ufeff')</code><code>        fieldnames = ['价格', '销量', '店铺位置']</code><code>        writer = csv.DictWriter(f, fieldnames=fieldnames)</code><code>        writer.writeheader()</code><code>    infos = []</code><code>    options = webdriver.ChromeOptions()</code><code>    options.add_experimental_option('debuggerAddress', '127.0.0.1:9222')</code><code>    # options.add_argument(f'--proxy-server={ip}')</code><code>    driver = webdriver.Chrome(options=options)</code><code>    driver.implicitly_wait(3)</code><code>    driver.get(url)</code><code>    # driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>    element = driver.find_element(By.ID, 'mainsrp-itemlist').find_element(By.XPATH, './/div[@class="items"]')</code><code>    items = element.find_elements(By.XPATH, './/div[@data-category="auctions"]')</code><code>    for index, item in enumerate(items):</code><code>        price = item.find_element(By.XPATH, './div[2]/div[1]/div[contains(@class,"price")]').text</code><code>        paid_num_data = item.find_element(By.XPATH, './div[2]/div[1]/div[@class="deal-cnt"]').text</code><code>        store_location = item.find_element(By.XPATH, './div[2]/div[3]/div[@class="location"]').text</code><code>        infos.append(</code><code>            {'价格': price,</code><code>             '销量': paid_num_data,</code><code>             '店铺位置': store_location})</code><code>    try:</code><code>        driver.find_element(By.PARTIAL_LINK_TEXT, '下一').click()</code><code>    except Exception as e:</code><code>        driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>        driver.find_element(By.PARTIAL_LINK_TEXT, '下一').click()</code><code>    for i in range(9):</code><code>        time.sleep(1)</code><code>        driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>        element = driver.find_element(By.ID, 'mainsrp-itemlist').find_element(By.XPATH, './/div[@class="items"]')</code><code>        items = element.find_elements(By.XPATH, './/div[@data-category="auctions"]')</code><code>        for index, item in enumerate(items):</code><code>            try:</code><code>                price = item.find_element(By.XPATH, './div[2]/div[1]/div[contains(@class,"price")]').text</code><code>            except Exception:</code><code>                time.sleep(1)</code><code>                driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>                price = item.find_element(By.XPATH, './div[2]/div[1]/div[contains(@class,"price")]').text</code><code>            paid_num_data = item.find_element(By.XPATH, './div[2]/div[1]/div[@class="deal-cnt"]').text</code><code>            store_location = item.find_element(By.XPATH, './div[2]/div[3]/div[@class="location"]').text</code><code>            infos.append(</code><code>                {'价格': price,</code><code>                 '销量': paid_num_data,</code><code>                 '店铺位置': store_location})</code><code>        try:</code><code>            driver.find_element(By.PARTIAL_LINK_TEXT, '下一').click()</code><code>        except Exception as e:</code><code>            driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')</code><code>            driver.find_element(By.PARTIAL_LINK_TEXT, '下一').click()</code><code>        # 一页结束</code><code>        for info in infos:</code><code>            print(info)</code><code>        with open('前十页销量和金额.csv', 'a+', encoding='utf-8', newline='') as f:</code><code>            fieldnames = ['价格', '销量', '店铺位置']</code><code>            writer = csv.DictWriter(f, fieldnames=fieldnames)</code><code>            for info in infos:</code><code>                writer.writerow(info)</code>

<code>if __name__ == '__main__':</code><code>    url = 'https://s.taobao.com/search?q=%E5%B0%8F%E9%B1%BC%E9%9B%B6%E9%A3%9F&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.21814703.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170306&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=2%2C48&amp;s=0'</code><code>    # get_10_pages_datas()</code><code>    # tongji()</code><code>    # get_the_top_10(url)</code><code>    # get_top_10_comments(url)</code><code>    get_top_10_comments_wordcloud()</code></pre> 
<p>通过上面的代码，我们能获取到想要获取的数据，然后再Bar和Geo进行柱状图和地理位置分布展示，这两块大家可以去摸索一下。</p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d76923d6c26e8e1a2933d383c98d1c0e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">使用锐捷RG-EG210G-E路由器实现两个IP地址冲突的局域网互通</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1569498a3899f141cc43e9293fe3da24/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">教你用Python写一个京东自动下单抢购脚本（Python实现京东自动抢购）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>