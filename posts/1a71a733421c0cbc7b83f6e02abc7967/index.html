<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习参数技巧 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习参数技巧" />
<meta property="og:description" content="1：优化器
机器学习训练的目的在于更新参数，优化目标函数，常见优化器有SGD，Adagrad，Adadelta，Adam，Adamax，Nadam。
其中SGD和Adam优化器是最为常用的两种优化器，SGD根据每个batch的数据计算一次局部的估计，最小化代价函数。学习速率决定了每次步进的大小，因此我们需要选择一个合适的学习速率进行调优。学习速率太大会导致不收敛，速率太小收敛速度慢。
因此SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。Adam优化器结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，能够自动调整学习速率，收敛速度更快，在复杂网络中表现更优。
2：学习速率
学习速率的设置第一次可以设置大一点的学习率加快收敛，后续慢慢调整；也可以采用动态变化学习速率的方式（比如，每一轮乘以一个衰减系数或者根据损失的变化动态调整学习速率）。 3：dropout
数据第一次跑模型的时候可以不加dropout，后期调优的时候dropout用于防止过拟合有比较明显的效果，特别是数据量相对较小的时候。
4：变量初始化。 常见的变量初始化有零值初始化、随机初始化、均匀分布初始值、正态分布初始值和正交分布初始值。一般采用正态分布或均匀分布的初始化值，有的论文说正交分布的初始值能带来更好的效果。实验的时候可以才正态分布和正交分布初始值做一个尝试。
5：训练轮数
模型收敛即可停止迭代，一般可采用验证集作为停止迭代的条件。如果连续几轮模型损失都没有相应减少，则停止迭代。
6：正则化
为了防止过拟合，可通过加入l1、l2正则化。从公式可以看出，加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。
7：预训练
对需要训练的语料进行预训练可以加快训练速度，并且对于模型最终的效果会有少量的提升，常用的预训练工具有word2vec和glove。
8：激活函数
常用的激活函数为sigmoid、tanh、relu、leaky relu、elu。采用sigmoid激活函数计算量较大，而且sigmoid饱和区变换缓慢，求导趋近于0，导致梯度消失。sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。
tanh它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。relu从公式上可以看出，解决了gradient vanishing问题并且计算简单更容易优化，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）；leaky relu有relu的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明leaky relu总是好于relu。
elu也是为解决relu存在的问题而提出，elu有relu的基本所有优点，但计算量稍大，并且没有完全证明elu总是好于relu。
9：特征学习函数
常用的特征学习函数有cnn、rnn、lstm、gru。cnn注重词位置上的特征，而具有时序关系的词采用rnn、lstm、gru抽取特征会更有效。gru是简化版的lstm，具有更少的参数，训练速度更快。但是对于足够的训练数据，为了追求更好的性能可以采用lstm模型。
10：特征抽取
max-pooling、avg-pooling是深度学习中最常用的特征抽取方式。max-pooling是抽取最大的信息向量，然而当存在多个有用的信息向量时，这样的操作会丢失大量有用的信息。avg-pooling是对所有信息向量求平均，当仅仅部分向量相关而大部分向量无关时，会导致有用信息向量被噪声淹没。
针对这样的情况，在有多个有用向量的情形下尽量在最终的代表向量中保留这些有用的向量信息，又想在只有一个显著相关向量的情形下直接提取该向量做代表向量，避免其被噪声淹没。那么解决方案只有：加权平均，即Attention。
11：每轮训练数据乱序 每轮数据迭代保持不同的顺序，避免模型每轮都对相同的数据进行计算。
12：batch_size选择 对于小数据量的模型，可以全量训练，这样能更准确的朝着极值所在的方向更新。但是对于大数据，全量训练将会导致内存溢出，因此需要选择一个较小的batch_size。如果这时选择batch_size为1，则此时为在线学习，每次修正方向为各自样本的梯度方向修正，难以达到收敛。batch_size增大，处理相同数据量的时间减少，但是达到相同精度的轮数增多。
实际中可以逐步增大batch_size，随着batch_size增大，模型达到收敛，并且训练时间最为合适" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/1a71a733421c0cbc7b83f6e02abc7967/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-07-13T19:24:59+08:00" />
<meta property="article:modified_time" content="2017-07-13T19:24:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习参数技巧</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">1：优化器</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">机器学习训练的目的在于更新参数，优化目标函数，常见优化器有SGD，Adagrad，Adadelta，Adam，Adamax，Nadam。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">其中SGD和Adam优化器是最为常用的两种优化器，SGD根据每个batch的数据计算一次局部的估计，最小化代价函数。学习速率决定了每次步进的大小，因此我们需要选择一个合适的学习速率进行调优。学习速率太大会导致不收敛，速率太小收敛速度慢。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">因此SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。Adam优化器结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，能够自动调整学习速率，收敛速度更快，在复杂网络中表现更优。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">2：学习速率</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">学习速率的设置第一次可以设置大一点的学习率加快收敛，后续慢慢调整；也可以采用动态变化学习速率的方式（比如，每一轮乘以一个衰减系数或者根据损失的变化动态调整学习速率）。<br style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"> </span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">3：dropout</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"></span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">数据第一次跑模型的时候可以不加dropout，后期调优的时候dropout用于防止过拟合有比较明显的效果，特别是数据量相对较小的时候。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">4：变量初始化。</span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"><br style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"> </span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">常见的变量初始化有零值初始化、随机初始化、均匀分布初始值、正态分布初始值和正交分布初始值。一般采用正态分布或均匀分布的初始化值，有的论文说正交分布的初始值能带来更好的效果。实验的时候可以才正态分布和正交分布初始值做一个尝试。</span></p> 
<p style='margin-top:0px; margin-bottom:0px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; word-wrap:break-word!important'> <br style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"> </p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">5：训练轮数</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">模型收敛即可停止迭代，一般可采用验证集作为停止迭代的条件。如果连续几轮模型损失都没有相应减少，则停止迭代。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">6：正</span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">则化</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"></span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">为了防止过拟合，可通过加入l1、l2正则化。从公式可以看出，加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">7</span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">：预训练</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"></span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">对需要训练的语料进行预训练可以加快训练速度，并且对于模型最终的效果会有少量的提升，常用的预训练工具有word2vec和glove。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">8：激活函数</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">常用的激活函数为sigmoid、tanh、relu、leaky relu、elu。采用sigmoid激活函数计算量较大，而且sigmoid饱和区变换缓慢，求导趋近于0，导致梯度消失。sigmoid函数的输出值恒大于0，这会导致模型训练的收敛速度变慢。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">tanh它解决了zero-centered的输出问题，然而，gradient vanishing的问题和幂运算的问题仍然存在。relu从公式上可以看出，解决了gradient vanishing问题并且计算简单更容易优化，但是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）；leaky relu有relu的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明leaky relu总是好于relu。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">elu也是为解决relu存在的问题而提出，elu有relu的基本所有优点，但计算量稍大，并且没有完全证明elu总是好于relu。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">9：特征学习函数</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">常用的特征学习函数有cnn、rnn、lstm、gru。cnn注重词位置上的特征，而具有时序关系的词采用rnn、lstm、gru抽取特征会更有效。gru是简化版的lstm，具有更少的参数，训练速度更快。但是对于足够的训练数据，为了追求更好的性能可以采用lstm模型。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">10：特征抽取</span></span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"></span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">max-pooling、avg-pooling是深度学习中最常用的特征抽取方式。max-pooling是抽取最大的信息向量，然而当存在多个有用的信息向量时，这样的操作会丢失大量有用的信息。avg-pooling是对所有信息向量求平均，当仅仅部分向量相关而大部分向量无关时，会导致有用信息向量被噪声淹没。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">针对这样的情况，在有多个有用向量的情形下尽量在最终的代表向量中保留这些有用的向量信息，又想在只有一个显著相关向量的情形下直接提取该向量做代表向量，避免其被噪声淹没。<span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">那么解决方案只有：加权平均，即Attention</span>。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">11：每轮训练数据乱序</span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"><br style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"> </span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">每轮数据迭代保持不同的顺序，避免模型每轮都对相同的数据进行计算。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important">12：batch_size选择</span></span><span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px"><br style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important"> </span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">对于小数据量的模型，可以全量训练，这样能更准确的朝着极值所在的方向更新。但是对于大数据，全量训练将会导致内存溢出，因此需要选择一个较小的batch_size。如果这时选择batch_size为1，则此时为在线学习，每次修正方向为各自样本的梯度方向修正，难以达到收敛。batch_size增大，处理相同数据量的时间减少，但是达到相同精度的轮数增多。</span></p> 
<p style='margin-top:0px; margin-bottom:20px; padding-top:0px; padding-bottom:0px; max-width:100%; clear:both; min-height:1em; color:rgb(62,62,62); font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",Arial,sans-serif; font-size:16px; line-height:1.75em; word-wrap:break-word!important'> <span style="margin:0px; padding:0px; max-width:100%; word-wrap:break-word!important; font-size:15px">实际中可以逐步增大batch_size，随着batch_size增大，模型达到收敛，并且训练时间最为合适</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d81e71b205bfac224deb26f4b031aeb5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">tensorflow 变量生成 变量管理 tf.Variable、tf.get_variable、tf.variable_scope</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/aa30b90f16cfda5550d5bdcf785df2cd/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">修改oracle数据库 db_name,instace_name,sid_name</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>