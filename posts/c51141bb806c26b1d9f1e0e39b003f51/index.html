<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>了解神经网络的超参数 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="了解神经网络的超参数" />
<meta property="og:description" content="神经网络超参数有哪些？ 神经网路中的超参数主要包括1. 学习率 ηη，2. 正则化参数 λλ，3. 神经网络的层数 LL，4. 每一个隐层中神经元的个数 jj，5. 学习的回合数EpochEpoch，6. 小批量数据 minibatchminibatch 的大小，7. 输出神经元的编码方式，8. 代价函数的选择，9. 权重初始化的方法，10. 神经元激活函数的种类，11.参加训练模型数据的规模 这十一类超参数。
隐藏层层数和每层神经元个数的设定 有一般经验公式。
详情请看BP神经网络隐藏层节点数如何确定
注意 数据正则化检查结果数据预处理数据规范化
注意数据归一化，数据标准化，正则化的区别
详情请看我搭的神经网络不work该怎么办！看看这11条新手最容易犯的错误 sklearn.MLPClassifier的主要参数 hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
The ith element represents the number of neurons in the ith hidden layer.
用于设置隐藏层大小，很重要。可以自动调参
activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’
Activation function for the hidden layer.
‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/c51141bb806c26b1d9f1e0e39b003f51/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-24T16:30:34+08:00" />
<meta property="article:modified_time" content="2019-03-24T16:30:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">了解神经网络的超参数</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3><a id="_0"></a>神经网络超参数有哪些？</h3> 
<blockquote> 
 <p>神经网路中的超参数主要包括1. 学习率 ηη，2. 正则化参数 λλ，3. 神经网络的层数 LL，4. 每一个隐层中神经元的个数 jj，5. 学习的回合数EpochEpoch，6. 小批量数据 minibatchminibatch 的大小，7. 输出神经元的编码方式，8. 代价函数的选择，9. 权重初始化的方法，10. 神经元激活函数的种类，11.参加训练模型数据的规模 这十一类超参数。</p> 
</blockquote> 
<h3><a id="_4"></a>隐藏层层数和每层神经元个数的设定</h3> 
<p>有一般经验公式。<br> 详情请看<a href="http://www.ilovematlab.cn/thread-209458-1-1.html" rel="nofollow">BP神经网络隐藏层节点数如何确定</a></p> 
<h3><a id="_7"></a>注意</h3> 
<ul><li>数据正则化</li><li>检查结果</li><li>数据预处理</li><li>数据规范化<br> 注意数据归一化，数据标准化，正则化的区别<br> 详情请看<a href="http://www.sohu.com/a/191189706_610300" rel="nofollow">我搭的神经网络不work该怎么办！看看这11条新手最容易犯的错误 </a></li></ul> 
<h3><a id="sklearnMLPClassifier_16"></a>sklearn.MLPClassifier的主要参数</h3> 
<p><code>hidden_layer_sizes</code> : tuple, length = n_layers - 2, default (100,)<br> The ith element represents the number of neurons in the ith hidden layer.<br> 用于设置隐藏层大小，很重要。可以自动调参</p> 
<p><code>activation</code> : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’<br> Activation function for the hidden layer.<br> ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x<br> ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).<br> ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).<br> ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)<br> 激活函数，默认‘relu’。可以手动调参。</p> 
<p><code>solver</code> : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’<br> The solver for weight optimization.<br> ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.<br> ‘sgd’ refers to stochastic gradient descent.<br> ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba<br> Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better.<br> 大数据集上，adam更好。根据先验知识，手动调参。</p> 
<p><code>alpha</code> : float, optional, default 0.0001<br> L2 penalty (regularization term) parameter.<br> 正则化系数，影响模型泛化能力。</p> 
<p><code>batch_size</code> : int, optional, default ‘auto’<br> Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)<br> 每次训练的的样本数目</p> 
<p><code>learning_rate</code> : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’<br> Learning rate schedule for weight updates.<br> ‘constant’ is a constant learning rate given by ‘learning_rate_init’.<br> ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)<br> ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5.<br> Only used when solver=‘sgd’.<br> 当solver是sgd的时候，才使用的参数。</p> 
<p><code>learning_rate_init</code> : double, optional, default 0.001<br> The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.<br> 初始的学习率。</p> 
<p><code>power_t</code> : double, optional, default 0.5<br> The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.<br> 当solver是sgd时使用。</p> 
<p><code>max_iter</code> : int, optional, default 200<br> Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.<br> 最大迭代次数</p> 
<p><code>shuffle</code> : bool, optional, default True<br> Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’.</p> 
<p><code>random_state</code> : int, RandomState instance or None, optional, default None<br> If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.</p> 
<p><code>tol</code> : float, optional, default 1e-4<br> Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.<br> 停止训练的阈值。</p> 
<p><code>verbose</code> : bool, optional, default False<br> Whether to print progress messages to stdout.<br> 是否打印训练过程的信息。</p> 
<p><code>warm_start</code> : bool, optional, default False<br> When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary.</p> 
<p><code>momentum</code> : float, default 0.9<br> Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.</p> 
<p><code>nesterovs_momentum</code> : boolean, default True<br> Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum &gt; 0.</p> 
<p><code>early_stopping</code> : bool, default False<br> Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. Only effective when solver=’sgd’ or ‘adam’</p> 
<p><code>validation_fraction</code> : float, optional, default 0.1<br> The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True</p> 
<p><code>beta_1</code> : float, optional, default 0.9<br> Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=’adam’</p> 
<p><code>beta_2</code> : float, optional, default 0.999<br> Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=’adam’</p> 
<p><code>epsilon</code> : float, optional, default 1e-8<br> Value for numerical stability in adam. Only used when solver=’adam’</p> 
<p><code>n_iter_no_change</code> : int, optional, default 10<br> Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’</p> 
<h3><a id="Reference_108"></a>Reference</h3> 
<p><a href="http://www.ilovematlab.cn/thread-209458-1-1.html" rel="nofollow">BP神经网络隐藏层节点数如何确定</a><br> <a href="https://blog.csdn.net/dugudaibo/article/details/77366245">如何选择神经网络的超参数</a><br> <a href="https://blog.csdn.net/weixin_38278334/article/details/83023958">sklearn 神经网络MLPclassifier参数详解</a><br> <a href="https://blog.csdn.net/qq_37274615/article/details/81147013">卷积神经网络训练三个概念（epoch，迭代次数，batchsize）</a><br> <a href="http://www.sohu.com/a/191189706_610300" rel="nofollow">我搭的神经网络不work该怎么办！看看这11条新手最容易犯的错误 </a><br> <a href="https://blog.csdn.net/tianguiyuyu/article/details/80694669">归一化，标准化，正则化的概念和区别</a><br> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" rel="nofollow">sklearn.neural_network.MLPClassifier</a><br> <a href="https://www.jqr.com/article/000544" rel="nofollow">如何得出神经网络需要多少隐藏层、每层需要多少神经元？</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/082aef5d8e1e95940c34d5e98070f59e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于XAMPP创建数据库与本地数据库的使用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/362dd075c033bea7e79e4cca2d38b28c/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">keras预测函数采坑实录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>