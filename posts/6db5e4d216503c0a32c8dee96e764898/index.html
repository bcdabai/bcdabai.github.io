<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>基于 k8s和docker 构建一个高可用的 web 集群 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="基于 k8s和docker 构建一个高可用的 web 集群" />
<meta property="og:description" content="目录
一、项目整体架构图
二、项目描述
三、项目准备
1.修改主机名
2.规划项目集群机器的IP地址
3.关闭selinux和firewall
4.升级系统（也可以不升级）
5.添加域名解析
四、项目步骤
1.kubeadm安装k8s单master的集群环境
1.1.配置主机之间无密码登录
1.2.关闭交换分区
1.3.修改机器内核参数
1.4.配置阿里云的repo源
1.5.配置时间同步
1.6.安装docker服务（这一步也可以放在最前面，先安装好docker，再做其他的操作）
1.7.安装kubeadm,kubelet和kubectl（初始化k8s需要的软件包）
1.8.部署Kubernetes的Master节点
1.9.node节点服务器加入k8s集群
1.10.实现master上的pod和node节点上的pod之间通信
2.部署ansible完成相关软件的自动化运维工作
2.1.建立免密通道 在ansible主机上生成密钥对
2.2.上传公钥到所有服务器的root用户家目录下
2.3.验证是否实现免密码密钥认证
2.4.在管理节点上安装ansible
2.5.编写主机配置
3.部署nfs服务器，为整个web集群提供数据
3.1.搭建好nfs服务器
3.2.设置共享目录
3.3.挂载共享目录
3.4.创建pv及pvc使用nfs服务器上的共享目录
4.部署镜像仓库harbor
4.1.前提
4.2.搭建harbor
5.搭建gitlab
5.1.官方部署文档
5.2.安装和配置必须的依赖项
5.3.下载/安装极狐GitLab
5.4.查看密码
5.5.部署成功后使用
6.部署简单的nginx业务
6.1.打包使用python&#43;flask完成的简单项目，制作成镜像
6.2.镜像上传至本地harbor仓库
6.3.在k8s集群上安装ingress-nginx来暴露应用
6.4.使用dashboard对整个集群资源进行掌控
6.5.手工部署应用镜像到k8s使用
6.6.部署过程有报错，报错及解决
PS：拓展
一、项目整体架构图 二、项目描述 三、项目准备 1.修改主机名 hostnamcectl set-hostname k8s-master
hostnamcectl set-hostname k8s-node1
hostnamcectl set-hostname k8s-node2hostnamcectl set-hostname k8s-node3 2.规划项目集群机器的IP地址 serveripk8s-master192.168.205.143k8s-node1192.168.205.144k8s-node2192.168.205.145k8s-node3192.168.205.146ansible192.168.205.138nfs192.168.205.136harbor192.168.205.135prometheus192.168.205.134gitlab192.168.205.190 [root@k8s-master network-scripts]# cd /etc/sysconfig/network-scripts/" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6db5e4d216503c0a32c8dee96e764898/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-20T16:15:01+08:00" />
<meta property="article:modified_time" content="2023-09-20T16:15:01+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于 k8s和docker 构建一个高可用的 web 集群</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE" rel="nofollow">一、项目整体架构图</a></p> 
<p id="%E4%BA%8C%E3%80%81%E9%A1%B9%E7%9B%AE%E6%8F%8F%E8%BF%B0-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E9%A1%B9%E7%9B%AE%E6%8F%8F%E8%BF%B0" rel="nofollow">二、项目描述</a></p> 
<p id="%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E5%87%86%E5%A4%87-toc" style="margin-left:0px;"><a href="#%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E5%87%86%E5%A4%87" rel="nofollow">三、项目准备</a></p> 
<p id="1.%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D-toc" style="margin-left:160px;"><a href="#1.%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D" rel="nofollow">1.修改主机名</a></p> 
<p id="2.%E8%A7%84%E5%88%92%E9%A1%B9%E7%9B%AE%E9%9B%86%E7%BE%A4%E6%9C%BA%E5%99%A8%E7%9A%84IP%E5%9C%B0%E5%9D%80-toc" style="margin-left:160px;"><a href="#2.%E8%A7%84%E5%88%92%E9%A1%B9%E7%9B%AE%E9%9B%86%E7%BE%A4%E6%9C%BA%E5%99%A8%E7%9A%84IP%E5%9C%B0%E5%9D%80" rel="nofollow">2.规划项目集群机器的IP地址</a></p> 
<p id="3.%E5%85%B3%E9%97%ADselinux%E5%92%8Cfirewall-toc" style="margin-left:160px;"><a href="#3.%E5%85%B3%E9%97%ADselinux%E5%92%8Cfirewall" rel="nofollow">3.关闭selinux和firewall</a></p> 
<p id="4.%E5%8D%87%E7%BA%A7%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%B8%8D%E5%8D%87%E7%BA%A7%EF%BC%89-toc" style="margin-left:160px;"><a href="#4.%E5%8D%87%E7%BA%A7%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%B8%8D%E5%8D%87%E7%BA%A7%EF%BC%89" rel="nofollow">4.升级系统（也可以不升级）</a></p> 
<p id="5.%E6%B7%BB%E5%8A%A0%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90-toc" style="margin-left:160px;"><a href="#5.%E6%B7%BB%E5%8A%A0%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90" rel="nofollow">5.添加域名解析</a></p> 
<p id="%E5%9B%9B%E3%80%81%E9%A1%B9%E7%9B%AE%E6%AD%A5%E9%AA%A4-toc" style="margin-left:0px;"><a href="#%E5%9B%9B%E3%80%81%E9%A1%B9%E7%9B%AE%E6%AD%A5%E9%AA%A4" rel="nofollow">四、项目步骤</a></p> 
<p id="1.kubeadm%E5%AE%89%E8%A3%85k8s%E5%8D%95master%E7%9A%84%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83-toc" style="margin-left:80px;"><a href="#1.kubeadm%E5%AE%89%E8%A3%85k8s%E5%8D%95master%E7%9A%84%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83" rel="nofollow">1.kubeadm安装k8s单master的集群环境</a></p> 
<p id="1.1.%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95-toc" style="margin-left:160px;"><a href="#1.1.%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95" rel="nofollow">1.1.配置主机之间无密码登录</a></p> 
<p id="1.2.%E5%85%B3%E9%97%AD%E4%BA%A4%E6%8D%A2%E5%88%86%E5%8C%BA-toc" style="margin-left:160px;"><a href="#1.2.%E5%85%B3%E9%97%AD%E4%BA%A4%E6%8D%A2%E5%88%86%E5%8C%BA" rel="nofollow">1.2.关闭交换分区</a></p> 
<p id="1.3.%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0-toc" style="margin-left:160px;"><a href="#1.3.%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0" rel="nofollow">1.3.修改机器内核参数</a></p> 
<p id="1.4.%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84repo%E6%BA%90-toc" style="margin-left:160px;"><a href="#1.4.%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84repo%E6%BA%90" rel="nofollow">1.4.配置阿里云的repo源</a></p> 
<p id="1.5.%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5-toc" style="margin-left:160px;"><a href="#1.5.%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5" rel="nofollow">1.5.配置时间同步</a></p> 
<p id="1.6.%E5%AE%89%E8%A3%85docker%E6%9C%8D%E5%8A%A1%EF%BC%88%E8%BF%99%E4%B8%80%E6%AD%A5%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%94%BE%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%EF%BC%8C%E5%85%88%E5%AE%89%E8%A3%85%E5%A5%BDdocker%EF%BC%8C%E5%86%8D%E5%81%9A%E5%85%B6%E4%BB%96%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%89-toc" style="margin-left:160px;"><a href="#1.6.%E5%AE%89%E8%A3%85docker%E6%9C%8D%E5%8A%A1%EF%BC%88%E8%BF%99%E4%B8%80%E6%AD%A5%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%94%BE%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%EF%BC%8C%E5%85%88%E5%AE%89%E8%A3%85%E5%A5%BDdocker%EF%BC%8C%E5%86%8D%E5%81%9A%E5%85%B6%E4%BB%96%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%89" rel="nofollow">1.6.安装docker服务（这一步也可以放在最前面，先安装好docker，再做其他的操作）</a></p> 
<p id="1.7.%E5%AE%89%E8%A3%85kubeadm%2Ckubelet%E5%92%8Ckubectl%EF%BC%88%E5%88%9D%E5%A7%8B%E5%8C%96k8s%E9%9C%80%E8%A6%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85%EF%BC%89-toc" style="margin-left:160px;"><a href="#1.7.%E5%AE%89%E8%A3%85kubeadm%2Ckubelet%E5%92%8Ckubectl%EF%BC%88%E5%88%9D%E5%A7%8B%E5%8C%96k8s%E9%9C%80%E8%A6%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85%EF%BC%89" rel="nofollow">1.7.安装kubeadm,kubelet和kubectl（初始化k8s需要的软件包）</a></p> 
<p id="1.8.%E9%83%A8%E7%BD%B2Kubernetes%E7%9A%84Master%E8%8A%82%E7%82%B9-toc" style="margin-left:160px;"><a href="#1.8.%E9%83%A8%E7%BD%B2Kubernetes%E7%9A%84Master%E8%8A%82%E7%82%B9" rel="nofollow">1.8.部署Kubernetes的Master节点</a></p> 
<p id="1.9.node%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A0%E5%85%A5k8s%E9%9B%86%E7%BE%A4-toc" style="margin-left:160px;"><a href="#1.9.node%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A0%E5%85%A5k8s%E9%9B%86%E7%BE%A4" rel="nofollow">1.9.node节点服务器加入k8s集群</a></p> 
<p id="1.10.%E5%AE%9E%E7%8E%B0master%E4%B8%8A%E7%9A%84pod%E5%92%8Cnode%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84pod%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1-toc" style="margin-left:160px;"><a href="#1.10.%E5%AE%9E%E7%8E%B0master%E4%B8%8A%E7%9A%84pod%E5%92%8Cnode%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84pod%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1" rel="nofollow">1.10.实现master上的pod和node节点上的pod之间通信</a></p> 
<p id="2.%E9%83%A8%E7%BD%B2ansible%E5%AE%8C%E6%88%90%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E4%BD%9C-toc" style="margin-left:80px;"><a href="#2.%E9%83%A8%E7%BD%B2ansible%E5%AE%8C%E6%88%90%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E4%BD%9C" rel="nofollow">2.部署ansible完成相关软件的自动化运维工作</a></p> 
<p id="2.1.%E5%BB%BA%E7%AB%8B%E5%85%8D%E5%AF%86%E9%80%9A%E9%81%93%20%E5%9C%A8ansible%E4%B8%BB%E6%9C%BA%E4%B8%8A%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9-toc" style="margin-left:160px;"><a href="#2.1.%E5%BB%BA%E7%AB%8B%E5%85%8D%E5%AF%86%E9%80%9A%E9%81%93%20%E5%9C%A8ansible%E4%B8%BB%E6%9C%BA%E4%B8%8A%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9" rel="nofollow">2.1.建立免密通道 在ansible主机上生成密钥对</a></p> 
<p id="2.2.%E4%B8%8A%E4%BC%A0%E5%85%AC%E9%92%A5%E5%88%B0%E6%89%80%E6%9C%89%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84root%E7%94%A8%E6%88%B7%E5%AE%B6%E7%9B%AE%E5%BD%95%E4%B8%8B-toc" style="margin-left:160px;"><a href="#2.2.%E4%B8%8A%E4%BC%A0%E5%85%AC%E9%92%A5%E5%88%B0%E6%89%80%E6%9C%89%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84root%E7%94%A8%E6%88%B7%E5%AE%B6%E7%9B%AE%E5%BD%95%E4%B8%8B" rel="nofollow">2.2.上传公钥到所有服务器的root用户家目录下</a></p> 
<p id="2.3.%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%AE%9E%E7%8E%B0%E5%85%8D%E5%AF%86%E7%A0%81%E5%AF%86%E9%92%A5%E8%AE%A4%E8%AF%81-toc" style="margin-left:160px;"><a href="#2.3.%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%AE%9E%E7%8E%B0%E5%85%8D%E5%AF%86%E7%A0%81%E5%AF%86%E9%92%A5%E8%AE%A4%E8%AF%81" rel="nofollow">2.3.验证是否实现免密码密钥认证</a></p> 
<p id="2.4.%E5%9C%A8%E7%AE%A1%E7%90%86%E8%8A%82%E7%82%B9%E4%B8%8A%E5%AE%89%E8%A3%85ansible-toc" style="margin-left:160px;"><a href="#2.4.%E5%9C%A8%E7%AE%A1%E7%90%86%E8%8A%82%E7%82%B9%E4%B8%8A%E5%AE%89%E8%A3%85ansible" rel="nofollow">2.4.在管理节点上安装ansible</a></p> 
<p id="2.5.%E7%BC%96%E5%86%99%E4%B8%BB%E6%9C%BA%E9%85%8D%E7%BD%AE-toc" style="margin-left:160px;"><a href="#2.5.%E7%BC%96%E5%86%99%E4%B8%BB%E6%9C%BA%E9%85%8D%E7%BD%AE" rel="nofollow">2.5.编写主机配置</a></p> 
<p id="3.%E9%83%A8%E7%BD%B2nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%8C%E4%B8%BA%E6%95%B4%E4%B8%AAweb%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BE%9B%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;"><a href="#3.%E9%83%A8%E7%BD%B2nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%8C%E4%B8%BA%E6%95%B4%E4%B8%AAweb%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BE%9B%E6%95%B0%E6%8D%AE" rel="nofollow">3.部署nfs服务器，为整个web集群提供数据</a></p> 
<p id="3.1.%E6%90%AD%E5%BB%BA%E5%A5%BDnfs%E6%9C%8D%E5%8A%A1%E5%99%A8-toc" style="margin-left:160px;"><a href="#3.1.%E6%90%AD%E5%BB%BA%E5%A5%BDnfs%E6%9C%8D%E5%8A%A1%E5%99%A8" rel="nofollow">3.1.搭建好nfs服务器</a></p> 
<p id="3.2.%E8%AE%BE%E7%BD%AE%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95-toc" style="margin-left:160px;"><a href="#3.2.%E8%AE%BE%E7%BD%AE%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95" rel="nofollow">3.2.设置共享目录</a></p> 
<p id="3.3.%E6%8C%82%E8%BD%BD%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95-toc" style="margin-left:160px;"><a href="#3.3.%E6%8C%82%E8%BD%BD%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95" rel="nofollow">3.3.挂载共享目录</a></p> 
<p id="3.4.%E5%88%9B%E5%BB%BApv%E5%8F%8Apvc%E4%BD%BF%E7%94%A8nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95-toc" style="margin-left:160px;"><a href="#3.4.%E5%88%9B%E5%BB%BApv%E5%8F%8Apvc%E4%BD%BF%E7%94%A8nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95" rel="nofollow">3.4.创建pv及pvc使用nfs服务器上的共享目录</a></p> 
<p id="4.%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor-toc" style="margin-left:80px;"><a href="#4.%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor" rel="nofollow">4.部署镜像仓库harbor</a></p> 
<p id="4.1.-toc" style="margin-left:160px;"><a href="#4.1." rel="nofollow">4.1.前提</a></p> 
<p id="4.2.-toc" style="margin-left:160px;"><a href="#4.2." rel="nofollow">4.2.搭建harbor</a></p> 
<p id="5.-toc" style="margin-left:80px;"><a href="#5." rel="nofollow">5.搭建gitlab</a></p> 
<p id="5.1.%E5%AE%98%E6%96%B9%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3-toc" style="margin-left:160px;"><a href="#5.1.%E5%AE%98%E6%96%B9%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3" rel="nofollow">5.1.官方部署文档</a></p> 
<p id="5.2.%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%BF%85%E9%A1%BB%E7%9A%84%E4%BE%9D%E8%B5%96%E9%A1%B9-toc" style="margin-left:160px;"><a href="#5.2.%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%BF%85%E9%A1%BB%E7%9A%84%E4%BE%9D%E8%B5%96%E9%A1%B9" rel="nofollow">5.2.安装和配置必须的依赖项</a></p> 
<p id="4.3.-toc" style="margin-left:160px;"><a href="#4.3." rel="nofollow">5.3.下载/安装极狐GitLab</a></p> 
<p id="4.4.-toc" style="margin-left:160px;"><a href="#4.4." rel="nofollow">5.4.查看密码</a></p> 
<p id="4.5.-toc" style="margin-left:160px;"><a href="#4.5." rel="nofollow">5.5.部署成功后使用</a></p> 
<p id="4.6.-toc" style="margin-left:80px;"><a href="#4.6." rel="nofollow">6.部署简单的nginx业务</a></p> 
<p id="6.1.%E6%89%93%E5%8C%85%E4%BD%BF%E7%94%A8python%2Bflask%E5%AE%8C%E6%88%90%E7%9A%84%E7%AE%80%E5%8D%95%E9%A1%B9%E7%9B%AE%EF%BC%8C%E5%88%B6%E4%BD%9C%E6%88%90%E9%95%9C%E5%83%8F-toc" style="margin-left:160px;"><a href="#6.1.%E6%89%93%E5%8C%85%E4%BD%BF%E7%94%A8python%2Bflask%E5%AE%8C%E6%88%90%E7%9A%84%E7%AE%80%E5%8D%95%E9%A1%B9%E7%9B%AE%EF%BC%8C%E5%88%B6%E4%BD%9C%E6%88%90%E9%95%9C%E5%83%8F" rel="nofollow">6.1.打包使用python+flask完成的简单项目，制作成镜像</a></p> 
<p id="6.2.%E9%95%9C%E5%83%8F%E4%B8%8A%E4%BC%A0%E8%87%B3%E6%9C%AC%E5%9C%B0harbor%E4%BB%93%E5%BA%93-toc" style="margin-left:160px;"><a href="#6.2.%E9%95%9C%E5%83%8F%E4%B8%8A%E4%BC%A0%E8%87%B3%E6%9C%AC%E5%9C%B0harbor%E4%BB%93%E5%BA%93" rel="nofollow">6.2.镜像上传至本地harbor仓库</a></p> 
<p id="6.3.%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85ingress-nginx%E6%9D%A5%E6%9A%B4%E9%9C%B2%E5%BA%94%E7%94%A8-toc" style="margin-left:160px;"><a href="#6.3.%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85ingress-nginx%E6%9D%A5%E6%9A%B4%E9%9C%B2%E5%BA%94%E7%94%A8" rel="nofollow">6.3.在k8s集群上安装ingress-nginx来暴露应用</a></p> 
<p id="6.4.%E4%BD%BF%E7%94%A8dashboard%E5%AF%B9%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%BF%9B%E8%A1%8C%E6%8E%8C%E6%8E%A7-toc" style="margin-left:160px;"><a href="#6.4.%E4%BD%BF%E7%94%A8dashboard%E5%AF%B9%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%BF%9B%E8%A1%8C%E6%8E%8C%E6%8E%A7" rel="nofollow">6.4.使用dashboard对整个集群资源进行掌控</a></p> 
<p id="6.5.%E6%89%8B%E5%B7%A5%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E9%95%9C%E5%83%8F%E5%88%B0k8s%E4%BD%BF%E7%94%A8-toc" style="margin-left:160px;"><a href="#6.5.%E6%89%8B%E5%B7%A5%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E9%95%9C%E5%83%8F%E5%88%B0k8s%E4%BD%BF%E7%94%A8" rel="nofollow">6.5.手工部署应用镜像到k8s使用</a></p> 
<p id="6.6.%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E6%9C%89%E6%8A%A5%E9%94%99%EF%BC%8C%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3-toc" style="margin-left:160px;"><a href="#6.6.%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E6%9C%89%E6%8A%A5%E9%94%99%EF%BC%8C%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3" rel="nofollow">6.6.部署过程有报错，报错及解决</a></p> 
<p id="PS%EF%BC%9A%E6%8B%93%E5%B1%95-toc" style="margin-left:200px;"><a href="#PS%EF%BC%9A%E6%8B%93%E5%B1%95" rel="nofollow">PS：拓展</a></p> 
<hr id="hr-toc"> 
<p></p> 
<h2 id="%E4%B8%80%E3%80%81%E9%A1%B9%E7%9B%AE%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E5%9B%BE">一、项目整体架构图</h2> 
<p><img alt="" height="857" src="https://images2.imgbox.com/49/e1/evJ3BThG_o.png" width="1200"></p> 
<h2 id="%E4%BA%8C%E3%80%81%E9%A1%B9%E7%9B%AE%E6%8F%8F%E8%BF%B0">二、项目描述</h2> 
<h2 id="%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E5%87%86%E5%A4%87">三、项目准备</h2> 
<h6 id="1.%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D">1.修改主机名</h6> 
<blockquote> 
 <ol><li> <p>hostnamcectl set-hostname k8s-master</p> </li><li> <p>hostnamcectl set-hostname k8s-node1</p> </li><li>hostnamcectl set-hostname k8s-node2</li><li>hostnamcectl set-hostname k8s-node3</li></ol> 
</blockquote> 
<h6 id="2.%E8%A7%84%E5%88%92%E9%A1%B9%E7%9B%AE%E9%9B%86%E7%BE%A4%E6%9C%BA%E5%99%A8%E7%9A%84IP%E5%9C%B0%E5%9D%80">2.规划项目集群机器的IP地址</h6> 
<table border="1" cellpadding="1" cellspacing="1" style="width:500px;"><tbody><tr><td><strong>server</strong></td><td><strong>ip</strong></td></tr><tr><td>k8s-master</td><td>192.168.205.143</td></tr><tr><td>k8s-node1</td><td>192.168.205.144</td></tr><tr><td>k8s-node2</td><td>192.168.205.145</td></tr><tr><td>k8s-node3</td><td>192.168.205.146</td></tr><tr><td>ansible</td><td>192.168.205.138</td></tr><tr><td>nfs</td><td>192.168.205.136</td></tr><tr><td>harbor</td><td>192.168.205.135</td></tr><tr><td>prometheus</td><td>192.168.205.134</td></tr><tr><td>gitlab</td><td>192.168.205.190</td></tr></tbody></table> 
<blockquote> 
 <p>[root@k8s-master network-scripts]# cd /etc/sysconfig/network-scripts/<br> [root@k8s-master network-scripts]# vim ifcfg-ens33 </p> 
 <p>[root@k8s-master network-scripts]# cat ifcfg-ens33 <br> BOOTPROTO=none<br> NAME="ens33"<br> DEVICE="ens33"<br> ONBOOT="yes"<br> IPADDR=192.168.205.143<br> NETMASK=255.255.255.0<br> DNS1=114.114.114.114</p> 
</blockquote> 
<h6 id="3.%E5%85%B3%E9%97%ADselinux%E5%92%8Cfirewall">3.关闭selinux和firewall</h6> 
<blockquote> 
 <p># 防火墙并且设置防火墙开启不启动<br> service firewalld stop &amp;&amp; systemctl disable firewalld<br> # 临时关闭seLinux<br> setenforce 0<br> # 永久关闭seLinux<br> sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config<br> [root@k8s-master network-scripts]# getenforce<br> Disabled</p> 
</blockquote> 
<h6 id="4.%E5%8D%87%E7%BA%A7%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%B8%8D%E5%8D%87%E7%BA%A7%EF%BC%89">4.升级系统（也可以不升级）</h6> 
<blockquote> 
 <p>yum update -y</p> 
</blockquote> 
<h6 id="5.%E6%B7%BB%E5%8A%A0%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90">5.添加域名解析</h6> 
<blockquote> 
 <p>[root@k8s-master network-scripts]# vim /etc/hosts<br> [root@k8s-master network-scripts]# cat /etc/hosts<br> 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br> ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br> 192.168.205.143 k8s-master<br> 192.168.205.144 k8s-node1<br> 192.168.205.145 k8s-node2<br> 192.168.205.146 k8s-node3 </p> 
</blockquote> 
<h2 id="%E5%9B%9B%E3%80%81%E9%A1%B9%E7%9B%AE%E6%AD%A5%E9%AA%A4">四、项目步骤</h2> 
<h4 id="1.kubeadm%E5%AE%89%E8%A3%85k8s%E5%8D%95master%E7%9A%84%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83">1.kubeadm安装k8s单master的集群环境</h4> 
<h6 id="1.1.%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95">1.1.配置主机之间无密码登录</h6> 
<blockquote> 
 <p>master节点：</p> 
 <p>[root@k8s-master ~]# ssh-keygen           #一路回车，不输入密码<br> Generating public/private rsa key pair.<br> Enter file in which to save the key (/root/.ssh/id_rsa): <br> Created directory '/root/.ssh'.<br> Enter passphrase (empty for no passphrase): <br> Enter same passphrase again: <br> Your identification has been saved in /root/.ssh/id_rsa.<br> Your public key has been saved in /root/.ssh/id_rsa.pub.<br> The key fingerprint is:<br> SHA256:H5E2l2k7iN/XajAlHJjLCcQYUjfdlofaZSNCKH6NPl0 root@k8s-master<br> The key's randomart image is:<br> +---[RSA 2048]----+<br> |    ..o=++.+ o   |<br> |     .o.+.=.Bo=  |<br> |     . . ==O=* . |<br> |      . oo**E..  |<br> |       oS.o.oo   |<br> |        oo.oo. . |<br> |         .o .o. .|<br> |             ... |<br> |             ..  |<br> +----[SHA256]-----+<br>  </p> 
 <p>[root@k8s-master ~]#ssh-copy-id k8s-master</p> 
 <p>[root@k8s-master ~]#ssh-copy-id k8s-node1</p> 
 <p>[root@k8s-master ~]#ssh-copy-id k8s-node2</p> 
 <p>[root@k8s-master ~]#ssh-copy-id k8s-node3</p> 
 <p>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"<br> The authenticity of host 'k8s-node3 (192.168.205.13)' can't be established.<br> ECDSA key fingerprint is SHA256:lSTuuChFfqoAbSkAzqiWh3mx36qL9vU+640WXMtb70o.<br> ECDSA key fingerprint is MD5:15:9a:e7:5d:39:01:85:d7:ce:26:0f:43:84:9f:ac:1d.<br> Are you sure you want to continue connecting (yes/no)? yes  ----这里填yes<br> /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br> /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys<br> root@k8s-node3's password:                                                   ----这里输入对应远程主机密码</p> 
 <p>Number of key(s) added: 1</p> 
 <p>Now try logging into the machine, with:   "ssh 'k8s-node3'"<br> and check to make sure that only the key(s) you wanted were added.</p> 
</blockquote> 
<h6 id="1.2.%E5%85%B3%E9%97%AD%E4%BA%A4%E6%8D%A2%E5%88%86%E5%8C%BA">1.2.关闭交换分区</h6> 
<blockquote> 
 <p>原因：Swap是交换分区，如果机器内存不够，会使用swap分区，但是swap分区的性能较低，k8s设计的时候为了能提升性能，默认是不允许使用交换分区的。Kubeadm初始化的时候会检测swap是否关闭，如果没关闭，那就初始化失败。如果不想要关闭交换分区，安装k8s的时候可以指定 --ignore-preflight-errors=Swap 来解决。</p> 
</blockquote> 
<blockquote> 
 <p># 临时关闭</p> 
 <p>[root@k8s-master ~]# swapoff -a<br> [root@k8s-node1~]# swapoff -a<br> [root@k8s-node2 ~]# swapoff -a<br> [root@k8s-node3~]# swapoff -a<br>  </p> 
 <p># 永久关闭<span style="color:#0d0016;">：注释swap挂载，给swap这行开头加一下注释</span><br> [root@k8s-master ~]# vim /etc/fstab<br> [root@k8s-master ~]# cat /etc/fstab </p> 
 <p>#<br> # /etc/fstab<br> # Created by anaconda on Wed Mar 22 00:23:15 2023<br> #<br> # Accessible filesystems, by reference, are maintained under '/dev/disk'<br> # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info<br> #<br> /dev/mapper/centos-root /                       xfs     defaults        0 0<br> UUID=e61480a4-6186-435e-839c-9d8ed1c4f824 /boot                   xfs     defaults        0 0<br> #/dev/mapper/centos-swap swap                    swap    defaults        0 0      -------&gt;这行加注释<br> [root@k8s-master ~]# </p> 
</blockquote> 
<h6 id="1.3.%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0">1.3.修改机器内核参数</h6> 
<blockquote> 
 <p>[root@k8s-master ~]# modprobe br_netfilter<br> [root@k8s-master ~]# echo "modprobe br_netfilter" &gt;&gt; /etc/profile<br> [root@k8s-master ~]# cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOF<br> &gt; net.bridge.bridge-nf-call-ip6tables = 1<br> &gt; net.bridge.bridge-nf-call-iptables = 1<br> &gt; net.ipv4.ip_forward = 1<br> &gt; EOF<br> [root@k8s-master ~]# sysctl -p /etc/sysctl.d/k8s.conf<br> net.bridge.bridge-nf-call-ip6tables = 1<br> net.bridge.bridge-nf-call-iptables = 1<br> net.ipv4.ip_forward = 1</p> 
 <p>同步到node1/node2/node3节点</p> 
</blockquote> 
<blockquote> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">问题1：sysctl是做什么的？</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">在运行时配置内核参数</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">  -p   从指定的文件加载系统参数，如不指定即从/etc/sysctl.conf中加载</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">问题2：为什么要执行modprobe br_netfilter？</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">修改/etc/sysctl.d/k8s.conf文件，增加如下三行参数：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.bridge.bridge-nf-call-ip6tables = 1</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.bridge.bridge-nf-call-iptables = 1</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.ipv4.ip_forward = 1</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">sysctl -p /etc/sysctl.d/k8s.conf出现报错：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-ip6tables: No such file or directory</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">解决方法：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">modprobe br_netfilter</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">问题3：为什么开启net.bridge.bridge-nf-call-iptables内核参数？</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">在centos下安装docker，执行docker info出现如下警告：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">WARNING: bridge-nf-call-iptables is disabled</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">WARNING: bridge-nf-call-ip6tables is disabled</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">解决办法：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">vim  /etc/sysctl.d/k8s.conf</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.bridge.bridge-nf-call-ip6tables = 1</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.bridge.bridge-nf-call-iptables = 1</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">问题4：为什么要开启net.ipv4.ip_forward = 1参数？</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">kubeadm初始化k8s如果报错：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;"><img alt="" height="56" src="https://images2.imgbox.com/23/ae/KKVhw2fp_o.png" width="1108"></span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">就表示没有开启ip_forward，需要开启。</span></p> 
 <p style="margin-left:.0001pt;"></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">net.ipv4.ip_forward是数据包转发：</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">出于安全考虑，Linux系统默认是禁止数据包转发的。所谓转发即当主机拥有多于一块的网卡时，其中一块收到数据包，根据数据包的目的ip地址将数据包发往本机另一块网卡，该网卡根据路由表继续发送数据包。这通常是路由器所要实现的功能。</span></p> 
 <p style="margin-left:.0001pt;"><span style="color:#0d0016;">要让Linux系统具有路由转发功能，需要配置一个Linux的内核参数net.ipv4.ip_forward。这个参数指定了Linux系统当前对路由转发功能的支持情况；其值为0时表示禁止进行IP转发；如果是1,则说明IP转发功能已经打开。</span></p> 
</blockquote> 
<h6 id="1.4.%E9%85%8D%E7%BD%AE%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84repo%E6%BA%90" style="margin-left:.0001pt;">1.4.配置阿里云的repo源</h6> 
<blockquote> 
 <p>[root@k8s-master ~]# vim  /etc/yum.repos.d/kubernetes.repo</p> 
 <p>[root@k8s-master yum.repos.d]# cat kubernetes.repo <br> [kubernetes]<br> name=Kubernetes<br> baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64<br> enabled=1<br> gpgcheck=0<br> repo_gpgcheck=0<br> gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</p> 
</blockquote> 
<h6 id="1.5.%E9%85%8D%E7%BD%AE%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5" style="margin-left:.0001pt;">1.5.配置时间同步</h6> 
<blockquote> 
 <p style="margin-left:.0001pt;">master节点：</p> 
 <p style="margin-left:.0001pt;">#把时间同步做成计划任务</p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# crontab -e</p> 
 <p style="margin-left:.0001pt;">* */1 * * * /usr/sbin/ntpdate   cn.pool.ntp.org</p> 
 <p style="margin-left:.0001pt;">#重启crond服务</p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# service crond restart</p> 
 <p style="margin-left:.0001pt;">node1/2/3节点：</p> 
 <p style="margin-left:.0001pt;">#把时间同步做成计划任务</p> 
 <p style="margin-left:.0001pt;">[root@k8s-node1 ~]#crontab -e</p> 
 <p style="margin-left:.0001pt;">* */1 * * * /usr/sbin/ntpdate   cn.pool.ntp.org</p> 
 <p style="margin-left:.0001pt;">#重启crond服务</p> 
 <p style="margin-left:.0001pt;">[root@k8s-node1 ~]#service crond restart</p> 
 <p style="margin-left:.0001pt;">Redirecting to /bin/systemctl restart crond.service</p> 
</blockquote> 
<h6 id="1.6.%E5%AE%89%E8%A3%85docker%E6%9C%8D%E5%8A%A1%EF%BC%88%E8%BF%99%E4%B8%80%E6%AD%A5%E4%B9%9F%E5%8F%AF%E4%BB%A5%E6%94%BE%E5%9C%A8%E6%9C%80%E5%89%8D%E9%9D%A2%EF%BC%8C%E5%85%88%E5%AE%89%E8%A3%85%E5%A5%BDdocker%EF%BC%8C%E5%86%8D%E5%81%9A%E5%85%B6%E4%BB%96%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%89" style="margin-left:.0001pt;">1.6.安装docker服务（这一步也可以放在最前面，先安装好docker，再做其他的操作）</h6> 
<blockquote> 
 <p>master和node节点上都操作：</p> 
 <p>--&gt; 安装yum相关的工具，下载docker-ce.repo文件<br> [root@k8s-master ~]# yum install -y yum-utils -y<br> [root@k8s-master ~]#yum-config-manager \<br>     --add-repo \<br>     https://download.docker.com/linux/centos/docker-ce.repo<br> --&gt; 下载docker-ce.repo文件存放在/etc/yum.repos.d<br> [root@k8s-master yum.repos.d]# pwd<br> /etc/yum.repos.d<br> [root@k8s-master yum.repos.d]# ls<br> CentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repo          docker-ce.repo<br> CentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo  nginx.repo<br> [root@k8s-master yum.repos.d]# <br> # 安装docker-ce软件<br> [root@k8s-master yum.repos.d]# yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y</p> 
 <p># container engine 容器引擎<br> # docker是一个容器管理的软件<br> # docker-ce 是服务器端软件 server<br> # docker-ce-cli 是客户端软件 client<br> # docker-compose-plugin 是compose插件，用来批量启动很多容器，在单台机器上<br> # containerd.io  底层用来启动容器的</p> 
 <p>--&gt; 启动docker，设置开机自启</p> 
 <p>[root@k8s-master ~]# systemctl start docker &amp;&amp; systemctl enable docker.service</p> 
 <p>--&gt; 查看docker的版本</p> 
 <p>[root@k8s-master yum.repos.d]# docker --version<br> Docker version 24.0.5, build ced0996</p> 
</blockquote> 
<h6 id="1.7.%E5%AE%89%E8%A3%85kubeadm%2Ckubelet%E5%92%8Ckubectl%EF%BC%88%E5%88%9D%E5%A7%8B%E5%8C%96k8s%E9%9C%80%E8%A6%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85%EF%BC%89" style="margin-left:.0001pt;">1.7.安装kubeadm,kubelet和kubectl（初始化k8s需要的软件包）</h6> 
<blockquote> 
 <p style="margin-left:.0001pt;">master上操作：</p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# yum install -y kubelet kubeadm kubectl  --&gt;不指定版本默认下最新版</p> 
 <p> --&gt;最好指定版本，因为1.24的版本默认的容器运行时环境不是docker了</p> 
 <p>[root@k8s-master ~]# yum install -y kubelet-1.23.6 kubeadm-1.23.6 kubectl-1.23.6</p> 
 <p> --&gt;设置开机自启，因为kubelet是k8s在node节点上的代理，必须开机要运行的<br> [root@k8s-master ~]# systemctl enable  kubelet</p> 
 <p># kubeadm:  kubeadm是一个工具，用来初始化k8s集群的</p> 
 <p># kubelet:   安装在集群所有节点上，用于启动Pod的</p> 
 <p># kubectl:   通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件</p> 
 <p></p> 
</blockquote> 
<h6 id="1.8.%E9%83%A8%E7%BD%B2Kubernetes%E7%9A%84Master%E8%8A%82%E7%82%B9">1.8.部署Kubernetes的Master节点</h6> 
<blockquote> 
 <p>[root@k8s-master ~]#  docker pull  coredns/coredns:1.8.4<br> [root@k8s-master ~]# docker pull  coredns/coredns</p> 
 <p>[root@k8s-master ~]# docker tag coredns/coredns:1.8.4 registry.aliyuncs.com/google_containers/coredns:v1.8.4</p> 
 <p>#初始化操作在master服务器上执行<br> [root@k8s-master ~]# kubeadm init \<br>     --apiserver-advertise-address=192.168.205.143 \<br>     --image-repository registry.aliyuncs.com/google_containers \<br>     --service-cidr=10.1.0.0/16 \<br>     --pod-network-cidr=10.244.0.0/16</p> 
 <p># 192.168.205.10 是master的ip</p> 
 <p>--&gt; 执行命令出现提示操作，按照提示进行操作</p> 
 <p>[root@k8s-master ~]# mkdir -p $HOME/.kube</p> 
 <p>[root@k8s-master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br> [root@k8s-master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config</p> 
</blockquote> 
<h6 id="1.9.node%E8%8A%82%E7%82%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%8A%A0%E5%85%A5k8s%E9%9B%86%E7%BE%A4">1.9.node节点服务器加入k8s集群</h6> 
<blockquote> 
 <p style="margin-left:.0001pt;">node1/2/3节点：</p> 
 <p style="margin-left:.0001pt;">[root@k8s-node1 ~]# kubeadm join 192.168.205.143:6443 --token 2fiwt1.47ss9cjmyaztw58b --discovery-token-ca-cert-hash </p> 
 <p style="margin-left:.0001pt;">[root@k8s-node2 ~]# kubeadm join 192.168.205.143:6443 --token 2fiwt1.47ss9cjmyaztw58b --discovery-token-ca-cert-hash </p> 
 <p style="margin-left:.0001pt;">[root@k8s-node3 ~]# kubeadm join 192.168.205.143:6443 --token 2fiwt1.47ss9cjmyaztw58b --discovery-token-ca-cert-hash </p> 
 <p style="margin-left:.0001pt;">--&gt; 查看节点状态</p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# kubectl get nodes<br> NAME         STATUS   ROLES                  AGE   VERSION<br> k8s-master   Ready    control-plane,master   31d   v1.23.6<br> k8s-node1    Ready    &lt;none&gt;                 31d   v1.23.6<br> k8s-node2    Ready    &lt;none&gt;                 31d   v1.23.6<br> k8s-node3    Ready    &lt;none&gt;                 31d   v1.23.6</p> 
</blockquote> 
<h6 id="1.10.%E5%AE%9E%E7%8E%B0master%E4%B8%8A%E7%9A%84pod%E5%92%8Cnode%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84pod%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1" style="margin-left:.0001pt;">1.10.实现master上的pod和node节点上的pod之间通信</h6> 
<blockquote> 
 <p style="margin-left:.0001pt;"># 安装网络插件flannel(在master节点执行):</p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# vim kube-flannel.yml <br> [root@k8s-master ~]# cat kube-flannel.yml <br> ---<br> kind: Namespace<br> apiVersion: v1<br> metadata:<br>   name: kube-flannel<br>   labels:<br>     pod-security.kubernetes.io/enforce: privileged<br> ---<br> kind: ClusterRole<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: flannel<br> rules:<br> - apiGroups:<br>   - ""<br>   resources:<br>   - pods<br>   verbs:<br>   - get<br> - apiGroups:<br>   - ""<br>   resources:<br>   - nodes<br>   verbs:<br>   - list<br>   - watch<br> - apiGroups:<br>   - ""<br>   resources:<br>   - nodes/status<br>   verbs:<br>   - patch<br> ---<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: flannel<br> roleRef:<br>   apiGroup: rbac.authorization.k8s.io<br>   kind: ClusterRole<br>   name: flannel<br> subjects:<br> - kind: ServiceAccount<br>   name: flannel<br>   namespace: kube-flannel<br> ---<br> apiVersion: v1<br> kind: ServiceAccount<br> metadata:<br>   name: flannel<br>   namespace: kube-flannel<br> ---<br> kind: ConfigMap<br> apiVersion: v1<br> metadata:<br>   name: kube-flannel-cfg<br>   namespace: kube-flannel<br>   labels:<br>     tier: node<br>     app: flannel<br> data:<br>   cni-conf.json: |<br>     {<!-- --><br>       "name": "cbr0",<br>       "cniVersion": "0.3.1",<br>       "plugins": [<br>         {<!-- --><br>           "type": "flannel",<br>           "delegate": {<!-- --><br>             "hairpinMode": true,<br>             "isDefaultGateway": true<br>           }<br>         },<br>         {<!-- --><br>           "type": "portmap",<br>           "capabilities": {<!-- --><br>             "portMappings": true<br>           }<br>         }<br>       ]<br>     }<br>   net-conf.json: |<br>     {<!-- --><br>       "Network": "10.244.0.0/16",<br>       "Backend": {<!-- --><br>         "Type": "vxlan"<br>       }<br>     }<br> ---<br> apiVersion: apps/v1<br> kind: DaemonSet<br> metadata:<br>   name: kube-flannel-ds<br>   namespace: kube-flannel<br>   labels:<br>     tier: node<br>     app: flannel<br> spec:<br>   selector:<br>     matchLabels:<br>       app: flannel<br>   template:<br>     metadata:<br>       labels:<br>         tier: node<br>         app: flannel<br>     spec:<br>       affinity:<br>         nodeAffinity:<br>           requiredDuringSchedulingIgnoredDuringExecution:<br>             nodeSelectorTerms:<br>             - matchExpressions:<br>               - key: kubernetes.io/os<br>                 operator: In<br>                 values:<br>                 - linux<br>       hostNetwork: true<br>       priorityClassName: system-node-critical<br>       tolerations:<br>       - operator: Exists<br>         effect: NoSchedule<br>       serviceAccountName: flannel<br>       initContainers:<br>       - name: install-cni-plugin<br>        #image: flannelcni/flannel-cni-plugin:v1.1.0 for ppc64le and mips64le (dockerhub limitations may apply)<br>         image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.0<br>         command:<br>         - cp<br>         args:<br>         - -f<br>         - /flannel<br>         - /opt/cni/bin/flannel<br>         volumeMounts:<br>         - name: cni-plugin<br>           mountPath: /opt/cni/bin<br>       - name: install-cni<br>        #image: flannelcni/flannel:v0.19.2 for ppc64le and mips64le (dockerhub limitations may apply)<br>         image: docker.io/rancher/mirrored-flannelcni-flannel:v0.19.2<br>         command:<br>         - cp<br>         args:<br>         - -f<br>         - /etc/kube-flannel/cni-conf.json<br>         - /etc/cni/net.d/10-flannel.conflist<br>         volumeMounts:<br>         - name: cni<br>           mountPath: /etc/cni/net.d<br>         - name: flannel-cfg<br>           mountPath: /etc/kube-flannel/<br>       containers:<br>       - name: kube-flannel<br>        #image: flannelcni/flannel:v0.19.2 for ppc64le and mips64le (dockerhub limitations may apply)<br>         image: docker.io/rancher/mirrored-flannelcni-flannel:v0.19.2<br>         command:<br>         - /opt/bin/flanneld<br>         args:<br>         - --ip-masq<br>         - --kube-subnet-mgr<br>         resources:<br>           requests:<br>             cpu: "100m"<br>             memory: "50Mi"<br>           limits:<br>             cpu: "100m"<br>             memory: "50Mi"<br>         securityContext:<br>           privileged: false<br>           capabilities:<br>             add: ["NET_ADMIN", "NET_RAW"]<br>         env:<br>         - name: POD_NAME<br>           valueFrom:<br>             fieldRef:<br>               fieldPath: metadata.name<br>         - name: POD_NAMESPACE<br>           valueFrom:<br>             fieldRef:<br>               fieldPath: metadata.namespace<br>         - name: EVENT_QUEUE_DEPTH<br>           value: "5000"<br>         volumeMounts:<br>         - name: run<br>           mountPath: /run/flannel<br>         - name: flannel-cfg<br>           mountPath: /etc/kube-flannel/<br>         - name: xtables-lock<br>           mountPath: /run/xtables.lock<br>       volumes:<br>       - name: run<br>         hostPath:<br>           path: /run/flannel<br>       - name: cni-plugin<br>         hostPath:<br>           path: /opt/cni/bin<br>       - name: cni<br>         hostPath:<br>           path: /etc/cni/net.d<br>       - name: flannel-cfg<br>         configMap:<br>           name: kube-flannel-cfg<br>       - name: xtables-lock<br>         hostPath:<br>           path: /run/xtables.lock<br>           type: FileOrCreate<br> [root@k8s-master ~]# <br> #  部署flannel<br> [root@k8s-master ~]# kubectl apply -f kube-flannel.yml </p> 
 <p style="margin-left:.0001pt;">[root@k8s-master ~]# ps aux|grep flannel<br> root       3810  0.0  1.3 1335164 25748 ?       Ssl  16:06   0:17 /opt/bin/flanneld --ip-masq --kube-subnet-mgr<br> root      94146  0.0  0.0 112828   980 pts/1    S+   21:26   0:00 grep --color=auto flannel</p> 
 <p style="margin-left:.0001pt;"># 查看各个节点的详细信息<br> [root@k8s-master ~]# kubectl get nodes -n kube-system -o wide<br> NAME         STATUS   ROLES                  AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION                CONTAINER-RUNTIME<br> k8s-master   Ready    control-plane,master   31d   v1.23.6   192.168.205.143   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.95.1.el7.x86_64   docker://24.0.5<br> k8s-node1    Ready    &lt;none&gt;                 31d   v1.23.6   192.168.205.144   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.95.1.el7.x86_64   docker://24.0.5<br> k8s-node2    Ready    &lt;none&gt;                 31d   v1.23.6   192.168.205.145   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.95.1.el7.x86_64   docker://24.0.5<br> k8s-node3    Ready    &lt;none&gt;                 31d   v1.23.6   192.168.205.146   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1160.95.1.el7.x86_64   docker://24.0.5<br> [root@k8s-master ~]# </p> 
</blockquote> 
<h4 id="2.%E9%83%A8%E7%BD%B2ansible%E5%AE%8C%E6%88%90%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E4%BD%9C" style="margin-left:.0001pt;">2.部署ansible完成相关软件的自动化运维工作</h4> 
<h6 id="2.1.%E5%BB%BA%E7%AB%8B%E5%85%8D%E5%AF%86%E9%80%9A%E9%81%93%20%E5%9C%A8ansible%E4%B8%BB%E6%9C%BA%E4%B8%8A%E7%94%9F%E6%88%90%E5%AF%86%E9%92%A5%E5%AF%B9">2.1.建立免密通道 在ansible主机上生成密钥对</h6> 
<blockquote> 
 <p style="margin-left:.0001pt;"># 一路回车，不输入密码</p> 
 <p style="margin-left:.0001pt;">[root@ansible ~]# ssh-keygen -t ecdsa<br> Generating public/private ecdsa key pair.<br> Enter file in which to save the key (/root/.ssh/id_ecdsa): <br> Created directory '/root/.ssh'.<br> Enter passphrase (empty for no passphrase): <br> Enter same passphrase again: <br> Your identification has been saved in /root/.ssh/id_ecdsa.<br> Your public key has been saved in /root/.ssh/id_ecdsa.pub.<br> The key fingerprint is:<br> SHA256:d1s0FfCXztHrv/NmWouZqJv4s5ubvHeOdaDgiy+1a9k root@ansible<br> The key's randomart image is:<br> +---[ECDSA 256]---+<br> |             ...o|<br> |              ..o|<br> |              o+o|<br> |             .o.+|<br> |        S.. ...+ |<br> |        .o...oo  |<br> |        ..+... o.|<br> |       .+==E+.=o=|<br> |       o=^@oo=.**|<br> +----[SHA256]-----+</p> 
 <p style="margin-left:.0001pt;">[root@ansible ~]# cd /root/.ssh<br> [root@ansible .ssh]# ls<br> id_ecdsa  id_ecdsa.pub</p> 
</blockquote> 
<h6 id="2.2.%E4%B8%8A%E4%BC%A0%E5%85%AC%E9%92%A5%E5%88%B0%E6%89%80%E6%9C%89%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84root%E7%94%A8%E6%88%B7%E5%AE%B6%E7%9B%AE%E5%BD%95%E4%B8%8B">2.2.上传公钥到所有服务器的root用户家目录下</h6> 
<blockquote> 
 <p># 所有服务器上开启ssh服务 ，开放22号端口，允许root用户登录</p> 
 <p>--&gt; k8s-master</p> 
 <p>[root@ansible .ssh]# ssh-copy-id -i id_ecdsa.pub root@192.168.205.143<br> /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "id_ecdsa.pub"<br> The authenticity of host '192.168.205.143 (192.168.205.143)' can't be established.<br> ECDSA key fingerprint is SHA256:lSTuuChFfqoAbSkAzqiWh3mx36qL9vU+640WXMtb70o.<br> ECDSA key fingerprint is MD5:15:9a:e7:5d:39:01:85:d7:ce:26:0f:43:84:9f:ac:1d.<br> Are you sure you want to continue connecting (yes/no)? yes<br> /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br> /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys<br> root@192.168.205.143's password: </p> 
 <p>Number of key(s) added: 1</p> 
 <p>Now try logging into the machine, with:   "ssh 'root@192.168.205.143'"<br> and check to make sure that only the key(s) you wanted were added.</p> 
 <p></p> 
 <p>--&gt; k8s-node1/node2/node3同样的方式（IP地址不同）</p> 
</blockquote> 
<h6 id="2.3.%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%AE%9E%E7%8E%B0%E5%85%8D%E5%AF%86%E7%A0%81%E5%AF%86%E9%92%A5%E8%AE%A4%E8%AF%81">2.3.验证是否实现免密码密钥认证</h6> 
<blockquote> 
 <p>[root@ansible .ssh]# ssh root@192.168.205.143<br> Last login: Wed Sep 13 20:40:08 2023 from 192.168.205.1<br> [root@k8s-master ~]# exit<br> 登出<br> Connection to 192.168.205.143 closed.<br> [root@ansible .ssh]# ssh root@192.168.205.144<br> Last login: Wed Sep 13 19:07:12 2023 from 192.168.205.1<br> [root@k8s-node1 ~]# exit<br> 登出<br> Connection to 192.168.205.144 closed.<br> [root@ansible .ssh]# ssh root@192.168.205.145<br> Last login: Wed Sep 13 19:07:02 2023 from 192.168.205.1<br> [root@k8s-node2 ~]# exit<br> 登出<br> Connection to 192.168.205.145 closed.<br> [root@ansible .ssh]# ssh root@192.168.205.146<br> Last failed login: Wed Sep 13 22:55:00 CST 2023 from 192.168.205.138 on ssh:notty<br> There was 1 failed login attempt since the last successful login.<br> Last login: Wed Sep 13 19:09:31 2023 from 192.168.205.1<br> [root@k8s-node3 ~]# exit<br> 登出<br> Connection to 192.168.205.146 closed.</p> 
</blockquote> 
<h6 id="2.4.%E5%9C%A8%E7%AE%A1%E7%90%86%E8%8A%82%E7%82%B9%E4%B8%8A%E5%AE%89%E8%A3%85ansible">2.4.在管理节点上安装ansible</h6> 
<blockquote> 
 <p>[root@ansible .ssh]# yum install epel-release -y</p> 
 <p>[root@ansible .ssh]#  yum install ansible -y<br> [root@ansible .ssh]# ansible --version<br> ansible 2.9.27<br>   config file = /etc/ansible/ansible.cfg<br>   configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']<br>   ansible python module location = /usr/lib/python2.7/site-packages/ansible<br>   executable location = /usr/bin/ansible<br>   python version = 2.7.5 (default, Oct 14 2020, 14:45:30) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]</p> 
</blockquote> 
<h6 id="2.5.%E7%BC%96%E5%86%99%E4%B8%BB%E6%9C%BA%E9%85%8D%E7%BD%AE">2.5.编写主机配置</h6> 
<blockquote> 
 <p>[root@ansible .ssh]# cd /etc/ansible/<br> [root@ansible ansible]# ls<br> ansible.cfg  hosts  roles<br> [root@ansible ansible]# vim hosts <br> [root@ansible ansible]# cat hosts<br> # This is the default ansible 'hosts' file.<br> #<br> # It should live in /etc/ansible/hosts<br> #<br> #   - Comments begin with the '#' character<br> #   - Blank lines are ignored<br> #   - Groups of hosts are delimited by [header] elements<br> #   - You can enter hostnames or ip addresses<br> #   - A hostname/ip can be a member of multiple groups</p> 
 <p># Ex 1: Ungrouped hosts, specify before any group headers.</p> 
 <p>## green.example.com<br> ## blue.example.com<br> ## 192.168.100.1<br> ## 192.168.100.10</p> 
 <p># Ex 2: A collection of hosts belonging to the 'webservers' group</p> 
 <p>## [webservers]<br> ## alpha.example.org<br> ## beta.example.org<br> ## 192.168.1.100<br> ## 192.168.1.110<br> [k8s-master]<br> 192.168.205.143</p> 
 <p>[k8s-node]<br> 192.168.205.144<br> 192.168.205.145<br> 192.168.205.146</p> 
 <p>[nfs]<br> 192.168.205.136</p> 
 <p>[prometheus]<br> 192.168.205.134</p> 
 <p>[harbor]<br> 192.168.205.135</p> 
 <p>[gitlab]<br> 192.168.205.198</p> 
 <p># If you have multiple hosts following a pattern you can specify<br> # them like this:</p> 
 <p>## www[001:006].example.com</p> 
 <p># Ex 3: A collection of database servers in the 'dbservers' group</p> 
 <p>## [dbservers]<br> ## <br> ## db01.intranet.mydomain.net<br> ## db02.intranet.mydomain.net<br> ## 10.25.1.56<br> ## 10.25.1.57</p> 
 <p># Here's another example of host ranges, this time there are no<br> # leading 0s:</p> 
 <p>## db-[99:101]-node.example.com<br> [root@ansible ansible]# </p> 
</blockquote> 
<h4 id="3.%E9%83%A8%E7%BD%B2nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%EF%BC%8C%E4%B8%BA%E6%95%B4%E4%B8%AAweb%E9%9B%86%E7%BE%A4%E6%8F%90%E4%BE%9B%E6%95%B0%E6%8D%AE">3.部署nfs服务器，为整个web集群提供数据</h4> 
<h6 id="3.1.%E6%90%AD%E5%BB%BA%E5%A5%BDnfs%E6%9C%8D%E5%8A%A1%E5%99%A8">3.1.搭建好nfs服务器</h6> 
<blockquote> 
 <p>nfs的搭建参考以下：</p> 
 <p><a href="https://blog.csdn.net/weixin_56302282/article/details/129987136?spm=1001.2014.3001.5501" title="模拟企业业务构建基于nginx的高可用web集群_花雨292的博客-CSDN博客">模拟企业业务构建基于nginx的高可用web集群_花雨292的博客-CSDN博客</a></p> 
 <p>所有的web业务pod通过pv、pvc和卷挂载实现。</p> 
 <p>k8s-master/node1/node2/node3:</p> 
 <p>[root@k8s-master ~]# yum install nfs-utils -y<br> [root@k8s-master ~]# service nfs start<br> Redirecting to /bin/systemctl start nfs.service<br> [root@k8s-master ~]# service nfs restart<br> Redirecting to /bin/systemctl restart nfs.service<br> [root@k8s-master ~]# ps aux|grep nfs<br> root      28363  0.0  0.0      0     0 ?        S&lt;   23:18   0:00 [nfsd4_callbacks]<br> root      28369  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28370  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28371  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28372  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28373  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28374  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28375  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28376  0.0  0.0      0     0 ?        S    23:18   0:00 [nfsd]<br> root      28530  0.0  0.0 112824   976 pts/1    S+   23:18   0:00 grep --color=auto nfs</p> 
</blockquote> 
<h6 id="3.2.%E8%AE%BE%E7%BD%AE%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95">3.2.设置共享目录</h6> 
<blockquote> 
 <p>[root@nfs ~]# vim /etc/exports</p> 
 <p>[root@nfs ~]# cat /etc/exports</p> 
 <p>/data 192.168.205.0/24(rw,no_root_squash,no_all_squash,sync)</p> 
 <p>[root@nfs ~]# exportfs -r              # 输出所有共享目录</p> 
 <p>[root@nfs ~]# exportfs -v             # 显示输出的共享目录</p> 
 <p>/data             192.168.205.0/24(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)<br>  </p> 
</blockquote> 
<h6 id="3.3.%E6%8C%82%E8%BD%BD%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95">3.3.挂载共享目录</h6> 
<blockquote> 
 <p>[root@k8s-master ~]# mkdir /nfs-pv<br> [root@k8s-master ~]# mount  192.168.205.136:/data /nfs-pv/<br> [root@k8s-master ~]# df -Th|grep nfs<br> 192.168.205.136:/data   nfs4       17G  1.6G   16G    9% /nfs-pv</p> 
 <p># ps：取消挂载：umount /nfs-pv</p> 
</blockquote> 
<h6 id="3.4.%E5%88%9B%E5%BB%BApv%E5%8F%8Apvc%E4%BD%BF%E7%94%A8nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95">3.4.创建pv及pvc使用nfs服务器上的共享目录</h6> 
<blockquote> 
 <p>k8s官方文档：<a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#introduction" rel="nofollow" title="持久卷 | Kubernetes">持久卷 | Kubernetes</a></p> 
 <p><strong>1.在master节点上创建pv（持久卷PersistentVolume）</strong></p> 
 <p>[root@k8s-master pv]# vim nfs-pv.yaml<br> [root@k8s-master pv]# cat nfs-pv.yaml <br> apiVersion: v1<br> kind: PersistentVolume<br> metadata:<br>   name: pv-data<br>   labels:<br>     type: pv-data<br> spec:<br>   capacity:<br>     storage: 10Gi<br>   volumeMode: Filesystem<br>   accessModes:<br>     - ReadWriteMany<br>   persistentVolumeReclaimPolicy: Recycle<br>   storageClassName: nfs<br>   nfs:<br>     path: /data<br>     server: 192.168.205.136<br> [root@k8s-master pv]# pwd<br> /root/pv<br> [root@k8s-master pv]# kubectl apply -f nfs-pv.yaml <br> persistentvolume/pv-data created<br> [root@k8s-master pv]# kubectl get pv<br> NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br> pv-data   10Gi       RWX            Recycle          Available           nfs                     12s<br> [root@k8s-master pv]# <br><strong>2.创建pvc（持久卷申领PersistentVolumeClaim）使用pv</strong></p> 
 <p>[root@k8s-master pv]# vim nfs-pvc.yaml<br> [root@k8s-master pv]# cat nfs-pvc.yaml <br> apiVersion: v1<br> kind: PersistentVolumeClaim<br> metadata:<br>   name: pvc-data<br> spec:<br>   accessModes:<br>   - ReadWriteMany<br>   volumeMode: Filesystem      <br>   resources:<br>      requests:<br>        storage: 1Gi<br>   storageClassName: nfs<br> [root@k8s-master pv]# kubectl apply -f nfs-pvc.yaml<br> persistentvolumeclaim/pvc-data created<br> [root@k8s-master pv]# kubectl get pvc<br> NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br> pvc-data   Bound    pv-data   10Gi       RWX            nfs            14s<br> [root@k8s-master pv]# <br><strong>使用：创建pod使用pvc</strong></p> 
</blockquote> 
<h4 id="4.%E9%83%A8%E7%BD%B2%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor">4.部署镜像仓库harbor</h4> 
<h6 id="4.1.">4.1.前提</h6> 
<blockquote> 
 <p>前提条件：安装好docker和docker compose</p> 
 <p>[root@harbor ~]# yum install -y yum-utils -y<br> [root@harbor ~]# yum-config-manager \<br> &gt;     --add-repo \<br> &gt;     https://download.docker.com/linux/centos/docker-ce.repo<br> [root@harbor yum.repos.d]# pwd<br> /etc/yum.repos.d<br> [root@harbor yum.repos.d]# yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y<br> [root@harbor yum.repos.d]# docker --version<br> Docker version 24.0.6, build ed223bc<br> [root@harbor yum.repos.d]# systemctl start docker<br> [root@harbor yum.repos.d]# ps aux|grep docker<br> root       8069  2.6  2.6 968988 48508 ?        Ssl  18:39   0:00 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock<br> root       8197  0.0  0.0 112824   976 pts/0    S+   18:39   0:00 grep --color=auto docker<br> [root@harbor yum.repos.d]# systemctl enable docker<br> Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.<br> [root@harbor ~]# DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}<br> [root@harbor ~]# echo $DOCKER_CONFIG<br> /root/.docker<br> [root@harbor ~]# mkdir -p $DOCKER_CONFIG/cli-plugins<br> [root@harbor ~]# mv docker-compose /root/.docker/cli-plugins/<br> [root@harbor ~]# cd /root/.docker/cli-plugins/<br> [root@harbor cli-plugins]# ls<br> docker-compose<br> [root@harbor cli-plugins]# chmod +x docker-compose <br> [root@harbor cli-plugins]# docker compose version<br> Docker Compose version v2.7.0<br> [root@harbor cli-plugins]# </p> 
</blockquote> 
<h6 id="4.2.">4.2.搭建harbor</h6> 
<blockquote> 
 <p>1.下载harbor源码包</p> 
 <p><a href="https://github.com/goharbor/harbor/releases/tag/v2.7.3" title="Release v2.7.3 · goharbor/harbor · GitHub">Release v2.7.3 · goharbor/harbor · GitHub</a></p> 
 <p>2.进入源码包网址，最下面点击下载源码包即可</p> 
 <p><img alt="" height="1200" src="https://images2.imgbox.com/b8/4f/Tai0XF68_o.png" width="1200"></p> 
</blockquote> 
<blockquote> 
 <p>3.解压源码包，安装配置harbor仓库</p> 
 <p>[root@harbor ~]# mkdir harbor<br> [root@harbor ~]# ls<br> anaconda-ks.cfg  kube-flannel.yml   pv<br> harbor           onekey_install.sh<br> [root@harbor ~]# cd harbor<br> [root@harbor harbor]# ls<br> [root@harbor harbor]# ls<br> harbor-offline-installer-v2.7.3.tgz<br> [root@harbor harbor]# tar xf harbor-offline-installer-v2.7.3.tgz <br> [root@harbor harbor]# ls<br> harbor  harbor-offline-installer-v2.7.3.tgz<br> [root@harbor harbor]# cd harbor<br> [root@harbor harbor]# ls<br> common.sh             harbor.yml.tmpl  LICENSE<br> harbor.v2.7.3.tar.gz  install.sh       prepare<br> [root@harbor harbor]# cp harbor.yml.tmpl harbor.yml<br> [root@harbor harbor]# vim harbor.yml</p> 
 <p><img alt="" height="505" src="https://images2.imgbox.com/84/a8/t6szo72b_o.png" width="1200"></p> 
 <p>[root@harbor harbor]# ./install.sh </p> 
 <p><img alt="" height="544" src="https://images2.imgbox.com/e2/58/7RJLWRoA_o.png" width="1200"></p> 
 <p>安装成功的样子</p> 
 <p>在windows机器上访问网站，去配置harbor<br> http://192.168.205.135:8089/</p> 
 <p>默认的登录的用户名和密码<br> admin<br> Harbor12345</p> 
 <p><img alt="" height="1200" src="https://images2.imgbox.com/48/10/zkzGgLG1_o.png" width="1200"></p> 
</blockquote> 
<h4 id="5.">5.搭建gitlab</h4> 
<h6 id="5.1.%E5%AE%98%E6%96%B9%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3">5.1.官方部署文档</h6> 
<blockquote> 
 <p><a href="https://gitlab.cn/install/" rel="nofollow" title="GitLab下载安装_GitLab最新中文免费版下载安装-极狐GitLab">GitLab下载安装_GitLab最新中文免费版下载安装-极狐GitLab</a></p> 
</blockquote> 
<h6 id="5.2.%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%E5%BF%85%E9%A1%BB%E7%9A%84%E4%BE%9D%E8%B5%96%E9%A1%B9">5.2.安装和配置必须的依赖项</h6> 
<blockquote> 
 <p>[root@gitlab ~]#sudo yum install -y curl policycoreutils-python openssh-server perl [root@gitlab ~]#sudo systemctl enable sshd</p> 
 <p>[root@gitlab ~]#sudo systemctl start sshd</p> 
 <p>[root@gitlab ~]#sudo firewall-cmd --permanent --add-service=http [root@gitlab ~]#sudo firewall-cmd --permanent --add-service=https</p> 
 <p>[root@gitlab ~]#sudo systemctl reload firewalld</p> 
</blockquote> 
<h6 id="4.3.">5.3.下载/安装极狐GitLab</h6> 
<blockquote> 
 <p>[root@gitlab ~]# curl -fsSL https://packages.gitlab.cn/repository/raw/scripts/setup.sh | /bin/bash</p> 
 <p>[root@gitlab ~]# sudo EXTERNAL_URL="http://192.168.205.190" yum install -y gitlab-jh</p> 
</blockquote> 
<h6 id="4.4.">5.4.查看密码</h6> 
<blockquote> 
 <p>[root@gitlab ~]# cat /etc/gitlab/initial_root_password <br> # WARNING: This value is valid only in the following conditions<br> #          1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run).<br> #          2. Password hasn't been changed manually, either via UI or via command line.<br> #<br> #          If the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password.</p> 
 <p>Password: lfIo17qJgCLdR6H0daLxZs6pRMTITCiSOFRvfaJx/QI=</p> 
 <p># NOTE: This file will be automatically deleted in the first reconfigure run after 24 hours.<br> [root@gitlab ~]# </p> 
</blockquote> 
<h6 id="4.5.">5.5.部署成功后使用</h6> 
<blockquote> 
 <p>使用文档：<a href="https://docs.gitlab.cn/jh/user/project/index.html" rel="nofollow" title="创建项目 | 极狐GitLab">创建项目 | 极狐GitLab</a></p> 
</blockquote> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/f3/7c/cY3EKQpa_o.png" width="1200"></p> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/c9/b0/A3Up4VzG_o.png" width="1200"></p> 
<h4 id="4.6.">6.部署简单的nginx业务</h4> 
<h6 id="6.1.%E6%89%93%E5%8C%85%E4%BD%BF%E7%94%A8python%2Bflask%E5%AE%8C%E6%88%90%E7%9A%84%E7%AE%80%E5%8D%95%E9%A1%B9%E7%9B%AE%EF%BC%8C%E5%88%B6%E4%BD%9C%E6%88%90%E9%95%9C%E5%83%8F">6.1.打包使用python+flask完成的简单项目，制作成镜像</h6> 
<blockquote> 
 <p>TIPS：代码应上传至gitlab再打镜像，由于gitlab访问速度太慢，本次直接在linux打镜像</p> 
</blockquote> 
<blockquote> 
 <p>上传代码到虚拟机</p> 
 <p>[root@harbor ~]# cd flask/<br> [root@harbor flask]# ls<br> app.py  config  data.sqlite  Dockerfile  models  requirements.txt  router  server.py  static  templates</p> 
 <p>[root@harbor flask]# docker build -t flask:v1 .</p> 
 <p>[root@harborflask]# docker images<br> REPOSITORY   TAG       IMAGE ID       CREATED          SIZE<br> flask        v1        23b6a850131c   31 seconds ago   1.07GB</p> 
</blockquote> 
<h6 id="6.2.%E9%95%9C%E5%83%8F%E4%B8%8A%E4%BC%A0%E8%87%B3%E6%9C%AC%E5%9C%B0harbor%E4%BB%93%E5%BA%93">6.2.镜像上传至本地harbor仓库</h6> 
<blockquote> 
 <p>[root@harbor docker]# docker tag flask:v1 192.168.205.135:8090/harbor/flask/flask:v1</p> 
 <p>要设置私有仓库地址为http才可以上传，否则需要为harbor配置https证书</p> 
 <p>[root@harbor docker]# vim /etc/docker/daemon.json</p> 
 <p>[root@harbor harbor]# cat /etc/docker/daemon.json<br> {<!-- --><br>     "insecure-registries": ["192.168.205.135:8090"] <br> }<br> [root@harbordocker]# systemctl daemon-reload<br> [root@harbor docker]# systemctl restart docker<br> [root@harbor ~]# docker login 192.168.205.135:8090<br> Username: admin<br> Password: <br> WARNING! Your password will be stored unencrypted in /root/.docker/config.json.<br> Configure a credential helper to remove this warning. See<br> https://docs.docker.com/engine/reference/commandline/login/#credentials-store</p> 
 <p>Login Succeeded<br> [root@harbor docker]# docker push  192.168.205.135:8090/harbor/flask/flask:v1</p> 
 <p><img alt="" height="592" src="https://images2.imgbox.com/df/25/0WHNcrGa_o.png" width="1200"></p> 
 <p><img alt="" height="1200" src="https://images2.imgbox.com/6a/0b/B1Z4tGxl_o.png" width="1200"></p> 
</blockquote> 
<h6 id="6.3.%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%8A%E5%AE%89%E8%A3%85ingress-nginx%E6%9D%A5%E6%9A%B4%E9%9C%B2%E5%BA%94%E7%94%A8">6.3.在k8s集群上安装ingress-nginx来暴露应用</h6> 
<blockquote> 
 <p>1.部署ingress-nginx</p> 
 <p>[root@k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.7.0/deploy/static/provider/baremetal/deploy.yaml<br> 2.部署完后查看</p> 
 <p>[root@k8s-master ~]# kubectl get pod,svc -n ingress-nginx<br> NAME                                            READY   STATUS              RESTARTS   AGE<br> pod/ingress-nginx-admission-create-f69pd        0/1     ErrImagePull        0          51s<br> pod/ingress-nginx-admission-patch-npf9v         0/1     ImagePullBackOff    0          51s<br> pod/ingress-nginx-controller-7557ffd88d-w76tv   0/1     ContainerCreating   0          51s</p> 
 <p>NAME                                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE<br> service/ingress-nginx-controller             NodePort    10.1.245.103   &lt;none&gt;        80:31774/TCP,443:32409/TCP   51s<br> service/ingress-nginx-controller-admission   ClusterIP   10.1.51.224    &lt;none&gt;        443/TCP                      51s<br> [root@k8s-master ~]# </p> 
</blockquote> 
<h6 id="6.4.%E4%BD%BF%E7%94%A8dashboard%E5%AF%B9%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E8%BF%9B%E8%A1%8C%E6%8E%8C%E6%8E%A7">6.4.使用dashboard对整个集群资源进行掌控</h6> 
<blockquote> 
 <p>[root@k8s-master dashboard]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml<br> --2023-09-20 14:47:53--  https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml<br> 正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...<br> 正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... 已连接。<br> 已发出 HTTP 请求，正在等待回应... 200 OK<br> 长度：7621 (7.4K) [text/plain]<br> 正在保存至: “recommended.yaml”</p> 
 <p>100%[===================================================================================================================&gt;] 7,621       --.-K/s 用时 0s      </p> 
 <p>2023-09-20 14:47:54 (19.8 MB/s) - 已保存 “recommended.yaml” [7621/7621])</p> 
 <p>[root@k8s-master dashboard]# ls<br> recommended.yaml<br> [root@k8s-master dashboard]# kubectl apply -f recommended.yaml<br> [root@k8s-master dashboard]# kubectl get ns<br> NAME                   STATUS   AGE<br> default                Active   40d<br> ingress-nginx          Active   36m<br> kube-flannel           Active   40d<br> kube-node-lease        Active   40d<br> kube-public            Active   40d<br> kube-system            Active   40d<br> kubernetes-dashboard   Active   15s<br> [root@k8s-master dashboard]# kubectl get pod -n kubernetes-dashboard<br> NAME                                         READY   STATUS    RESTARTS   AGE<br> dashboard-metrics-scraper-799d786dbf-mbmwx   1/1     Running   0          53m<br> kubernetes-dashboard-546cbc58cd-9j6q6        1/1     Running   0          53m</p> 
 <p>#查看dashboard对应的服务，因为发布服务的类型是ClusterIP ，外面的机器不能访问，不便于我们通过浏览器访问，因此需要改成NodePort</p> 
 <p>[root@k8s-master dashboard]# kubectl get svc -n kubernetes-dashboard<br> NAME                        TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE<br> dashboard-metrics-scraper   ClusterIP   10.1.182.10   &lt;none&gt;        8000/TCP   54m<br> kubernetes-dashboard        ClusterIP   10.1.252.34   &lt;none&gt;        443/TCP    54m<br> [root@k8s-master dashboard]# kubectl delete svc kubernetes-dashboard -n kubernetes-dashboard<br> service "kubernetes-dashboard" deleted<br> [root@k8s-master dashboard]# vim dashboard-svc.yml<br> [root@k8s-master dashboard]# cat dashboard-svc.yml<br> kind: Service<br> apiVersion: v1<br> metadata:<br>   labels:<br>     k8s-app: kubernetes-dashboard<br>   name: kubernetes-dashboard<br>   namespace: kubernetes-dashboard<br> spec:<br>   type: NodePort<br>   ports:<br>     - port: 443<br>       targetPort: 8443<br>   selector:<br>     k8s-app: kubernetes-dashboard<br> [root@k8s-master dashboard]# kubectl apply -f dashboard-svc.yml<br> service/kubernetes-dashboard created<br> [root@k8s-master dashboard]# kubectl get svc -n kubernetes-dashboard<br> NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE<br> dashboard-metrics-scraper   ClusterIP   10.1.182.10    &lt;none&gt;        8000/TCP        57m<br> kubernetes-dashboard        NodePort    10.1.250.147   &lt;none&gt;        443:31312/TCP   17s<br> [root@k8s-master dashboard]# vim dashboard-svc-account.yaml<br> [root@k8s-master dashboard]# cat dashboard-svc-account.yaml <br> apiVersion: v1<br> kind: ServiceAccount<br> metadata:<br>   name: dashboard-admin<br>   namespace: kube-system<br> ---<br> kind: ClusterRoleBinding<br> apiVersion: rbac.authorization.k8s.io/v1<br> metadata:<br>   name: dashboard-admin<br> subjects:<br>   - kind: ServiceAccount<br>     name: dashboard-admin<br>     namespace: kube-system<br> roleRef:<br>   kind: ClusterRole<br>   name: cluster-admin<br>   apiGroup: rbac.authorization.k8s.io<br> [root@k8s-master dashboard]# </p> 
 <p>[root@k8s-master dashboard]# kubectl apply -f dashboard-svc-account.yaml<br> serviceaccount/dashboard-admin created<br> clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin created<br> [root@k8s-master dashboard]# kubectl get secret -n kube-system|grep admin|awk '{print $1}'<br> dashboard-admin-token-m58pp<br> [root@k8s-master dashboard]# kubectl describe secret dashboard-admin-token-m58pp -n kube-system<br> Name:         dashboard-admin-token-m58pp<br> Namespace:    kube-system<br> Labels:       &lt;none&gt;<br> Annotations:  kubernetes.io/service-account.name: dashboard-admin<br>               kubernetes.io/service-account.uid: a35b61d0-3642-4664-a5cf-7509a8a029cc</p> 
 <p>Type:  kubernetes.io/service-account-token</p> 
 <p>Data<br> ====<br> ca.crt:     1099 bytes<br> namespace:  11 bytes<br> token:      eyJhbGciOiJSUzI1NiIsImtpZCI6Il83MjJseTFSM3pHMXZMZUpYbFdJbl9DbTNxWXZHVC1rQnlmMHZkNnExSGMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbTU4cHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTM1YjYxZDAtMzY0Mi00NjY0LWE1Y2YtNzUwOWE4YTAyOWNjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.TyoBrihRQEeVnjEgur5MM3sv5se5VpvB_F2OtiEIYLmjTJLi2YI1Pul13JcqgdXvD1Vud7j7cywJvU9B0A54p3KV0E7S16F0pZ-WNnJklBEmO4S7ZUooN7K0mbP-WrPnvCMh5mn8Aw5gsH9IfLSPTqNpXnPjtzww8DIcU3nzCLDTV19R4nPe0JfkXX6ulsV0vQwkxchC1yFcZJDVMZ6nBfkj0ci71J6ygxFerEF_HhJ7b6LMp-SdC1iwiLOlSxf5FzACV_oxs5QIcjdjWRcghhny10gbmDQMD1gRanAaw69HJTtgZkmim1jIqq-zld6AU7FCbxfMPJHPbcs184STJg<br> [root@k8s-master dashboard]# kubectl describe secret dashboard-admin-token-m58pp -n kube-system|awk '/^token/ {print $2}'<br> eyJhbGciOiJSUzI1NiIsImtpZCI6Il83MjJseTFSM3pHMXZMZUpYbFdJbl9DbTNxWXZHVC1rQnlmMHZkNnExSGMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbTU4cHAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYTM1YjYxZDAtMzY0Mi00NjY0LWE1Y2YtNzUwOWE4YTAyOWNjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.TyoBrihRQEeVnjEgur5MM3sv5se5VpvB_F2OtiEIYLmjTJLi2YI1Pul13JcqgdXvD1Vud7j7cywJvU9B0A54p3KV0E7S16F0pZ-WNnJklBEmO4S7ZUooN7K0mbP-WrPnvCMh5mn8Aw5gsH9IfLSPTqNpXnPjtzww8DIcU3nzCLDTV19R4nPe0JfkXX6ulsV0vQwkxchC1yFcZJDVMZ6nBfkj0ci71J6ygxFerEF_HhJ7b6LMp-SdC1iwiLOlSxf5FzACV_oxs5QIcjdjWRcghhny10gbmDQMD1gRanAaw69HJTtgZkmim1jIqq-zld6AU7FCbxfMPJHPbcs184STJg<br> [root@k8s-master dashboard]# kubectl get svc -n kubernetes-dashboard<br> NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE<br> dashboard-metrics-scraper   ClusterIP   10.1.182.10    &lt;none&gt;        8000/TCP        60m<br> kubernetes-dashboard        NodePort    10.1.250.147   &lt;none&gt;        443:31312/TCP   3m17s<br> 浏览器访问：<a href="https://192.168.205.143:31312/#/login" rel="nofollow" title="https://192.168.205.143:31312/#/login">https://192.168.205.143:31312/#/login</a></p> 
 <p>必须要加https前缀，选择无视风险继续访问即可，输入token</p> 
 <p><img alt="" height="1200" src="https://images2.imgbox.com/24/df/khwOzDZN_o.png" width="1200"></p> 
</blockquote> 
<h6 id="6.5.%E6%89%8B%E5%B7%A5%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E9%95%9C%E5%83%8F%E5%88%B0k8s%E4%BD%BF%E7%94%A8">6.5.手工部署应用镜像到k8s使用</h6> 
<p>yaml文件内容如下：</p> 
<blockquote> 
 <p>apiVersion: apps/v1<br> kind: Deployment<br> metadata:<br>   labels:<br>     app: flask<br>   name: falsk<br> spec:<br>   replicas: 1<br>   selector:<br>     matchLabels:<br>       app: flask<br>   template:<br>     metadata:<br>       labels:<br>         app: flask<br>     spec:<br>       containers:<br>       - image: 192.168.205.135:8090/harbor/flask/flask:v1<br>         name: flask<br> ---<br> apiVersion: v1<br> kind: Service<br> metadata:<br>   labels:<br>     app: flask<br>   name: flask<br> spec:<br>   ports:<br>   - name: http-port<br>     port: 5678<br>     protocol: TCP<br>     targetPort: 80<br>   selector:<br>     app: flask<br>   type: ClusterIP<br> ---<br> apiVersion: networking.k8s.io/v1<br> kind: Ingress<br> metadata:<br>   name: flask<br> spec:<br>   ingressClassName: nginx<br>   rules:<br>   - host: "test.flask.com"<br>     http:<br>       paths:<br>       - path: /<br>         pathType: Prefix<br>         backend:<br>           service:<br>             name: flask<br>             port:<br>               number: 5678</p> 
</blockquote> 
<h6 id="6.6.%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E6%9C%89%E6%8A%A5%E9%94%99%EF%BC%8C%E6%8A%A5%E9%94%99%E5%8F%8A%E8%A7%A3%E5%86%B3">6.6.部署过程有报错，报错及解决</h6> 
<blockquote> 
 <p>#报错信息</p> 
 <p>[root@k8s-master flask]# kubectl apply -f flask.yml <br> deployment.apps/falsk unchanged<br> service/flask unchanged<br> Error from server (InternalError): error when creating "flask.yml": Internal error occurred: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.1.51.224:443: connect: connection refused</p> 
 <p>#解决方案</p> 
 <p>[root@k8s-master flask]# kubectl get validatingwebhookconfigurations<br> NAME                      WEBHOOKS   AGE<br> ingress-nginx-admission   1          105m<br> [root@k8s-master flask]# kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission<br> validatingwebhookconfiguration.admissionregistration.k8s.io "ingress-nginx-admission" deleted</p> 
 <p>#重新部署</p> 
 <p>[root@k8s-master flask]# kubectl apply -f flask.yml <br> deployment.apps/falsk unchanged<br> service/flask unchanged<br> ingress.networking.k8s.io/flask created</p> 
</blockquote> 
<h6 id="PS%EF%BC%9A%E6%8B%93%E5%B1%95">PS：拓展</h6> 
<blockquote> 
 <p>构建双master集群：通过keepalive+nginx实现k8s apiserver节点高可用</p> 
 <p style="margin-left:.0001pt;">nginx：脚本安装:<a href="https://blog.csdn.net/weixin_56302282/article/details/129987136?spm=1001.2014.3001.5502" title="模拟企业业务构建基于nginx的高可用web集群_花雨292的博客-CSDN博客">模拟企业业务构建基于nginx的高可用web集群_花雨292的博客-CSDN博客</a></p> 
 <p style="margin-left:.0001pt;">keepalive：yum install keepalived -y</p> 
</blockquote> 
<p style="margin-left:.0001pt;"></p> 
<p style="margin-left:.0001pt;"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/baddf657713b2afbf1e9d31990aab6ba/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">golang实现远程控制主机</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/47d022e3d42f4b038df0ffe6c1f61f83/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">FreeRTOS移植以及核心功能</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>