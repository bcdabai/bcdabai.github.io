<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>跨模态检索研究进展综述【跨模态检索的核心工作在于：①不同模态数据的特征提取、②不同模态数据之间内容的相关性度量】【主流研究方法：基于传统统计分析的技术、基于深度学习的技术】【哈希编码提高检索速度】 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="跨模态检索研究进展综述【跨模态检索的核心工作在于：①不同模态数据的特征提取、②不同模态数据之间内容的相关性度量】【主流研究方法：基于传统统计分析的技术、基于深度学习的技术】【哈希编码提高检索速度】" />
<meta property="og:description" content="随着互联网上多媒体数据的爆炸式增长,单一模态的检索已经无法满足用户需求,跨模态检索应运而生.
跨模态检索旨在以一种模态的数据去检索另一种模态的相关数据。
跨模态检索的核心任务是：数据特征提取 和 不同模态数据之间内容的相关性度量。
文中梳理了跨模态检索领域近期的研究进展,从以下角度归纳论述了跨模态检索领域的研究成果.：
传统方法；深度学习方法；手工特征的哈希编码方法；深度学习的哈希编码方法 在此基础上,对比分析了各类算法在跨模态检索常用标准数据集上的性能。最后,分析了跨模态检索研究存在的问题,并对该领域未来发展趋势以及应用进行了展望.
一、概述 随着互联网上数据规模的不断壮大,数据类型越来越呈现多样化的特点,用户感兴趣的数据模态不再单一,用户的检索需求也越来越呈现出从单一模态到跨模态的发展态势.模态是指数据的表达形式,包括文本、图像、视频和音频等.
跨模态检索是至少两种模态的数据之间互相检索，通常是以一种模态作为查询来检索另一种模态的相关数据。通过找出不同模态数据之间的潜在关联，实现相对准确的交叉匹配.
如以文本检索相关图像,文本集为 T ＝ { t , … , t n } T＝\{t_,…,t_n\} T＝{t,​…,tn​}, 图像集为 V ＝ { v , … , v n } V＝\{v_,…,v_n\} V＝{v,​…,vn​}，针对查询文本 t q , q ∈ [ 1 , n ] t_q,q∈[1,n] tq​,q∈[1,n],检索出与 t q t_q tq​ 最相似的图像 v q ＝ { v i ∣ m a x s i m ( t q , v i ) , i ∈ [ 1 , n ] } v_q＝\{v_i|maxsim(t_q,v_i),i∈[1,n]\} vq​＝{vi​∣maxsim(tq​,vi​),i∈[1,n]}." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/bc340d8bddca9ebd89370c6d0f2a12ec/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-05T12:24:28+08:00" />
<meta property="article:modified_time" content="2022-03-05T12:24:28+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">跨模态检索研究进展综述【跨模态检索的核心工作在于：①不同模态数据的特征提取、②不同模态数据之间内容的相关性度量】【主流研究方法：基于传统统计分析的技术、基于深度学习的技术】【哈希编码提高检索速度】</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>随着互联网上多媒体数据的爆炸式增长,单一模态的检索已经无法满足用户需求,跨模态检索应运而生.</p> 
<p><font color="red"><strong>跨模态检索</strong></font>旨在<font color="blue"><strong>以一种模态的数据去检索另一种模态的相关数据</strong></font>。</p> 
<p><font color="red"><strong>跨模态检索的核心任务</strong></font>是：<font color="blue"><strong>数据特征提取</strong></font> 和 <font color="blue"><strong>不同模态数据之间内容的相关性度量</strong></font>。</p> 
<p>文中梳理了跨模态检索领域近期的研究进展,从以下角度归纳论述了跨模态检索领域的研究成果.：</p> 
<ul><li>传统方法；</li><li>深度学习方法；</li><li>手工特征的哈希编码方法；</li><li>深度学习的哈希编码方法</li></ul> 
<p>在此基础上,对比分析了各类算法在跨模态检索常用标准数据集上的性能。最后,分析了跨模态检索研究存在的问题,并对该领域未来发展趋势以及应用进行了展望.</p> 
<h2><a id="_16"></a>一、概述</h2> 
<p>随着互联网上数据规模的不断壮大,数据类型越来越呈现多样化的特点,用户感兴趣的数据模态不再单一,用户的检索需求也越来越呈现出从单一模态到跨模态的发展态势.模态是指数据的表达形式,包括文本、图像、视频和音频等.</p> 
<p>跨模态检索是<font color="blue">至少两种模态的数据之间互相检索，通常是以一种模态作为查询来检索另一种模态的相关数据</font>。通过找出不同模态数据之间的潜在关联，实现相对准确的交叉匹配.</p> 
<p>如以文本检索相关图像,文本集为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         T 
        
       
         ＝ 
        
       
         { 
        
        
        
          t 
         
        
          , 
         
        
       
         … 
        
       
         , 
        
        
        
          t 
         
        
          n 
         
        
       
         } 
        
       
      
        T＝\{t_,…,t_n\} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathdefault" style="margin-right: 0.13889em;">T</span><span class="mord cjk_fallback">＝</span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mpunct mtight">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span>, 图像集为 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         V 
        
       
         ＝ 
        
       
         { 
        
        
        
          v 
         
        
          , 
         
        
       
         … 
        
       
         , 
        
        
        
          v 
         
        
          n 
         
        
       
         } 
        
       
      
        V＝\{v_,…,v_n\} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord mathdefault" style="margin-right: 0.22222em;">V</span><span class="mord cjk_fallback">＝</span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mpunct mtight">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span>，针对查询文本<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          t 
         
        
          q 
         
        
       
         , 
        
       
         q 
        
       
         ∈ 
        
       
         [ 
        
       
         1 
        
       
         , 
        
       
         n 
        
       
         ] 
        
       
      
        t_q,q∈[1,n] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.901188em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">q</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span></span>,检索出与 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          t 
         
        
          q 
         
        
       
      
        t_q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.901188em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span> 最相似的图像 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          v 
         
        
          q 
         
        
       
         ＝ 
        
       
         { 
        
        
        
          v 
         
        
          i 
         
        
       
         ∣ 
        
       
         m 
        
       
         a 
        
       
         x 
        
       
         s 
        
       
         i 
        
       
         m 
        
       
         ( 
        
        
        
          t 
         
        
          q 
         
        
       
         , 
        
        
        
          v 
         
        
          i 
         
        
       
         ) 
        
       
         , 
        
       
         i 
        
       
         ∈ 
        
       
         [ 
        
       
         1 
        
       
         , 
        
       
         n 
        
       
         ] 
        
       
         } 
        
       
      
        v_q＝\{v_i|maxsim(t_q,v_i),i∈[1,n]\} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.03611em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mord cjk_fallback">＝</span><span class="mopen">{<!-- --></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: -0.03588em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathdefault">n</span><span class="mclose">]</span><span class="mclose">}</span></span></span></span></span>.</p> 
<p>在互联网海量多模态数据的背景下,跨模态检索一直是学术界的研究热点.</p> 
<p><strong>不同模态数据之间的内容相关性度量</strong>是跨模态检索任务的核心与挑战，现有研究：</p> 
<ul><li>一方面通过<font color="blue">改善特征提取和公共空间映射等方法</font>减少多模态数据间的跨模态差异,使跨模态检索<strong>更精准</strong>;</li><li>另一方面通过采用<font color="blue">哈希编码的方式</font>提升搜索效率,使跨模态检索任务<strong>更高效</strong>.</li></ul> 
<p>由于跨模态检索包括但不限于两种模态,因此设计方法还应考虑模型的可扩展性.尽管国内外学者采用各种方法从不同角度提出了解决跨模态检索的方案,但该领域研究目前仍面临诸多挑战和困难,具体表现在:</p> 
<ul><li><strong>多模态表达差异</strong>：不同模态的数据表达之间存在较大差异,如文本和图像之间差异巨大,如何度量不同模态数据之间的相关性成为跨模态检索任务必须应对的挑战之一.</li><li><strong>语义鸿沟</strong>：不同模态之间特征分析的困难在于底层表达和高层语义之间的断层,也称为语义鸿沟,如何挖掘不同模态之间的数据高级语义成为挑战之一.</li><li><strong>样本增量学习</strong>：在检索数据库中,一旦加入新数据,则需要花费大量时间重新训练模型或者<br> 重新计算,此时样本的增量学习就显得尤为重要.</li></ul> 
<p>一次性解决上述挑战是不现实的,跨模态检索也远未取得令人满意的效果.</p> 
<p>对比分析现有研究进展及存在问题,有助于研究人员未来在此基础上找到更好的解决方案.围绕跨模态检索研究进展,</p> 
<ul><li>Liu 等[１]在2010年分析梳理了传统方法在跨模态检索领域的应用研究.</li><li>Wang 等[２]在2016年总结 分析了跨模态检索领域的相关动态,重点从实值表示和二值表示的角度对训练形式进行梳理.不同于上述文献,</li></ul> 
<p>本文一方面增加了深度学习相关方法的论述,另一方面侧重从技术方法角度归纳分析目前主流的跨模态检索解决方案,重点关注近年来的最新研究进展,指出现阶段该领域仍然存在的问题及挑战,并对该领域的未来方向进行展望.本文的主要贡献总结如下:</p> 
<ol><li>探讨了跨模态检索存在的挑战及困难,系统地分析了跨模态检索领域主流方法以及最新研究进展,为跨模态检索领域的初学者提供了便利;</li><li>针对跨模态检索研究面临的不同挑战,讨论了不同应对策略,有助于正在做跨模态检索研究的读者更好地理解该问题,并且可以使其从中了解各种解决方案;</li><li>梳理了用于跨模态检索研究的多种常用标准数据集以及一些针对特殊场景的数据集,并对相关算法在数据集上的性能做出对比;</li><li>分析总结了目前跨模态检索存在的问题,并展望了未来可能的发展方向.</li></ol> 
<p>跨模态检索研究近年来发展迅速,在跨模态检索任务中利用深度学习的方法也越来越丰富.</p> 
<p>当前跨模态检索的主流方法是<strong>基于公共空间学习的方法</strong>,其依据是不同模态数据具有相似的语义,如数据中的对象、方位、背景等语义信息,而<font color="red">相似语义的数据具有潜在相关性</font>,从而使得构建公共空间并将不同媒体类型的数据投影到这个空间进行相关性度量成为可能.</p> 
<p>基于公共空间学习的方法的目的是学习一个公共空间,并显式地将不同媒体类型的数据投影到这个空间进行相关性度量.跨模态检索的流程框架如图 １ 所示.</p> 
<p><img src="https://images2.imgbox.com/69/86/auKNuxU5_o.png" alt="在这里插入图片描述" width="600"><br> 基于公共空间学习的方法：</p> 
<ul><li>首先分别提取不同模态数据的特征；</li><li>然后学习多模态数据的公共表示；</li><li>最后进行跨模态的匹配与排名；</li></ul> 
<p>从图1可以看出,文本、图像、视频等模态数据可以在一个<font color="red"><strong>公共高级语义空间</strong></font>中彼此接近。</p> 
<p>本文从技术方法的角度出发,主要将跨模态检索研究分为三大类,包括：</p> 
<ul><li>传统方法、</li><li>基于深度学习的方法、</li><li>基于哈希编码的方法,<br> <img src="https://images2.imgbox.com/c5/57/q6JeYm1g_o.png" alt="在这里插入图片描述" width="500"></li></ul> 
<p>研究方法的分类架构如图2 所示.其中：</p> 
<ul><li>深度学习方法主要侧重于提升检索准确性,</li><li>哈希编码方法主要侧重于提升检索效率.哈希方法中的深度学习哈希方法则是深度学习与哈希编码的融合,以平衡检索的准确性与效率.</li></ul> 
<h2><a id="_72"></a>一、传统方法</h2> 
<p>特征提取和内容相关性度量是跨模态检索研究要解决的关键问题。早期特征提取主要围绕文本、视觉及音频展开,本节重点介绍这些模态特征提取的传统方法.</p> 
<h3><a id="1_75"></a>1、特征提取</h3> 
<h4><a id="11__76"></a>1.1 文本特征提取</h4> 
<p>词袋法(Bag-of-Words,BoW)是一种简单的文本表示方式,它将每个输入文本视为多个单词集合,不考虑其复杂的语义或者语法.也就是说,文本中每个词的出现都是独立的,不影响文中其他词的出现.</p> 
<p>在标准的词袋表示法的背景下,Zhu 等[４]对大规模公共数据集的不对称程度进行了度量,提 出了新的不对称差异,分析了标准 BoW 在这种情况下的局限性.TF-IDF通过对每个词进行加权来改进 BoW,从而可以识别出输入文本所特有的关键性词汇.其中,当文本较长时,TF-IDF 可以有效地提取输入文本的特征.</p> 
<p>Blei 等[６]于2003 年提出了三层贝叶斯主题模型(Latent Dirichlet Allocation,LDA),LDA 通过无监督的学习方法获取输入文本中隐含的主题信息,隐性的语义分析实际上是利用文本中的词项的共现特征来体现文本的主题或概念结构,LDA 模型能够以更精炼的尺度表示文本.</p> 
<h4><a id="12__84"></a>1.2 视觉特征提取</h4> 
<p>尺度不变特征变化(Scale-Invariant Feature Transform,SIFT)是一种检测图像局部特征的算法,该算法通过求一幅图像中的特征点及其尺度上和方向上的相关描述子得到图像特征并进行图像特征点匹配.</p> 
<p>方向梯度直方图(Histogramof Oriented Gradient,HOG)特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子,它通过计算和统计图像局部区域的梯度方向直方图来构成图像特征.</p> 
<p>SIFT 的缺点是需要专业的图像处理器来实现,而加速稳健特征(Speeded Up Robust Features,SURF)把 SIFT 中的卷积平滑操作简化成加减运算,提高了算法的鲁棒性,降低了复杂度,但在运行时间上提升并不显著.</p> 
<p>ORB(Oriented Fast and Ro-tated Brief)特征描述算子在运行时间上实现了质的飞跃.</p> 
<p>HOG 特征结合 SVM分类器已经被广泛应用于图像识别中,代表性工作包括 ２００５ 年 CVPR 会议上 Dalal 等[７]提出的并获得极大成功的行人检测模型,文献[８]使用具有 HOG 特性的线性 SVM分类器也在字符检测中取得了不错的性能.针对视频中的运动特征的提取方法有光流方程、贝叶斯方法等.</p> 
<h4><a id="13__95"></a>1.3 音频特征提取</h4> 
<p>声波是一种音频信号.</p> 
<p>针对音频的相关工作必须对音频信号进行预处理和特征提取.</p> 
<p>在传统的语音信号处理中,语音特征以短时音频帧(audio frame)的方式提取.</p> 
<p>早期提出的 音频特征提取方法主要有线性预测倒谱系数(Linear Predic-tion Cepstrum Coefficient,LPCC)和梅尔频率倒谱系数(Mel- Frequency Cepstrum Cofficients,MFCC ),其中：</p> 
<ul><li>LPCC 特征对辅音描述能力弱,抗噪性能差.</li><li>而 MFCC 是基于人的听觉特征提取出来的特征参数,更符合人的听觉特性,因此在对音频信号进行特征提取时通常使用 MFCC 特征.但 MFCC 相邻帧特征相对独立,忽略了信号可能的内在结构,如相邻帧的强关联.文献[９-１１]在提取音频特征时都采用该思想.</li></ul> 
<h3><a id="2_106"></a>2、内容相关性度量</h3> 
<p>多模态检索主要实现文本、图像等不同模态数据的相互检索,这种相互检索的前提是文本内容和图像内容的相互关联.</p> 
<h4><a id="21__CCA_109"></a>2.1 典型相关分析 （CCA）</h4> 
<p>2004年 Hardoon 等提出的<strong>典型相关分析</strong> (Canonic Correlation Analysis,<strong>CCA</strong>)用于跨模态检索中的内容相关性度量，是一个里程碑式的工作，其通过最大化两种模态投影之间的相关性来学习公共子空间,在跨模态检索的公共子空间方法中,迅速成为此后同类算法的基准算法.</p> 
<p>尽管 CCA 因其简单和高效而广受欢迎,但其缺点也很显著：</p> 
<ul><li>经典 CCA 无法理解类标签等高级语义信息,未能充分利用类别信息,因而其学习到的公共子空间本质上判别力较弱.与 CCA 类 似,偏 最 小 二 乘(Partial Least Squares,PLS)和双线性模型[１４]等方法也尝试通过学习子空间来进行跨模态检索,但这些方法都依靠两个模态间的显式配对来建立对应关系.事实上,某个模态的某项数据可能存在不止一个语义,因此,仅关注成对耦合还远远不够,以这种方式学习到的通用表示也无法完全保留数据中潜在的跨模态语义结构.</li></ul> 
<p>总体来说,上述方法都未利用多标签信息.要想准确表达图像中存在的多个概念,必须充分考虑多标签信息,精确建模不同模态之间的相关性.</p> 
<h4><a id="22__mlCCA_117"></a>2.2 多标签典型相关分析 （ml-CCA）</h4> 
<p>2015年,Ranjan 等[15]提出了多标签典型相关分析 ml-CCA,ml-CCA 是 CCA 的扩展,主要用于学习共享子空间,同时考虑了多标签注释等高级语义信息.与 CCA 不同,ml-CCA 不依赖于模态之间一对一的显式配对,而是使用多标签信息来建立对应关系,因而可产生一对多、多对一等配对情况,形成一个判别子空间,更适合于跨模态检索任务.实际应用中,ml-CCA 的性能胜过大多数其他CCA扩展方法.</p> 
<h4><a id="23__CCAKCCA_120"></a>2.3 基于内核的 CCA（KCCA）</h4> 
<p>如前所述,ml-CCA 依赖于模态对应关系的预建立,因此 针对难以线性建模的更复杂相关性分析问题,ml-CCA 的表现不很理想.</p> 
<p>为此,Hwang 等提出了基于内核的 CCA(简 称 KCCA)方法,用于发现图像与文本模态的共享特征空间.</p> 
<p>KCCA 增加了特征选择的灵活性,是一种在机器学习领域提取非线性特征的有效方法.</p> 
<h4><a id="24_mlKCCA_127"></a>2.4 多标签核典型相关分析（ml-KCCA）</h4> 
<p>在 KCCA 的基础上,Jia 等提 出了多标签核典型相关分析(简称 ml-KCCA),通过多标签注释中的高级语义信息对 KCCA 进行增强.从多标签中提取相关性进行核化,可以测量不同模态之间更复杂的非线性相关性,从而学习更适合跨模态检索任务的判别子空间.</p> 
<h4><a id="25__CCA_130"></a>2.5 集群 CCA</h4> 
<p>而集群 CCA(简称 C-CCA)通过来自跨模态数据对之间的一一对应关系,使用标准的 CCA 来学习投影,可以保留更多的语义信息.随着深度神经网络的发展,涌现出了结合深度神经网络的典型相关分析方法 DCCA[１９],它可以不需要 KCCA 的内积计算学习非线性变换.</p> 
<h2><a id="_133"></a>二、深度学习方法</h2> 
<p>2016年,AlphaGo 战胜李世石,直接引发了深度学习技 术的快速发展及其在诸多领域的应用研究,仅在图像领域,包括目标检测、目标跟踪、图像检索、图像分割等任务都开始借助深度学习不断提升各自的性能.</p> 
<p>基于深度学习的跨模态检索研究也取得了很多进展,检索准确性得到大幅提升.</p> 
<p><font color="red">跨模态检索的核心工作关注于<font color="blue"><strong>数据特征提取</strong></font>和<font color="blue"><strong>不同模态数据之间内容的相关性度量</strong></font>方面</font>,本节将围绕深度学习技术方法的应用介绍近几年跨模态检索领域的相关工作.</p> 
<h3><a id="1_140"></a>1、特征提取</h3> 
<h4><a id="11__141"></a>1.1 基本网络结构</h4> 
<p>近年来：</p> 
<ul><li>图像特征提取主要采用卷积神经网络(Convolu-tional Neural Networks,CNN),</li><li>文本特征提取主要采用长短期记忆(Long Short-Term Memory,LSTM)或循环神经网络 (Recurrent Neural Network,RNN),</li><li>音频主要是先对音频信号进行降噪及处理,再采用 CNN 或 LSTM提取特征,</li><li>视频特征提取主要采用 3D-CNN,</li></ul> 
<p>本文将这些方法都归类为基本网络结构.根据具体任务的不同,跨模态检索领域的研究人员在采用深度学习网络进行特征提取时提出了不同的改进和组合方法.</p> 
<p>针对自然语言对象检索任务同时涉及场景中的对象和全局场景上下文空间信息,Hu等[２０]提出一种新颖的<strong>空间上下文循环模型</strong>(简称 SCRC)作为对象检索候选框的评分函数,将空间配置和全局场景级上下文信息集成到网络中.该模型不仅考虑了图像全局上下文特征,而且考虑了图像中具体对象的特征,因此可以更精准地通过文本信息检索到对应的图像信息.</p> 
<p>考虑到图像与文本之间存在语义鸿沟,用户难以使用文本准确描述出所需图像,Vo 等[２１]提出一种使用查询文本特征修改查询图像特征的方法,称为文本图像剩余门(Text Image Residual Gating,TIRG),其采用剩余连接组合图像和文本特征,生成一个新的查询特征.查询输入可以是一张图像或一段文本,通过文本微调图像特征使其符合预期的查询结果.<br> <img src="https://images2.imgbox.com/0b/cc/gIQkWi7a_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>如图 ３ 所示,输入为一个白天人满为患的埃菲尔铁塔图像和一段文本“无人且转为夜间”,最终输出结果是夜间无人的埃菲尔铁塔图像.该模型在输入图像的基础上改变了某些特征,有效地缓解了语义鸿沟.</p> 
<p>针对文本特征提取面临的不同语言之间的差异性以及词汇拼写错误等问题,Wehrmann 等[２２]提出采用简单有效的字符级卷积架构来替换词嵌入和RNN,使用字符级而不是词级文本表示,用一小组有限字符构建跨多种语言的描述.使用字符级卷积学习句子表示,对输入噪声也具有鲁棒性.</p> 
<p>文本图像检索领域有一类特殊任务是菜谱检索,其特殊性表现在:文本内容包含烹饪说明和食材,其中烹饪说明是有序文本而食材是无序文本,<br> <img src="https://images2.imgbox.com/91/54/D1o6harF_o.png" alt="在这里插入图片描述" width="500"><br> 如图 ４ 所示.针对此类应用,Sal- vador 等[２３]同时考虑了正向和反向排序,食材样本使用双向 LSTM模型,烹饪说明样本因其有序而使用正向 LSTM 模型,最终两个编码器的输出被串联并嵌入食谱图像联合空间中.根据任务的特殊性,合理地使用模型不仅能提升检索准确性,对于大规模样本还会提升效率.</p> 
<p>在视频特征提取方面,２D-CNN 的主要作用是提取视觉空间特征,但由于视频是一个连续的序列,只使用 ２D-CNN会丢失时间信息,因此 ３D-CNN 的作用就是可以保留视频中的时间特征,更符合视频的特点.因此,Yamaguchi 等[２４]采 用局部与全局上下文结合,并使用 ２D-CNN 与 ３D-CNN 结合的方式提取视频特征.其累计提取 ６ 种类型的特征,最后全部串联起来作为完整的一个特征向量.文献[９]提出了音频与文本检索的架构,与文献[１０-１１]一致,均使用 MFCCs 提取音频特征.不同的是,２０１７ 年 Google 公司[２５]发现卷积神经网络在图像处理方面非常有效, 同样在音频分类方面也显示出了应用前景.通过对不同CNN 架构进行实验,发现 CNN 在音频分类任务上的效果很好,在大量的 YouTube 数据集上训练得到类 VGG 模型 VG- Gish.此后,许多研究都使用 VGGish 提取音频特征,如文献[２６-２７].</p> 
<h4><a id="12__166"></a>1.2 序列权重机制</h4> 
<p>序列权重机制已经被成功应用于包括目标检测和细粒度图像分类等在内的许多计算机视觉任务中.</p> 
<p>其主要目的是,针对不同任务,采用为信息赋权重的方式提取已知序列中的重要信息,忽略无关信息.</p> 
<p>近年来,在跨模态检索领域也逐渐涌现出一些研究工作采用序列权重机制来提升检索性能.</p> 
<p>Deng 等[２８]提出一种文本-图像协同注意网,可以有效地关注 图像中细粒度的局部特征,更好地与文本进行匹配,然后将有注意的特征输入哈希层得到二进制哈希表示,从而更容易比较文本和图像的内容相似性.</p> 
<p>Li 等[２９]提出一种具有门控神 经注意机制的递归神经网络(简称 GNA-RNN),网络的输入包括描述语句和人物图像,网络的输出是语句和图像间的亲和度,在机场等人员密集的公共场所作为安全监控有广泛的应用前景.具体做法是:将句子作为网络输入,逐个单词进行处理,采用单元级注意机制加权不同单元对不同单词的贡献,同时采用字级门估计不同单词对于自适应单词级加权的重要性.该项研究的贡献还有:整理收集了 CUHK 人员描述数据集(简称 CUHK-PEDES),包括有详细自然语言注释的大规模人员描述数据集和各种来源的人工样本集.在视觉问答任务中,与传统多模态特征融合相反,Li 等[３０]采用自然语言统 一所有输入信息,提取上下文和疑问词之间最相关的特征,在模型中构造了上下文到问题的注意力和问题到上下文的注意力,从而将 VQA 转换为机器阅读理解问题,有效缓解了跨模态差异.</p> 
<p>不同于上述两项研究在语言特征提取上采用序列权重的方式,Dey 等[３１]提出使用 LSTM 为单个图像计算n个不同且 有序依赖的注意力图,然后将该注意力图映射到公共子空间中与其他模态查询特征进行比较,最终进行相关性度量.人员搜索场景下,人们通常可能会分组走路,即使独自走路,出现在同一场景中的其他邻近行人也会包含重要的背景线索.针对该现象,Yan 等[３２]提出采用上下文实例扩展模块,使用 相对注意块来搜索和过滤场景中有用的上下文信息,</p> 
<p><img src="https://images2.imgbox.com/31/24/FUHzCKto_o.png" alt="在这里插入图片描述" width="1000"><br> 如图 ５所示;同时构建一个图学习框架,有效使用上下文对来更新目标相似性.通过融合对象外部的上下文特征,增加了有效信息,提高了检索结果的准确性.</p> 
<p>与单模态特征提取不同,序列权重机制可以应用于多种模态之间.Mithun 等[３３]提出一种弱监督的基于视觉语义嵌 入的联合框架,使用视频级句子描述从视频中学习相关片段的数量.其基本思想是:借助文本引导注意力(Text-Guided Attention,TGA)充分利用视频帧和句子描述之间的潜在对 齐,亦即,使用查询文本引导提取出视频中的关键连续时刻.</p> 
<p>Song 等[３４]引 入 一 种 多 义 实 例 嵌 入 网 络 (简 称 PIE- Nets),通过多头自我关注和残差学习将全局背景与本地引导 的特征相结合,计算实例的多种表征.</p> 
<p>在单模态以及多模态之间采用序列权重机制的基础上,Chen 等[３５]提出了一种新颖的注意力引导多模态相关(简称 AMC 模型)学习方法,采用序列权重机制来自适应平衡不同模态特征的重要性.</p> 
<p><img src="https://images2.imgbox.com/82/db/wxSUy8f5_o.png" alt="在这里插入图片描述" width="1000"></p> 
<p>如图 ６ 所示,AMC 模型由联合学习的内部和外部注意网络的层次结构组成,其中,内部注意网络主要包括视觉内注意网络和语言内注意网络.多模态间的外部注意网络主要关注与查询最相关的模态重要性.其不足之处在于序列权重机制仅关注了两种模态的最主要特征,忽略了图像和参照表达之间可能存在多个综合的文本-视觉对的事实.针对该问题,Liu 等[３６]设计了一种新颖的跨模态注意力引导 擦除方法,丢弃来自文本或视觉域的最主要信息,在线生成硬训练样本,并驱动模型发现互补的文本视觉对应.序列权重机制的应用非常广泛,在跨模态检索的特征提取阶段,应用该机制可以有效地保留重要特征信息.</p> 
<h3><a id="2_192"></a>2、内容相关性度量</h3> 
<p>解决跨模态检索中不同模态之间的差异性问题,主要依赖于模态间的内容相关性度量.目前深度学习方法中流行的度量策略包括视觉关系和图网络、对抗博弈机制等.</p> 
<h4><a id="21__195"></a>2.1 视觉关系和图网络</h4> 
<p>成对关系的推理对于各类计算机视觉相关研究都非常关键.</p> 
<p>近年来,在对象识别、视觉问答和动作识别等领域都涌现出一些采用深度神经网络学习视觉关系的研究工作.</p> 
<p>通常把一幅图像中的对象、属性和关系的明确表示成为图像的详细语义,对详细语义的准确识别有利于对图像的理解.</p> 
<p>场景图作为一种图像详细语义描述方法,被广泛用于图像检索任务.</p> 
<p>针对同一场景中对象之间的关系可能很复杂等问题,Johnson等[３７]提出一种基于视觉场景的条件随机场(Conditional Ran-dom Field,CRF)模型,使用场景图概念显式建模对象、对象的属性以及对象之间的关系并捕获详细语义,以场景图替换文本来检索相关图像.这种结构化图模型将文本结构化地表示出来,可以更好地表达图像中对象之间的关系,在文本图像检索中表现优异.有关场景图的生,Yang 等[３８]提出一种 Graph R-CNN 框架,采用图网络建模,同时利用关系提议网 络(简称 RePN)计算对象对之间的相似性,并修剪图中的边; 采用注意力图卷积网络(简称 aGCN)提取高级上下文信息, 更新场景图以获得更好的表示.</p> 
<p>受视觉问答相关研究[３９-４１]启发,在自然语言视频时刻检索领域,Liu 等[４２]提出一种时间模块化网络(简称 TMN)模 型,使用查询的底层语言结构动态组装相应的模块化神经网络,然后通过该神经网络对视频进行组合推理,最后输出查询和视频间的对应关系.在此基础上,Zhang 等[４３]提出一种时刻对齐网络(Moment Alignment Network,MAN),通过迭代图调整网络将单次前馈网络中的候选时刻编码和时间结构推理统一起来,以端到端的形式共同学习图网络的最佳结构,来准确表达对象之间的关系.利用场景图可以非常直接清晰地表达文本中对象之间的关系,可以很大程度上弥补语义鸿沟.通过图网络来迭代更新网络的方式具有更强的鲁棒性.</p> 
<h4><a id="22__209"></a>2.2 对抗博弈机制</h4> 
<p>生成对抗网络(Generative Adversarial Networks,GAN)自 ２０１４ 年诞生以来,迅速成为深度学习领域的研究热点.</p> 
<p>Wang 等[４４]提出的对抗性跨模态检索与 Peng 等[４５]提出的跨 模态生成对抗网络的结构非常类似,都是对不同模态数据的联合分布进行建模.</p> 
<p>模态间和模态内的相关性可以在生成模型和判别模型中同时探索,通过相互间的搏斗,促进跨模态相关学习.</p> 
<p>其共同特征包括:</p> 
<ul><li>１)采用跨模态卷积自动编码器构成生成模型,确保有效利用跨模态相关性和重构信息;</li><li>２)两种判别模型同时进行模态内区分和模态间区分,以实现跨模态对抗训练.</li></ul> 
<p>Wang 等[４６]提出一种新颖的对抗性跨模态嵌入(Adver-sarial Cross-Modal Embedding,ACME)方法,使一种模态的嵌入能够在另一种模态中恢复对应实例的一些重要信息.其目标是通过采用新的三重态损失方案和有效采样策略来学习两种模态间的公共嵌入特征空间,借助对抗性学习策略进行<br> 模态对齐.</p> 
<p>类似地,针对说话人识别问题,文献[４７]提出一种声视频多模态生成对抗及三重态损失网络(AVGATN),语音部分采用传统语音识别中常用的梅尔倒谱系数(Mel Fre-quency Cepstrum Coefficients,MFCC)特征,并增加了基于三元组的身份识别及特征匹配判断网络,获得了良好的性能.</p> 
<p>与文献[４６]类似,Gu 等[４８]也将生成过程结合到跨模态特征 嵌入中,不仅可以学习全局抽象特征,还可以学习局部基础特征.</p> 
<p>为了探索以程序文本生成图像来解决检索问题的可能性,Zhu 等[４９]提出一种基于 GAN 的食谱检索(简称 R２GAN)架构,</p> 
<p><img src="https://images2.imgbox.com/0a/b6/ebO4GrT0_o.png" alt="在这里插入图片描述" width="1000"></p> 
<p>如图 ７ 所示.该架构包括一个生成器和两个鉴别器,旨在探索以程序文本生成图像来解决检索问题的可能性.</p> 
<p>采用 GAN 以多种方式学习兼容的跨模态特征,并通过显示从食谱生成的图像来解释搜索结果,第一个鉴别器用于区分真实图像和伪造图像,第二个鉴别器用于区分图像来源.</p> 
<p>生成模型在无监督领域占据重要地位,负责捕捉样本数据的分布;判别模型主要用于判别生成的数据是否真实.两者互相博弈调整,最终使模型达到平衡.从上述研究可以看出,GAN 可以通过文本数据生成图像数据来检索图像,从而有效地降低跨模态差异.</p> 
<h2><a id="_238"></a>三、哈希编码方法</h2> 
<p>随着多媒体数据量的急剧增长,研究人员开始采用跨模态哈希方法来解决跨模态检索问题,通过将多媒体数据转化为二进制编码,投影跨媒体数据到公共汉明空间,<font color="red">提高检索速度的同时缩减存储空间</font>.</p> 
<p>相较于传统方法,哈希编码方法提升了效率,同时深度哈希方法也在检索准确性和效率方面达到了很好的平衡,从而被广泛应用于各种检索研究领域.</p> 
<h3><a id="1_244"></a>1、手工特征的哈希编码</h3> 
<p>手工特征的哈希编码可简单划分为 ３ 类：无监督哈希、有监督哈希和半监督哈希.</p> 
<ul><li>Weiss 等[５０]提出的无监督哈希侧重 于捕获未标记数据中的基础结构信息,通过保留样本之间的相似性来学习紧凑的二进制编码.</li><li>有监督哈希的代表性工作是由 Liu 等[５１]提出的基于内核的监督哈希(简称 KSH),其采 用内核化定制哈希函数,使哈希码损失函数最小.文献[５２]提出使用深度学习方法提取视频特征,使用传统哈希方法对视频特征生成二值哈希,但不同的是视频特征的学习能够与哈希学习相结合,是一种端到端的视频哈希算法,特征提取与生成哈希过程可以互相反馈,从而缓解高度依赖特征提取的情况.</li><li>半监督哈希方法的代表性工作有文献[５３]和文献[５４],它们都充分利用了有标签和无标签的训练数据信息.</li></ul> 
<h3><a id="2_251"></a>2、深度学习的哈希编码</h3> 
<p>传统的手工特征的哈希编码方法虽然改善了检索速度和存储性能,但不可避免地损失了检索精度.</p> 
<p>为此,近几年研究人员开始探索基于深度学习的哈希编码方法.</p> 
<p>Salakhutdinov 等[５５]提出的语义哈希是利用深度学习技术进行哈希编码的早期工作之一,它采用受限玻尔兹曼机(Restricted Boltzmann Machine,RBM)学习哈希码以进行可视搜索.</p> 
<p>Xia 等[５６]和Liong 等[５７]提出了一种两阶段哈希方法,包括特征学习阶段 和哈希码生成阶段.其关键技术有:</p> 
<ul><li>设计了深度神经网络从图像-文本输入对中学习非线性变换,实现统一的二进制编码;</li><li>以概率方式设计特定于模态的神经网络,在该网络中,根据推断出的二进制代码对隐变量进行建模.</li></ul> 
<p>文献[５８]首次提出利用深度异构哈希网络来实现跨模态人脸检索的方法,能够将异构空间的人脸图像和视频数据映射到同一空间,生成二值哈希表示.该方法提供了一套深度网络的通用框架,不仅可以用于图像与视频之间,还可以扩展到其他跨模态的检索任务.</p> 
<p>Dai 等[５９]和 Long 等[６０]研究工作的共同点是利用三元组 排序损失来建模图像之间的相似关系.</p> 
<p>Yao 等[６１]设计了一 个双流框架,该框架结合了哈希编码和分类,不仅可以保留图像之间的相对相似性,还可以保留图像的语义结构.</p> 
<p>Long 等[６０]使用对抗性学习,利用源知识以及未标记的或仅标记稀疏的目标数据来学习目标域中的新模型,设计了一个域识别器来处理域转移,在特征空间中使源分布和目标分布更接近.</p> 
<p>Qiu 等[６２]提出基于 GAN 的深度语义哈希(Deep Seman- tic Hashing,DSH-GANs),使用由生成对抗网络生成的合成图像来扩大训练数据,以更好地进行哈希学习,如图 ８ 所示.</p> 
<p><img src="https://images2.imgbox.com/9f/0d/SpdlcB0e_o.png" alt="在这里插入图片描述" width="1000"><br> 深度监督哈希方法,一般需要用到多类别的大量训练数据,当出现先前未有的数据类别时,必须重新训练 CNN 模型再次为所有数据库图像生成哈希码.针对此问题,Wu 等[６３]提出一种新的深度增量哈希网络(Deep Incremental HashingNetwork,DIHN),以增量方式学习哈希码,同时保持数据库 中原有图像的哈希码表示不变.通过保留训练数据之间的相似性来学习查询集的深度哈希函数,在缩短训练时间的同时确保了检索的准确性.其以增量方式学习哈希编码使新数据可以直接进行编码,无需重新训练模型,解决了跨模态检索中增量学习的挑战.针对现有监督哈希效率较低等问题,</p> 
<p>Jiang 等[６４]提出一种新的深度监督哈希方法,称为非对称深度监督哈希(简称 ADSH),适用于大规模最近邻搜.ADSH 以非对称方式处理查询点和数据库点,仅为查询点学习深度哈希函数,而对于数据库点则直接学习其哈希码,可以大大缩短训练时间,表现出比传统对称深度监督哈希方法更好的性能.<br> Panyapanuwat 等[６５]提出了针对音频检索的无监督的深度学 习哈希方法,其主要贡献在于对目标函数的改进,增加了目标函数的独立性、平衡性和相似性保持特性,即可以最小化输入空间中的真实搜索结果与哈希编码空间中的近似搜索结果之间的差异.</p> 
<h2><a id="_280"></a>四、未来发展展望【跨模态检索领域存在的问题及未来发展趋势进行分析】</h2> 
<p>从研究方法层面来看,多数工作关注对图像、视频等数据的全局特征学习,而忽略了这些数据中包含的细粒度语义信息;</p> 
<p>从应用层面来看,跨模态检索应用领域主要局限于安全监控与搜索引擎,还有待进一步扩展.</p> 
<p>基于此,本文认为,未来一段时间,跨模态检索领域的研究将呈现如下发展趋势：</p> 
<h3><a id="1_287"></a>1、方法改进</h3> 
<p>在深度学习方法与知识图谱融合、细粒度分析等方面仍有提升空间,甚至可以将情感、环境等信息融合到公共表示学习中,以获取更强的相关,从而进一步改善跨模态检索的准确性.</p> 
<ol><li><strong>融合知识图谱</strong>.跨模态检索着重于数据内容的高级语义信息,知识图谱将高级语义信息连接成关系网络,称为语义网,根据检索的输入,可以检索出与输入语义相关的内容,从而增加检索结果的多样性和准确性.</li><li><strong>加入细粒度分析</strong>.在目前的跨模态检索研究中,对于视频、图像等数据的特征提取,大多是针对数据进行整体的提取,若加入细粒度分析,对图像或视频等内容进行如对象级地探究,则对于高级语义信息的提取有积极作用.</li><li><strong>融合丰富信息</strong>.在跨模态检索任务中,对输入内容可以融合情感、环境、背景等丰富信息,有效区分数据个体之间的差异性,从而可以使检索结果更准确且更具判别性.</li></ol> 
<h3><a id="2_294"></a>2、应用领域</h3> 
<p>跨模态检索在搜索引擎和安全监控等方面已经有了一些成功应用,未来可以加入环境、情感等上下文<br> 信息进行全面检索、数据整理、语音匹配,甚至是诗画创作等.</p> 
<ol><li><strong>融合丰富信息的检索</strong>：对于仅针对内容的图像检索,计算机难以真正理解人的检索意图,因此加入环境、情感等上下文信息进行全面检索,可以有效地理解检索意图,提高检索精度.</li><li><strong>数据整理</strong>：海量的多媒体数据往往不易整理和归纳,需要花费大量人力.若利用跨模态检索技术,可以自动识别数据中的内容,进行自动分类归纳,从而减少人力消耗和时间成本.</li><li><strong>语音匹配</strong>：音频数据通常需要人工进行匹配整理,而通过跨模态检索可以实现音频关键内容的提取,并且可以根据搜索的输入自动定位出音频时刻位置.</li><li><strong>诗画创作</strong>：跨模态检索的一个重要任务就是通过提取各个模态数据的高级语义信息,将其关联起来.而诗画创作则可以利用文字搜索图像或由图像搜索诗句的方式实现.</li></ol> 
<p>结束语 本文梳理分析了解决跨模态检索任务的主流研究方法,包括：</p> 
<ul><li>传统方法、</li><li>基于深度学习的方法、</li><li>基于哈希编码的方法；深度学习与哈希编码融合的方法等.</li></ul> 
<p>围绕深度学习方法在跨模态检索领域应用的关键性突破和进展,分析了各主要模块的关键问题以及代表性工作,涵盖了跨模态检索领域的最新相关工作.本文最后总结了跨模态检索研究的常用数据集和性能评价,分析明确了当前存在的主要问题并指出了该领域的未来发展趋势.</p> 
<hr> 
<hr> 
<hr> 
<p>参考资料：<br> <a href="" rel="nofollow">跨模态检索研究进展综述 ComputerScience 计算机科学 Vol．４８,No．８,Aug．２０２１</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/db1158a3df9b377333982198ae93a375/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Qt QTableView之QSqlTableModel数据库显示及插入删除操作</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/c83f0503b43b8287f6e0c46d1a641074/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">计操实验 多级反馈队列C语言</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>