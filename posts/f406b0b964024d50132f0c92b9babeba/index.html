<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>EMNLP&#39;23 京东：深度语义召回中的超参自适应调整 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="EMNLP&#39;23 京东：深度语义召回中的超参自适应调整" />
<meta property="og:description" content="作者 | 李国趸 整理 | NewBeeNLP https://zhuanlan.zhihu.com/p/675751178
后台留言『交流』，加入 NewBee讨论组
大家好，这里是 NewBeeNLP。京东搞了一种在召回阶段简单的自适应调整温度系数和 margin 的方法。
参考论文：Adaptive Hyper-parameter Learning for Deep Semantic Retrieval
公司：京东
链接：https://aclanthology.org/2023.emnlp-industry.72.pdf
会议：EMNLP2023
省流版 京东搜索搞了一种在召回阶段简单的自适应调整温度系数和margin的方法，比WWW那篇论文更简单。主要看两个大盘指标：UV值（每个独立访客的收入）和UCVR（订单数/UV）和一个中间指标：过了相关性模块后参与排名阶段(prank)的item数量（笔者注：怀疑prank是粗排）。2023年的10天的线上AB实验显示，效果还是蛮好的，p值也比较低。
动机 召回或者说检索中，最常用的就是sampled softmax loss或者margin loss，其中利用margin和温度系数来去显式控制正负例之间的“分辨率“。
之前的工作都是手拍的超参，很明显，更合理的方式是针对不同query，不同user实现一个自适应的温度或者margin。最近也有一篇www论文（Adap-τ : Adaptively modulating embedding magnitude for recommendation）在探讨温度系数怎么自适应调整，但是作者认为该论文方法比较复杂，且在搜索系统中是不可用的。文中提到的原因是因为现实中query是不可知的，而不像推荐系统中user是给定的。
作者也通过一系列实验表明两种loss的超参对batch recall@k的结果是比较敏感的：
因此，作者就设计了一种更简单而且可以自适应调整温度系数和margin的方法，并且在京东线上AB实验上取得了收益。
双塔模型上常用的两个loss 双塔召回模型主要是通过负采样或batch内负样本进行训练。给定一个样本列表，其中包含一个正样本和n个负样本，即。目标是将负样本表征远离query并将正样本表征靠近query，即：
为了达到上述目标，有两种loss可以用来优化：
margin loss ：旨在通过固定的超参数margin来区分正负例，控制度量空间中的决策边界。
softmax loss ：softmax损失函数能够实现良好的训练稳定性，并与ranking metric相匹配。通常比其他方法表现更好，因此在检索中受到广泛关注。其中温度系数用于平滑训练数据的整体拟合分布。更小值意味模型完全适应监督信号并更专注难负例，反之亦然。
原文翻译，看看就好：上述损失函数取决于超参数，这在性能方面起着重要作用。具体实验将在下一节中讨论。不幸的是，传统方法在自适应选择超参数方面存在问题。此外，在个性化场景中，每对需要特定的边界和温度值，使得学习或选择适当的值更加具有挑战性。虽然其他领域，如推荐系统，已通过双层或统计学习解决了这个问题，但我们认为这些方法不适用于检索场景。检索场景涉及到来自在线系统的不同于推荐的输入查询，因为输入查询是丰富和不可知的。因此，需要一种无参数的方法来生成特定的值。为此，我们首先提出了一种通过内积计算值的启发式方法，然后提出了一种对称度量学习方法来缓解训练过程中的崩溃问题。
方法 在度量空间中，最难的负样本的位置非常接近正样本，而容易或随机的负样本则远离正样本。我们需要细分负样本。给定一对，如果是最难的负样本，则查询和正样本v的相似度应该更高，换句话说，边缘损失应该更小。同样，对于最难的负样本，温度在softmax损失中也应该更小。
所以，给定一个pair对，作者针对这个pair对定义了一个动态的，即：
其实呢，这个动态，是指的根据正负item之间的内积来动态决定，依然需要一个全局的参数和。这两个参数可以是trainable的。
虽然启发式方法很简单，但作者觉得它在训练过程中会遭受模型崩溃的问题，导致所有item在度量空间中聚集在一起。从梯度的角度来看，可以知道自适应的margin将影响正负item的更新方向。
为了避免表征崩塌，最简单的方法就是停梯度：
所以最后的loss就变成：
除此以外，作者还觉得，糟糕的初始化方式还是有崩溃风险，比如。这种情况下，q和v的距离会大于v和的距离。为了避免这种情况，搞了一个对称的loss来约束，即将v作为锚点，q作为正样本项，旨在将负样本项远离正样本项:
总的loss就是：
softmax loss也是一样：
效果 batch recall@k作为指标
列一下牛逼的基线：
DSSM：经典的不说了，Learning deep structured semantic models for web search using clickthrough data" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/f406b0b964024d50132f0c92b9babeba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-23T11:05:50+08:00" />
<meta property="article:modified_time" content="2024-01-23T11:05:50+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">EMNLP&#39;23 京东：深度语义召回中的超参自适应调整</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:justify;"><img src="https://images2.imgbox.com/74/f6/J0lX1Em4_o.png" alt="format,png"><br></p> 
 <blockquote> 
  <p>作者 | 李国趸  </p> 
  <p>整理 | NewBeeNLP </p> 
  <p>https://zhuanlan.zhihu.com/p/675751178</p> 
 </blockquote> 
 <p style="text-align:center;"><em>后台留言『交流』，加入 NewBee讨论组</em></p> 
 <p style="text-align:justify;">大家好，这里是 NewBeeNLP。京东搞了一种在召回阶段简单的自适应调整温度系数和 margin 的方法。</p> 
 <ul><li><p>参考论文：Adaptive Hyper-parameter Learning for Deep Semantic Retrieval</p></li><li><p>公司：京东</p></li><li><p>链接：https://aclanthology.org/2023.emnlp-industry.72.pdf</p></li><li><p>会议：EMNLP2023</p></li></ul> 
 <img src="https://images2.imgbox.com/2d/25/XtBJAFkZ_o.png" alt="335f6c0ecf9d4cf37253b724bf4ff9fe.png"> 
 <h3>省流版</h3> 
 <p style="text-align:justify;">京东搜索搞了一种在召回阶段简单的自适应调整温度系数和margin的方法，比WWW那篇论文更简单。主要看两个大盘指标：UV值（每个独立访客的收入）和UCVR（订单数/UV）和一个中间指标：过了相关性模块后参与排名阶段(prank)的item数量（笔者注：怀疑prank是粗排）。2023年的10天的线上AB实验显示，效果还是蛮好的，p值也比较低。</p> 
 <img src="https://images2.imgbox.com/a5/b0/5FW84prw_o.png" alt="827e2ddfb993ce066daae9653362730b.png"> 
 <h3>动机</h3> 
 <p style="text-align:justify;">召回或者说检索中，最常用的就是sampled softmax loss或者margin loss，其中利用margin和温度系数来去显式控制正负例之间的“分辨率“。</p> 
 <p style="text-align:justify;">之前的工作都是手拍的超参，很明显，更合理的方式是针对不同query，不同user实现一个自适应的温度或者margin。最近也有一篇www论文（Adap-τ : Adaptively modulating embedding magnitude for recommendation）在探讨温度系数怎么自适应调整，但是作者认为该论文方法比较复杂，且在搜索系统中是不可用的。文中提到的原因是因为现实中query是不可知的，而不像推荐系统中user是给定的。</p> 
 <p style="text-align:justify;">作者也通过一系列实验表明两种loss的超参对batch recall@k的结果是比较敏感的：<img src="https://images2.imgbox.com/b2/e2/hznJuJEd_o.png" alt="566bae6d15a056df0cfbabce0d3fa027.png"></p> 
 <p style="text-align:justify;">因此，作者就设计了一种更简单而且可以自适应调整温度系数和margin的方法，并且在京东线上AB实验上取得了收益。</p> 
 <h3><strong> 双塔模型上常用的两个loss</strong></h3> 
 <p style="text-align:justify;">双塔召回模型主要是通过负采样或batch内负样本进行训练。给定一个样本列表，其中包含一个正样本和n个负样本，即。目标是将负样本表征远离query并将正样本表征靠近query，即：</p> 
 <img src="https://images2.imgbox.com/f6/78/PpU72tw6_o.png" alt="609d189ad17521933778895d0b2376e0.png"> 
 <p style="text-align:justify;">为了达到上述目标，有两种loss可以用来优化：</p> 
 <ul><li><p><strong> margin loss </strong>：旨在通过固定的超参数margin来区分正负例，控制度量空间中的决策边界。</p></li></ul> 
 <img src="https://images2.imgbox.com/84/b0/T4oapSDa_o.png" alt="18cef7a96ca895e36c39c8a502cba746.png"> 
 <ul><li><p><strong> softmax loss </strong>：softmax损失函数能够实现良好的训练稳定性，并与ranking metric相匹配。通常比其他方法表现更好，因此在检索中受到广泛关注。其中温度系数用于平滑训练数据的整体拟合分布。更小值意味模型完全适应监督信号并更专注难负例，反之亦然。</p></li></ul> 
 <img src="https://images2.imgbox.com/31/82/mxnmFobD_o.png" alt="4b9cd9bea5b18d07970a87bc80e8b35e.png"> 
 <blockquote> 
   
  <p style="text-align:justify;">原文翻译，看看就好：上述损失函数取决于超参数，这在性能方面起着重要作用。具体实验将在下一节中讨论。不幸的是，传统方法在自适应选择超参数方面存在问题。此外，在个性化场景中，每对需要特定的边界和温度值，使得学习或选择适当的值更加具有挑战性。虽然其他领域，如推荐系统，已通过双层或统计学习解决了这个问题，但我们认为这些方法不适用于检索场景。检索场景涉及到来自在线系统的不同于推荐的输入查询，因为输入查询是丰富和不可知的。因此，需要一种无参数的方法来生成特定的值。为此，我们首先提出了一种通过内积计算值的启发式方法，然后提出了一种对称度量学习方法来缓解训练过程中的崩溃问题。</p> 
   
 </blockquote> 
 <h3><strong> 方法</strong></h3> 
 <p style="text-align:justify;">在度量空间中，最难的负样本的位置非常接近正样本，而容易或随机的负样本则远离正样本。我们需要细分负样本。给定一对，如果是最难的负样本，则查询和正样本v的相似度应该更高，换句话说，边缘损失应该更小。同样，对于最难的负样本，温度在softmax损失中也应该更小。</p> 
 <p style="text-align:justify;">所以，给定一个pair对，作者针对这个pair对定义了一个动态的，即：</p> 
 <img src="https://images2.imgbox.com/5d/ff/JjA15k7R_o.png" alt="d9bf8a577664fb5de7bd5ebfeb7cc467.png"> 
 <p style="text-align:justify;">其实呢，这个动态，是指的根据正负item之间的内积来动态决定，依然需要一个全局的参数和。这两个参数可以是trainable的。</p> 
 <p style="text-align:justify;">虽然启发式方法很简单，但作者觉得它在训练过程中会遭受模型崩溃的问题，导致所有item在度量空间中聚集在一起。从梯度的角度来看，可以知道自适应的margin将影响正负item的更新方向。</p> 
 <img src="https://images2.imgbox.com/06/01/hNVStwd3_o.png" alt="98783c3e25186772a92f2edb01d683d0.png"> 
 <p style="text-align:justify;">为了避免表征崩塌，最简单的方法就是停梯度：</p> 
 <img src="https://images2.imgbox.com/d1/b0/JtokGbO8_o.png" alt="051d49ec712830d99c76d1e3b947fed0.png"> 
 <p style="text-align:justify;">所以最后的loss就变成：</p> 
 <img src="https://images2.imgbox.com/79/93/yXWCFkRb_o.png" alt="51f1ec58831911966740bd10924f9064.png"> 
 <p style="text-align:justify;">除此以外，作者还觉得，糟糕的初始化方式还是有崩溃风险，比如。这种情况下，q和v的距离会大于v和的距离。为了避免这种情况，搞了一个对称的loss来约束，即将v作为锚点，q作为正样本项，旨在将负样本项远离正样本项:</p> 
 <img src="https://images2.imgbox.com/fd/42/MYBjfdy0_o.png" alt="c2cb5a227ae0cb11b99952786a1dacc9.png"> 
 <img src="https://images2.imgbox.com/64/f9/OhsElPHx_o.png" alt="688dc1618ba041079588aa468604268d.png"> 
 <p style="text-align:justify;">总的loss就是：</p> 
 <img src="https://images2.imgbox.com/bd/94/dnKlx6FN_o.png" alt="dcfd198d9ac089476a96df0de0e65589.png"> 
 <p style="text-align:justify;"><strong> softmax loss也是一样：</strong></p> 
 <img src="https://images2.imgbox.com/a1/bf/YpJJY5ED_o.png" alt="23116a1f8d6e1e9f2630f11985d0f938.png"> 
 <h3><strong> 效果</strong></h3> 
 <ul><li><p style="text-align:justify;">batch recall@k作为指标</p></li><li><p style="text-align:justify;">列一下牛逼的基线：</p> 
   <ul><li><p>DSSM：经典的不说了，Learning deep structured semantic models for web search using clickthrough data</p></li><li><p>MMSE：京东23年出的一篇多目标EBR召回论文：Learning multi-stage multigrained semantic embeddings for e-commerce search</p></li><li><p>DPSR：京东20年出的一篇EBR召回论文：Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning</p></li><li><p>LTR：经典的不说了，Learning to rank for information retrieval</p></li><li><p>RSR：京东22年出的一篇EBR召回论文，backbone是bert，Pre-training tasks for user intent detection and embedding retrieval in e-commerce search</p></li></ul></li><li><p style="text-align:justify;">作者也提到了这个方法可以用到淘宝的MGDSPR中。</p></li></ul> 
 <img src="https://images2.imgbox.com/db/33/lgK0Y1K4_o.png" alt="0d2fcc3afb04487db950c8499305c9a6.png"> 
 <blockquote> 
   
  <p style="text-align:justify;">维度设置为128，批量大小设置为350，IVF-PQ的n-list设置为32768，并在Faiss ANNS库1中使用索引构建。softmax的默认温度为1/30，边距设置为0.1。设置为0.5，设置为0.01，设置为1/30。w的默认值设置为0.05。采用Adam优化器，初始学习率为5e-5。查询和序列的最大长度分别为30和100。</p> 
   
 </blockquote> 
 <p style="text-align:justify;">此外消融了下这些loss的效果，看起来确实涨了一些。同时对称loss是很必要的。</p> 
 <img src="https://images2.imgbox.com/1b/16/P1rUiHyc_o.png" alt="53820c6afc7903a3912cebe925da9e5f.png"> 
 <h3><strong> 总结</strong></h3> 
 <p style="text-align:justify;">可以试试。</p> 
 <p style="text-align:center;">一起交流</p> 
 <p style="text-align:left;">想和你一起学习进步！『<strong>NewBeeNLP』</strong>目前已经建立了多个不同方向交流群（<strong>机器学习 / 深度学习 / 自然语言处理 / 搜索推荐 / 图网络 / 面试交流 / </strong>等），名额有限，赶紧添加下方微信加入一起讨论交流吧！（注意一定o要<strong>备注信息</strong>才能通过）</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/ae/cd/rUkRLnA6_o.jpg" alt="d0a071ce367cde9583189eb996fc3582.jpeg"></p> 
 <p style="text-align:center;"><img height="792" width="1068" src="https://images2.imgbox.com/aa/39/gM3xiUr9_o.gif" alt="631ff750a28839bdb736fb6fad9372fe.gif"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/a0e0b6551a48b09c6926b0d8f0fd738c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">我的文档不见了怎么恢复？3个方法快速恢复！</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2590fd10f29826cd2fd65b5345364009/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">sudo 授权问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>