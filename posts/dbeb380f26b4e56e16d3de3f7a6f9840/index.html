<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Transformer 优秀开源工作：timm 库 vision transformer 代码解读 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Transformer 优秀开源工作：timm 库 vision transformer 代码解读" />
<meta property="og:description" content="timm库（PyTorchImageModels，简称timm）是一个巨大的PyTorch代码集合，已经被官方使用了。
参考：timm 视觉库中的 create_model 函数详解
p r e t r a i n e d \color{red}{pretrained} pretrained
如果我们传入 pretrained=True，那么 timm 会从对应的 URL 下载模型权重参数并载入模型，只有当第一次（即本地还没有对应模型参数时）会去下载，之后会直接从本地加载模型权重参数。
model = timm.create_model(&#39;resnet34&#39;, pretrained=True) 输出：
Downloading: &#34;https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth&#34; to /home/song/.cache/torch/hub/checkpoints/resnet34-43635321.pth 查 看 安 装 的 t i m m 库 中 可 以 使 用 哪 些 模 型 ： \color{red}{查看安装的timm库中可以使用哪些模型：} 查看安装的timm库中可以使用哪些模型：
参考：Pytorch视觉模型库–timm
找到swin transformer的模型：
&#39;swin_base_patch4_window7_224&#39;, &#39;swin_base_patch4_window7_224_in22k&#39;, &#39;swin_base_patch4_window12_384&#39;, &#39;swin_base_patch4_window12_384_in22k&#39;, &#39;swin_large_patch4_window7_224&#39;, &#39;swin_large_patch4_window7_224_in22k&#39;, &#39;swin_large_patch4_window12_384&#39;, &#39;swin_large_patch4_window12_384_in22k&#39;, &#39;swin_small_patch4_window7_224&#39;, &#39;swin_tiny_patch4_window7_224&#39;, 一、修改num_classes提取分类前和后的特征 我们使用模型：vit_tiny_patch16_224" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/dbeb380f26b4e56e16d3de3f7a6f9840/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-06T21:01:25+08:00" />
<meta property="article:modified_time" content="2022-05-06T21:01:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformer 优秀开源工作：timm 库 vision transformer 代码解读</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>timm库（PyTorchImageModels，简称timm）是一个巨大的PyTorch代码集合，已经被官方使用了。<br> 参考：<a href="https://blog.csdn.net/weixin_44966641/article/details/121364784">timm 视觉库中的 create_model 函数详解</a><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           p 
          
         
           r 
          
         
           e 
          
         
           t 
          
         
           r 
          
         
           a 
          
         
           i 
          
         
           n 
          
         
           e 
          
         
           d 
          
         
        
       
      
        \color{red}{pretrained} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord" style="color: red;"><span class="mord mathdefault" style="color: red;">p</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: red;">r</span><span class="mord mathdefault" style="color: red;">e</span><span class="mord mathdefault" style="color: red;">t</span><span class="mord mathdefault" style="margin-right: 0.02778em; color: red;">r</span><span class="mord mathdefault" style="color: red;">a</span><span class="mord mathdefault" style="color: red;">i</span><span class="mord mathdefault" style="color: red;">n</span><span class="mord mathdefault" style="color: red;">e</span><span class="mord mathdefault" style="color: red;">d</span></span></span></span></span></span><br> 如果我们传入 pretrained=True，那么 timm 会从对应的 URL 下载模型权重参数并载入模型，只有当第一次（即本地还没有对应模型参数时）会去下载，之后会直接从本地加载模型权重参数。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> timm<span class="token punctuation">.</span>create_model<span class="token punctuation">(</span><span class="token string">'resnet34'</span><span class="token punctuation">,</span> pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">Downloading<span class="token punctuation">:</span> <span class="token string">"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth"</span> to <span class="token operator">/</span>home<span class="token operator">/</span>song<span class="token operator">/</span><span class="token punctuation">.</span>cache<span class="token operator">/</span>torch<span class="token operator">/</span>hub<span class="token operator">/</span>checkpoints<span class="token operator">/</span>resnet34<span class="token operator">-</span><span class="token number">43635321</span><span class="token punctuation">.</span>pth
</code></pre> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           查 
          
         
           看 
          
         
           安 
          
         
           装 
          
         
           的 
          
         
           t 
          
         
           i 
          
         
           m 
          
         
           m 
          
         
           库 
          
         
           中 
          
         
           可 
          
         
           以 
          
         
           使 
          
         
           用 
          
         
           哪 
          
         
           些 
          
         
           模 
          
         
           型 
          
         
           ： 
          
         
        
       
      
        \color{red}{查看安装的timm库中可以使用哪些模型：} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord" style="color: red;"><span class="mord cjk_fallback" style="color: red;">查</span><span class="mord cjk_fallback" style="color: red;">看</span><span class="mord cjk_fallback" style="color: red;">安</span><span class="mord cjk_fallback" style="color: red;">装</span><span class="mord cjk_fallback" style="color: red;">的</span><span class="mord mathdefault" style="color: red;">t</span><span class="mord mathdefault" style="color: red;">i</span><span class="mord mathdefault" style="color: red;">m</span><span class="mord mathdefault" style="color: red;">m</span><span class="mord cjk_fallback" style="color: red;">库</span><span class="mord cjk_fallback" style="color: red;">中</span><span class="mord cjk_fallback" style="color: red;">可</span><span class="mord cjk_fallback" style="color: red;">以</span><span class="mord cjk_fallback" style="color: red;">使</span><span class="mord cjk_fallback" style="color: red;">用</span><span class="mord cjk_fallback" style="color: red;">哪</span><span class="mord cjk_fallback" style="color: red;">些</span><span class="mord cjk_fallback" style="color: red;">模</span><span class="mord cjk_fallback" style="color: red;">型</span><span class="mord cjk_fallback" style="color: red;">：</span></span></span></span></span></span><br> 参考：<a href="https://blog.csdn.net/qq_42003943/article/details/118382823">Pytorch视觉模型库–timm</a><br> <img src="https://images2.imgbox.com/b3/88/rJYxReQ3_o.png" alt="在这里插入图片描述"><br> 找到swin transformer的模型：</p> 
<pre><code> 'swin_base_patch4_window7_224',
 'swin_base_patch4_window7_224_in22k',
 'swin_base_patch4_window12_384',
 'swin_base_patch4_window12_384_in22k',
 'swin_large_patch4_window7_224',
 'swin_large_patch4_window7_224_in22k',
 'swin_large_patch4_window12_384',
 'swin_large_patch4_window12_384_in22k',
 'swin_small_patch4_window7_224',
 'swin_tiny_patch4_window7_224',
</code></pre> 
<p><img src="https://images2.imgbox.com/d8/03/uhSwXjG2_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="num_classes_34"></a>一、修改num_classes提取分类前和后的特征</h2> 
<p>我们使用模型：<mark>vit_tiny_patch16_224</mark></p> 
<h3><a id="11_num_classes0classify192_36"></a>1.1 修改为num_classes=0提取classify之前的特征192：</h3> 
<pre><code class="prism language-python">self<span class="token punctuation">.</span>transformer_model <span class="token operator">=</span> creat<span class="token punctuation">(</span><span class="token string">'vit_tiny_patch16_224'</span><span class="token punctuation">,</span> pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<p>打印模型结构可以发现：先经过PatchEmbed然后经历vit_tiny_patch16_224的<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          6 
         
        
       
      
        \color{red}{6} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord" style="color: red;"><span class="mord" style="color: red;">6</span></span></span></span></span></span>个Block，然后得到的是classifier 之前的特征<br> <img src="https://images2.imgbox.com/67/fc/CSovrMEM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/fd/09/GLdjTCvJ_o.png" alt="在这里插入图片描述"><br> 这里输入特征维度【b, 3, 224, 224】，输出特征维度【b，192】：<br> <img src="https://images2.imgbox.com/51/4f/u9hedhjZ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/b8/1e/4cN4Rtr1_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="12_t_q_feature__selftransformer_modelforward_featurest_q_x_46"></a>1.2 得到网络分类器之前的输出<code>t_q_feature = self.transformer_model.forward_features(t_q_x)</code></h3> 
<p>示例：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"如果设置num_classes，表示重设全连接层，该操作通常用于迁移学习"</span><span class="token punctuation">)</span>
m <span class="token operator">=</span> timm<span class="token punctuation">.</span>create_model<span class="token punctuation">(</span><span class="token string">'resnet50'</span><span class="token punctuation">,</span> pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>num_classes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
m<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
o <span class="token operator">=</span> m<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Classification layer shape: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>o<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token comment">#输出flatten层或者global_pool层的前一层的数据（flatten层和global_pool层通常接分类层）</span>
o <span class="token operator">=</span> m<span class="token punctuation">.</span>forward_features<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Feature shape: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>o<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>代码执行输出如下所示：</p> 
<pre><code class="prism language-python">如果设置num_classes，表示重设全连接层，该操作通常用于迁移学习
Classification layer shape<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Feature shape<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="13_head_65"></a>1.3 正常修改head的类别</h3> 
<p>打印模型结构，前面的到normal都一样，最后的head Linear层发生变化(head): <code>Linear(in_features=192, out_features=3600, bias=True)</code><br> <img src="https://images2.imgbox.com/41/5e/FlxPPVDw_o.png" alt="在这里插入图片描述"><br> 这里输入特征维度【b, 3, 224, 224】，输出特征维度【b，3600】<mark>3600是我们修改的最后一层输出</mark>：<br> <img src="https://images2.imgbox.com/47/9b/MmOXpDRX_o.png" alt="在这里插入图片描述"><br> PS：直接修改num_classes=3600，就可以不用添加一层self.transformer_model.head了，是一样的模型结构和结果：<br> <img src="https://images2.imgbox.com/69/2c/QgTCIveJ_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/8f/97/ro8qeI3O_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="13_VIT_73"></a>1.3 VIT模型结构</h3> 
<p>参考：<a href="https://blog.csdn.net/CHENCHENCHEN0526/article/details/121311456">【超详细】初学者包会的Vision Transformer（ViT）的PyTorch实现代码学习</a><br> 可以看到一张图片</p> 
<ul><li>经过Encoder Block，再经过MLP Block全连接层变为197*768的特征图。</li><li>接着下一个块层标准化…最后堆叠完块之后。</li><li>堆叠完Block，出来经过一个层标准化变为197* 768、提取类别Token变为1* 768、经过MLP Head最后输出了1*class的特征向量。<br> <img src="https://images2.imgbox.com/99/da/NNU2zjZB_o.png" alt="在这里插入图片描述"></li></ul> 
<p><img src="https://images2.imgbox.com/e9/12/4MuIUwDF_o.png" alt="在这里插入图片描述"></p> 
<p>timm库中的features_only=True不适用于vision transformer模型，会报错：<code>RuntimeError: features_only not implemented for Vision Transformer models.</code></p> 
<h3><a id="summaryselftransformer_model_3_224_224_85"></a>使用<code>summary(self.transformer_model, (3, 224, 224))</code>打印网络结构</h3> 
<h4><a id="PStorchsummary_86"></a>PS：torchsummary能够查看模型的输入和输出的形状，可以更加清楚地输出模型的结构。</h4> 
<p>参考：<a href="https://blog.csdn.net/qq_41468616/article/details/121164258">pytorch 中的torchsummary</a></p> 
<ul><li>第一个参数是model：pytorch 模型，必须继承自 nn.Module</li><li>第二个参数是输入的尺寸，input_size：模型输入 size，形状为 C，H ，W，不包括batchsize</li><li>device：“cuda"或者"cpu”<br> 使用时需要注意，默认device=‘cuda’，如果是在‘cpu’，那么就需要更改。不匹配就会出现下面的错误：</li></ul> 
<pre><code>RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
</code></pre> 
<p><img src="https://images2.imgbox.com/9a/b0/kdPuTccI_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="ViT_99"></a>二、ViT操作流程</h2> 
<p>关于位置编码等详细信息参考：<a href="https://blog.csdn.net/qq_39478403/article/details/118704747">【机器学习】详解 Vision Transformer (ViT)</a></p> 
<p>ViT 中的位置编码没有采用原版 Transformer 中的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         s 
        
       
         i 
        
       
         n 
        
       
         c 
        
       
         o 
        
       
         s 
        
       
      
        sincos 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em; vertical-align: 0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span></span></span></span></span> 编码，而是直接设置为<mark>可学习的 Positional Encoding</mark>。对训练好的 Positional Encoding 进行可视化，如下图所示。我们可以看到，位置越接近，往往具有更相似的位置编码。此外，出现了行列结构，同一行/列中的 patch 具有相似的位置编码。<br> <img src="https://images2.imgbox.com/d3/43/TAfVVZ8N_o.png" alt="在这里插入图片描述"></p> 
<p>论文中也对学习到的位置编码进行了可视化，发现相近的图像块的位置编码较相似，且同行或列的位置编码也相近： <img src="https://images2.imgbox.com/0f/b9/dQD6UVnA_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/9d/80/QJEJRZOL_o.png" alt="在这里插入图片描述"><br> (https://blog.csdn.net/qq_39478403/article/details/118704747)<br> vit论文地址：<a href="https://arxiv.org/abs/1706.03762" rel="nofollow">Attention Is All You Need 2016</a><br> 中文讲解 李宏毅老师的视频：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=23" rel="nofollow">(强推)李宏毅2021/2022春机器学习课程</a>、<a href="https://www.youtube.com/watch?v=ugWDIIOHtPA" rel="nofollow">YouTube</a></p> 
<p>参考：<a href="https://blog.csdn.net/weixin_44966641/article/details/118733341?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.pc_relevant_default&amp;utm_relevant_index=5">Vision Transformer（ViT）PyTorch代码全解析（附图解）</a></p> 
<h3><a id="21_ViT_113"></a>2.1 下图是ViT的整体框架图，我们在解析代码时会参照此图：</h3> 
<p><img src="https://images2.imgbox.com/37/b6/8fABqymT_o.png" alt="在这里插入图片描述"></p> 
<p>ViT的各个结构都写在了__init__()里，不再细讲，通过forward()来看ViT的整个前向传播过程（操作流程）。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">ViT</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> image_size<span class="token punctuation">,</span> patch_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> mlp_dim<span class="token punctuation">,</span> pool <span class="token operator">=</span> <span class="token string">'cls'</span><span class="token punctuation">,</span> channels <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span> dim_head <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> dropout <span class="token operator">=</span> <span class="token number">0.</span><span class="token punctuation">,</span> emb_dropout <span class="token operator">=</span> <span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        image_height<span class="token punctuation">,</span> image_width <span class="token operator">=</span> pair<span class="token punctuation">(</span>image_size<span class="token punctuation">)</span>
        patch_height<span class="token punctuation">,</span> patch_width <span class="token operator">=</span> pair<span class="token punctuation">(</span>patch_size<span class="token punctuation">)</span>
        <span class="token keyword">assert</span> image_height <span class="token operator">%</span> patch_height <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> image_width <span class="token operator">%</span> patch_width <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'Image dimensions must be divisible by the patch size.'</span>
        num_patches <span class="token operator">=</span> <span class="token punctuation">(</span>image_height <span class="token operator">//</span> patch_height<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>image_width <span class="token operator">//</span> patch_width<span class="token punctuation">)</span>
        patch_dim <span class="token operator">=</span> channels <span class="token operator">*</span> patch_height <span class="token operator">*</span> patch_width
        <span class="token keyword">assert</span> pool <span class="token keyword">in</span> <span class="token punctuation">{<!-- --></span><span class="token string">'cls'</span><span class="token punctuation">,</span> <span class="token string">'mean'</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token string">'pool type must be either cls (cls token) or mean (mean pooling)'</span>
        self<span class="token punctuation">.</span>to_patch_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            Rearrange<span class="token punctuation">(</span><span class="token string">'b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)'</span><span class="token punctuation">,</span> p1 <span class="token operator">=</span> patch_height<span class="token punctuation">,</span> p2 <span class="token operator">=</span> patch_width<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>patch_dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_patches <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (1,65,1024)</span>
        self<span class="token punctuation">.</span>cls_token <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>emb_dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> Transformer<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dim_head<span class="token punctuation">,</span> mlp_dim<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> pool
        self<span class="token punctuation">.</span>to_latent <span class="token operator">=</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># img: (1, 3, 256, 256)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>to_patch_embedding<span class="token punctuation">(</span>img<span class="token punctuation">)</span>     <span class="token comment"># (1, 64, 1024)</span>
        b<span class="token punctuation">,</span> n<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        cls_tokens <span class="token operator">=</span> repeat<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cls_token<span class="token punctuation">,</span> <span class="token string">'() n d -&gt; b n d'</span><span class="token punctuation">,</span> b <span class="token operator">=</span> b<span class="token punctuation">)</span>    <span class="token comment"># (1, 1, 1024)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>cls_tokens<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (1, 65, 1024)</span>
        x <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">(</span>n <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>   <span class="token comment"># (1, 65, 1024)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                    <span class="token comment"># (1, 65, 1024)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token comment"># (1, 65, 1024)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>pool <span class="token operator">==</span> <span class="token string">'mean'</span> <span class="token keyword">else</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>      <span class="token comment"># (1, 1024)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>to_latent<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>mlp_head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

</code></pre> 
<p>整体流程：</p> 
<ul><li>首先对输入进来的img(256<em>256大小)，划分为32</em>32大小的patch，共有8*8个。并将patch转换成embedding。（对应第26行代码）</li><li>生成cls_tokens （对应第28行代码）</li><li>将cls_tokens沿dim=1维与x进行拼接 （对应第29行代码）</li><li>生成随机的position embedding，每个embedding都是1024维 （对应代码14行和30行）</li><li>对输入经过Transformer进行编码（对应代码第32行）</li><li>如果是分类任务的话，截取第一个可学习的class embedding</li><li>最后过一个MLP Head用于分类。</li></ul> 
<h3><a id="22_vit_165"></a>2.2 vit张量维度变化</h3> 
<p><img src="https://images2.imgbox.com/45/e5/s7Ze9Yvn_o.png" alt="在这里插入图片描述"><br> <mark>！！！注意，经过Transformer Encoder块的输入和输出都为（b，65，1024），只不过timm中是将经过特征提取后的[:,0]出来（维度为b，1，1024）用于后续mlp head的输入。</mark></p> 
<h4><a id="PStimmcls_token0_168"></a>PS：timm中将cls_token拼接在前，所以提取[:,<strong>0</strong>]</h4> 
<p><img src="https://images2.imgbox.com/87/bb/ie50WQkd_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="timmVisionTransformer_172"></a>三、timm库中VisionTransformer代码解读</h2> 
<h4><a id="PStorchnnIdentity_173"></a>PS：torch.nn.Identity()</h4> 
<p>今天看源码时，遇到的这个恒等函数，就如同名字那样<br> 占位符，并没有实际操作<br> 主要使用场景：<br> 不区分参数的占位符标识运算符<br> if 某个操作 else Identity()<br> 在增减网络过程中，可以使得整个网络层数据不变，便于迁移权重数据。</p> 
<h3><a id="31_forward_featuresvitxx__selfpatch_embedx_181"></a>3.1 forward_features函数（一）<mark>输入</mark>进vit的x维度为，首先经过<code>x = self.patch_embed(x)</code>：</h3> 
<p><img src="https://images2.imgbox.com/af/6e/d2ej7Rwa_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/1c/6d/vgxtU7ES_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="x__selfpatch_embedxforward_185"></a>首先经过<code>x = self.patch_embed(x)</code>内的forward函数：</h4> 
<ul><li>输入x维度（b，3，224，224），经过<code>x = self.proj(x)</code>后变为（b，192，14，14）</li><li>经过<code>x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC</code>【用到了flatten(2)将BCHW -&gt; BNC，之后变为196（14<em>14）<em>768（16</em> 16</em> 3通道）】，14*14合并为192，这个出来后变为（16，196，192）</li><li>最后经过<code>x = self.norm(x)</code>后return<br> <img src="https://images2.imgbox.com/45/5e/Phg1Sw6k_o.png" alt="在这里插入图片描述"><br> 经过<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
          
          
            p 
           
          
            a 
           
          
            t 
           
          
            c 
           
           
           
             h 
            
           
             e 
            
           
          
            m 
           
          
            b 
           
          
            e 
           
          
            d 
           
          
         
        
       
         \color{red}{patch_embed} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord" style="color: red;"><span class="mord mathdefault" style="color: red;">p</span><span class="mord mathdefault" style="color: red;">a</span><span class="mord mathdefault" style="color: red;">t</span><span class="mord mathdefault" style="color: red;">c</span><span class="mord" style="color: red;"><span class="mord mathdefault" style="color: red;">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight" style="color: red;"><span class="mord mathdefault mtight" style="color: red;">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault" style="color: red;">m</span><span class="mord mathdefault" style="color: red;">b</span><span class="mord mathdefault" style="color: red;">e</span><span class="mord mathdefault" style="color: red;">d</span></span></span></span></span></span>后维度变为：<br> <img src="https://images2.imgbox.com/50/6f/1cCyWs49_o.png" alt="在这里插入图片描述"></li></ul> 
<h4><a id="PSpythonflatten43_192"></a>PS：python：flatten()参数详解【此处用来将4维变为3维—&gt;降维】</h4> 
<p>参考：<a href="https://blog.csdn.net/kuan__/article/details/116987162">python：flatten()参数详解</a></p> 
<ul><li>flatten()是对多维数据的降维函数。</li><li>flatten(),默认缺省参数为0，也就是说flatten()和flatte(0)效果一样。</li><li>python里的flatten(dim)表示，从第dim个维度开始展开，将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。</li></ul> 
<h3><a id="32_forward_featuresvitforward_featuresself_x_198"></a>3.2 forward_features函数（二）vit特征提取`forward_features(self, x):</h3> 
<ul><li>经过提取cls_token</li><li>拼接token后变为（b，197，192）</li><li>加入位置嵌入<code>x = self.pos_drop(x + self.pos_embed)</code></li><li>进入堆叠的block后输出为（b，197，92）不变</li><li>标准化x = self.norm(x)—&gt;<code>self.norm = norm_layer(embed_dim)</code></li><li><code>return x[:, 0]</code>，<mark>即返回下标0的可学习的cls_token用于后续的mlp head分类</mark></li></ul> 
<h4><a id="cls_token_206"></a>提取cls_token</h4> 
<p><img src="https://images2.imgbox.com/de/08/WUzO3NgA_o.png" alt="在这里插入图片描述"></p> 
<p>通过<code>nn.Parameter(torch.zeros(1, 1, embed_dim))</code>将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter。<mark>详解参考</mark>：<a href="https://blog.csdn.net/weixin_44966641/article/details/118730730">PyTorch中的torch.nn.Parameter() 详解</a><br> <img src="https://images2.imgbox.com/ce/20/abaX6qdv_o.png" alt="在这里插入图片描述"></p> 
<p>注意blocks中堆叠块是使用<code>nn.Sequential(*</code>配合下面list的用法，重复depth个Block：</p> 
<pre><code class="prism language-java"><span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">for</span> i in <span class="token function">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token class-name">Out</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<h4><a id="PSlistlist_217"></a>PS:list变量前加一个星号*，目的是将该list变量拆解开多个独立的参数，传入函数中</h4> 
<p>参考：<a href="https://blog.csdn.net/gaolijing_/article/details/106155679?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&amp;utm_relevant_index=2">Python中的list（列表）和dict（字典）变量前面加星号*的作用</a><br> 例如：</p> 
<pre><code class="prism language-python">list1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span>list1<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token number">1</span> <span class="token number">2</span> <span class="token number">3</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/20/19/YeucXnDI_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/10/b7/Q7k1rddf_o.png" alt="在这里插入图片描述"></p> 
<ul><li>之后标准化<code>x = self.norm(x)</code></li><li>最后修改了输出如下，提取后的特征维度变为（b，197，102）<br> <img src="https://images2.imgbox.com/af/e6/aquSpfBt_o.png" alt="在这里插入图片描述"></li></ul> 
<h3><a id="33_forwardforward_featuresself_xforward_237"></a>3.3 forward函数，上面的<code>forward_features(self, x)</code>是forward中的第一步</h3> 
<p><img src="https://images2.imgbox.com/f6/10/ZcheKqGc_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/924efc26d68c3982d2dbeb2426682330/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">定义一个函数求n(阶乘）,再在主函数中通过调用该函数计算: 1（阶乘）&#43;3（阶乘）&#43;5（阶乘）&#43;...&#43;(2n-1)(阶乘）输入提示信息格式: “Input n:\n“输入数据格式要求</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fbd090b930028137ed196afd981dd38b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">用MATLAB实现求椭球上任意两点的最短弧长</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>