<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>神经网络是通过类比什么得到的数学模型 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="神经网络是通过类比什么得到的数学模型" />
<meta property="og:description" content="人工神经网络的基础数学模型来自哪里 “纯意念控制”人工神经康复机器人系统2014年6月14日在天津大学和天津市人民医院共同举办的发表会上，由双方共同研制的人工神经康复机器人“神工一号”正式亮相。
人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。
基本特征：
（1）非线性 非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现为一种非线性关系。具有阈值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。
（2）非局限性 一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。
（3）非常定性 人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身也在不断变化。经常采用迭代过程描写动力系统的演化过程。
人工神经网络，人工神经网络是什么意思 一、 人工神经网络的概念。
人工神经网络（Artificial Neural Network，ANN）简称神经网络(NN)，是基于生物学中神经网络的基本原理，在理解和抽象了人脑结构和外界刺激响应机制后，以网络拓扑知识为理论基础，模拟人脑的神经系统对复杂信息的处理机制的一种数学模型。该模型以并行分布的处理能力、高容错性、智能化和自学习等能力为特征，将信息的加工和存储结合在一起，以其独特的知识表示方式和智能化的自适应学习能力，引起各学科领域的关注。它实际上是一个有大量简单元件相互连接而成的复杂网络，具有高度的非线性，能够进行复杂的逻辑操作和非线性关系实现的系统。
神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激活函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重（weight），神经网络就是通过这种方式来模拟人类的记忆。网络的输出则取决于网络的结构、网络的连接方式、权重和激活函数。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。神经网络的构筑理念是受到生物的神经网络运作启发而产生的。人工神经网络则是把对生物神经网络的认识与数学统计模型相结合，借助数学统计工具来实现。另一方面在人工智能学的人工感知领域，我们通过数学统计学的方法，使神经网络能够具备类似于人的决定能力和简单的判断能力，这种方法是对传统逻辑学演算的进一步延伸。
人工神经网络中，神经元处理单元可表示不同的对象，例如特征、字母、概念，或者一些有意义的抽象模式。网络中处理单元的类型分为三类：输入单元、输出单元和隐单元。输入单元接受外部世界的信号与数据；输出单元实现系统处理结果的输出；隐单元是处在输入和输出单元之间，不能由系统外部观察的单元。神经元间的连接权值反映了单元间的连接强度，信息的表示和处理体现在网络处理单元的连接关系中。人工神经网络是一种非程序化、适应性、大脑风格的信息处理，其本质是通过网络的变换和动力学行为得到一种并行分布式的信息处理功能，并在不同程度和层次上模仿人脑神经系统的信息处理功能。
神经网络，是一种应用类似于大脑神经突触连接结构进行信息处理的数学模型，它是在人类对自身大脑组织结合和思维机制的认识理解基础之上模拟出来的，它是根植于神经科学、数学、思维科学、人工智能、统计学、物理学、计算机科学以及工程科学的一门技术。
二、 人工神经网络的发展。
神经网络的发展有悠久的历史。其发展过程大致可以概括为如下4个阶段。
1. 第一阶段----启蒙时期。
(1)、M-P神经网络模型：20世纪40年代，人们就开始了对神经网络的研究。1943 年，美国心理学家麦克洛奇（Mcculloch）和数学家皮兹（Pitts）提出了M-P模型，此模型比较简单，但是意义重大。在模型中，通过把神经元看作个功能逻辑器件来实现算法，从此开创了神经网络模型的理论研究。
(2)、Hebb规则：1949 年，心理学家赫布（Hebb）出版了《The Organization of Behavior》（行为组织学），他在书中提出了突触连接强度可变的假设。这个假设认为学习过程最终发生在神经元之间的突触部位，突触的连接强度随之突触前后神经元的活动而变化。这一假设发展成为后来神经网络中非常著名的Hebb规则。这一法则告诉人们，神经元之间突触的联系强度是可变的，这种可变性是学习和记忆的基础。Hebb法则为构造有学习功能的神经网络模型奠定了基础。
(3)、感知器模型：1957 年，罗森勃拉特（Rosenblatt）以M-P 模型为基础，提出了感知器（Perceptron）模型。感知器模型具有现代神经网络的基本原则，并且它的结构非常符合神经生理学。这是一个具有连续可调权值矢量的MP神经网络模型，经过训练可以达到对一定的输入矢量模式进行分类和识别的目的，它虽然比较简单，却是第一个真正意义上的神经网络。Rosenblatt 证明了两层感知器能够对输入进行分类，他还提出了带隐层处理元件的三层感知器这一重要的研究方向。Rosenblatt 的神经网络模型包含了一些现代神经计算机的基本原理，从而形成神经网络方法和技术的重大突破。
(4)、ADALINE网络模型： 1959年，美国著名工程师威德罗（B.Widrow）和霍夫（M.Hoff）等人提出了自适应线性元件(Adaptive linear element，简称Adaline)和Widrow-Hoff学习规则（又称最小均方差算法或称δ规则）的神经网络训练方法，并将其应用于实际工程，成为第一个用于解决实际问题的人工神经网络，促进了神经网络的研究应用和发展。ADALINE网络模型是一种连续取值的自适应线性神经元网络模型，可以用于自适应系统。
2. 第二阶段----低潮时期。
人工智能的创始人之一Minsky和Papert对以感知器为代表的网络系统的功能及局限性从数学上做了深入研究，于1969年发表了轰动一时《Perceptrons》一书，指出简单的线性感知器的功能是有限的，它无法解决线性不可分的两类样本的分类问题，如简单的线性感知器不可能实现“异或”的逻辑关系等。这一论断给当时人工神经元网络的研究带来沉重的打击。开始了神经网络发展史上长达10年的低潮期。
(1)、自组织神经网络SOM模型：1972年，芬兰的KohonenT.教授，提出了自组织神经网络SOM(Self-Organizing feature map)。后来的神经网络主要是根据KohonenT.的工作来实现的。SOM网络是一类无导师学习网络，主要用于模式识别﹑语音识别及分类问题。它采用一种“胜者为王”的竞争学习算法，与先前提出的感知器有很大的不同，同时它的学习训练方式是无指导训练，是一种自组织网络。这种学习训练方式往往是在不知道有哪些分类类型存在时，用作提取分类信息的一种训练。
(2)、自适应共振理论ART：1976年，美国Grossberg教授提出了著名的自适应共振理论ART(Adaptive Resonance Theory)，其学习过程具有自组织和自稳定的特征。
3. 第三阶段----复兴时期。
(1)、Hopfield模型：1982年，美国物理学家霍普菲尔德（Hopfield）提出了一种离散神经网络，即离散Hopfield网络，从而有力地推动了神经网络的研究。在网络中，它首次将李雅普诺夫（Lyapunov）函数引入其中，后来的研究学者也将Lyapunov函数称为能量函数。证明了网络的稳定性。1984年，Hopfield 又提出了一种连续神经网络，将网络中神经元的激活函数由离散型改为连续型。1985 年，Hopfield和Tank利用Hopfield神经网络解决了著名的旅行推销商问题（Travelling Salesman Problem）。Hopfield神经网络是一组非线性微分方程。Hopfield的模型不仅对人工神经网络信息存储和提取功能进行了非线性数学概括，提出了动力方程和学习方程，还对网络算法提供了重要公式和参数，使人工神经网络的构造和学习有了理论指导，在Hopfield模型的影响下，大量学者又激发起研究神经网络的热情，积极投身于这一学术领域中。因为Hopfield 神经网络在众多方面具有巨大潜力，所以人们对神经网络的研究十分地重视，更多的人开始了研究神经网络，极大地推动了神经网络的发展。
(2)、Boltzmann机模型：1983年，Kirkpatrick等人认识到模拟退火算法可用于NP完全组合优化问题的求解，这种模拟高温物体退火过程来找寻全局最优解的方法最早由Metropli等人1953年提出的。1984年，Hinton与年轻学者Sejnowski等合作提出了大规模并行网络学习机，并明确提出隐单元的概念，这种学习机后来被称为Boltzmann机。
Hinton和Sejnowsky利用统计物理学的感念和方法，首次提出的多层网络的学习算法，称为Boltzmann 机模型。
(3)、BP神经网络模型：1986年，儒默哈特（D.E.Ru melhart）等人在多层神经网络模型的基础上，提出了多层神经网络权值修正的反向传播学习算法----BP算法（Error Back-Propagation），解决了多层前向神经网络的学习问题，证明了多层神经网络具有很强的学习能力，它可以完成许多学习任务，解决许多实际问题。
(4)、并行分布处理理论：1986年，由Rumelhart和McCkekkand主编的《Parallel Distributed Processing：Exploration in the Microstructures of Cognition》，该书中，他们建立了并行分布处理理论，主要致力于认知的微观研究，同时对具有非线性连续转移函数的多层前馈网络的误差反向传播算法即BP算法进行了详尽的分析，解决了长期以来没有权值调整有效算法的难题。可以求解感知机所不能解决的问题，回答了《Perceptrons》一书中关于神经网络局限性的问题，从实践上证实了人工神经网络有很强的运算能力。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/6abfa045ce3623ecd1828f5354447015/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-12T23:29:06+08:00" />
<meta property="article:modified_time" content="2022-07-12T23:29:06+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络是通过类比什么得到的数学模型</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p class="img-center"><img alt="" src="https://images2.imgbox.com/c6/09/SUMMqO2j_o.png"></p> 
<h3>人工神经网络的基础数学模型来自哪里</h3> 
<p></p> 
<p>“纯意念控制”人工神经康复机器人系统2014年6月14日在天津大学和天津市人民医院共同举办的发表会上，由双方共同研制的人工神经康复机器人“神工一号”正式亮相。</p> 
<p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。</p> 
<p>基本特征：</p> 
<p>（1）非线性 非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现为一种非线性关系。具有阈值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。</p> 
<p>（2）非局限性 一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。</p> 
<p>（3）非常定性 人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身也在不断变化。经常采用迭代过程描写动力系统的演化过程。</p> 
<p></p> 
<p></p> 
<h3>人工神经网络，人工神经网络是什么意思</h3> 
<p></p> 
<p>一、 人工神经网络的概念。</p> 
<p>人工神经网络（Artificial Neural Network，ANN）简称神经网络(NN)，是基于生物学中神经网络的基本原理，在理解和抽象了人脑结构和外界刺激响应机制后，以网络拓扑知识为理论基础，模拟人脑的神经系统对复杂信息的处理机制的一种数学模型。该模型以并行分布的处理能力、高容错性、智能化和自学习等能力为特征，将信息的加工和存储结合在一起，以其独特的知识表示方式和智能化的自适应学习能力，引起各学科领域的关注。它实际上是一个有大量简单元件相互连接而成的复杂网络，具有高度的非线性，能够进行复杂的逻辑操作和非线性关系实现的系统。</p> 
<p>神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激活函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重（weight），神经网络就是通过这种方式来模拟人类的记忆。网络的输出则取决于网络的结构、网络的连接方式、权重和激活函数。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。神经网络的构筑理念是受到生物的神经网络运作启发而产生的。人工神经网络则是把对生物神经网络的认识与数学统计模型相结合，借助数学统计工具来实现。另一方面在人工智能学的人工感知领域，我们通过数学统计学的方法，使神经网络能够具备类似于人的决定能力和简单的判断能力，这种方法是对传统逻辑学演算的进一步延伸。</p> 
<p>人工神经网络中，神经元处理单元可表示不同的对象，例如特征、字母、概念，或者一些有意义的抽象模式。网络中处理单元的类型分为三类：输入单元、输出单元和隐单元。输入单元接受外部世界的信号与数据；输出单元实现系统处理结果的输出；隐单元是处在输入和输出单元之间，不能由系统外部观察的单元。神经元间的连接权值反映了单元间的连接强度，信息的表示和处理体现在网络处理单元的连接关系中。人工神经网络是一种非程序化、适应性、大脑风格的信息处理，其本质是通过网络的变换和动力学行为得到一种并行分布式的信息处理功能，并在不同程度和层次上模仿人脑神经系统的信息处理功能。</p> 
<p>神经网络，是一种应用类似于大脑神经突触连接结构进行信息处理的数学模型，它是在人类对自身大脑组织结合和思维机制的认识理解基础之上模拟出来的，它是根植于神经科学、数学、思维科学、人工智能、统计学、物理学、计算机科学以及工程科学的一门技术。</p> 
<p>二、 人工神经网络的发展。</p> 
<p>神经网络的发展有悠久的历史。其发展过程大致可以概括为如下4个阶段。</p> 
<p>1. 第一阶段----启蒙时期。</p> 
<p>(1)、M-P神经网络模型：20世纪40年代，人们就开始了对神经网络的研究。1943 年，美国心理学家麦克洛奇（Mcculloch）和数学家皮兹（Pitts）提出了M-P模型，此模型比较简单，但是意义重大。在模型中，通过把神经元看作个功能逻辑器件来实现算法，从此开创了神经网络模型的理论研究。</p> 
<p>(2)、Hebb规则：1949 年，心理学家赫布（Hebb）出版了《The Organization of Behavior》（行为组织学），他在书中提出了突触连接强度可变的假设。这个假设认为学习过程最终发生在神经元之间的突触部位，突触的连接强度随之突触前后神经元的活动而变化。这一假设发展成为后来神经网络中非常著名的Hebb规则。这一法则告诉人们，神经元之间突触的联系强度是可变的，这种可变性是学习和记忆的基础。Hebb法则为构造有学习功能的神经网络模型奠定了基础。</p> 
<p>(3)、感知器模型：1957 年，罗森勃拉特（Rosenblatt）以M-P 模型为基础，提出了感知器（Perceptron）模型。感知器模型具有现代神经网络的基本原则，并且它的结构非常符合神经生理学。这是一个具有连续可调权值矢量的MP神经网络模型，经过训练可以达到对一定的输入矢量模式进行分类和识别的目的，它虽然比较简单，却是第一个真正意义上的神经网络。Rosenblatt 证明了两层感知器能够对输入进行分类，他还提出了带隐层处理元件的三层感知器这一重要的研究方向。Rosenblatt 的神经网络模型包含了一些现代神经计算机的基本原理，从而形成神经网络方法和技术的重大突破。</p> 
<p>(4)、ADALINE网络模型： 1959年，美国著名工程师威德罗（B.Widrow）和霍夫（M.Hoff）等人提出了自适应线性元件(Adaptive linear element，简称Adaline)和Widrow-Hoff学习规则（又称最小均方差算法或称δ规则）的神经网络训练方法，并将其应用于实际工程，成为第一个用于解决实际问题的人工神经网络，促进了神经网络的研究应用和发展。ADALINE网络模型是一种连续取值的自适应线性神经元网络模型，可以用于自适应系统。</p> 
<p>2. 第二阶段----低潮时期。</p> 
<p>人工智能的创始人之一Minsky和Papert对以感知器为代表的网络系统的功能及局限性从数学上做了深入研究，于1969年发表了轰动一时《Perceptrons》一书，指出简单的线性感知器的功能是有限的，它无法解决线性不可分的两类样本的分类问题，如简单的线性感知器不可能实现“异或”的逻辑关系等。这一论断给当时人工神经元网络的研究带来沉重的打击。开始了神经网络发展史上长达10年的低潮期。</p> 
<p>(1)、自组织神经网络SOM模型：1972年，芬兰的KohonenT.教授，提出了自组织神经网络SOM(Self-Organizing feature map)。后来的神经网络主要是根据KohonenT.的工作来实现的。SOM网络是一类无导师学习网络，主要用于模式识别﹑语音识别及分类问题。它采用一种“胜者为王”的竞争学习算法，与先前提出的感知器有很大的不同，同时它的学习训练方式是无指导训练，是一种自组织网络。这种学习训练方式往往是在不知道有哪些分类类型存在时，用作提取分类信息的一种训练。</p> 
<p>(2)、自适应共振理论ART：1976年，美国Grossberg教授提出了著名的自适应共振理论ART(Adaptive Resonance Theory)，其学习过程具有自组织和自稳定的特征。</p> 
<p>3. 第三阶段----复兴时期。</p> 
<p>(1)、Hopfield模型：1982年，美国物理学家霍普菲尔德（Hopfield）提出了一种离散神经网络，即离散Hopfield网络，从而有力地推动了神经网络的研究。在网络中，它首次将李雅普诺夫（Lyapunov）函数引入其中，后来的研究学者也将Lyapunov函数称为能量函数。证明了网络的稳定性。1984年，Hopfield 又提出了一种连续神经网络，将网络中神经元的激活函数由离散型改为连续型。1985 年，Hopfield和Tank利用Hopfield神经网络解决了著名的旅行推销商问题（Travelling Salesman Problem）。Hopfield神经网络是一组非线性微分方程。Hopfield的模型不仅对人工神经网络信息存储和提取功能进行了非线性数学概括，提出了动力方程和学习方程，还对网络算法提供了重要公式和参数，使人工神经网络的构造和学习有了理论指导，在Hopfield模型的影响下，大量学者又激发起研究神经网络的热情，积极投身于这一学术领域中。因为Hopfield 神经网络在众多方面具有巨大潜力，所以人们对神经网络的研究十分地重视，更多的人开始了研究神经网络，极大地推动了神经网络的发展。</p> 
<p>(2)、Boltzmann机模型：1983年，Kirkpatrick等人认识到模拟退火算法可用于NP完全组合优化问题的求解，这种模拟高温物体退火过程来找寻全局最优解的方法最早由Metropli等人1953年提出的。1984年，Hinton与年轻学者Sejnowski等合作提出了大规模并行网络学习机，并明确提出隐单元的概念，这种学习机后来被称为Boltzmann机。</p> 
<p>Hinton和Sejnowsky利用统计物理学的感念和方法，首次提出的多层网络的学习算法，称为Boltzmann 机模型。</p> 
<p>(3)、BP神经网络模型：1986年，儒默哈特（D.E.Ru melhart）等人在多层神经网络模型的基础上，提出了多层神经网络权值修正的反向传播学习算法----BP算法（Error Back-Propagation），解决了多层前向神经网络的学习问题，证明了多层神经网络具有很强的学习能力，它可以完成许多学习任务，解决许多实际问题。</p> 
<p>(4)、并行分布处理理论：1986年，由Rumelhart和McCkekkand主编的《Parallel Distributed Processing：Exploration in the Microstructures of Cognition》，该书中，他们建立了并行分布处理理论，主要致力于认知的微观研究，同时对具有非线性连续转移函数的多层前馈网络的误差反向传播算法即BP算法进行了详尽的分析，解决了长期以来没有权值调整有效算法的难题。可以求解感知机所不能解决的问题，回答了《Perceptrons》一书中关于神经网络局限性的问题，从实践上证实了人工神经网络有很强的运算能力。</p> 
<p>(5)、细胞神经网络模型：1988年，Chua和Yang提出了细胞神经网络（CNN）模型，它是一个细胞自动机特性的大规模非线性计算机仿真系统。Kosko建立了双向联想存储模型（BAM），它具有非监督学习能力。</p> 
<p>(6)、Darwinism模型：Edelman提出的Darwinism模型在90年代初产生了很大的影响，他建立了一种神经网络系统理论。</p> 
<p>(7)、1988年，Linsker对感知机网络提出了新的自组织理论，并在Shanon信息论的基础上形成了最大互信息理论，从而点燃了基于NN的信息应用理论的光芒。</p> 
<p>(8)、1988年，Broomhead和Lowe用径向基函数(Radialbasis function, RBF)提出分层网络的设计方法，从而将NN的设计与数值分析和线性适应滤波相挂钩。</p> 
<p>(9)、1991年，Haken把协同引入神经网络，在他的理论框架中，他认为，认知过程是自发的，并断言模式识别过程即是模式形成过程。</p> 
<p>(10)、1994年，廖晓昕关于细胞神经网络的数学理论与基础的提出，带来了这个领域新的进展。通过拓广神经网络的激活函数类，给出了更一般的时滞细胞神经网络(DCNN)、Hopfield神经网络（HNN）、双向联想记忆网络（BAM）模型。</p> 
<p>(11)、90年代初，Vapnik等提出了支持向量机(Supportvector machines, SVM)和VC(Vapnik-Chervonenkis)维数的概念。</p> 
<p>经过多年的发展，已有上百种的神经网络模型被提出。</p> 
<p></p> 
<p></p> 
<p><strong><a href="http://www.a8u.net/" rel="nofollow" title="A8U神经网络">A8U神经网络</a></strong></p> 
<h3>神经网络算法的人工神经网络</h3> 
<p></p> 
<p>人工神经网络（Artificial Neural Networks，ANN）系统是 20 世纪 40 年代后出现的。它是由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信 息存储、良好的自组织自学习能力等特点。BP（Back Propagation）算法又称为误差 反向传播算法，是人工神经网络中的一种监督式的学习算法。BP 神经网络算法在理 论上可以逼近任意函数，基本的结构由非线性变化单元组成，具有很强的非线性映射能力。而且网络的中间层数、各层的处理单元数及网络的学习系数等参数可根据具体情况设定，灵活性很大，在优化、信号处理与模式识别、智能控制、故障诊断等许 多领域都有着广泛的应用前景。 人工神经元的研究起源于脑神经元学说。19世纪末，在生物、生理学领域，Waldeger等人创建了神经元学说。人们认识到复杂的神经系统是由数目繁多的神经元组合而成。大脑皮层包括有100亿个以上的神经元，每立方毫米约有数万个，它们互相联结形成神经网络，通过感觉器官和神经接受来自身体内外的各种信息，传递至中枢神经系统内，经过对信息的分析和综合，再通过运动神经发出控制信息，以此来实现机体与内外环境的联系，协调全身的各种机能活动。</p> 
<p>神经元也和其他类型的细胞一样，包括有细胞膜、细胞质和细胞核。但是神经细胞的形态比较特殊，具有许多突起，因此又分为细胞体、轴突和树突三部分。细胞体内有细胞核，突起的作用是传递信息。树突是作为引入输入信号的突起，而轴突是作为输出端的突起，它只有一个。</p> 
<p>树突是细胞体的延伸部分，它由细胞体发出后逐渐变细，全长各部位都可与其他神经元的轴突末梢相互联系，形成所谓“突触”。在突触处两神经元并未连通，它只是发生信息传递功能的结合部，联系界面之间间隙约为（15～50)×10米。突触可分为兴奋性与抑制性两种类型，它相应于神经元之间耦合的极性。每个神经元的突触数目正常，最高可达10个。各神经元之间的连接强度和极性有所不同，并且都可调整、基于这一特性，人脑具有存储信息的功能。利用大量神经元相互联接组成人工神经网络可显示出人的大脑的某些特征。</p> 
<p>人工神经网络是由大量的简单基本元件——神经元相互联接而成的自适应非线性动态系统。每个神经元的结构和功能比较简单，但大量神经元组合产生的系统行为却非常复杂。</p> 
<p>人工神经网络反映了人脑功能的若干基本特性，但并非生物系统的逼真描述，只是某种模仿、简化和抽象。</p> 
<p>与数字计算机比较，人工神经网络在构成原理和功能特点等方面更加接近人脑，它不是按给定的程序一步一步地执行运算，而是能够自身适应环境、总结规律、完成某种运算、识别或过程控制。</p> 
<p>人工神经网络首先要以一定的学习准则进行学习，然后才能工作。现以人工神经网络对于写“A”、“B”两个字母的识别为例进行说明，规定当“A”输入网络时，应该输出“1”，而当输入为“B”时，输出为“0”。</p> 
<p>所以网络学习的准则应该是：如果网络作出错误的的判决，则通过网络的学习，应使得网络减少下次犯同样错误的可能性。首先，给网络的各连接权值赋予(0，1)区间内的随机值，将“A”所对应的图象模式输入给网络，网络将输入模式加权求和、与门限比较、再进行非线性运算，得到网络的输出。在此情况下，网络输出为“1”和“0”的概率各为50%，也就是说是完全随机的。这时如果输出为“1”(结果正确)，则使连接权值增大，以便使网络再次遇到“A”模式输入时，仍然能作出正确的判断。</p> 
<p>如果输出为“0”(即结果错误)，则把网络连接权值朝着减小综合输入加权值的方向调整，其目的在于使网络下次再遇到“A”模式输入时，减小犯同样错误的可能性。如此操作调整，当给网络轮番输入若干个手写字母“A”、“B”后，经过网络按以上学习方法进行若干次学习后，网络判断的正确率将大大提高。这说明网络对这两个模式的学习已经获得了成功，它已将这两个模式分布地记忆在网络的各个连接权值上。当网络再次遇到其中任何一个模式时，能够作出迅速、准确的判断和识别。一般说来，网络中所含的神经元个数越多，则它能记忆、识别的模式也就越多。 （1）人类大脑有很强的自适应与自组织特性，后天的学习与训练可以开发许多各具特色的活动功能。如盲人的听觉和触觉非常灵敏；聋哑人善于运用手势；训练有素的运动员可以表现出非凡的运动技巧等等。</p> 
<p>普通计算机的功能取决于程序中给出的知识和能力。显然，对于智能活动要通过总结编制程序将十分困难。</p> 
<p>人工神经网络也具有初步的自适应与自组织能力。在学习或训练过程中改变突触权重值，以适应周围环境的要求。同一网络因学习方式及内容不同可具有不同的功能。人工神经网络是一个具有学习能力的系统，可以发展知识，以致超过设计者原有的知识水平。通常，它的学习训练方式可分为两种，一种是有监督或称有导师的学习，这时利用给定的样本标准进行分类或模仿；另一种是无监督学习或称无为导师学习，这时，只规定学习方式或某些规则，则具体的学习内容随系统所处环境 （即输入信号情况）而异，系统可以自动发现环境特征和规律性，具有更近似人脑的功能。</p> 
<p>（2）泛化能力</p> 
<p>泛化能力指对没有训练过的样本，有很好的预测能力和控制能力。特别是，当存在一些有噪声的样本，网络具备很好的预测能力。</p> 
<p>(3)非线性映射能力</p> 
<p>当对系统对于设计人员来说，很透彻或者很清楚时，则一般利用数值分析，偏微分方程等数学工具建立精确的数学模型，但当对系统很复杂，或者系统未知，系统信息量很少时，建立精确的数学模型很困难时，神经网络的非线性映射能力则表现出优势，因为它不需要对系统进行透彻的了解，但是同时能达到输入与输出的映射关系，这就大大简化设计的难度。</p> 
<p>(4)高度并行性</p> 
<p>并行性具有一定的争议性。承认具有并行性理由：神经网络是根据人的大脑而抽象出来的数学模型，由于人可以同时做一些事，所以从功能的模拟角度上看，神经网络也应具备很强的并行性。</p> 
<p>多少年以来，人们从医学、生物学、生理学、哲学、信息学、计算机科学、认知学、组织协同学等各个角度企图认识并解答上述问题。在寻找上述问题答案的研究过程中，这些年来逐渐形成了一个新兴的多学科交叉技术领域，称之为“神经网络”。神经网络的研究涉及众多学科领域，这些领域互相结合、相互渗透并相互推动。不同领域的科学家又从各自学科的兴趣与特色出发，提出不同的问题，从不同的角度进行研究。</p> 
<p>下面将人工神经网络与通用的计算机工作特点来对比一下：</p> 
<p>若从速度的角度出发，人脑神经元之间传递信息的速度要远低于计算机，前者为毫秒量级，而后者的频率往往可达几百兆赫。但是，由于人脑是一个大规模并行与串行组合处理系统，因而，在许多问题上可以作出快速判断、决策和处理，其速度则远高于串行结构的普通计算机。人工神经网络的基本结构模仿人脑，具有并行处理特征，可以大大提高工作速度。</p> 
<p>人脑存贮信息的特点为利用突触效能的变化来调整存贮内容，也即信息存贮在神经元之间连接强度的分布上，存贮区与计算机区合为一体。虽然人脑每日有大量神经细胞死亡 （平均每小时约一千个），但不影响大脑的正常思维活动。</p> 
<p>普通计算机是具有相互独立的存贮器和运算器，知识存贮与数据运算互不相关，只有通过人编出的程序使之沟通，这种沟通不能超越程序编制者的预想。元器件的局部损坏及程序中的微小错误都可能引起严重的失常。 心理学家和认知科学家研究神经网络的目的在于探索人脑加工、储存和搜索信息的机制，弄清人脑功能的机理，建立人类认知过程的微结构理论。</p> 
<p>生物学、医学、脑科学专家试图通过神经网络的研究推动脑科学向定量、精确和理论化体系发展，同时也寄希望于临床医学的新突破；信息处理和计算机科学家研究这一问题的目的在于寻求新的途径以解决不能解决或解决起来有极大困难的大量问题，构造更加逼近人脑功能的新一代计算机。</p> 
<p>人工神经网络早期的研究工作应追溯至上世纪40年代。下面以时间顺序，以著名的人物或某一方面突出的研究成果为线索，简要介绍人工神经网络的发展历史。</p> 
<p>1943年，心理学家W·Mcculloch和数理逻辑学家W·Pitts在分析、总结神经元基本特性的基础上首先提出神经元的数学模型。此模型沿用至今，并且直接影响着这一领域研究的进展。因而，他们两人可称为人工神经网络研究的先驱。</p> 
<p>1945年冯·诺依曼领导的设计小组试制成功存储程序式电子计算机，标志着电子计算机时代的开始。1948年，他在研究工作中比较了人脑结构与存储程序式计算机的根本区别，提出了以简单神经元构成的再生自动机网络结构。但是，由于指令存储式计算机技术的发展非常迅速，迫使他放弃了神经网络研究的新途径，继续投身于指令存储式计算机技术的研究，并在此领域作出了巨大贡献。虽然，冯·诺依曼的名字是与普通计算机联系在一起的，但他也是人工神经网络研究的先驱之一。</p> 
<p>50年代末，F·Rosenblatt设计制作了“感知机”，它是一种多层的神经网络。这项工作首次把人工神经网络的研究从理论探讨付诸工程实践。当时，世界上许多实验室仿效制作感知机，分别应用于文字识别、声音识别、声纳信号识别以及学习记忆问题的研究。然而，这次人工神经网络的研究高潮未能持续很久，许多人陆续放弃了这方面的研究工作，这是因为当时数字计算机的发展处于全盛时期，许多人误以为数字计算机可以解决人工智能、模式识别、专家系统等方面的一切问题，使感知机的工作得不到重视；其次，当时的电子技术工艺水平比较落后，主要的元件是电子管或晶体管，利用它们制作的神经网络体积庞大，价格昂贵，要制作在规模上与真实的神经网络相似是完全不可能的；另外，在1968年一本名为《感知机》的著作中指出线性感知机功能是有限的，它不能解决如异感这样的基本问题，而且多层网络还不能找到有效的计算方法，这些论点促使大批研究人员对于人工神经网络的前景失去信心。60年代末期，人工神经网络的研究进入了低潮。</p> 
<p>另外，在60年代初期，Widrow提出了自适应线性元件网络，这是一种连续取值的线性加权求和阈值网络。后来，在此基础上发展了非线性多层自适应网络。当时，这些工作虽未标出神经网络的名称，而实际上就是一种人工神经网络模型。</p> 
<p>随着人们对感知机兴趣的衰退，神经网络的研究沉寂了相当长的时间。80年代初期，模拟与数字混合的超大规模集成电路制作技术提高到新的水平，完全付诸实用化，此外，数字计算机的发展在若干应用领域遇到困难。这一背景预示，向人工神经网络寻求出路的时机已经成熟。美国的物理学家Hopfield于1982年和1984年在美国科学院院刊上发表了两篇关于人工神经网络研究的论文，引起了巨大的反响。人们重新认识到神经网络的威力以及付诸应用的现实性。随即，一大批学者和研究人员围绕着 Hopfield提出的方法展开了进一步的工作，形成了80年代中期以来人工神经网络的研究热潮。</p> 
<p>1985年，Ackley、Hinton和Sejnowski将模拟退火算法应用到神经网络训练中，提出了Boltzmann机，该算法具有逃离极值的优点，但是训练时间需要很长。</p> 
<p>1986年，Rumelhart、Hinton和Williams提出了多层前馈神经网络的学习算法，即BP算法。它从证明的角度推导算法的正确性，是学习算法有理论依据。从学习算法角度上看，是一个很大的进步。</p> 
<p>1988年，Broomhead和Lowe第一次提出了径向基网络：RBF网络。</p> 
<p>总体来说，神经网络经历了从高潮到低谷，再到高潮的阶段，充满曲折的过程。</p> 
<p></p> 
<p></p> 
<h3>什么是人工神经元算法</h3> 
<p></p> 
<p>人工神经网络算法</p> 
<p>“人工神经网络”(ARTIFICIAL NEURAL NETWORK，简称ANN)是在对人脑组织结构和运行机制的认识理解基础之上模拟其结构和智能行为的一种工程系统。早在本世纪40年代初期，心理学家McCulloch、数学家Pitts就提出了人工神经网络的第一个数学模型，从此开创了神经科学理论的研究时代。其后，F Rosenblatt、Widrow和J. J .Hopfield等学者又先后提出了感知模型，使得人工神经网络技术得以蓬勃发展。</p> 
<p>神经系统的基本构造是神经元(神经细胞)，它是处理人体内各部分之间相互信息传递的基本单元。据神经生物学家研究的结果表明，人的一个大脑一般有1010～1011个神经元。每个神经元都由一个细胞体，一个连接其他神经元的轴突和一些向外伸出的其它较短分支——树突组成。轴突的功能是将本神经元的输出信号(兴奋)传递给别的神经元。其末端的许多神经末梢使得兴奋可以同时传送给多个神经元。树突的功能是接受来自其它神经元的兴奋。神经元细胞体将接受到的所有信号进行简单处理(如：加权求和，即对所有的输入信号都加以考虑且对每个信号的重视程度——体现在权值上——有所不同)后由轴突输出。神经元的树突与另外的神经元的神经末梢相连的部分称为突触。</p> 
<p></p> 
<p></p> 
<h3>人工神经网络是哪个流派的基础</h3> 
<p></p> 
<p>“纯意念控制”人工神经康复机器人系统2014年6月14日在天津大学和天津市人民医院共同举办的发表会上，由双方共同研制的人工神经康复机器人“神工一号”正式亮相。</p> 
<p>中文名</p> 
<p>“纯意念控制”人工神经康复机器人系统。</p> 
<p>发布时间</p> 
<p>2014年6月14日</p> 
<p>快速</p> 
<p>导航</p> 
<p>产品特色发展历史</p> 
<p>功能配置</p> 
<p>“纯意念控制”人工神经康复机器人系统在复合想象动作信息解析与处理、异步脑——机接口训练与识别、皮层——肌肉活动同步耦合优化、中风后抑郁脑电非线性特征提取与筛查等关键技术上取得了重大突破。</p> 
<p>“纯意念控制”人工神经康复机器人系统包括无创脑电传感模块、想象动作特征检测模块、运动意图识别模块、指令编码接口模块、刺激信息调理模块、刺激电流输出模块6部分。</p> 
<p>产品特色</p> 
<p>“纯意念控制”人工神经康复机器人系统最新研究成果将让不少中风、瘫痪人士燃起重新独立生活的希望。现已拥有包括23项授权国家发明专利、1项软件著作权在内的自主知识产权集群，是全球首台适用于全肢体中风康复的“纯意念控制”人工神经机器人系统。[1]。</p> 
<p>脑控机械外骨骼是利用被动机械牵引，非肌肉主动收缩激活。而“神工一号”则利用神经肌肉电刺激，模拟神经冲动的电刺激引起肌肉产生主动收缩，带动骨骼和关节产生自主动作，与人体自主运动原理一致。</p> 
<p>体验者需要把装有电极的脑电探测器戴在头部，并在患病肢体的肌肉上安装电极，借助“神工一号”的连接，就可以用“意念”来“控制”自己本来无法行动的肢体了。[2]。</p> 
<p>发展历史</p> 
<p>“纯意念控制”人工神经康复机器人系统技术历时10年，是国家“863计划“、“十二五”国家科技支撑计划和国家优秀青年科学基金重点支持项目。</p> 
<p>人工神经网络（Artificial Neural Network，即ANN ），是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。在工程与学术界也常直接简称为神经网络或类神经网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。</p> 
<p>最近十多年来，人工神经网络的研究工作不断深入，已经取得了很大的进展，其在模式识别、智能机器人、自动控制、预测估计、生物、医学、经济等领域已成功地解决了许多现代计算机难以解决的实际问题，表现出了良好的智能特性。</p> 
<p>中文名</p> 
<p>人工神经网络</p> 
<p>外文名</p> 
<p>artificial neural network。</p> 
<p>别称</p> 
<p>ANN</p> 
<p>应用学科</p> 
<p>人工智能</p> 
<p>适用领域范围</p> 
<p>模式分类</p> 
<p>精品荐读</p> 
<p>“蠢萌”的神经网络</p> 
<p>作者：牛油果进化论</p> 
<p>快速</p> 
<p>导航</p> 
<p>基本特征发展历史网络模型学习类型分析方法特点优点研究方向发展趋势应用分析。</p> 
<p>神经元</p> 
<p>如图所示</p> 
<p>a1~an为输入向量的各个分量。</p> 
<p>w1~wn为神经元各个突触的权值。</p> 
<p>b为偏置</p> 
<p>f为传递函数，通常为非线性函数。以下默认为hardlim()。</p> 
<p>t为神经元输出</p> 
<p>数学表示 t=f(WA'+b)。</p> 
<p>W为权向量</p> 
<p>A为输入向量，A'为A向量的转置。</p> 
<p>b为偏置</p> 
<p>f为传递函数</p> 
<p>可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。</p> 
<p>单个神经元的作用：把一个n维向量空间用一个超平面分割成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。</p> 
<p>该超平面的方程: Wp+b=0。</p> 
<p>W权向量</p> 
<p>b偏置</p> 
<p>p超平面上的向量</p> 
<p>基本特征</p> 
<p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。人工神经网络具有四个基本特征：</p> 
<p>（1）非线性 非线性关系是自然界的普遍特性。大脑的智慧就是一种非线性现象。人工神经元处于激活或抑制二种不同的状态，这种行为在数学上表现为一种非线性关系。具有阈值的神经元构成的网络具有更好的性能，可以提高容错性和存储容量。</p> 
<p>人工神经网络</p> 
<p>（2）非局限性 一个神经网络通常由多个神经元广泛连接而成。一个系统的整体行为不仅取决于单个神经元的特征，而且可能主要由单元之间的相互作用、相互连接所决定。通过单元之间的大量连接模拟大脑的非局限性。联想记忆是非局限性的典型例子。</p> 
<p>（3）非常定性 人工神经网络具有自适应、自组织、自学习能力。神经网络不但处理的信息可以有各种变化，而且在处理信息的同时，非线性动力系统本身也在不断变化。经常采用迭代过程描写动力系统的演化过程。</p> 
<p>（4）非凸性 一个系统的演化方向，在一定条件下将取决于某个特定的状态函数。例如能量函数，它的极值相应于系统比较稳定的状态。非凸性是指这种函数有多个极值，故系统具有多个较稳定的平衡态，这将导致系统演化的多样性。</p> 
<p>人工神经网络中，神经元处理单元可表示不同的对象，例如特征、字母、概念，或者一些有意义的抽象模式。网络中处理单元的类型分为三类：输入单元、输出单元和隐单元。输入单元接受外部世界的信号与数据；输出单元实现系统处理结果的输出；隐单元是处在输入和输出单元之间，不能由系统外部观察的单元。神经元间的连接权值反映了单元间的连接强度，信息的表示和处理体现在网络处理单元的连接关系中。人工神经网络是一种非程序化、适应性、大脑风格的信息处理 ，其本质是通过网络的变换和动力学行为得到一种并行分布式的信息处理功能，并在不同程度和层次上模仿人脑神经系统的信息处理功能。它是涉及神经科学、思维科学、人工智能、计算机科学等多个领域的交叉学科。</p> 
<p>人工神经网络</p> 
<p>人工神经网络是并行分布式系统，采用了与传统人工智能和信息处理技术完全不同的机理，克服了传统的基于逻辑符号的人工智能在处理直觉、非结构化信息方面的缺陷，具有自适应、自组织和实时学习的特点。[1]。</p> 
<p>发展历史</p> 
<p>1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts建立了神经网络和数学模型，称为MP模型。他们通过MP模型提出了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。1949年，心理学家提出了突触联系强度可变的设想。60年代，人工神经网络得到了进一步发展，更完善的神经网络模型被提出，其中包括感知器和自适应线性元件等。M.Minsky等仔细分析了以感知器为代表的神经网络系统的功能及局限后，于1969年出版了《Perceptron》一书，指出感知器不能解决高阶谓词问题。他们的论点极大地影响了神经网络的研究，加之当时串行计算机和人工智能所取得的成就，掩盖了发展新型计算机和人工智能新途径的必要性和迫切性，使人工神经网络的研究处于低潮。在此期间，一些人工神经网络的研究者仍然致力于这一研究，提出了适应谐振理论（ART网）、自组织映射、认知机网络，同时进行了神经网络数学理论的研究。以上研究为神经网络的研究和发展奠定了基础。1982年，美国加州工学院物理学家J.J.Hopfield提出了Hopfield神经网格模型，引入了“计算能量”概念，给出了网络稳定性判断。 1984年，他又提出了连续时间Hopfield神经网络模型，为神经计算机的研究做了开拓性的工作，开创了神经网络用于联想记忆和优化计算的新途径，有力地推动了神经网络的研究，1985年，又有学者提出了波耳兹曼模型，在学习中采用统计热力学模拟退火技术，保证整个系统趋于全局稳定点。1986年进行认知微观结构地研究，提出了并行分布处理的理论。1986年，Rumelhart, Hinton, Williams发展了BP算法。Rumelhart和McClelland出版了《Parallel distribution processing: explorations in the microstructures of cognition》。迄今，BP算法已被用于解决大量实际问题。1988年，Linsker对感知机网络提出了新的自组织理论，并在Shanon信息论的基础上形成了最大互信息理论，从而点燃了基于NN的信息应用理论的光芒。1988年，Broomhead和Lowe用径向基函数(Radial basis function, RBF)提出分层网络的设计方法，从而将NN的设计与数值分析和线性适应滤波相挂钩。90年代初，Vapnik等提出了支持向量机(Support vector machines, SVM)和VC(Vapnik-Chervonenkis)维数的概念。人工神经网络的研究受到了各个发达国家的重视，美国国会通过决议将1990年1月5日开始的十年定为“脑的十年”，国际研究组织号召它的成员国将“脑的十年”变为全球行为。在日本的“真实世界计算（RWC）”项目中，人工智能的研究成了一个重要的组成部分。</p> 
<p>人工神经网络</p> 
<p>网络模型</p> 
<p>人工神经网络模型主要考虑网络连接的拓扑结构、神经元的特征、学习规则等。目前，已有近40种神经网络模型，其中有反传网络、感知器、自组织映射、Hopfield网络、波耳兹曼机、适应谐振理论等。根据连接的拓扑结构，神经网络模型可以分为：[1]。</p> 
<p>人工神经网络</p> 
<p>前向网络</p> 
<p>网络中各个神经元接受前一级的输入，并输出到下一级，网络中没有反馈，可以用一个有向无环路图表示。这种网络实现信号从输入空间到输出空间的变换，它的信息处理能力来自于简单非线性函数的多次复合。网络结构简单，易于实现。反传网络是一种典型的前向网络。[2]。</p> 
<p>反馈网络</p> 
<p>网络内神经元间有反馈，可以用一个无向的完备图表示。这种神经网络的信息处理是状态的变换，可以用动力学系统理论处理。系统的稳定性与联想记忆功能有密切关系。Hopfield网络、波耳兹曼机均属于这种类型。</p> 
<p>学习类型</p> 
<p>学习是神经网络研究的一个重要内容，它的适应性是通过学习实现的。根据环境的变化，对权值进行调整，改善系统的行为。由Hebb提出的Hebb学习规则为神经网络的学习算法奠定了基础。Hebb规则认为学习过程最终发生在神经元之间的突触部位，突触的联系强度随着突触前后神经元的活动而变化。在此基础上，人们提出了各种学习规则和算法，以适应不同网络模型的需要。有效的学习算法，使得神经网络能够通过连接权值的调整，构造客观世界的内在表示，形成具有特色的信息处理方法，信息存储和处理体现在网络的连接中。</p> 
<p>人工神经网络</p> 
<p>分类</p> 
<p>根据学习环境不同，神经网络的学习方式可分为监督学习和非监督学习。在监督学习中，将训练样本的数据加到网络输入端，同时将相应的期望输出与网络输出相比较，得到误差信号，以此控制权值连接强度的调整，经多次训练后收敛到一个确定的权值。当样本情况发生变化时，经学习可以修改权值以适应新的环境。使用监督学习的神经网络模型有反传网络、感知器等。非监督学习时，事先不给定标准样本，直接将网络置于环境之中，学习阶段与工作阶段成为一体。此时，学习规律的变化服从连接权值的演变方程。非监督学习最简单的例子是Hebb学习规则。竞争学习规则是一个更复杂的非监督学习的例子，它是根据已建立的聚类进行权值调整。自组织映射、适应谐振理论网络等都是与竞争学习有关的典型模型。</p> 
<p>分析方法</p> 
<p>研究神经网络的非线性动力学性质，主要采用动力学系统理论、非线性规划理论和统计理论，来分析神经网络的演化过程和吸引子的性质，探索神经网络的协同行为和集体计算功能，了解神经信息处理机制。为了探讨神经网络在整体性和模糊性方面处理信息的可能，混沌理论的概念和方法将会发挥作用。混沌是一个相当难以精确定义的数学概念。一般而言，“混沌”是指由确定性方程描述的动力学系统中表现出的非确定性行为，或称之为确定的随机性。“确定性”是因为它由内在的原因而不是外来的噪声或干扰所产生，而“随机性”是指其不规则的、不能预测的行为，只可能用统计的方法描述。</p> 
<p></p> 
<p></p> 
<h3>人工神经元的基本构成</h3> 
<p></p> 
<p>人脑的神经元模型如图8.6所示。</p> 
<p>图中一个神经元由细胞核、一个轴突、多个树突、突触组成。生物电信号从树突传入，经过细胞核处理，从轴突输出一个电脉冲信号。神经元通过树突与轴突之间的突触与其他神经元相连构成一个复杂的大规模并行网络。</p> 
<p>图8.6 人脑的神经元模型[8]。</p> 
<p>1943年心理学家McCulloch和数学家Pitt将生物模型抽象化，建立了人工神经网络的数学模型——MP模型，如图8.7所示。</p> 
<p>图8.7 人工神经元模型[8]。</p> 
<p>该人工神经元具有以下6点特征:。</p> 
<p>(1)每个神经元是一个多输入单输出单元;。</p> 
<p>(2)突触分兴奋和抑制两种;</p> 
<p>(3)神经元有空间整合性和阀值;。</p> 
<p>(4)神经元的输入输出有固定的时间滞后，主要取决于突触延搁;。</p> 
<p>(5)忽略时间整合及不应期;</p> 
<p>(6)神经元是非时变的，即突触延时和突触强度均为常数。</p> 
<p>显然，上述假定是对生物神经元信息处理过程的简化和概括。以上内容可以由式(8.25)进行抽象和概括:。</p> 
<p>地球物理反演教程</p> 
<p>其中:xi(t)表示t时刻神经元j接受到来自神经元i的信息输入;oj(t)表示t时刻神经元j的输出;τij为输入输出间的突触时延;Tj为神经元j的阀值;wij为神经元i到神经元j的突触连接系数或权值;f{ }为神经元的转移函数，有时又称激励函数。</p> 
<p>为简单起见，将上式中的突触时延取为单位时间，则式(8.25)变为。</p> 
<p>地球物理反演教程</p> 
<p>上式描述的神经元数学模型全面表达了神经元模型的6点假设。xi(t)有多个，而oj(t)只有一个，体现了“多输入单输出”。权重值wij的正负体现了“突触的兴奋和抑制”。输入总和net'j(t)称为神经元在t时刻的净输入:。</p> 
<p>地球物理反演教程</p> 
<p>上式体现了神经元j的“空间整合性”而忽略了“时间整合作用和不应期”。当net'j(t)-Tj＞0时，神经元才被激活。oj(t+1)与xi(t)的单位时间差代表所有神经元具有相同的、恒定的工作节律，对应于“突触延搁”。wij与时间无关，体现了“非时变”。</p> 
<p></p> 
<p></p> 
<h3>神经网络算法原理</h3> 
<p></p> 
<p>4.2.1 概述</p> 
<p>人工神经网络的研究与计算机的研究几乎是同步发展的。1943年心理学家McCulloch和数学家Pitts合作提出了形式神经元的数学模型，20世纪50年代末，Rosenblatt提出了感知器模型，1982年，Hopfiled引入了能量函数的概念提出了神经网络的一种数学模型，1986年，Rumelhart及LeCun等学者提出了多层感知器的反向传播算法等。</p> 
<p>神经网络技术在众多研究者的努力下，理论上日趋完善，算法种类不断增加。目前，有关神经网络的理论研究成果很多，出版了不少有关基础理论的著作，并且现在仍是全球非线性科学研究的热点之一。</p> 
<p>神经网络是一种通过模拟人的大脑神经结构去实现人脑智能活动功能的信息处理系统，它具有人脑的基本功能，但又不是人脑的真实写照。它是人脑的一种抽象、简化和模拟模型，故称之为人工神经网络（边肇祺，2000）。</p> 
<p>人工神经元是神经网络的节点，是神经网络的最重要组成部分之一。目前，有关神经元的模型种类繁多，最常用最简单的模型是由阈值函数、Sigmoid 函数构成的模型（图 4-3）。</p> 
<p>图4-3 人工神经元与两种常见的输出函数。</p> 
<p>神经网络学习及识别方法最初是借鉴人脑神经元的学习识别过程提出的。输入参数好比神经元接收信号，通过一定的权值（相当于刺激神经兴奋的强度）与神经元相连，这一过程有些类似于多元线性回归，但模拟的非线性特征是通过下一步骤体现的，即通过设定一阈值（神经元兴奋极限）来确定神经元的兴奋模式，经输出运算得到输出结果。经过大量样本进入网络系统学习训练之后，连接输入信号与神经元之间的权值达到稳定并可最大限度地符合已经经过训练的学习样本。在被确认网络结构的合理性和学习效果的高精度之后，将待预测样本输入参数代入网络，达到参数预测的目的。</p> 
<p>4.2.2 反向传播算法（BP法）</p> 
<p>发展到目前为止，神经网络模型不下十几种，如前馈神经网络、感知器、Hopfiled 网络、径向基函数网络、反向传播算法（BP法）等，但在储层参数反演方面，目前比较成熟比较流行的网络类型是误差反向传播神经网络（BP-ANN）。</p> 
<p>BP网络是在前馈神经网络的基础上发展起来的，始终有一个输入层（它包含的节点对应于每个输入变量）和一个输出层（它包含的节点对应于每个输出值），以及至少有一个具有任意节点数的隐含层（又称中间层）。在 BP-ANN中，相邻层的节点通过一个任意初始权值全部相连，但同一层内各节点间互不相连。对于 BP-ANN，隐含层和输出层节点的基函数必须是连续的、单调递增的，当输入趋于正或负无穷大时，它应该接近于某一固定值，也就是说，基函数为“S”型（Kosko，1992）。BP-ANN 的训练是一个监督学习过程，涉及两个数据集，即训练数据集和监督数据集。</p> 
<p>给网络的输入层提供一组输入信息，使其通过网络而在输出层上产生逼近期望输出的过程，称之为网络的学习，或称对网络进行训练，实现这一步骤的方法则称为学习算法。BP网络的学习过程包括两个阶段：第一个阶段是正向过程，将输入变量通过输入层经隐层逐层计算各单元的输出值；第二阶段是反向传播过程，由输出误差逐层向前算出隐层各单元的误差，并用此误差修正前层权值。误差信息通过网络反向传播，遵循误差逐步降低的原则来调整权值，直到达到满意的输出为止。网络经过学习以后，一组合适的、稳定的权值连接权被固定下来，将待预测样本作为输入层参数，网络经过向前传播便可以得到输出结果，这就是网络的预测。</p> 
<p>反向传播算法主要步骤如下：首先选定权系数初始值，然后重复下述过程直至收敛（对各样本依次计算）。</p> 
<p>（1）从前向后各层计算各单元Oj。</p> 
<p>储层特征研究与预测</p> 
<p>（2）对输出层计算δj</p> 
<p>储层特征研究与预测</p> 
<p>（3）从后向前计算各隐层δj</p> 
<p>储层特征研究与预测</p> 
<p>（4）计算并保存各权值修正量</p> 
<p>储层特征研究与预测</p> 
<p>（5）修正权值</p> 
<p>储层特征研究与预测</p> 
<p>以上算法是对每个样本作权值修正，也可以对各个样本计算δj后求和，按总误差修正权值。</p> 
<p></p> 
<p></p> 
<h3>人工神经网络是哪一年由谁提出来的</h3> 
<p></p> 
<p>人工神经网络是1943年，心理学家W.S.McCulloch和数理逻辑学家W.Pitts提出来。</p> 
<p>他们通过MP模型提出了神经元的形式化数学描述和网络结构方法，证明了单个神经元能执行逻辑功能，从而开创了人工神经网络研究的时代。</p> 
<p>1949年，心理学家提出了突触联系强度可变的设想。60年代，人工神经网络得到了进一步发展，更完善的神经网络模型被提出，其中包括感知器和自适应线性元件等。M.Minsky等仔细分析了以感知器为代表的神经网络系统的功能及局限后，于1969年出版了《Perceptron》一书，指出感知器不能解决高阶谓词问题。</p> 
<p>扩展资料</p> 
<p>人工神经网络的特点和优越性，主要表现在三个方面：</p> 
<p>第一，具有自学习功能。例如实现图像识别时，只在先把许多不同的图像样板和对应的应识别的结果输入人工神经网络，网络就会通过自学习功能，慢慢学会识别类似的图像。自学习功能对于预测有特别重要的意义。</p> 
<p>预期未来的人工神经网络计算机将为人类提供经济预测、市场预测、效益预测，其应用前途是很远大的。</p> 
<p>第二，具有联想存储功能。用人工神经网络的反馈网络就可以实现这种联想。</p> 
<p>第三，具有高速寻找优化解的能力。寻找一个复杂问题的优化解，往往需要很大的计算量，利用一个针对某问题而设计的反馈型人工神经网络，发挥计算机的高速运算能力，可能很快找到优化解。</p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/52c61dfea61d574e780895a28223055f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">perf进阶-event实践</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/77b0dc6924f57d86ee5c21e32b7e9a8b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">牛客网刷题记录 || 第一番</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>