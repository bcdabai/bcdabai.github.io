<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>四. 基于环视Camera的BEV感知算法-BEVFormer - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="四. 基于环视Camera的BEV感知算法-BEVFormer" />
<meta property="og:description" content="目标 前言0. 简述1. 算法动机&amp;开创性思路2. 主体结构3. 损失函数4. 性能对比5. BEVFormerv2总结下载链接参考 前言 自动驾驶之心推出的《国内首个BVE感知全栈系列学习教程》，链接。记录下个人学习笔记，仅供自己参考
本次课程我们来学习下课程第四章——基于环视Camera的BEV感知算法，一起去学习下 BEVFormer 感知算法
课程大纲可以看下面的思维导图
0. 简述 今天我们来给大家分享一下一个在 BEV 感知算法里面算是比较基础也是比较具有开创性的一个工作叫 BEVFormer
论文方面的讲解流程我们还是按照算法动机&amp;开创性思路、主体结构、损失函数然后一些性能对比，未来展望这些方面去展开。我们这节课程还是围绕 BEVFormer 的框架来进行讲解，后续会有一节单独的课程会给大家详细讲解一下 BEVFormer 有关代码方面的一些问题，包括环境配置、模块化的一些流程以及后续的训练等方面的讲解，会在一个额外的课程上面去给大家做单独的一个说明
那下面我们就正式开始 BEVFormer 的讲解
1. 算法动机&amp;开创性思路 论文题目其实是很能概括出作者的一个中心思想的，所以我们从题目入手，题目如下所示：
文章题目叫 BEVFormer，我们把它拆分来看前面是 BEV 后面其实是 Former，那对于 BEV 这一块我们其实已经讲过很多次了，所谓的 BEV 其实是鸟瞰图也就是俯视图。那 Former 是什么呢，我们在 CV 这个领域当中我们所讲的很多的 Former 它其实就是一个 Transformer 的结构
OK，那除了 BEV 和 Transformer 这两个内容之外题目还给了我们其他什么信息呢，那首先它提供了一个了从哪学 BEV 的一个信息，从哪学 BEV 呢，从 Multi-Camera Images 多视角图像当中去学 BEV，反过来它告诉我们什么呢，它提示我们它这个 BEVFormer 是基于 Multi-Camera 图像输入的，通过什么样的架构去提这个 BEV 呢，他提出了一个 Transformer 的一个架构，此外有一个形容词 Spatiotemporal 来修饰这个 Transformer，那 Spatiotemporal 又是什么意思呢，是一个空间上时序上的一个界定" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/90692727591f364c0c622b007615a178/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-10T21:25:47+08:00" />
<meta property="article:modified_time" content="2023-12-10T21:25:47+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">四. 基于环视Camera的BEV感知算法-BEVFormer</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>目标</h4> 
 <ul><li><ul><li><a href="#_1" rel="nofollow">前言</a></li><li><a href="#0__12" rel="nofollow">0. 简述</a></li><li><a href="#1__20" rel="nofollow">1. 算法动机&amp;开创性思路</a></li><li><a href="#2__64" rel="nofollow">2. 主体结构</a></li><li><a href="#3__150" rel="nofollow">3. 损失函数</a></li><li><a href="#4__165" rel="nofollow">4. 性能对比</a></li><li><a href="#5_BEVFormerv2_194" rel="nofollow">5. BEVFormerv2</a></li><li><a href="#_215" rel="nofollow">总结</a></li><li><a href="#_221" rel="nofollow">下载链接</a></li><li><a href="#_226" rel="nofollow">参考</a></li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h3><a id="_1"></a>前言</h3> 
<blockquote> 
 <p>自动驾驶之心推出的《国内首个BVE感知全栈系列学习教程》，<a href="https://www.zdjszx.com/p/t_pc/goods_pc_detail/goods_detail/course_2MjRdDQO8jGkz1Sx4AoJ0sytlIU" rel="nofollow">链接</a>。记录下个人学习笔记，<strong>仅供自己参考</strong></p> 
 <p>本次课程我们来学习下课程第四章——基于环视Camera的BEV感知算法，一起去学习下 BEVFormer 感知算法</p> 
 <p>课程大纲可以看下面的思维导图</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/17/53/4ekNRTqB_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="0__12"></a>0. 简述</h3> 
<p>今天我们来给大家分享一下一个在 BEV 感知算法里面算是比较基础也是比较具有开创性的一个工作叫 <strong>BEVFormer</strong></p> 
<p>论文方面的讲解流程我们还是按照算法动机&amp;开创性思路、主体结构、损失函数然后一些性能对比，未来展望这些方面去展开。我们这节课程还是围绕 BEVFormer 的框架来进行讲解，后续会有一节单独的课程会给大家详细讲解一下 BEVFormer 有关代码方面的一些问题，包括环境配置、模块化的一些流程以及后续的训练等方面的讲解，会在一个额外的课程上面去给大家做单独的一个说明</p> 
<p>那下面我们就正式开始 BEVFormer 的讲解</p> 
<h3><a id="1__20"></a>1. 算法动机&amp;开创性思路</h3> 
<p>论文题目其实是很能概括出作者的一个中心思想的，所以我们从题目入手，题目如下所示：</p> 
<p><img src="https://images2.imgbox.com/06/1d/g0w8ROo9_o.png" alt="在这里插入图片描述"></p> 
<p>文章题目叫 <strong>BEVFormer</strong>，我们把它拆分来看前面是 BEV 后面其实是 Former，那对于 BEV 这一块我们其实已经讲过很多次了，所谓的 BEV 其实是鸟瞰图也就是俯视图。那 Former 是什么呢，我们在 CV 这个领域当中我们所讲的很多的 Former 它其实就是一个 Transformer 的结构</p> 
<p>OK，那除了 BEV 和 Transformer 这两个内容之外题目还给了我们其他什么信息呢，那首先它提供了一个了从哪学 BEV 的一个信息，从哪学 BEV 呢，从 Multi-Camera Images 多视角图像当中去学 BEV，反过来它告诉我们什么呢，它提示我们它这个 BEVFormer 是基于 Multi-Camera 图像输入的，通过什么样的架构去提这个 BEV 呢，他提出了一个 Transformer 的一个架构，此外有一个形容词 Spatiotemporal 来修饰这个 Transformer，那 Spatiotemporal 又是什么意思呢，是一个空间上时序上的一个界定</p> 
<p>那所以从题目上我们这个文章的内容其实描述得已经非常清晰了，作者的目的是从 Multi-Camera 的图像当中去学习 BEV 的一个表征，使用的模块是上面提到的空间时序的 Transformer 模块，所以按照题目来归纳的话，文章其实具有两个核心点，第一个核心点是<strong>我们如何去做 Spatiotemporal Transformer 模块</strong>，另外一个核心点是<strong>我们如何去生成 BEV 的表征</strong>，后续我们主要按照这两块去给大家做一个讲解</p> 
<p>首先我们在介绍 BEVFormer 这个完整的流程之前，我们先来复习一下两个知识点，什么是 BEV 表征和什么是 Transformer？这两个点其实我们在之前的讲解过程中也提到过很多次了，我们这里给大家做一个复习</p> 
<p><img src="https://images2.imgbox.com/27/70/PWo56CyM_o.png" alt="在这里插入图片描述"></p> 
<p>那什么是 BEV 表征呢？我们这里简单总结下：</p> 
<ul><li>第一，它是一个<strong>重构空间</strong></li><li>第二，它是一个<strong>多传感器融合</strong>的空间</li><li>第三，它是固定视角（俯视视角）的空间</li></ul> 
<p>那对于 BEV 表征而言，它首先是一个重构空间，什么叫重构空间呢，重构空间其实意味着它这个空间并不是真实存在的，我们是不能通过传感器去获取到的，而是通过某些额外的手段去构造出来的一个空间。第二点它是一个多传感器的空间，我们常见的传感器包括激光雷达、毫米波雷达还有一个多视角相机等等，那这些传感器采集到的数据通过某些融合处理的方式后可以得到我们这里所谓的 BEV 空间。</p> 
<p>另外一点它是一个固定视角的空间，重构也好，融合也好，它需要对重构和融合的方向做一个界定，我们往哪重构，我们上哪去做融合呢，所以说 BEV 这里就给了一个比较明确的一个界定，它把这个融合的空间已经固定到俯视视角的空间了，所以有了我们这个 BEV 的表征，那我们这里就给大家先比较简单的复习一下视觉重构的俯视视角的特征是什么样子的</p> 
<p>我们再给大家复习一下 Transformer 是干什么的，我们在之前的课程中也强调过我们理解 Transformer 可以不用把它当成一个网络去理解，我们大家其实把它当成一个模块，和卷积、池化这些模块一样，是一个很小的功能性模块</p> 
<p><img src="https://images2.imgbox.com/95/4a/e6M2tYEj_o.png" alt="在这里插入图片描述"></p> 
<p>那它这个模块可以实现什么功能呢，它其实就是一个注意力的机制，通俗的讲它这个注意力是<strong>突出强调视觉特征中的某一部分</strong>，比如上图中奔驰车的示意图，通过什么样的特征去判断这辆车是奔驰车是最靠谱的呢，通过车标就可以，很明显我们通过合理的训练之后 Transformer 最后会关注哪个位置呢，显然是车标的位置</p> 
<p>这里再给大家扩展一下就是我们在 BEVSAN 课程中也提到过，里面涉及了一个 SE 模块，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/13/b4/Ds8Vlm4F_o.png" alt="在这里插入图片描述"></p> 
<p>这个 SE 模块本身也是一种注意力机制，它的注意力机制是作用在通道域上面的，也就是说从图中我们能看到最后输出的特征图是有不同颜色的，那不同的颜色我们认为它其实是表示了不同注意力的一个权重的。通过对通道数值的重新加权网络会自适应的关注或者忽略某一部分的特征，我们可以假定颜色比较深的区域权重比较大，通过加权之后权重比较大的地方特征会自适应的得到一个加强，同理颜色比较浅的区域权重就比较小，通过加权之后权重比较小的地方特征会得到一个削弱，后续我们在对前面的特征去做一个判断的时候，网络就自然而然地不会注意到权重很小的位置</p> 
<p>OK，这里给大家稍微了复习一下 BEV 和 Transformer 的相关内容，BEV 其实就是一个俯视图的空间，将我们前端传感器所采集到的图像数据、点云数据统一的映射到这个空间当中；Transformer 其实就是一个注意力机制，它会引导网络去忽略或者关注某一区域的特征从而对我们后续的任务起到一个促进的作用</p> 
<h3><a id="2__64"></a>2. 主体结构</h3> 
<p>从这里开始我们给大家具体介绍一下 BEVFormer 是一个怎样的流程，它的网络结构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/8c/59/X5KnOGAy_o.png" alt="在这里插入图片描述"></p> 
<p>看一个网络我们首先从输入输出看起，输入其实很清楚了，是 Multi-view input，是一个多视角的输入，这个我们前面提到过很多次了，BEVFormer 的输入是一个多视角图像，那输出在哪呢，它这个网络的图画得输出比较隐晦一点，那它的输出其实在上面，是一个分割和检测头用来做输出的，通过输入的多视角图像我们可以得到输出的结果。那明白了网络功能之后，我们按照网络的流程走向来看一下这个网络有哪些模块</p> 
<p>那以图像为输入的话，首先图像通过 Backbone 网络得到 Multi-Camera Features 图像特征，图像特征和历史 BEV 特征和 BEV Queries 查询向量同时输入到类似于 Transformer 的结构当中得到我们当前的 BEV 特征，我们再将这个 Current BEV Feature 送入到后续的分割还有检测网络头里面，我们就可以得到最终的一个结果</p> 
<p>按照这个流程来讲的话框架其实是很清晰的，输入是一个 Multi-view Input 通过 Backbone 我们可以得到 Feature，图像级别的特征，图像级别的 Feature 和历史数据和查询数据合在一起可以生成我们当前状态下俯视图的特征，然后利用这个俯视图的特征我们可以做很多的任务，无论是检测也好，分割也好，轨迹预测也好，我们可以利用这个 BEV 特征去做很多的任务，后续的任务其实可以依赖于我们个人的设计</p> 
<p>OK，我们对 BEVFormer 的流程有了一个基本了解之后，我们可以对每个模块去进行一个分开的讨论，我们首先讨论的是图像的 Backbone 部分，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/57/4f/zey5gVeY_o.png" alt="在这里插入图片描述"></p> 
<p>我们还是从 Backbone 的输入输出看起，首先输入是 Multi-view Input，是一个多视角的图像，那输出是什么呢，是 Multi-Camera Features，是输入的多视角图像所对应的多视角特征。那用什么网络去做这个事情呢，我们怎么样才能将输入变成特征呢，这就是 Backbone 网络要做的事情了。这个 Backbone 网络它就是一个图像特征提取的网络，所以我们常见的比如 ResNet、ResNet+FPN 等等这些网络都行，那这些网络属于很基础的深度图像处理网络了，我们这里就不再详细展开了</p> 
<p>那了解了 Backbone 网络，我们知道这个 Backbone 网络的输出是 Multi-Camera Features，那也就是说输出的是视觉图像特征，并且是多视角的视觉图像特征，后续我们怎么处理这个特征呢，我们怎么利用这个视觉图像特征去得到我们想要的 BEV 特征呢，那后续灰色部分的模块就是干这个事情了，后续这个框架其实是 BEVFormer 的一个核心内容，它涉及到我们如何把我们得到的 Multi-Camera Feature 去生成我们当前需要的 BEV Feature</p> 
<p><img src="https://images2.imgbox.com/0a/3d/5JFd3nb8_o.png" alt="在这里插入图片描述"></p> 
<p>为了看懂这个模块，我们还是从输入输出看起，它的输入其实包括三个部分，那首先第一个是 Multi-Camera Features，第二个输入是历史的俯视图特征，最后一个是 BEV Queries，那这个所谓的历史俯视图特征我们可以把它理解成一个时序的特征，因为 BEVFormer 文章的题目叫 <strong>SpatialTemporal attention</strong> 空间时序注意力，那这个 Multi-Camera Features 属于空间层面的，历史的 BEV 特征属于时序层面的，它们俩怎么做一个有机的融合呢，这个空间特征和历史特征是怎么做融合的呢，所以我们就用到了 BEV Queries，BEV Queries 其实是起到了在 Multi-Camera Feature 和历史的 BEV Feature 之间一个很好的桥接的作用，</p> 
<p>图中灰色部分是这个模块的一个详细结构，我们从流程上可以把这个模块再仔细看一下，它这个模块其实是一个从下往上的一个模块，那首先是将历史的 BEV 和 BEV Queries 输入 Temporal Self-Attention 的一个结构，一个时序的 Self-Attention 的结构，通过这个 Temporal Self-Attention 之后送到 Spatial Cross-Attention，先做时序的然后再做空间的，当做完两个之后我们会得到 Current BEV 也就是它当前状态下的俯视图的特征</p> 
<p>那我们之前讲过，我们做注意力机制的一个主要目的是得到需要强调的部分，那这个部分通道的也好，空间的也好，时序的也好是我们需要特别去关注的部分，所以我们才把它称之为注意力机制嘛，我们带着这样一个基本思想去进入到下一个模块。那按照这个逻辑结构来讲的话，我们还是先给大家强调一些 Tempora Self-Attention 然后再说 Spatial Cross-Attention，因为它是一个从下往上的一个结构，所以我们后续也是先讲时序注意力是怎么做的，然后再讲空间注意力是怎么做的</p> 
<p>OK，我们先讲一下 Tempora Self-Attention 也就是图中黄色部分的的时序注意力模块是怎么做的</p> 
<p><img src="https://images2.imgbox.com/6b/b7/CAaLByvx_o.png" alt="在这里插入图片描述"></p> 
<p>按照作者论文中所阐述的，<strong>时序注意力模块的设计主要考虑以下两个方面</strong>：</p> 
<ul><li>第一，如何引入前一时刻特征？</li><li>第二，考虑不同帧中车的偏移量不同，如何选择需要的呢？</li></ul> 
<p>第一个方面是我们要怎么引入前一时刻的特征，因为它既然称之为时序注意力，时序它是考虑前一帧和当前帧之间的一个关联的，所以说我们如何把前一帧时刻的特征引入到我们当前 BEV 特征的计算是一个很关键的问题，在上图中我们也能看到作者是想通过历史 BEV 信息来去对我们当前 BEV Queries 去做一个加强的；另外一个我们要谈论的点是它在不同帧中车的偏移量是不同的，所以我们如果设定一个固定的 attention 机制来去建模这样一个不确定性的话，它其实是不太合理的一个事情</p> 
<p>考虑到以上两点，作者引入了一个叫可形变注意力（<strong>Deformable Attention</strong>）的模块，那它这种注意力与我们通常意义上讲的注意力有什么区别呢，这里以一个九宫格举例，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/20/46/li6GuBXs_o.png" alt="在这里插入图片描述"></p> 
<p>传统的注意力是怎么做的呢，比如我想计算上图中 ❌ 这个位置特征应该是什么，那计算 ❌ 这个位置特征的时候我们可能会考虑临近点的一个特征，比如与它临近的三个 🔺 位置的特征，我们对这三个 🔺 位置特征去进行一个加权，乘以一定的系数，比如 0.5，0.2，0.3，乘到这个原始的特征上生成我们 ❌ 这个位置的特征，这就是我们通常意义上讲的视觉注意力的一个做法</p> 
<p><img src="https://images2.imgbox.com/a2/9a/5R9G63Fn_o.png" alt="在这里插入图片描述"></p> 
<p>那 Deformable Attention 可形变注意力是怎么做的呢，我们还是以九宫格举例，比如我们刚刚想算 ❌ 位置的特征，我们采用的是与它邻近的三个 🔺 的特征，我们后续实验可能发现这个特征并不能完全满足我们的要求，因为它是一个固定位置的，但这个固定位置并不适合于通用场景的，所以就提出了这个可形变 attention 的思路</p> 
<p>它会计算一个 offset 偏移，<strong>那这个 offset 其实表示的是我要计算的那个点位置的特征和我当前点它之间的一个偏移量，也就是一个距离</strong>。比如我想计算 ❌ 这个位置，那我采用的特征，我通过预测得出它的一个偏移量，可能离他很远，可能在 ⭕ 这个位置，那我利用 ⭕ 这个位置去计算 ❌ 这个位置的特征它可能是更准确的</p> 
<p>我们以前的通用的方法是用 🔺 的位置来计算，那这样子可能在某些场景下不合理的，那我们通过这个 offset 这样一个偏移量的预测，我们发现可能采用三个 ⭕ 去计算 ❌ 的特征可能是更合理的，所以也就提出了我们这个可形变注意力的网络，利用一个可适应的可学习的一个 offset 偏移去对特征去做一个更好的提取预测</p> 
<p><img src="https://images2.imgbox.com/d3/3c/JjXQz7Ko_o.png" alt="在这里插入图片描述"></p> 
<p>还是以上图为基准，网络可以自行预测偏移量，它是可以预测出来我哪个点的特征对我当前点特征是有增益的，利用这个预测出来的点去对我当前点去做一个加权，那这样可以得到一个逻辑更自洽的特征提取</p> 
<p>OK，明白了这点之后我们再回到这个时序注意力上，对于每一个 BEV Query 我们可以理解成它是一个 ❌ 的这个点，网络会来判断哪个 BEV 对我当前 BEV Query 是有用的，用这个方式我们就恰好能利用这个历史的 BEV 特征，比如在 BEV Queries 的 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         x 
        
       
         , 
        
       
         y 
        
       
         ) 
        
       
      
        (x,y) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mclose">)</span></span></span></span></span> 上我们会去找哪一个历史 BEV 对我当前的这个 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
       
         x 
        
       
         , 
        
       
         y 
        
       
         ) 
        
       
      
        (x,y) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mclose">)</span></span></span></span></span> 是有用的呢，我们可以找到褐色的点和蓝色的点对我这个 BEV 是有用的，所以我们就会把它引入进来，那这就是第一个模块</p> 
<p>那我们把历史 BEV 计算完之后，BEVFormer 会同时的去在它原始的特征图中去做第二次的 self-attention，也就是上图中深绿色的位置和浅蓝色的位置，考虑的是 BEV Query 上对我当前点有增益的一个位置，它是一个 self-attention 的一个计算过程，利用这两个方式来自于历史 BEV 和 BEV Query 它自身的一个特征加权，通过这种方式 BEVFormer 的作者认为是可以得到一个很好的查询先验的，因为 BEV Query 本身是作为一个查询向量产生的。那有了一个历史 BEV 的引导之后 BEV Query 的查询的先验会更好，那也就是说我们对当前应该生成怎样的 BEV 是有一个初步预期的，那这个初步预期其实来源于我们历史 BEV 的数据的</p> 
<p>后续我们再结合我们当前提取到的空间特征和已经有很强的这个 BEV 先验的 BEV Query 我们就能生成更好的 BEV Feature，那后续我们有了这个 BEV Query，有了我们已经提取好的 Multi-Camera Feature，我们怎么样去生成我们想要的这个 BEV Feature 呢，而且是当前状态下的 BEV Feature 呢，所以作者提出另外一个主体结构叫做 <strong>Spatial Attention 空间注意力</strong>的结构</p> 
<p><img src="https://images2.imgbox.com/96/5d/pwz7O12W_o.png" alt="在这里插入图片描述"></p> 
<p>那 BEVFormer 的作者还是考虑了以下两个方面：</p> 
<ul><li>第一，还是讨论如何选择需要的特征呢？</li><li>第二，如何建模高度信息？</li></ul> 
<p>首先第一个方面讨论一下如何选择需要的特征，因为 BEV 空间其实很大的，那这个空间中它并不是所有信息全都是我要去构造这个俯视图特征所需要的信息，其中也包含了很多这种无用的信息。那我们如何选择我们需要的特征呢；那另外一个点是考虑的一个比较有意思的信息就是我们如何建模高度</p> 
<p>那我们首先讨论第一个问题，如何选择我们需要的这个特征，那这个自然离不开我们上面所生成的这个已经有很强先验信息的 BEV Query，利用这个 BEV Query 去对我们的 Multi-Camera Feature 去进行一个特征的查询，去问一问当前位置是否存在特征。我们举个例子，那比如我们想查询 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          ′ 
         
        
       
         , 
        
        
        
          y 
         
        
          ′ 
         
        
       
         ) 
        
       
      
        (x',y') 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0019em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 这个位置的特征，它会怎么做呢，它会按照我们 Query 提供的这个索引去找我们所对应的这个视角下的特征，空间位置的特征。</p> 
<p>那我们怎么找呢，同样还是一样的引入这个可形变注意力的机制，我们将 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          ′ 
         
        
       
         , 
        
       
         y 
        
       
         ’ 
        
       
         ) 
        
       
      
        (x',y’) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0019em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mord">’</span><span class="mclose">)</span></span></span></span></span> 先映射到它这个视角下所对应的这个位置上，通过去找这个位置邻近的相邻点的一个特征去进行一个融合，然后生成它当前视角下需要融合的特征，那后续的话它把这个多视角全都查询完之后会生成当前这个 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          ′ 
         
        
       
         , 
        
        
        
          y 
         
        
          ′ 
         
        
       
         ) 
        
       
      
        (x',y') 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0019em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7519em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 位置上通过 Multi-Camera Feature 融合好的一个特征。那这个特征显然也是经过了 attention 机制的嘛，我们刚提到是在每一个视角下去做这个 Deformable Attention 去做这个可形变注意力来提取空间位置上我们需要额外关注的一些特征</p> 
<p>第二点 BEVFormer 作者考虑了一个很有意思的一个事情，是这个高度信息，我们为什么要考虑高度信息呢，是因为 BEVFormer 作者认为我们如果是不同的目标的话，它在高度上是具有显著差异的，比如公交车或者卡车很高，路牌可能很矮，它们是有显著差异的，因为目标种类的不同会导致这个高度上的差异变化。那一般我们所讲的俯视图它是拍扁的，拍扁的话会损失掉高度维度的这个特征，所以说 BEVFormer 它在这里设置了一系列的 anchor 用于匹配不同高度的这个目标，那这点我们其实在讲解 BEV-SAN 这篇文章中已经详细分析过了，如果大家忘记了的话可以再去复习一下</p> 
<h3><a id="3__150"></a>3. 损失函数</h3> 
<p>那我们说到这里 BEVFormer 的一个主要模块已经给大家讲解完了，那按照我们前面的分析我们对里面的一些很重要的模块去做了一个比较详细的一个讲解，那我们接下来会结合损失函数把这个整体的流程再复习一遍</p> 
<p><img src="https://images2.imgbox.com/26/35/feh6n1JJ_o.png" alt="在这里插入图片描述"></p> 
<p>BEVFormer 的整体流程我们经过前面的拆解应该相对而言是比较清晰的，那首先是经过一个 Multi-View Input 然后经过一个 Backbone 网络去得到一个 Multi-Camera Feature 多视角图像特征，通过多视角图像特征、历史的 BEV 特征和这个 BEV Query 的特征我们通过一个所谓的叫 SpatialTemporal attention 的一个网络去生成 Current BEV，也就是我们当前 BEV 空间的特征，利用当前 BEV 空间的特征就可以做一些我们后续很关心的任务，比如说检测也好，分割也好，或者说轨迹预测，轨迹规划等等，那后续这个任务是我们可以人为自定义的</p> 
<p>那因为我们这里提到了损失函数，损失函数这个部分其实是根据我们后面子任务去定义的，后面 Loss 这一块是与我们任务相关的，比如说 3D 检测任务那就是一个边界框分类和回归的函数，那要是 3D 分割任务呢，那就是点云或者网格分类损失函数等等</p> 
<p>那这个流程清楚了之后实验设计的部分我们也可以推理出来 BEVFormer 作者他会做哪些实验呢，按照我们流程来讲的话，它首先是一个 Multi-view 的一个 input，是一个多视角的输入通过 Backbone 去得到特征，那首先第一个实验前序的图像处理这个 Backbone 它对我们整体性能有没有影响呢，这是第一个实验。那后续第二个实验也很明了了，前面一个 Multi-view 的 Feature，然后一个历史 BEV，一个 BEV Query，通过时序 attention 通过空间 attention 可以得到最后的 BEV Feature Map，那这个 Temporal self attention 它是可去掉的网络，我们通过刚刚讲解也知道它这个 temporal 时序 attention 它最后输出的是一个带有强烈先验信息的 BEV Query</p> 
<p>那我们把这个网络去掉那无非就是说它这个先验信息没有了，它不会对这个网络的输入输出有其他额外的影响，输入输出的数目还是固定的，它有 BEV Query 还是有 BEV Query，那无非是 BEV Query 本身额外的已经编码好的这个先验信息没有了，那后续消融实验我们就显然可以把它去掉，把这个 Temporal Self-Attention 给去掉看他对网络的性能是否有很大的影响。那另外一个我们刚才也提到无论是空间的 attention 还是时序的 attention，它俩都离不开一个叫可形变注意力的一个模块，也就是我们刚才讲的 Deformable Attention，那这个 Deformable Attention 是否对网络很重要呢</p> 
<h3><a id="4__165"></a>4. 性能对比</h3> 
<p>那我们带着上面的推断可以进入到性能对比的方面</p> 
<p><img src="https://images2.imgbox.com/bb/bb/t6NdOCU3_o.png" alt="在这里插入图片描述"></p> 
<p>那性能对比我们强调两个方面，前面第一个方面是一个总体结果，从总体结果上来看的话，比一些比较基础的 3D 检测方法还是有一定的性能提升的，比如在使用同等 Backbone 的情况下 BEVFormer 比 Fcos 高了十个点左右，那如果我们换一个更好的 Backbone，比如说换到了 V2-99*，这个性能提升就更明显了，BEVFormer 如果是 V2-99* 的 Backbone 比 R101 要好将近 0.03 个点</p> 
<p>那另外一个实验其实我们刚刚也提到过，整个时序注意力模块是可以拿掉的，那我们这里的 BEVFormer-S 其实就是拿掉时序注意力之后的网络，我们也可以比较下这个 BEVFormer-S 和 BEVFormer 之间的一个差异，可以发现如果没有时序注意力模块的话，我们本身的这个 BEV Query 其实是没有历史 BEV 的这个先验信息的，性能是下降了很多的大家可以从表中看到一个是 53.5 一个是 46.2，它这个性能其实下降特别多的</p> 
<p><img src="https://images2.imgbox.com/54/c7/QV72PNvs_o.png" alt="在这里插入图片描述"></p> 
<p>那 BEVFormer 中讨论第三个点是不是说我们这个网络非得使用这个可形变卷积不可呢，能不能使用别的一个这样的一个注意力机制呢，那这里作者其实也是给了一个性能比较的，显然是可以使用其他注意力机制的，那性能也是会受到一定的影响，作者在这里其实给了三种注意力机制，一个叫全局注意力也就是表中的 Global，还有一种是这个 points，一种是 local</p> 
<p><img src="https://images2.imgbox.com/a9/fb/agXyMuJe_o.png" alt="在这里插入图片描述"></p> 
<p>我们还是以九宫格举例来说明一下这三种 attention 有什么样的一个区别，以上图为例，它这里提到的 Global Attention 是什么意思呢，是考虑这个九宫格内的所有特征去给它做一个加权，我们会得到这个 Global 的特征，是一个全局性质的一个特征；那什么叫 point 呢，还是以九宫格特征，point 的特征其实是我们会对这个区域内去做一些点的指定，比如说我要算区域的特征，那我会指定我这个区域特征的四个点为代表，无论什么区域我全都是用这四个点的特征作为我这个区域的一个 reference 特征一个代表性的特征，</p> 
<p>那另外一个就是 local 的方式，那这个 local 的方式就是我们提到过很多次的，是文章中所提到的可形变注意力的一个方式，比如我们同样想提取这个区域的特征，那我们这个 local 方式选择的特征是非固定的，是基于 offset 网络预测出来的，它是根据网络按照不同的情况通过不同的输入去预测出来不同的 offset，所有会产生不一样的特征组合的方式，那显然按照这种可形变的方式，这个网络会更加的灵活也能更加的自适应不同的场景</p> 
<p>OK，以上就是我们今天 BEVFormer 部分整体的一个讲解</p> 
<p>我们对 BEVFormer 可以再做一个总结，BEVFormer 它的一个核心内容其实是我们如何去生成 BEV 特征，就是这个 BEV 特征要怎么构造，那至于这个 BEV 特征生成好之后去做什么任务，我们可以选择我们合适的一个分割头也好，检测头也好，去做适合的这个子任务。那核心的内容就是说我们怎么去生成 BEV 特征呢，其实 BEVFormer 包含两个模块，一个模块是 Temporal Attention，另一个模块是 Spatial Attention，通过 Temporal Attention 我们可以把历史 BEV 信息融合到当前的 BEV Query 上，通过 Spatial Attention 我们可以利用 BEV Query 提取到我们想要的空间位置的信息，空间的一个特征来生成我们最终想要的 current BEV，也就是当前 BEV 视角下的特征</p> 
<p>我们今天主要是对这个 BEVFormer 它的一个整体流程还有一些功能概念做一些介绍，后续会有对 BEVFormer 完整代码的一个详细讲解，也欢迎大家持续关注</p> 
<h3><a id="5_BEVFormerv2_194"></a>5. BEVFormerv2</h3> 
<p>另外我们这里给大家稍微提一下 BEVFormerv2 这个方法，我们说的 BEVFormer 的改进版，它的框图如下：</p> 
<p><img src="https://images2.imgbox.com/23/a1/i7cSeSz2_o.png" alt="在这里插入图片描述"></p> 
<p>我们这里主要看一下它做了哪些改进，我们还是按照输入输出的流程来看，输入是什么呢，是一个多视角图像，一个 Multi-view images，输出是什么呢，是 predictions，那也就是我们 3D 检测结果。输入的多视角图像通过 Backbone 网络可以得到 Multi-view Features，也就是我们说的图像特征，我们在课程前面提到我们说的 BEVFormer 是利用 Multi-view Feature 通过 Spatial attention 空间注意力，通过 Temporal attention 时序注意力可以得到 BEV 特征</p> 
<p>那在 BEVFormerv2 当中除了 BEVFormer 原本框架当中的 Spatial Encoder 和 Temporal Encoder 还有一个额外的支路，我们从图中也能看到是 Perspective 3D Head 是有额外的监督信息的。这个额外的支路其实是 BEVFormerv2 的核心内容，那为什么需要这个支路呢，我们可以这样想如果把这个支路去掉，图像 Backbone 网络当中参数的监督信息来自于哪里呢，是来自于我们最后的一个检测结果，最后检测结果的损失，用梯度回传来更新图像 Backbone 的参数，那 BEVFormerv2 的作者认为像这种监督其实不是一种明显的监督，梯度回传是一步一步最终才能到达图像 Backbone 的参数上的，那我们怎么把这种远端的监督来变成一种近端的监督呢，那我们怎么把这种很不直接很不明显的监督变成又直接又明显的呢</p> 
<p>BEVFormerv2 作者的想法也很直接，我们直接利用 Backbone 输出的图像特征可以得到一个初步的预测结果，初步预测结果如果有监督信息的话可以直接用来更新 Backbone 网络参数，通过这样的方式，那它的这个 head 的设计也很简单，我们前面是有了一个 Multi-view Feature 也就是多视角的图像特征，我们可以利用一些单目的 3D 检测框架的 Head，可以在图像上预测 3D Box，虽然这个 Box 可能是不太准的，可能是比较粗糙的，没有关系，它的 Perspective head 的预测结果是不会作为我们最终的检测结果的，它的一个更主要的作用其实是图像网络的参数更新的作用</p> 
<p>那 BEVFormerv2 它其实也是属于那种 Two-Stage 这种框架我们叫两阶段检测器，两阶段检测器哪里是第一阶段呢，额外的支路输出结果是第一阶段，我们说第一阶段检测结果一般是不太好的，很粗糙的结果，那后续通过我们说第二阶段去对第一阶段的结果做更新做 refine 可以得到一个更好的一个更准确的结果</p> 
<p>我们把第一阶段得到的初步的检测结果送入到第二阶段当中，它怎么送呢，我们第一阶段它不是有一个 proposal 吗，proposal 可以和我们原本初始的 BEVFormer 当中随机初始化的一些 Object Query 做混合，一个混合的 Object Query，那这个 Query 其实是有两个方面的，有一方面是我们第一阶段的检测结果，另外一方面其实是我们初始化的一些随机的，可更新的一些 Query，以上两个 Query 混合来做我们最后的 BEV Predictions</p> 
<p>那以上其实就是 BEVFormerv2 的一些主要内容的改进，详细的内容我们这里就不再赘述了</p> 
<p>那 OK 我们本小节内容就到此为止</p> 
<h3><a id="_215"></a>总结</h3> 
<blockquote> 
 <p>这节课程我们学习了一个非常经典的框架叫做 BEVFormer，BEVFormer 的整体流程是多视角图像通过 Backbone 提取到多视角图像特征，多视角图像特征加上历史 BEV 特征和 BEV Queries 一起输入到一个叫 SpatialTemporal Attention 的结构中得到我们当前的 BEV 空间特征。那其中最重要的是 SpatialTemporal Attention 结构中的 Temporal Self-Attention 时序注意力模块和 Spatial Attention 空间注意力模块，通过时序注意力模块我们可以把历史 BEV 信息融合生成一个具有强烈先验信息的 BEV Query，通过空间注意力模块我们可以提取我们想要空间位置的信息来生成我们最终想要的 current BEV。此外我们还简单介绍了一下 BEVFormerv2，它的主要改进在于引入了一条额外的支路来监督图像网络参数的更新。</p> 
 <p>OK，以上就是 BEVFormerv2 的全部内容了，下节我们学习另外一篇非常经典的环视算法 BEVDet，敬请期待😄</p> 
</blockquote> 
<h3><a id="_221"></a>下载链接</h3> 
<ul><li><a href="https://pan.baidu.com/s/1QmH51czKVOyVDmD_m_n2CA" rel="nofollow">论文下载链接【提取码：6463】</a></li><li><a href="https://pan.baidu.com/s/1MS_A2-YRvFj1LsL85RuQiQ" rel="nofollow">数据集下载链接【提取码：data】</a></li></ul> 
<h3><a id="_226"></a>参考</h3> 
<p>[1] <a href="https://arxiv.org/pdf/2203.17270.pdf" rel="nofollow">Li et al. Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers</a></p> 
<p>[2] <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf" rel="nofollow">Yang et al. BEVFormer v2: Adapting Modern Image Backbones to Bird’s-Eye-View Recognition via Perspective Supervision</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/d85e4d3d91649a30bb1f13b27afed71d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">四. 基于环视Camera的BEV感知算法-DETR3D</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b0e7298de08533d9b6d74147982af419/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">原来JMeter 分布式执行原理这么简单，为什么没有早点看到呢！</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>