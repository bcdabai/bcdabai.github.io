<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>人体姿态识别方案详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="人体姿态识别方案详解" />
<meta property="og:description" content="文章目录 姿态迁移简介方案详解MediapipeMediapipe数据获取多人姿态识别方向探索 PoseNetMoveNetOpenPoseOpenMMD 总结参考链接 姿态迁移简介 目前AR，VR，元宇宙都比较火，需要实际场景和虚拟中进行交互的情况，因此研究了通过摄像头获取图像进行识别，本文主要概述了在人体身体姿势识别跟踪方面的一些调研和尝试。
通过各个方案，我们可以从RGB视频帧中推断出整个身体的关键特征点，从而根据这些关键特征点去做扩展，比如迁移到unity模型中等。
从识别角度来说，我们可以分成两个大方向，一是人体身体关键特征点识别，这里特征点分为2d特征点和3d特征点，部分方案只支持2d特征点；二是人体动作识别，比如用户在做什么动作，举一个很简单的例子，我们可以通过mediapipe识别出用户在做俯卧撑或者深蹲等。
从用户体验角度来说，我们可以分成实时摄像头传输和对视频进行处理两个方向。摄像头实时传输的话就必须要做到对视频每一帧RGB图像做到即时处理，这里就牵扯到优化效率问题。对视频进行处理的话可以采用对视频后期处理的形式去处理，一般动捕方案都是这么做的。
方案详解 Mediapipe Google出品，可以实现人脸检测、姿态识别，手势识别等很多效果，并且可以在多平台高效的输出。这里强调下Mediapipe检测出来的特征点数据均为3d，有空间感。
Mediapipe检测是基于BlazeFace模型，明确地预测了两个额外的虚拟关键点，这些虚拟关键点将人体中心、旋转和缩放牢固地描述为一个圆圈。受莱昂纳多的维特鲁威人的启发，我们预测了一个人臀部的中点，外接整个人的圆的半径，以及连接肩部和臀部中点的线的倾斜角。
Mediapipe数据获取 从Mediapipe上获取的身体的33个特征点，具体如下图，对此33个特征点进行判断。
主要通过角度来判断：
float Angle(NormalizedLandmark ver1, NormalizedLandmark ver2, NormalizedLandmark ver3, NormalizedLandmark ver4) { return Vector3.Angle(new Vector3(ver1.X, ver1.Y, ver1.Z) - new Vector3(ver2.X, ver2.Y, ver2.Z), new Vector3(ver3.X, ver3.Y, ver3.Z) - new Vector3(ver4.X, ver4.Y, ver4.Z)); } 多人姿态识别方向探索 Mediapipe 在人脸上是支持多人的，但是在姿态识别上目前只支持单人。在实验了网上能搜到的各种方案之后，有一种方案目前是可行的，但是在性能上会比较卡顿。
既然Mediapipe支持单人，那我们把视频画面中的多人画面拆分成多个单人就行了。
这里我采用的是OpenVINO中的行人检测模型。（我尝试了多种方案，YOLO-unity、Barracuda-Image-Classification和OpenVINO后发现OpenVINO效果最佳）OpenVINO ToolKit是英特尔发布的一套深度学习推断引擎，支持各种网络框架。对此不熟悉的同学可以参考OpenVINO开发系列文章汇总进行较为系统的学习。
具体思路就是通过OpenVINO识别出人物范围框，然后使用Opencv进行图像分割，单人图像传递给Mediapipe sdk去做单人姿态识别，然后进行汇总。目前该方案在移动端测试效果较为卡顿，不是很理想。
PoseNet PoseNet是TensorFlow和谷歌创意实验室合作发布的专门用于姿态估计的一种技术方案。PoseNet可用于估计单个姿势或多个姿势，该算法有一个版本可以仅检测图像/视频中的一个人，另一个版本可以检测图像/视频中的多个人。
流程主要有两个阶段：
输入RGB 图像通过卷积神经网络馈送。单姿势或多姿势解码算法用于解码来自模型输出的姿势、姿势置信度分数、关键点位置 和关键点置信度分数。 同Mediapipe一样，我们来看看PoseNet给到我们的关键姿势点。PoseNet提供了17个关键点，不同于Mediapipe提供的3d数据点，PoseNet提供的是关键点的2d坐标，x和y。以及关键点可信度分数，使用者可以根据实际情况去做判断，范围在0.0-1.0之间，越接近1.0表示识别出来的点越正确。
ok，我们来看下我把posenet这一套通过tf-lite加载的形式在unity上的呈现效果。
MoveNet MoveNet是一种超快速且准确的模型，可检测身体的 17 个关键点。该模型在TF Hub上提供，有两种变体，称为 Lightning 和 Thunder。Lightning 适用于延迟关键的应用程序，而 Thunder 适用于需要高精度的应用程序。在大多数现代台式机、笔记本电脑和手机上，这两种模型的运行速度都比实时 (30&#43; FPS) 快，这对于现场健身、健康和保健应用至关重要。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/807c10c32565533778b6c5528900262e/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-27T00:25:54+08:00" />
<meta property="article:modified_time" content="2022-01-27T00:25:54+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人体姿态识别方案详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">姿态迁移简介</a></li><li><a href="#_11" rel="nofollow">方案详解</a></li><li><ul><li><a href="#Mediapipe_12" rel="nofollow">Mediapipe</a></li><li><ul><li><a href="#Mediapipe_19" rel="nofollow">Mediapipe数据获取</a></li><li><a href="#_31" rel="nofollow">多人姿态识别方向探索</a></li></ul> 
   </li><li><a href="#PoseNet_38" rel="nofollow">PoseNet</a></li><li><a href="#MoveNet_50" rel="nofollow">MoveNet</a></li><li><a href="#OpenPose_142" rel="nofollow">OpenPose</a></li><li><a href="#OpenMMD_155" rel="nofollow">OpenMMD</a></li></ul> 
  </li><li><a href="#_163" rel="nofollow">总结</a></li><li><a href="#_168" rel="nofollow">参考链接</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>姿态迁移简介</h2> 
<p>目前AR，VR，元宇宙都比较火，需要实际场景和虚拟中进行交互的情况，因此研究了通过摄像头获取图像进行识别，本文主要概述了在人体身体姿势识别跟踪方面的一些调研和尝试。</p> 
<p>通过各个方案，我们可以从RGB视频帧中推断出整个身体的关键特征点，从而根据这些关键特征点去做扩展，比如迁移到unity模型中等。</p> 
<p>从识别角度来说，我们可以分成两个大方向，一是人体身体关键特征点识别，这里特征点分为2d特征点和3d特征点，部分方案只支持2d特征点；二是人体动作识别，比如用户在做什么动作，举一个很简单的例子，我们可以通过mediapipe识别出用户在做俯卧撑或者深蹲等。</p> 
<p>从用户体验角度来说，我们可以分成实时摄像头传输和对视频进行处理两个方向。摄像头实时传输的话就必须要做到对视频每一帧RGB图像做到即时处理，这里就牵扯到优化效率问题。对视频进行处理的话可以采用对视频后期处理的形式去处理，一般动捕方案都是这么做的。</p> 
<h2><a id="_11"></a>方案详解</h2> 
<h3><a id="Mediapipe_12"></a>Mediapipe</h3> 
<p>Google出品，可以实现人脸检测、姿态识别，手势识别等很多效果，并且可以在多平台高效的输出。这里强调下Mediapipe检测出来的特征点数据均为3d，有空间感。</p> 
<p>Mediapipe检测是基于BlazeFace模型，明确地预测了两个额外的虚拟关键点，这些虚拟关键点将人体中心、旋转和缩放牢固地描述为一个圆圈。受莱昂纳多的维特鲁威人的启发，我们预测了一个人臀部的中点，外接整个人的圆的半径，以及连接肩部和臀部中点的线的倾斜角。<br> <img src="https://images2.imgbox.com/44/50/92ezb0HC_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="Mediapipe_19"></a>Mediapipe数据获取</h4> 
<p>从Mediapipe上获取的身体的33个特征点，具体如下图，对此33个特征点进行判断。<br> <img src="https://images2.imgbox.com/89/8e/Bdi8njQ5_o.png" alt="在这里插入图片描述"><br> 主要通过角度来判断：</p> 
<pre><code class="prism language-python"><span class="token builtin">float</span> Angle<span class="token punctuation">(</span>NormalizedLandmark ver1<span class="token punctuation">,</span> NormalizedLandmark ver2<span class="token punctuation">,</span> NormalizedLandmark ver3<span class="token punctuation">,</span> NormalizedLandmark ver4<span class="token punctuation">)</span>
<span class="token punctuation">{<!-- --></span> 
	<span class="token keyword">return</span> Vector3<span class="token punctuation">.</span>Angle<span class="token punctuation">(</span>new Vector3<span class="token punctuation">(</span>ver1<span class="token punctuation">.</span>X<span class="token punctuation">,</span> ver1<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> ver1<span class="token punctuation">.</span>Z<span class="token punctuation">)</span> <span class="token operator">-</span> new Vector3<span class="token punctuation">(</span>ver2<span class="token punctuation">.</span>X<span class="token punctuation">,</span> ver2<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> ver2<span class="token punctuation">.</span>Z<span class="token punctuation">)</span><span class="token punctuation">,</span>
                new Vector3<span class="token punctuation">(</span>ver3<span class="token punctuation">.</span>X<span class="token punctuation">,</span> ver3<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> ver3<span class="token punctuation">.</span>Z<span class="token punctuation">)</span> <span class="token operator">-</span> new Vector3<span class="token punctuation">(</span>ver4<span class="token punctuation">.</span>X<span class="token punctuation">,</span> ver4<span class="token punctuation">.</span>Y<span class="token punctuation">,</span> ver4<span class="token punctuation">.</span>Z<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h4><a id="_31"></a>多人姿态识别方向探索</h4> 
<p>Mediapipe 在人脸上是支持多人的，但是在姿态识别上目前只支持单人。在实验了网上能搜到的各种方案之后，有一种方案目前是可行的，但是在性能上会比较卡顿。<br> 既然Mediapipe支持单人，那我们把视频画面中的多人画面拆分成多个单人就行了。<br> 这里我采用的是OpenVINO中的行人检测模型。（我尝试了多种方案，YOLO-unity、Barracuda-Image-Classification和OpenVINO后发现OpenVINO效果最佳）OpenVINO ToolKit是英特尔发布的一套深度学习推断引擎，支持各种网络框架。对此不熟悉的同学可以参考<a href="https://mp.weixin.qq.com/s?__biz=MzA4MDExMDEyMw==&amp;mid=2247485999&amp;idx=2&amp;sn=e6136823390b8ea64504897d7c2036f7&amp;chksm=9fa87d6ba8dff47dabbb59894cd1f64794b3b80e4f0cc2e4691161f80485d6bd3092f06da383&amp;scene=21#wechat_redirect" rel="nofollow">OpenVINO开发系列文章汇总</a>进行较为系统的学习。<br> <img src="https://images2.imgbox.com/9a/c2/oTQthPwW_o.png" alt="在这里插入图片描述"><br> 具体思路就是通过OpenVINO识别出人物范围框，然后使用Opencv进行图像分割，单人图像传递给Mediapipe sdk去做单人姿态识别，然后进行汇总。目前该方案在移动端测试效果较为卡顿，不是很理想。</p> 
<h3><a id="PoseNet_38"></a>PoseNet</h3> 
<p>PoseNet是TensorFlow和谷歌创意实验室合作发布的专门用于姿态估计的一种技术方案。PoseNet可用于估计单个姿势或多个姿势，该算法有一个版本可以仅检测图像/视频中的一个人，另一个版本可以检测图像/视频中的多个人。<br> 流程主要有两个阶段：</p> 
<ol><li>输入RGB 图像通过卷积神经网络馈送。</li><li>单姿势或多姿势解码算法用于解码来自模型输出的姿势、姿势置信度分数、关键点位置 和关键点置信度分数。</li></ol> 
<p>同Mediapipe一样，我们来看看PoseNet给到我们的关键姿势点。PoseNet提供了17个关键点，不同于Mediapipe提供的3d数据点，PoseNet提供的是关键点的2d坐标，x和y。以及关键点可信度分数，使用者可以根据实际情况去做判断，范围在0.0-1.0之间，越接近1.0表示识别出来的点越正确。<br> <img src="https://images2.imgbox.com/5a/0e/HpqLMANH_o.png" alt="在这里插入图片描述"><br> ok，我们来看下我把posenet这一套通过tf-lite加载的形式在unity上的呈现效果。<br> <img src="https://images2.imgbox.com/cd/6a/ijH4gk3K_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="MoveNet_50"></a>MoveNet</h3> 
<p>MoveNet是一种超快速且准确的模型，可检测身体的 17 个关键点。该模型在TF Hub上提供，有两种变体，称为 Lightning 和 Thunder。Lightning 适用于延迟关键的应用程序，而 Thunder 适用于需要高精度的应用程序。在大多数现代台式机、笔记本电脑和手机上，这两种模型的运行速度都比实时 (30+ FPS) 快，这对于现场健身、健康和保健应用至关重要。</p> 
<p>我们可以看到movenet的识别出来的点的效果如下：<br> <img src="https://images2.imgbox.com/98/0f/cP2gTyKk_o.png" alt="在这里插入图片描述"><br> 官方从tf hub 加载模型代码：</p> 
<pre><code class="prism language-python">model_name <span class="token operator">=</span> <span class="token string">"movenet_lightning"</span>

<span class="token keyword">if</span> <span class="token string">"tflite"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
  <span class="token keyword">if</span> <span class="token string">"movenet_lightning_f16"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    !wget <span class="token operator">-</span>q <span class="token operator">-</span>O model<span class="token punctuation">.</span>tflite https<span class="token punctuation">:</span><span class="token operator">//</span>tfhub<span class="token punctuation">.</span>dev<span class="token operator">/</span>google<span class="token operator">/</span>lite<span class="token operator">-</span>model<span class="token operator">/</span>movenet<span class="token operator">/</span>singlepose<span class="token operator">/</span>lightning<span class="token operator">/</span>tflite<span class="token operator">/</span>float16<span class="token operator">/</span><span class="token number">4</span>?lite<span class="token operator">-</span><span class="token builtin">format</span><span class="token operator">=</span>tflite
    input_size <span class="token operator">=</span> <span class="token number">192</span>
  <span class="token keyword">elif</span> <span class="token string">"movenet_thunder_f16"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    !wget <span class="token operator">-</span>q <span class="token operator">-</span>O model<span class="token punctuation">.</span>tflite https<span class="token punctuation">:</span><span class="token operator">//</span>tfhub<span class="token punctuation">.</span>dev<span class="token operator">/</span>google<span class="token operator">/</span>lite<span class="token operator">-</span>model<span class="token operator">/</span>movenet<span class="token operator">/</span>singlepose<span class="token operator">/</span>thunder<span class="token operator">/</span>tflite<span class="token operator">/</span>float16<span class="token operator">/</span><span class="token number">4</span>?lite<span class="token operator">-</span><span class="token builtin">format</span><span class="token operator">=</span>tflite
    input_size <span class="token operator">=</span> <span class="token number">256</span>
  <span class="token keyword">elif</span> <span class="token string">"movenet_lightning_int8"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    !wget <span class="token operator">-</span>q <span class="token operator">-</span>O model<span class="token punctuation">.</span>tflite https<span class="token punctuation">:</span><span class="token operator">//</span>tfhub<span class="token punctuation">.</span>dev<span class="token operator">/</span>google<span class="token operator">/</span>lite<span class="token operator">-</span>model<span class="token operator">/</span>movenet<span class="token operator">/</span>singlepose<span class="token operator">/</span>lightning<span class="token operator">/</span>tflite<span class="token operator">/</span>int8<span class="token operator">/</span><span class="token number">4</span>?lite<span class="token operator">-</span><span class="token builtin">format</span><span class="token operator">=</span>tflite
    input_size <span class="token operator">=</span> <span class="token number">192</span>
  <span class="token keyword">elif</span> <span class="token string">"movenet_thunder_int8"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    !wget <span class="token operator">-</span>q <span class="token operator">-</span>O model<span class="token punctuation">.</span>tflite https<span class="token punctuation">:</span><span class="token operator">//</span>tfhub<span class="token punctuation">.</span>dev<span class="token operator">/</span>google<span class="token operator">/</span>lite<span class="token operator">-</span>model<span class="token operator">/</span>movenet<span class="token operator">/</span>singlepose<span class="token operator">/</span>thunder<span class="token operator">/</span>tflite<span class="token operator">/</span>int8<span class="token operator">/</span><span class="token number">4</span>?lite<span class="token operator">-</span><span class="token builtin">format</span><span class="token operator">=</span>tflite
    input_size <span class="token operator">=</span> <span class="token number">256</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported model name: %s"</span> <span class="token operator">%</span> model_name<span class="token punctuation">)</span>

  <span class="token comment"># Initialize the TFLite interpreter</span>
  interpreter <span class="token operator">=</span> tf<span class="token punctuation">.</span>lite<span class="token punctuation">.</span>Interpreter<span class="token punctuation">(</span>model_path<span class="token operator">=</span><span class="token string">"model.tflite"</span><span class="token punctuation">)</span>
  interpreter<span class="token punctuation">.</span>allocate_tensors<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">movenet</span><span class="token punctuation">(</span>input_image<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """</span>
    <span class="token comment"># TF Lite format expects tensor type of uint8.</span>
    input_image <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>input_image<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    input_details <span class="token operator">=</span> interpreter<span class="token punctuation">.</span>get_input_details<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output_details <span class="token operator">=</span> interpreter<span class="token punctuation">.</span>get_output_details<span class="token punctuation">(</span><span class="token punctuation">)</span>
    interpreter<span class="token punctuation">.</span>set_tensor<span class="token punctuation">(</span>input_details<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'index'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> input_image<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># Invoke inference.</span>
    interpreter<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># Get the model prediction.</span>
    keypoints_with_scores <span class="token operator">=</span> interpreter<span class="token punctuation">.</span>get_tensor<span class="token punctuation">(</span>output_details<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'index'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> keypoints_with_scores

<span class="token keyword">else</span><span class="token punctuation">:</span>
  <span class="token keyword">if</span> <span class="token string">"movenet_lightning"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    module <span class="token operator">=</span> hub<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"https://tfhub.dev/google/movenet/singlepose/lightning/4"</span><span class="token punctuation">)</span>
    input_size <span class="token operator">=</span> <span class="token number">192</span>
  <span class="token keyword">elif</span> <span class="token string">"movenet_thunder"</span> <span class="token keyword">in</span> model_name<span class="token punctuation">:</span>
    module <span class="token operator">=</span> hub<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"https://tfhub.dev/google/movenet/singlepose/thunder/4"</span><span class="token punctuation">)</span>
    input_size <span class="token operator">=</span> <span class="token number">256</span>
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported model name: %s"</span> <span class="token operator">%</span> model_name<span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">movenet</span><span class="token punctuation">(</span>input_image<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Runs detection on an input image.

    Args:
      input_image: A [1, height, width, 3] tensor represents the input image
        pixels. Note that the height/width should already be resized and match the
        expected input resolution of the model before passing into this function.

    Returns:
      A [1, 1, 17, 3] float numpy array representing the predicted keypoint
      coordinates and scores.
    """</span>
    model <span class="token operator">=</span> module<span class="token punctuation">.</span>signatures<span class="token punctuation">[</span><span class="token string">'serving_default'</span><span class="token punctuation">]</span>

    <span class="token comment"># SavedModel format expects tensor type of int32.</span>
    input_image <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>input_image<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    <span class="token comment"># Run model inference.</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>input_image<span class="token punctuation">)</span>
    <span class="token comment"># Output is a [1, 1, 17, 3] tensor.</span>
    keypoints_with_scores <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token string">'output_0'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> keypoints_with_scores
</code></pre> 
<p>实际生产过程中，movenet主要通过tf-lite去加载计算，可以分为单人和多人。tf-hub地址如下：<a href="https://tfhub.dev/s?q=movenet" rel="nofollow">地址</a>。测试发现，从帧率和效果上来说，movenet比mediapipe效果要好一点。<br> 但是在unity使用中，我发现movenet的tf-lite模型在unity中遇到了unity barracuda插件转换的问题，单人的tf-lite可用，但是多人无法转换。已知是unity barracuda插件问题，已提交issues，但目前还没走通。</p> 
<p>不熟悉unity barracuda的同学可以移步<a href="https://docs.unity3d.com/Packages/com.unity.barracuda@1.0/manual/index.html" rel="nofollow">这里</a><br> 主要用于在untiy中集成神经网络算法。</p> 
<h3><a id="OpenPose_142"></a>OpenPose</h3> 
<p>OpenPose是我在github上搜到的一个人体姿态识别的一个算法方案。<br> 主要功能：</p> 
<ol><li>2D实时多人关键点检测：15、18 或25 关键点身体/足部关键点估计，包括6 个足部关键点。运行时不受检测到的人数的影响。2x21-keypoint 手部关键点估计。运行时间取决于检测到的人数。70个关键点人脸关键点估计。运行时间取决于检测到的人数。</li><li>3D实时单人关键点检测</li></ol> 
<p>其实可以理解成在效果产出和姿态识别角度来说，是mediapipe和posenet的综合体。</p> 
<p>效果如下：<br> <img src="https://images2.imgbox.com/ed/cd/ZEIYIPyf_o.png" alt="在这里插入图片描述"></p> 
<h3><a id="OpenMMD_155"></a>OpenMMD</h3> 
<p>OpenMMD是一个可以直接分析现成视频（各种MP4, AVI等视频格式），自动生成vmd动作文件的工具。<br> 这个方案无法做到实时转换，即使你的输入源是摄像头的话，也必须录完以后生成动捕文件，然后再把动捕文件绑定到模型上才可以完成。<br> OpenMMD具体使用教程在b站上有一个大佬总结的很清楚了，但有一个问题，他只能绑定在mmd模型上，mmd模型在游戏里的通用性并不大，属于比较小范围的应用。尝试了几种模型之间互转的方案，Blender等，但无果，转换起来没那么简单。<br> 这是我迁移后的效果，左边是mmd模型，成功跑通。<br> <img src="https://images2.imgbox.com/a1/84/Wf2uz2p3_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_163"></a>总结</h2> 
<p>以上五种方案，简单总结如下：<br> <img src="https://images2.imgbox.com/14/1c/dAyFblTR_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_168"></a>参考链接</h2> 
<ol><li><a href="https://arxiv.org/abs/1907.05047" rel="nofollow">BlazeFace</a></li><li><a href="https://google.github.io/mediapipe/solutions/pose.html" rel="nofollow">Mediapipe Pose</a></li><li><a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" rel="nofollow">PoseNet</a></li><li><a href="https://www.tensorflow.org/hub/tutorials/movenet" rel="nofollow">MoveNet</a></li><li><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a></li><li><a href="https://github.com/peterljq/OpenMMD">OpenMMD</a></li></ol> 
<p>个人微信公共账号已上线，欢迎关注：<br> <img src="https://images2.imgbox.com/1f/f4/LUOl8Dj3_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/b8a924adf680274042de90c46ae31836/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PCL使用中遇到的常见问题以及解决办法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/399cff1a656ea9b1b53f4b2458e3fc34/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">VUE-PDF 实现pdf在线预览</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>