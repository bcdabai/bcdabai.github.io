<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>K8S监控（Prometheus&#43;Grafana&#43;alertmanager&#43;钉钉机器人告警）部署方案 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="K8S监控（Prometheus&#43;Grafana&#43;alertmanager&#43;钉钉机器人告警）部署方案" />
<meta property="og:description" content="架构就不细讲，网上一大堆都大同小异，但是自己在部署的过程中发现踩了很多坑，记录一下整个部署过程，开干！
一、部署K8S集群资源数据采集组件：kube-state-metrics yaml文件一共有5个：
1、cluster-role-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.3.0 name: kube-state-metrics roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: kube-system 2、cluster-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: exporter app.kubernetes.io/name: kube-state-metrics app.kubernetes.io/version: 2.3.0 name: kube-state-metrics rules: - apiGroups: - &#34;&#34; resources: - configmaps - secrets - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: - list - watch - apiGroups: - apps resources: - statefulsets - daemonsets - deployments - replicasets verbs: - list - watch - apiGroups: - batch resources: - cronjobs - jobs verbs: - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - list - watch - apiGroups: - authentication." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/ccb69e62c7860fa2f87d8d14c1038921/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-17T15:33:02+08:00" />
<meta property="article:modified_time" content="2023-10-17T15:33:02+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">K8S监控（Prometheus&#43;Grafana&#43;alertmanager&#43;钉钉机器人告警）部署方案</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p><span style="color:#0d0016;">架构就不细讲，网上一大堆都大同小异，但是自己在部署的过程中发现踩了很多坑，记录一下整个部署过程，开干！</span></p> 
<h3><span style="color:#0d0016;">一、部署K8S集群资源数据采集组件：kube-state-metrics</span></h3> 
<p><span style="color:#0d0016;">yaml文件一共有5个：</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="270" src="https://images2.imgbox.com/72/70/KqtszMS3_o.png" width="580"></span></p> 
<h4><span style="color:#0d0016;"><strong>1、cluster-role-binding.yaml</strong></span></h4> 
<pre><code class="hljs">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/version: 2.3.0
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: kube-system</code></pre> 
<h4><span style="color:#0d0016;"><strong>2、cluster-role.yaml </strong></span></h4> 
<pre><code class="hljs">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/version: 2.3.0
  name: kube-state-metrics
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  - nodes
  - pods
  - services
  - resourcequotas
  - replicationcontrollers
  - limitranges
  - persistentvolumeclaims
  - persistentvolumes
  - namespaces
  - endpoints
  verbs:
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets
  - daemonsets
  - deployments
  - replicasets
  verbs:
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - cronjobs
  - jobs
  verbs:
  - list
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - list
  - watch
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - list
  - watch
- apiGroups:
  - certificates.k8s.io
  resources:
  - certificatesigningrequests
  verbs:
  - list
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  - volumeattachments
  verbs:
  - list
  - watch
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - mutatingwebhookconfigurations
  - validatingwebhookconfigurations
  verbs:
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  - ingresses
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch</code></pre> 
<h4><span style="color:#0d0016;"><strong>3、deployment.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/version: 2.3.0
  name: kube-state-metrics
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  template:
    metadata:
      labels:
        app.kubernetes.io/component: exporter
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/version: 2.3.0
    spec:
      automountServiceAccountToken: true
      imagePullSecrets:
        - name: image-pull-secret
      containers:
      - image: # 我自己的私有镜像仓库/kube-state-metrics:v2.3.0，改镜像然后pull，往下看
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        name: kube-state-metrics
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 8081
          name: telemetry
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 65534
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: kube-state-metrics</code></pre> 
<h4><span style="color:#0d0016;"><strong>4、service-account.yaml</strong></span></h4> 
<pre><code class="hljs">apiVersion: v1
automountServiceAccountToken: false
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/version: 2.3.0
  name: kube-state-metrics
  namespace: kube-system</code></pre> 
<h4><span style="color:#0d0016;"><strong>5、service.yaml</strong></span></h4> 
<pre><code class="hljs">apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: exporter
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/version: 2.3.0
  name: kube-state-metrics
  namespace: kube-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
    targetPort: http-metrics
  - name: telemetry
    port: 8081
    targetPort: telemetry
  selector:
    app.kubernetes.io/name: kube-state-metrics</code></pre> 
<h4><span style="color:#0d0016;"><strong>6、以上文件准备好了，就可以执行deploy了</strong></span></h4> 
<blockquote> 
 <p><span style="color:#0d0016;">需要注意的是镜像k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0<br> 这个国内无法下载，需要通过国内源下载，然后tag打包push到自己的私有仓库<br> docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.0<br> docker tag 更换仓库地址<br> docker push 镜像</span></p> 
 <p><span style="color:#0d0016;">kubectl apply -f cluster-role-binding.yaml<br> kubectl apply -f cluster-role.yaml<br> kubectl apply -f deployment.yaml<br> kubectl apply -f service-account.yaml<br> kubectl apply -f service.yaml</span></p> 
</blockquote> 
<h3> 二、部署k8s集群节点监控</h3> 
<h4>1、下载</h4> 
<p><a href="https://github.com/prometheus/node_exporter/releases" title="Releases · prometheus/node_exporter · GitHub">Releases · prometheus/node_exporter · GitHub</a>下载安装包</p> 
<p>我这里使用旧版本：node_exporter-0.18.1.linux-amd64.tar.gz</p> 
<h4>2、安装</h4> 
<p>将安装包上传到k8s集群节点的/tmp目录，创建一下shell脚本并执行即可完成安装</p> 
<p>install_node_exporter.sh</p> 
<pre><code class="language-bash">#!/bin/bash


mkdir -p /usr/local/software/node-exporter
# Step 1: Download Node Exporter
mv /tmp/node_exporter-0.18.1.linux-amd64.tar.gz  /usr/local/software/node-exporter

# Step 2: Extract Node Exporter
cd /usr/local/software/node-exporter/
tar xvfz node_exporter-0.18.1.linux-amd64.tar.gz

# Step 3: Change to Node Exporter directory
cd node_exporter-0.18.1.linux-amd64

# Step 4: Copy Node Exporter binary to /usr/local/bin
sudo cp node_exporter /usr/local/bin/

# Step 5: Create systemd service unit file
sudo tee /etc/systemd/system/node_exporter.service &gt; /dev/null &lt;&lt;EOT
[Unit]
Description=Node Exporter
After=network.target

[Service]
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=default.target
EOT

# Step 6: Enable and start Node Exporter service
sudo systemctl enable node_exporter
sudo systemctl start node_exporter

# Step 7: Check Node Exporter status
sudo systemctl status node_exporter</code></pre> 
<p></p> 
<h3><span style="color:#0d0016;">三、部署prmoetheus</span></h3> 
<p><span style="color:#0d0016;">yaml文件7份、配置文件1份、告警规则文件1份</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="440" src="https://images2.imgbox.com/c7/eb/B6hfKJws_o.png" width="660"></span></p> 
<h4 style="background-color:transparent;"><span style="color:#0d0016;"><strong> 1、clusterRole.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: default
  namespace: monitoring</code></pre> 
<h4><span style="color:#0d0016;"><strong>2、prometheus--acme-tls.yml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: Secret
metadata:
  name: prometheus--acme-tls
  namespace: monitoring
type: kubernetes.io/tls
data:
  ca.crt: ""
  tls.crt: #域名证书SSL的crt 方法是： cat 证书.crt | base64
  tls.key: #域名证书SSL的key 方法是： cat 证书.key | base64</code></pre> 
<h4><span style="color:#0d0016;"><strong>3、prometheus-deployment.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
  labels:
    app: prometheus-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
            - --storage.tsdb.retention=60d
            - --web.enable-lifecycle
            - --web.enable-admin-api
          ports:
            - containerPort: 9090
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-storage-volume
            subPath: prometheus/conf
          - mountPath: /prometheus
            name: prometheus-storage-volume
            subPath: prometheus/data
      volumes:
        - name: prometheus-storage-volume
          persistentVolumeClaim:
            claimName: pvc-nas-prometheus
  </code></pre> 
<h4><span style="color:#0d0016;"><strong>4、prometheus-ingress.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/whitelist-source-range: "#需要加白名单的ip，无加白无法访问"
spec:
  tls:
  - hosts:
    # 自定义域名，访问prometheus用到
    - prometheus.xxx.xxx.com
    secretName: prometheus--acme-tls
  rules:
  - host: prometheus.xxx.xxx.com
    http:
      paths:
      - path: /
        backend:
          serviceName: prometheus-service
          servicePort: 8080</code></pre> 
<h4><span style="color:#0d0016;"><strong>5、prometheus-service.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: Service
metadata:
  name: prometheus-service
  namespace: monitoring
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port:   '9090'
  
spec:
  selector: 
    app: prometheus-server
  ports:
    - port: 8080
      targetPort: 9090 
</code></pre> 
<h4><span style="color:#0d0016;"><strong>6、10-pv.yml （这里使用的阿里云的Nas，可以根据实际情况创建pv与pvc）</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nas-prometheus
  labels:
    alicloud-pvname: pv-nas-prometheus
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  csi:
    driver: nasplugin.csi.alibabacloud.com
    volumeHandle: pv-nas-prometheus
    volumeAttributes:
      server: "xxxxxxxx.cn-shenzhen.nas.aliyuncs.com"
      path: "/prometheus"
  mountOptions:
  - nolock,tcp,noresvport
  - vers=3</code></pre> 
<h4><span style="color:#0d0016;"><strong>7、20-pvc.yml</strong></span></h4> 
<pre><code class="language-bash">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-nas-prometheus
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
  selector:
    matchLabels:
      alicloud-pvname: pv-nas-prometheus</code></pre> 
<h4><span style="color:#0d0016;">8、prometheus.rules（该文件放在创建的pv目录内,我这里是放在阿里云nas的/prometheus/prometheus/conf/）</span></h4> 
<pre><code class="language-bash">## CPU告警规则
groups:
- name: CpuAlertRule
  rules:
  - alert: PodCPU告警
    expr: (sum(rate(container_cpu_usage_seconds_total{image!="",pod!=""}[1m])) by (namespace, pod)) / (sum(container_spec_cpu_quota{image!="", pod!=""}) by(namespace, pod) / 100000) * 100 &gt; 80
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "CPU使用率大于80%"
      value: "{<!-- -->{$value}}%"
      #summary: 'CPU使用率大于80%，当前值为{<!-- -->{.Value}}%，CPU使用率: {<!-- -->{ printf `ceil(100 - ((avg by (instance)(irate(node_cpu_seconds_total{mode="idle",instance="%s"}[1m]))) *100))` $labels.instance | query | first | value }}%'
  - alert: NodeCPU告警
    expr: round(100-avg(irate(node_cpu_seconds_total{mode="idle"}[5m]))by(kubernetes_node)*100) &gt; 80
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "CPU使用率大于80%"
      value: "{<!-- -->{$value}}%"
      #summary: 'CPU使用率大于80%，当前值为{<!-- -->{.Value}}%，CPU使用率: {<!-- -->{ printf `ceil(100 - ((avg by (instance)(irate(node_cpu_seconds_total{mode="idle",instance="%s"}[1m]))) *100))` $labels.instance | query | first | value }}%'

## DISK告警规则
- name: DiskAlertRule
  rules:
  - alert: Node磁盘告警
    expr: round((1- node_filesystem_avail_bytes{fstype=~"ext.+|nfs.+",mountpoint!~".*docker.*"}/node_filesystem_size_bytes{fstype=~"ext.+|nfs.+",mountpoint!~".*docker.*"})*100) &gt; 85
    for: 1m
    labels:
      severity: warning
    annotations:
      description: "磁盘使用率大于85%"
      value: "{<!-- -->{$value}}%"

## MEM告警规则
- name: MemAlertRule
  rules:
  - alert: Pod内存告警
    expr: sum(container_memory_working_set_bytes{image!=""}) by(namespace, pod) / sum(container_spec_memory_limit_bytes{image!=""}) by(namespace, pod) * 100 != +inf &gt; 85
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "内存使用率大于85%"
      value: "{<!-- -->{$value}}%"
  - alert: Node内存告警
    expr: round(100-((node_memory_MemAvailable_bytes*100)/node_memory_MemTotal_bytes)) &gt; 80
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "内存使用率大于85%"
      value: "{<!-- -->{$value}}%"

## Pod意外重启
- name: PodRestartAlertRule
  rules:
  - alert: Pod重启告警
    expr: delta(kube_pod_container_status_restarts_total[1m]) &gt; 0
    for: 1s
    labels:
      severity: warning
    annotations:
      description: "Pod发生意外重启事件"

## JvmCMSOldGC
- name: PodJvmOldGCAlertRule
  rules:
  - alert: PodJvmCMSOldGC
    expr: round((jvm_memory_pool_bytes_used{pool=~".+Old Gen"}/jvm_memory_pool_bytes_max{pool=~".+Old Gen"})*100) &gt; 89
    for: 5s
    labels:
      severity: warning
    annotations:
      description: "Pod堆内存触发CMSOldGC"
      value: "{<!-- -->{$value}}%"

## Pod实例异常
- name: ContainerInstanceAlertRule
  rules:
  - alert: Pod实例异常
    expr: kube_pod_container_status_ready - kube_pod_container_status_running &gt; 0
    for: 20s
    labels:
      severity: warning
    annotations:
      description: "Container实例异常"

## Pod实例OOM
- name: ContainerOOMAlertRule
  rules:
  - alert: Pod实例OOM
    expr: kube_pod_container_status_terminated_reason{reason="OOMKilled"} &gt; 0
    for: 1s
    labels:
      severity: warning
    annotations:
      description: "Container实例OOM"

## Pod实例驱逐
- name: ContainerEvictionAlertRule
  rules:
  - alert: Pod实例驱逐
    expr: kube_pod_container_status_terminated_reason{reason="Evicted"} &gt; 0
    for: 1s
    labels:
      severity: warning
    annotations:
      description: "Container实例驱逐"

## MQ内存告警
- name: MQMemoryAlertRule
  rules:
  - alert: MQ内存水位线
    expr: rabbitmq_node_mem_alarm{job=~".*rabbitmq.*"} == 1
    for: 1s
    labels:
      severity: warning
    annotations:
      description: "RabbitMQ内存高水位线告警"
      summary: RabbitMQ {<!-- -->{`{<!-- -->{ $labels.instance }}`}} High Memory Alarm is going off.  Which means the node hit highwater mark and has cut off network connectivity, see RabbitMQ WebUI
  - alert: MQ内存使用告警
    expr: round(avg(rabbitmq_node_mem_used{job=~".*rabbitmq.*"} / rabbitmq_node_mem_limit{job=~".*rabbitmq.*"})by(node,kubernetes_namespace)*100) &gt; 90
    for: 10s
    labels:
      severity: warning
    annotations:
      description: "RabbitMQ使用告警"
      value: "{<!-- -->{$value}}%"
      summary: RabbitMQ {<!-- -->{`{<!-- -->{ $labels.instance }}`}} Memory Usage &gt; 90%

##PodJava进程异常
- name: PodJavaProcessAlertRule
  rules:
  - alert: PodJava进程异常
    expr: sum(up{job="kubernetes-pods-jvm"})by(kubernetes_container_name,kubernetes_pod_name) == 0
    for: 10s
    labels:
      severity: warning
    annotations:
      description: "PodJava进程异常"
      summary: "赶快看看吧，顶不住了"</code></pre> 
<h4><span style="color:#0d0016;"><strong>9、prometheus.yml （该文件放在创建的pv目录内,我这里是放在阿里云nas的/prometheus/prometheus/conf/）</strong></span></h4> 
<pre><code class="language-bash">    global:
      scrape_interval: 5s
      evaluation_interval: 5s
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager.monitoring.svc.cluster.local:9093"
    scrape_configs:
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_endpoints_name]
          regex: 'node-exporter'
          action: keep
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.kube-system.svc.cluster.local:8080']
      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
## 节点监控，根据实际情况编写
      - job_name: 'k8s-pro'
        static_configs:
        - targets: ['节点ip:9100']
          labels:
            instance: devops.105213.pro
        - targets: ['节点ip:9100']
          labels:
            instance: devops.104245.pro
        - targets: ['节点ip:9100']
          labels:
            instance: devops.104249.pro
        - targets: ['节点ip:9100']
          labels:
            instance: devops.105007.pro
        - targets: ['节点ip:9100']
          labels:
            instance: devops.105008.pro
        - targets: ['节点ip:9100']
          labels:
            instance: devops.104250.pro</code></pre> 
<h4><span style="color:#0d0016;">10、以上yaml文件准备好，即可deploy</span></h4> 
<blockquote> 
 <p><span style="color:#0d0016;">创建命名空间：kubectl create namespace monitoring</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  10-pv.yml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  20-pvc.yml</span></p> 
 <p><span style="color:#0d0016;">放文件到指定路径 prometheus.rules</span></p> 
 <p><span style="color:#0d0016;">放文件到指定路径 prometheus.yml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  clusterRole.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  prometheus-deployment.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  prometheus-service.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  prometheus--acme-tls.yml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f  prometheus-ingress</span></p> 
</blockquote> 
<p><span style="color:#0d0016;"> 验证：访问 https://prometheus.xxx.xxx.com </span></p> 
<p><span style="color:#0d0016;"><img alt="" height="985" src="https://images2.imgbox.com/56/b6/EMCqi99Z_o.png" width="1110"></span></p> 
<h3 style="background-color:transparent;"><span style="color:#0d0016;"> 四、部署altertmanager + webhook-dingtalk</span></h3> 
<p><span style="color:#0d0016;">部署需要准备yaml文件6份、配置文件1份、告警内容模版文件1份</span></p> 
<h4><span style="color:#0d0016;"><img alt="" height="486" src="https://images2.imgbox.com/b8/d6/yuN1KBEZ_o.png" width="850"><strong>1、10-pv.yml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nas-alertmanager
  labels:
    alicloud-pvname: pv-nas-alertmanager
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany
  csi:
    driver: nasplugin.csi.alibabacloud.com
    volumeHandle: pv-nas-alertmanager
    volumeAttributes:
      #使用了阿里云的nas，以实际为准
      server: "xxxx.cn-shenzhen.nas.aliyuncs.com"
      path: "/alertmanager"
  mountOptions:
  - nolock,tcp,noresvport
  - vers=3</code></pre> 
<h4><span style="color:#0d0016;"><strong>2、20-pvc.yml</strong></span></h4> 
<pre><code class="language-bash">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-nas-alertmanager
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
  selector:
    matchLabels:
      alicloud-pvname: pv-nas-alertmanager</code></pre> 
<h4><span style="color:#0d0016;"><strong>3、alertmanager--acme-tls.yml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: Secret
metadata:
  name: alertmanager--acme-tls
  namespace: monitoring
type: kubernetes.io/tls
data:
  ca.crt: ""
  tls.crt: # cat 域名证书ssl的crt文件|base64
  tls.key: # cat 域名证书ssl的key文件|base64</code></pre> 
<h4 style="background-color:transparent;"><span style="color:#0d0016;"><strong>4、alertmanager-deployment.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager-deployment
  namespace: monitoring
  labels:
    app: alertmanager-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager-server
  template:
    metadata:
      labels:
        app: alertmanager-server
    spec:
      containers:
        - name: alertmanager
          image: prom/alertmanager:latest
          args:
            - "--config.file=/etc/alertmanager/config.yml"
            - "--storage.path=/alertmanager/data"
            - --cluster.advertise-address=0.0.0.0:9093
          ports:
            - containerPort: 9093
              protocol: TCP
          volumeMounts:
          - mountPath: /etc/alertmanager
            name: alertmanager-storage-volume
            subPath: conf
          - mountPath: /alertmanager/data
            name: alertmanager-storage-volume
            subPath: data
      volumes:
        - name: alertmanager-storage-volume
          persistentVolumeClaim:
            claimName: pvc-nas-alertmanager</code></pre> 
<h4><span style="color:#0d0016;"><strong>5、alertmanager-ingress.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: alertmanager-service
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/whitelist-source-range: "填写需要加白的ip，无在白名单内无法访问的"
spec:
  tls:
  - hosts:
    - alert.xxx.com
    secretName: alertmanager--acme-tls
  rules:
  - host: alert.xxx.com
    http:
      paths:
      - path: /
        backend:
          serviceName: alertmanager
          servicePort: 9093</code></pre> 
<h4 style="background-color:transparent;"><span style="color:#0d0016;"><strong>6、alertmanager-service.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector: 
    app: alertmanager-server
  ports:
    - name: web
      port: 9093
      protocol: TCP
      targetPort: 9093</code></pre> 
<h4><span style="color:#0d0016;"><strong>7、config.yml (这里是配置钉钉机器人告警，没有配置邮件，如果需要按实际情况添加)</strong></span></h4> 
<pre><code class="language-bash">global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'severity', 'namespace']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 10s
  receiver: 'webhook'
  routes:
  - receiver: 'webhook'
    group_wait: 10s
    group_interval: 15s
    repeat_interval: 3h
templates:
- /etc/alertmanager/config/template.tmp1

receivers:
- name: 'webhook'
  webhook_configs:
  - url: 'http://webhook-dingtalk'
    send_resolved: true</code></pre> 
<h4><span style="color:#0d0016;"><strong>8、template.tmp1 （告警内容模版文件）</strong></span></h4> 
<pre><code class="language-bash">{<!-- -->{ define "wechat.default.message" }}
{<!-- -->{- if gt (len .Alerts.Firing) 0 -}}
{<!-- -->{- range $index, $alert := .Alerts -}}
{<!-- -->{- if eq $index 0 }}
===异常告警===
告警类型: {<!-- -->{ $alert.Labels.alertname }}
告警级别: {<!-- -->{ $alert.Labels.severity }}
告警详情: {<!-- -->{ $alert.Annotations.description}}
故障时间: {<!-- -->{ ($alert.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" }}
{<!-- -->{- if gt (len $alert.Labels.instance) 0 }}
实例信息: {<!-- -->{ $alert.Labels.instance }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.namespace) 0 }}
命名空间: {<!-- -->{ $alert.Labels.namespace }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.node) 0 }}
节点信息: {<!-- -->{ $alert.Labels.node }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.pod) 0 }}
实例名称: {<!-- -->{ $alert.Labels.pod }}
{<!-- -->{- end }}
===END===
{<!-- -->{- end }}
{<!-- -->{- end }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len .Alerts.Resolved) 0 -}}
{<!-- -->{- range $index, $alert := .Alerts -}}
{<!-- -->{- if eq $index 0 }}
===异常恢复===
告警类型: {<!-- -->{ $alert.Labels.alertname }}
告警级别: {<!-- -->{ $alert.Labels.severity }}
告警详情: {<!-- -->{ $alert.Annotations.description}}
故障时间: {<!-- -->{ ($alert.StartsAt.Add 28800e9).Format "2006-01-02 15:04:05" }}
恢复时间: {<!-- -->{ ($alert.EndsAt.Add 28800e9).Format "2006-01-02 15:04:05" }}
{<!-- -->{- if gt (len $alert.Labels.instance) 0 }}
实例信息: {<!-- -->{ $alert.Labels.instance }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.namespace) 0 }}
命名空间: {<!-- -->{ $alert.Labels.namespace }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.node) 0 }}
节点信息: {<!-- -->{ $alert.Labels.node }}
{<!-- -->{- end }}
{<!-- -->{- if gt (len $alert.Labels.pod) 0 }}
实例名称: {<!-- -->{ $alert.Labels.pod }}
{<!-- -->{- end }}
===END===
{<!-- -->{- end }}
{<!-- -->{- end }}
{<!-- -->{- end }}
{<!-- -->{- end }}</code></pre> 
<p><span style="color:#0d0016;">显示大致如下：</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="474" src="https://images2.imgbox.com/d7/a1/3IlWsLqj_o.png" width="714"></span></p> 
<h4><span style="color:#0d0016;"><strong> 9、webhook-dingtalk.yaml</strong></span></h4> 
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-dingtalk
  namespace: monitoring
  labels:
    app: webhook-dingtalk
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-dingtalk
  template:
    metadata:
      labels:
        app: webhook-dingtalk
    spec:
      containers:
        - name: webhook-dingtalk
          image: yangpeng2468/alertmanager-dingtalk-hook:v1
          env:
          - name: ROBOT_TOKEN
            valueFrom:
              secretKeyRef:
                name: dingtalk-secret
                key: token
          ports:
            - containerPort: 5000
              protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 500m
              memory: 500Mi

---

apiVersion: v1
kind: Service
metadata:
  labels:
    app: webhook-dingtalk
  name: webhook-dingtalk
  namespace: monitoring
  #需要和alertmanager在同一个namespace
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 5000
  selector:
    app: webhook-dingtalk
  type: ClusterIP</code></pre> 
<h4><span style="color:#0d0016;"><strong>10、在钉钉群创建机器人并获取token码</strong></span></h4> 
<p><span style="color:#0d0016;"><img alt="" height="1200" src="https://images2.imgbox.com/ff/df/By1snYVj_o.png" width="1200"></span></p> 
<p><span style="color:#0d0016;"><img alt="" height="1132" src="https://images2.imgbox.com/6f/80/Q70DRt6G_o.png" width="1200"></span></p> 
<h4><span style="color:#0d0016;"><strong>11、以上文件准备好，即可开始deploy</strong></span></h4> 
<blockquote> 
 <p><span style="color:#0d0016;">创建钉钉机器人的token文件,xxxxxxxxxx 为钉钉机器人的token：</span></p> 
 <p><span style="color:#0d0016;">kubectl create secret generic dingtalk-secret --from-literal=token=xxxxxxxxxx -n monitoring </span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f webhook-dingtalk.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f 10-pv.yml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f 20-pvc.yml</span></p> 
 <p><span style="color:#0d0016;">config.yml 文件放在创建的nas pv路径下的/alertmanager/conf</span></p> 
 <p><span style="color:#0d0016;">template.tmp1 文件放在创建的nas pv路径下的/alertmanager/conf/config/</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f alertmanager-deployment.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f alertmanager--acme-tls.yml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f alertmanager-service.yaml</span></p> 
 <p><span style="color:#0d0016;">kubectl deploy -f alertmanager-ingress.yaml</span></p> 
</blockquote> 
<p><span style="color:#0d0016;"> 访问web https://alert.xxx.com 验证</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="490" src="https://images2.imgbox.com/97/61/HHs9Whq5_o.png" width="1200"></span></p> 
<p><span style="color:#0d0016;">至此，数据采集以及告警都已经部署完成，接下来部署grafana展示数据</span></p> 
<h3 style="background-color:transparent;"><span style="color:#0d0016;"><strong>五、部署Grafana</strong></span></h3> 
<p><span style="color:#0d0016;">由于我实际环境与架构设计的原因，Grafana没有部署在k8s集群中，使用docker部署了，如果需要部署到k8s，可以将docker-compose文件转换成k8s的yaml文件部署即可。</span></p> 
<h4><span style="color:#0d0016;"><strong>1、docker-comepose.yml</strong></span></h4> 
<pre><code class="language-bash">version: "3"
services:
  grafana:
    image: grafana/grafana:8.1.5
    container_name: grafana
    restart: always
    network_mode: "host"
    # ports:
    #   - 3000:3000
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=密码
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_SECURITY_ALLOW_EMBEDDING=true  
    volumes:
      - /etc/localtime:/etc/localtime:ro
      -  /data/volumes/monitor/grafana:/var/lib/grafana:z
      # 这里可以把grafana的配置文件挂载出来，我本次部署还没对这里动手
      # - /data/volumes/monitor/grafana-cfg/grafana.ini:/etc/grafana/grafana.ini:z </code></pre> 
<p><span style="color:#0d0016;">访问web http://ip:3000 输入账号密码登录</span></p> 
<h4><span style="color:#0d0016;">2、添加prometheus数据源地址</span></h4> 
<p><span style="color:#0d0016;"><img alt="" height="558" src="https://images2.imgbox.com/6e/c7/Ucv60GA3_o.png" width="1200"><img alt="" height="795" src="https://images2.imgbox.com/e0/2c/vhoaiAES_o.png" width="1200"></span></p> 
<h4><span style="color:#0d0016;">3、导入k8s资源监控grafana模版</span></h4> 
<p><span style="color:#0d0016;">这里我是用了模版编号是：13105</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="455" src="https://images2.imgbox.com/5d/8f/LC7Vv8LZ_o.png" width="681"></span></p> 
<p><span style="color:#0d0016;"><img alt="" height="657" src="https://images2.imgbox.com/9e/0d/Tf0ZatzY_o.png" width="923"></span></p> 
<p><span style="color:#0d0016;">接下来根据实际情况去修改模版里面的prometheus语句等等操作了，这里就不一一介绍了。</span></p> 
<p><span style="color:#0d0016;">具体展示如下</span></p> 
<p><span style="color:#0d0016;"><img alt="" height="1018" src="https://images2.imgbox.com/50/cd/yBaM3VPb_o.png" width="1200"></span></p> 
<h3 style="background-color:transparent;"><span style="color:#0d0016;"><strong>六、踩坑记录</strong></span></h3> 
<h4><span style="color:#0d0016;">1、部署过程中prometheus告警调用alertmanager的时候报错</span></h4> 
<p><span style="color:#0d0016;">报错内容：</span></p> 
<blockquote> 
 <p><span style="color:#0d0016;">http://alertmanager.monitoring.svc.cluster.local:9093/api/v2/alerts count=1 msg="error sending alert" err="bad response status 404 not found"</span></p> 
</blockquote> 
<p><span style="color:#0d0016;">排查发现原来是alertmanager版本问题，我之前用的是<a href="https://hub.docker.com/layers/prom/alertmanager/v0.15.1/images/sha256-90f439897fb1efec31637452e25c97443957535fad39726d28b891a3bef7c451?context=explore" rel="nofollow" title="v0.15.1">v0.15.1</a>，解决方法更换alertmanager镜像为prom/alertmanager:latest就解决了。</span></p> 
<h4><span style="color:#0d0016;">2、部署过程中alertmanager调用webhook-dingtalk报错</span></h4> 
<p><span style="color:#0d0016;">报错内容：</span></p> 
<blockquote> 
 <p><span style="color:#0d0016;">error in app: exception on /dingtalk/send/ </span></p> 
</blockquote> 
<p><span style="color:#0d0016;"> 直接弃用使用的镜像 billy98/webhook-dingtalk:latest和端口8080，解决方法就是更换上述部署yaml文件即可。</span></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/0dfdd6df02418c268b923bd76a1fc143/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">超全教程丨如何正确使用java线程池</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/0fd378795aa88458dd99c18a852eb05b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">银行家算法-避免死锁的一个算法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>