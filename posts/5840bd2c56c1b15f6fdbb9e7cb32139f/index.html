<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>首个多视角自动驾驶场景视频生成世界模型 | DrivingDiffusion: BEV数据和仿真新思路... - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="首个多视角自动驾驶场景视频生成世界模型 | DrivingDiffusion: BEV数据和仿真新思路..." />
<meta property="og:description" content="点击下方卡片，关注“自动驾驶之心”公众号
ADAS巨卷干货，即可获取
&gt;&gt;自动驾驶之心【大模型】技术交流群
笔者的一些个人思考 在自动驾驶领域，随着BEV-based子任务/端到端方案的发展，高质量的多视图训练数据和相应的仿真场景构建愈发重要。针对当下任务的痛点，“高质量”可以解耦成三个方面：
不同维度上的长尾场景：如障碍物数据中近距离的车辆以及切车过程中精准的朝向角，以及车道线数据中不同曲率的弯道或较难采集的匝道/汇入/合流等场景。这些往往靠大量的数据采集和复杂的数据挖掘策略，成本高昂。
3D真值-图像的高度一致：当下的BEV数据获取往往受到传感器安装/标定，高精地图以及重建算法本身的误差影响。这导致了我们很难保证数据中的每一组【3D真值-图像-传感器参数】 的精确一致。
满足上述条件基础上的时序数据：连续帧的多视角图像和相应真值，这对于当前的感知/预测/决策/端到端等任务都是必不可少的。
而对仿真来说，可以直接通过布局进行满足上述条件的视频生成，无疑是最直接的multi-agent传感器输入的构造方式。而DrivingDiffusion则从一个新的角度解决了上述问题。
什么是DrivingDiffusion？ DrivingDiffusion是一个用于自动驾驶场景生成的扩散模型框架，实现了布局控制的多视角图像/视频生成并分别实现了SOTA。
DrivingDiffusion-Future作为自动驾驶世界模型有根据单帧图像预测未来场景视频并根据语言提示影响主车/他车运动规划的能力。
DrivingDiffusion生成效果是怎么样的？ 有需要的同学可以先看看项目主页：https://drivingdiffusion.github.io
（1）DrivingDiffusion 布局控制的多视角图像生成
图中展示了以布局投影作为输入的multi-view图像生成效果。
调整布局：精确控制生成结果
图中上半部分展示了生成结果的多样性以及下文中模块设计的重要性。下半部分展示了对正后方的车辆进行扰动的结果，包含移动，转向，碰撞甚至悬浮在空中的场景的生成效果。
布局控制的多视角视频生成
上：DrivingDiffusion在nuScenes数据上训练后的视频生成结果。下：DrivingDiffusion在大量私有真实数据上训练后的视频生成结果。
（2）DrivingDiffusion-Future 根据输入帧&#43;文本描述生成后续帧
使用单帧图像作为输入，根据对主车/他车的文本描述构建后续帧驾驶场景。图中前三行和第四行分别展示了对主车和他车行为进行文本描述控制后的生成效果。（绿框为输入，蓝框为输出)
根据输入帧直接生成后续帧
无需其他控制，仅使用单帧图像作为输入，预测后续帧驾驶场景。（绿框为输入，蓝框为输出)
DrivingDiffusion是如何解决上述问题的？ DrivingDiffusion首先人为地构造场景中的所有3D真值（障碍物/道路结构），在将真值投影为Layout图像后，以此为模型输入得到多相机视角下的真实图像/视频。之所以没有直接使用3D真值（BEV视图或按照编码后的实例）作为模型输入，而是使用参数进行投影后输入，是为了消除系统性的3D-2D一致性误差。（在这样的一组数据中，3D真值和车辆参数都是人为地按照实际需求构造的，前者带来了随意构造罕见场景数据能力，后者消除了传统数据生产中几何一致性的误差。）
此时还剩下一个问题：生成的图像/视频质量能否满足使用需求？
提到构造场景，大家往往会想到使用仿真引擎，然而其生成的数据和真实数据存在着较大的domain gap。GAN-based 方法的生成结果往往和实际真实数据的分布存在一定bias。而Diffusion Models基于马尔可夫链通过学习噪声来生成数据的特性，其生成结果的保真度较高，更适合替代真实数据使用。
DrivingDiffusion依照人为构造的场景和车辆参数，直接生成时序multi-view视图，不仅可以作为下游自动驾驶任务的训练数据，还可以构建用于反馈自动驾驶算法的仿真系统。
这里的“人为构造的场景”仅包含障碍物和道路结构信息，但DrivingDiffusion的框架可以轻松引入标志牌，红绿灯，施工区域等layout信息甚至low-level的occupancy grid/depth map等控制模式。
DrivingDiffusion方法概述 生成多视角视频时，有几个难点：
相较常见的图像生成，多视角视频生成新增了视角和时序两个维度，如何设计一个可以进行长视频生成的框架？如何保持跨视角一致性和跨帧一致性?
从自动驾驶任务的角度，场景中的实例至关重要，如何保证生成实例的质量?
DrivingDiffusion主要设计了一个通用的训练框架，将stable-diffusion-v1-4模型作为图像的预训练模型，并使用3D伪卷积将原有图像输入膨胀，用于处理视角/时序新增的维度后输入3D-Unet，在得到了处理新增维度的扩散模型后，进行了交替迭代式的视频扩展，通过关键帧控制和微调的操作保障了短时序和长时序的整体一致性。此外，DrivingDiffusion提出了Consistency Module和Local Prompt，分别解决了跨视角/跨帧一致性和实例质量的问题。
DrivingDiffusion生成长视频流程 单帧多视角模型：生成multi-view关键帧，
以关键帧作为额外控制，多视角共享的单视角时序模型：并行对各个view进行时序扩展，
以生成结果为额外控制的单帧多视角模型：时序并行地微调后续帧，
确定新关键帧并通过滑动窗口延长视频。
跨视角模型和时序模型的训练框架 对于multi-view模型和时序模型来说，3D-Unet的扩展维度分别为视角和时间。二者都有相同的布局控制器。作者认为后续帧可以从multi-view关键帧获取场景中的信息，并隐式地学习不同目标的关联信息。二者分别使用了不同的一致性注意力模块和相同的Local Prompt模块。
布局编码：障碍物类别/实例信息和道路结构分割布局，分别以不同的固定编码值编码为RGB图像，经过encode后输出布局token。
关键帧控制：所有的时序扩展过程，都采用了某一关键帧的multi-view图像，这是基于在短时序内的后续帧可以从关键帧获取信息的假设。所有的微调过程都以关键帧和其生成的后续某帧的multi-view图像作为额外控制，输出优化该帧跨视角一致性后multi-view图像。
基于特定视角的光流先验：对于时序模型，训练时只进行某个视角下数据的采样。额外使用提前统计的该视角图像下每个像素位置的光流先验值，编码后作为相机ID token，进行类似扩散过程中的time embedding对hidden层的交互控制。
Consistency Module &amp; Local Prompt Consistency Module分为两部分：一致性注意力机制和一致性关联损失。
一致性注意力机制关注了相邻视角和时序相关帧的交互，具体来说对于跨帧一致性仅仅关注存在overlap的左右相邻视角的信息交互，对于时序模型，每一帧只关注关键帧以及前一帧。这避免了全局交互带来的巨大计算量。
一致性关联损失通过像素级关联并回归位姿来添加几何约束，其梯度由一个预先训练的位姿回归器提供。该回归器基于LoFTR添加位姿回归head，并在相应数据集的真实数据上使用位姿真值进行训练。对于多视角模型和时序模型该模块分别监督相机相对位姿和主车运动位姿。
Local Prompt和Global Prompt配合，复用了CLIP和stable-diffusion-v1-4的参数语义，对特定类别实例区域进行局部增强。如图所示，在图像token和全局的文字描述提示的交叉注意力机制基础上，作者对某类别进行local prompt设计并使用该类别mask区域的图像token对local prompt进行查询。该过程最大程度地利用了原模型参数中在open domain的文本引导图像生成的概念。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5840bd2c56c1b15f6fdbb9e7cb32139f/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-23T07:30:12+08:00" />
<meta property="article:modified_time" content="2023-10-23T07:30:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">首个多视角自动驾驶场景视频生成世界模型 | DrivingDiffusion: BEV数据和仿真新思路...</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center;">点击下方<strong>卡片</strong>，关注“<strong>自动驾驶之心</strong>”公众号</p> 
 <p style="text-align:center;">ADAS巨卷干货，即可获取</p> 
 <p><strong>&gt;&gt;</strong><strong><a href="" rel="nofollow"><strong>自动驾驶之心【大模型】技术交流群</strong></a></strong></p> 
 <h4>笔者的一些个人思考</h4> 
 <p>在自动驾驶领域，随着BEV-based子任务/端到端方案的发展，高质量的<strong>多视图训练数据</strong>和相应的<strong>仿真场景构建</strong>愈发重要。针对当下任务的痛点，“高质量”可以解耦成三个方面：</p> 
 <ol><li><p><strong>不同维度上的长尾场景</strong>：如障碍物数据中近距离的车辆以及切车过程中精准的朝向角，以及车道线数据中不同曲率的弯道或较难采集的匝道/汇入/合流等场景。这些往往靠大量的数据采集和复杂的数据挖掘策略，成本高昂。</p></li><li><p><strong>3D真值-图像的高度一致</strong>：当下的BEV数据获取往往受到传感器安装/标定，高精地图以及重建算法本身的误差影响。这导致了我们很难保证数据中的每一组【3D真值-图像-传感器参数】 的精确一致。</p></li><li><p><strong>满足上述条件基础上的时序数据</strong>：连续帧的多视角图像和相应真值，这对于当前的感知/预测/决策/端到端等任务都是必不可少的。</p></li></ol> 
 <p>而对仿真来说，可以直接通过布局进行满足上述条件的视频生成，无疑是最直接的multi-agent传感器输入的构造方式。而DrivingDiffusion则从一个新的角度解决了上述问题。</p> 
 <h4>什么是DrivingDiffusion？</h4> 
 <ul><li><p>DrivingDiffusion是一个用于自动驾驶场景生成的扩散模型框架，实现了<strong>布局</strong>控制的<strong>多视角图像/视频</strong>生成并分别实现了SOTA。</p></li><li><p>DrivingDiffusion-Future作为自动驾驶世界模型有根据单帧图像<strong>预测未来场景视频</strong>并根据语言提示<strong>影响主车/他车运动规划</strong>的能力。</p></li></ul> 
 <h4>DrivingDiffusion生成效果是怎么样的？</h4> 
 <p>有需要的同学可以先看看项目主页：https://drivingdiffusion.github.io</p> 
 <h5>（1）DrivingDiffusion</h5> 
 <p><strong>布局控制的多视角图像生成</strong></p> 
 <img src="https://images2.imgbox.com/ec/9b/pNUMLoOB_o.png" alt="ad60e2913c204a1f7f434a0eaed9a45b.png"> 
 <p>图中展示了以布局投影作为输入的multi-view图像生成效果。</p> 
 <p><strong>调整布局：精确控制生成结果</strong></p> 
 <img src="https://images2.imgbox.com/65/c2/X9HzMb8J_o.png" alt="038a170b557167d8a58f4ff197caff72.png"> 
 <p>图中上半部分展示了生成结果的多样性以及下文中模块设计的重要性。下半部分展示了对正后方的车辆进行扰动的结果，包含移动，转向，碰撞甚至悬浮在空中的场景的生成效果。</p> 
 <p><strong>布局控制的多视角视频生成</strong></p> 
 <p style="text-align:center;"><strong><img src="https://images2.imgbox.com/b6/02/OtMhKoX3_o.gif" alt="58f50794f7cbffd7fa80811b9db62339.gif"></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/82/aa/y56KNI3I_o.gif" alt="643ecaba19ab82ce6e4ea85a0cef64ca.gif"></p> 
 <p>上：DrivingDiffusion在nuScenes数据上训练后的视频生成结果。下：DrivingDiffusion在大量私有真实数据上训练后的视频生成结果。</p> 
 <h5>（2）DrivingDiffusion-Future</h5> 
 <p><strong>根据输入帧+文本描述生成后续帧</strong></p> 
 <img src="https://images2.imgbox.com/03/59/x6CTaUbY_o.png" alt="83f6c1e40eb8a4ec79509b2404e73fd3.png"> 
 <p>使用单帧图像作为输入，根据对主车/他车的文本描述构建后续帧驾驶场景。图中前三行和第四行分别展示了对主车和他车行为进行文本描述控制后的生成效果。（绿框为输入，蓝框为输出)</p> 
 <p><strong>根据输入帧直接生成后续帧</strong></p> 
 <img src="https://images2.imgbox.com/74/10/xHjVATxu_o.png" alt="b25bb356cf97dffa4c22e834ee1cbf5a.png"> 
 <p>无需其他控制，仅使用单帧图像作为输入，预测后续帧驾驶场景。（绿框为输入，蓝框为输出)</p> 
 <h4>DrivingDiffusion是如何解决上述问题的？</h4> 
 <p>DrivingDiffusion首先人为地构造场景中的所有3D真值（障碍物/道路结构），在将真值投影为Layout图像后，以此为模型输入得到多相机视角下的真实图像/视频。之所以没有直接使用3D真值（BEV视图或按照编码后的实例）作为模型输入，而是使用参数进行投影后输入，是为了消除系统性的3D-2D一致性误差。（在这样的一组数据中，<strong>3D真值</strong>和<strong>车辆参数</strong>都是人为地按照实际需求构造的，前者带来了随意<strong>构造罕见场景数据能力</strong>，后者<strong>消除了传统数据生产中几何一致性的误差</strong>。）</p> 
 <p>此时还剩下一个问题：生成的图像/视频质量能否满足使用需求？</p> 
 <p>提到构造场景，大家往往会想到使用仿真引擎，然而其生成的数据和真实数据存在着较大的domain gap。GAN-based 方法的生成结果往往和实际真实数据的分布存在一定bias。而Diffusion Models基于马尔可夫链通过学习噪声来生成数据的特性，其生成结果的保真度较高，更适合替代真实数据使用。</p> 
 <p>DrivingDiffusion依照<strong>人为构造的场景</strong>和<strong>车辆参数</strong>，直接生成<strong>时序multi-view视图</strong>，不仅可以作为下游自动驾驶任务的训练数据，还可以构建用于反馈自动驾驶算法的仿真系统。</p> 
 <p>这里的“人为构造的场景”仅包含障碍物和道路结构信息，但DrivingDiffusion的框架可以轻松引入标志牌，红绿灯，施工区域等layout信息甚至low-level的occupancy grid/depth map等控制模式。</p> 
 <h4>DrivingDiffusion方法概述</h4> 
 <p><strong>生成多视角视频时，有几个难点：</strong></p> 
 <ul><li><p>相较常见的图像生成，多视角视频生成新增了<strong>视角</strong>和<strong>时序</strong>两个维度，如何设计一个可以进行长视频生成的框架？如何保持跨视角一致性和跨帧一致性?</p></li><li><p>从自动驾驶任务的角度，场景中的实例至关重要，如何保证生成实例的质量?</p></li></ul> 
 <p>DrivingDiffusion主要设计了一个通用的训练框架，将stable-diffusion-v1-4模型作为图像的预训练模型，并使用3D伪卷积将原有图像输入膨胀，用于处理视角/时序新增的维度后输入3D-Unet，在得到了处理新增维度的扩散模型后，进行了交替迭代式的视频扩展，通过<strong>关键帧控制</strong>和<strong>微调</strong>的操作保障了短时序和长时序的整体一致性。此外，DrivingDiffusion提出了Consistency Module和Local Prompt，分别解决了跨视角/跨帧一致性和实例质量的问题。</p> 
 <h5>DrivingDiffusion生成长视频流程</h5> 
 <img src="https://images2.imgbox.com/2c/db/vtrZRwcP_o.png" alt="061b0236dc840261d037b250270cb8e6.png"> 
 <ol><li><p>单帧多视角模型：生成multi-view关键帧，</p></li><li><p>以关键帧作为额外控制，多视角共享的单视角时序模型：并行对各个view进行时序扩展，</p></li><li><p>以生成结果为额外控制的单帧多视角模型：时序并行地微调后续帧，</p></li><li><p>确定新关键帧并通过滑动窗口延长视频。</p></li></ol> 
 <h5>跨视角模型和时序模型的训练框架</h5> 
 <img src="https://images2.imgbox.com/a0/e0/eqojW6uT_o.png" alt="c48d101e49c8f89fd818454757ac2fe7.png"> 
 <ul><li><p>对于multi-view模型和时序模型来说，3D-Unet的扩展维度分别为视角和时间。二者都有相同的布局控制器。作者认为后续帧可以从multi-view关键帧获取场景中的信息，并隐式地学习不同目标的关联信息。二者分别使用了不同的一致性注意力模块和相同的Local Prompt模块。</p></li><li><p>布局编码：障碍物类别/实例信息和道路结构分割布局，分别以不同的固定编码值编码为RGB图像，经过encode后输出布局token。</p></li><li><p>关键帧控制：所有的时序扩展过程，都采用了某一关键帧的multi-view图像，这是基于在短时序内的后续帧可以从关键帧获取信息的假设。所有的微调过程都以关键帧和其生成的后续某帧的multi-view图像作为额外控制，输出优化该帧跨视角一致性后multi-view图像。</p></li><li><p>基于特定视角的光流先验：对于时序模型，训练时只进行某个视角下数据的采样。额外使用提前统计的该视角图像下每个像素位置的光流先验值，编码后作为相机ID token，进行类似扩散过程中的time embedding对hidden层的交互控制。</p></li></ul> 
 <h5>Consistency Module &amp; Local Prompt</h5> 
 <img src="https://images2.imgbox.com/c1/10/J9wuVbuH_o.png" alt="13e3fca9be9bb782062f0ba1b56a9877.png"> 
 <p><strong>Consistency Module</strong>分为两部分：<strong>一致性注意力机制</strong>和<strong>一致性关联损失</strong>。</p> 
 <p>一致性注意力机制关注了相邻视角和时序相关帧的交互，具体来说对于跨帧一致性仅仅关注存在overlap的左右相邻视角的信息交互，对于时序模型，每一帧只关注关键帧以及前一帧。这避免了全局交互带来的巨大计算量。</p> 
 <p>一致性关联损失通过像素级关联并回归位姿来添加几何约束，其梯度由一个预先训练的位姿回归器提供。该回归器基于LoFTR添加位姿回归head，并在相应数据集的真实数据上使用位姿真值进行训练。对于多视角模型和时序模型该模块分别监督相机相对位姿和主车运动位姿。</p> 
 <p><strong>Local Prompt</strong>和<strong>Global Prompt</strong>配合，复用了CLIP和stable-diffusion-v1-4的参数语义，对特定类别实例区域进行局部增强。如图所示，在图像token和全局的文字描述提示的交叉注意力机制基础上，作者对某类别进行local prompt设计并使用该类别mask区域的图像token对local prompt进行查询。该过程最大程度地利用了原模型参数中在open domain的文本引导图像生成的概念。</p> 
 <h4>DrivingDiffusion-Future方法概述</h4> 
 <img src="https://images2.imgbox.com/d2/88/8UTc5i3S_o.png" alt="3fde2bf06cd32a2a250c5b156e207889.png"> 
 <p>对于未来场景构建任务来说，DrivingDiffusion-Future使用了两种方式：一种是直接通过第一帧图像预测后续帧图像（视觉分支），并使用帧间光流作为辅助损失。这种方式较简单，但根据文本描述对后续生成帧进行生成的效果一般。另一种方式是在前者基础上新增了概念分支，该分支通过第一帧BEV视图预测后续帧BEV视图，这是因为对BEV视图的预测有助于模型捕捉驾驶场景的核心信息和建立概念。此时文本描述同时作用于两个分支，并通过BEV2PV的视角转换模块将概念分支的特征作用于视觉分支，其中视角转换模块的部分参数是通过使用真值图像替代噪声输入预先训练的（并在后续训练中冻结）。值得注意的是，<strong>主车控制文本描述控制器</strong>和<strong>他车控制/环境文本描述控制器</strong>是解耦的。</p> 
 <h4>实验分析</h4> 
 <p>为了评估模型的性能，DrivingDiffusion采用帧级Fréchet Inception Distance (FID)来评估生成图像的质量，相应地使用FVD来评估生成视频质量。所有指标都是在nuScenes验证集上计算的。如表1所示，和自动驾驶场景中的图像生成任务BEVGen 和视频生成任务DriveDreamer相比，DrivingDiffusion在不同设定下的性能指标都有较大优势。</p> 
 <img src="https://images2.imgbox.com/dd/88/aPO8Z7SC_o.png" alt="e2406fba4d790f4216ec88d34fe62985.png"> 
 <p>尽管FID等方法通常用于衡量图像合成的质量，但它们并没有完全反馈任务的设计目标，也没有反映不同语义类别的合成质量。由于任务致力于生成与3D布局一致的多视图图像，DrivingDiffuison提出使用BEV感知模型指标来衡量一致性方面的性能：利用CVT和BEVFusion的官方模型作为评测器，采用与nuScenes验证集相同的以真实3D布局为条件的生成图像，对每组生成的图像进行CVT和BevFusion推理，然后将预测的结果与真实结果进行比较，对其中可驾驶区域的平均交叉路口(mIoU)分数和所有对象类的NDS进行了统计，如表2所示。实验结果表明，对合成数据评测集的感知指标和真实评测集的指标十分接近，这体现了生成结果和3D真值的高度一致性和图像质量的高保真度。</p> 
 <img src="https://images2.imgbox.com/4d/16/zDxVR6Mw_o.png" alt="a9bf1ef96bf6182fdac49be93d951dc7.png"> 
 <p>除了上述实验外，DrivingDiffusion针对其主要解决的问题——提升自动驾驶下游任务表现，进行了加入合成数据训练的实验。表3展示了合成数据增强在BEV感知任务中实现的性能改进。在原始训练数据中，存在长尾分布的问题，特别是小目标、近距车辆和车辆定向角度。DrivingDiffusion专注于为这些样本有限的类别生成额外的数据来解决这个问题。在增加了专注于改善障碍物朝向角度的分布的2000帧数据后，NDS略有改善，而mAOE从0.5613显著下降到0.5295。在使用6000帧更全面，更专注于罕见场景的合成数据来辅助训练后，可以观察到nuScenes验证集有显著的增强：NDS由0.412上升至0.434, mAOE由0.5613下降至0.5130。这证明了合成数据的数据增强给感知任务带来的显著的提升。使用者可以根据实际需求，对数据中各个维度的分布进行统计，再针对性地使用合成数据进行补充。</p> 
 <h4>DrivingDiffusion的意义和未来工作</h4> 
 <p>DrivingDiffuison同时实现了多视角的自动驾驶场景视频生成和未来预测的能力，对自动驾驶任务有着重大意义。其中<strong>布局</strong>和<strong>参数</strong>全部人为构造且3D-2D之间的转换通过投影而非依赖可学习的模型参数，这消除了在以往获取数据过程中的几何误差，有较强的实用价值。同时DrivingDiffuison的可扩展性极强，支持新增场景内容layout以及额外的controller，同时也可以通过超分辨率和视频插帧技术无损地提升生成质量。</p> 
 <p>在自动驾驶仿真中，关于Nerf的尝试越来越多。然而在街景生成这一任务上，对动静态内容的分离，大范围街区重建，解耦天气等维度的表观控制等等，带来了巨大工程量，此外Nerf往往需要再特定范围场景内进行训练后才可支持后续的仿真中的新视角合成任务。而DrivingDiffusion天然包含了一定的通识先验，包括视觉-文字的联系，对视觉内容的概念理解等，可以仅通过构造布局快速地根据需求建立一段场景。但正如上文所述，整个流程较为复杂，且对于长视频的生成需要后处理的模型微调和扩展。DrivingDiffusion将继续探索视角维度和时间维度的压缩，以及结合Nerf进行新视角生成和转换，持续提升生成质量以及可扩展性。</p> 
 <p>参考：</p> 
 <ul><li><p>论文：https://arxiv.org/pdf/2310.07771.pdf</p></li><li><p>代码：https://github.com/shalfun/DrivingDiffusion</p></li><li><p>单位：百度</p></li></ul> 
 <p style="text-align:center;"><strong>① 全网独家视频课程</strong></p> 
 <p style="text-align:left;"><strong>BEV感知</strong><strong>、毫米波雷达视觉融合</strong>、<strong>多传感器标定</strong>、<strong>多传感器融合</strong>、<strong>多模态3D目标检测</strong>、<strong>点云3D目标检测</strong>、<strong>目标跟踪</strong>、<strong>Occupancy、</strong><strong>cuda与TensorRT模型部署</strong>、<strong>协同感知</strong><strong>、</strong><strong>语义分割、</strong><strong>自动驾驶仿真、</strong><strong>传感器部署、</strong><strong>决策规划、轨迹预测</strong>等多个方向学习视频（<strong>扫码即可学习</strong>）</p> 
 <img src="https://images2.imgbox.com/e0/ff/yZFmtAjI_o.png" alt="9e97bc3de229f01d3106c846dad99027.png"> 
 <strong>视频官网：www.zdjszx.com</strong> 
 <p style="text-align:center;"><strong>② 国内首个自动驾驶学习社区</strong></p> 
 <p style="text-align:left;">近2000+人的交流社区，涉及30+自动驾驶技术栈学习路线，想要了解更多自动驾驶感知（2D检测、分割、2D/3D车道线、BEV感知、3D目标检测、Occupancy、多传感器融合、多传感器标定、目标跟踪、光流估计）、自动驾驶定位建图（SLAM、高精地图、局部在线地图）、自动驾驶规划控制/轨迹预测等领域技术方案、AI模型部署落地实战、行业动态、岗位发布，欢迎扫描下方二维码，加入自动驾驶之心知识星球，<strong>这是一个真正有干货的地方，与领域大佬交流入门、学习、工作、跳槽上的各类难题，日常分享论文+代码+视频</strong>，期待交流！</p> 
 <img src="https://images2.imgbox.com/f5/11/cYyTCead_o.png" alt="09c03ee8acdf85df786e8655b8fcd22e.png"> 
 <p style="text-align:center;"><strong>③【自动驾驶之心】技术交流群</strong></p> 
 <p style="text-align:left;">自动驾驶之心是首个自动驾驶开发者社区，聚焦<strong>目标检测、语义分割、全景分割、实例分割、关键点检测、车道线、目标跟踪、3D目标检测、BEV感知、多模态感知、Occupancy、多传感器融合、transformer、大模型、点云处理、端到端自动驾驶、SLAM、光流估计、深度估计、轨迹预测、高精地图、NeRF、规划控制、模型部署落地、自动驾驶仿真测试、产品经理、硬件配置、AI求职交流</strong>等方向。扫码添加汽车人助理微信邀请入群，备注：学校/公司+方向+昵称（快速入群方式）</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/6a/24/ATAk6QfE_o.jpg" alt="2990cd9a2ae6c312ee4947db7fc8cbcc.jpeg"></p> 
 <p style="text-align:center;"><strong>④【自动驾驶之心】平台矩阵，</strong><strong>欢迎联系我们！</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/14/01/DgFUZVjl_o.jpg" alt="d92bf36f8a01622785abd25f8797584e.jpeg"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c00ba5767820fc7affc32b4afc09ccbf/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">基于单片机智能停车系统的设计与实现(论文&#43;源码)_kaic</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/fa4acd2a7553ccf7de7cf9c53756d5c9/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">这是我见过对redis最直白的讲解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>