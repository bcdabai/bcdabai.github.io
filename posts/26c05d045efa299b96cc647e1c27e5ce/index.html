<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Ceph集群部署文档（4节点） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Ceph集群部署文档（4节点）" />
<meta property="og:description" content="Ceph集群部署文档（4节点）
一、 背景 Ceph是一个分布式存储，可以提供对象存储、块存储和文件存储，其中对象存储和块存储可以很好地和各大云平台集成。一个Ceph集群中有Monitor节点、MDS节点（可选，用于文件存储）、至少两个OSD守护进程。 Ceph OSD：OSD守护进程，用于存储数据、处理数据拷贝、恢复、回滚、均衡，并通过心跳程序向Monitor提供部分监控信息。一个Ceph集群中至少需要两个OSD守护进程。 Monitor：维护集群的状态映射信息，包括monitor、OSD、Placement Group（PG）。还维护了Monitor、OSD和PG的状态改变历史信息。 MDS：存储Ceph文件系统的元数据。
二、 环境规划 1、四节点Centos7主机，其中ceph-admin节点为管理节点和监控节点，ceph-1、ceph-2为osd节点，每个节点3个2T的磁盘（分别命名为sdb、sdc、sdd）。ceph-client为客户端，方便以后进行存储测试。所有节点都安装CeontOS7。
2、集群配置如下：
主机
IP
功能
ceph-admin
192.168.56.100
ceph-deploy、mon、ntp server
ceph-1
192.168.56.101
osd.0、mds
ceph-2
192.168.56.102
osd.1
ceph-client
192.168.56.103
客户端，主要利用它挂载ceph集群提供的存储进行测试
3、节点关系图
三、前期准备，安装ceph-deploy工具 所有的节点都是root用户登录的。 1、修改每个节点的主机名，并重启。 # hostnamectl set-hostname ceph-admin(ceph-1/ceph-2/ceph-client)
2、配置防火墙启动端口 需要在每个节点上执行以下命令：
# firewall-cmd--zone=public --add-port=6789/tcp –permanent
# firewall-cmd--zone=public --add-port=6800-7100/tcp–permanent
# firewall-cmd–reload
# firewall-cmd--zone=public --list-all
3、禁用selinux 需要在每个节点上执行以下命令：
# setenforce 0
# vim /etc/selinux/config
将SELINUX设置为disabled
4、配置ceph-admin节点的hosts文件 # vi /etc/hosts
192.168.56.100 ceph-admin
192.168.56.101 ceph-1
192.168.56.102 ceph-2" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/26c05d045efa299b96cc647e1c27e5ce/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2017-11-29T19:42:44+08:00" />
<meta property="article:modified_time" content="2017-11-29T19:42:44+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ceph集群部署文档（4节点）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p align="center"><strong><span style="font-size:24px">Ceph集群部署文档（4节点）</span></strong></p> 
<h2>一、    背景</h2> 
<p><span style="color:rgb(63,63,63)">Ceph</span><span style="color:rgb(63,63,63)">是一个分布式存储，可以提供对象存储、块存储和文件存储，其中对象存储和块存储可以很好地和各大云平台集成。一个</span><span style="color:rgb(63,63,63)">Ceph</span><span style="color:rgb(63,63,63)">集群中有</span><span style="color:rgb(63,63,63)">Monitor</span><span style="color:rgb(63,63,63)">节点、</span><span style="color:rgb(63,63,63)">MDS</span><span style="color:rgb(63,63,63)">节点（可选，用于文件存储）、至少两个</span><span style="color:rgb(63,63,63)">OSD</span><span style="color:rgb(63,63,63)">守护进程。</span><span style="color:rgb(63,63,63)"> </span><span style="color:#3F3F3F"><br> <em><strong>Ceph OSD</strong></em></span><span style="color:rgb(63,63,63)">：OSD</span>守护进程，用于存储数据、处理数据拷贝、恢复、回滚、均衡，并通过心跳程序向Monitor提供部分监控信息。一个Ceph集群中至少需要两个OSD守护进程。 <span style="color:#3F3F3F"><br> <em><strong>Monitor</strong></em></span><span style="color:rgb(63,63,63)">：维护集群的状态映射信息，包括monitor</span>、OSD、Placement Group（PG）。还维护了Monitor、OSD和PG的状态改变历史信息。 <span style="color:#3F3F3F"><br> <em><strong>MDS</strong></em></span><span style="color:rgb(63,63,63)">：存储Ceph</span>文件系统的元数据。</p> 
<h2>二、    环境规划</h2> 
<p>1、四节点Centos7主机，其中ceph-admin节点为管理节点和监控节点，ceph-1、ceph-2为osd节点，每个节点3个2T的磁盘（分别命名为sdb、sdc、sdd）。ceph-client为客户端，方便以后进行存储测试。所有节点都安装CeontOS7。</p> 
<p>2、集群配置如下：<code></code></p> 
<table border="1" cellspacing="0" cellpadding="0"><tbody><tr><td> <p align="center"><strong>主机</strong></p> </td><td> <p align="center"><strong>IP</strong></p> </td><td> <p align="center"><strong>功能</strong></p> </td></tr><tr><td> <p align="center">ceph-admin</p> </td><td> <p align="center">192.168.56.100</p> </td><td> <p align="center">ceph-deploy、mon、ntp server</p> </td></tr><tr><td> <p align="center">ceph-1</p> </td><td> <p align="center">192.168.56.101</p> </td><td> <p align="center">osd.0、mds</p> </td></tr><tr><td> <p align="center">ceph-2</p> </td><td> <p align="center">192.168.56.102</p> </td><td> <p align="center">osd.1</p> </td></tr><tr><td> <p align="center">ceph-client</p> </td><td> <p align="center">192.168.56.103</p> </td><td> <p align="center">客户端，主要利用它挂载ceph集群提供的存储进行测试</p> </td></tr></tbody></table> 
<p> </p> 
<p>3、节点关系图</p> 
<p><img src="https://images2.imgbox.com/8d/83/P27tIta3_o.png" width="700" height="400" alt=""><br> </p> 
<p align="center"></p> 
<h2>三、前期准备，安装ceph-deploy工具</h2> 
<p>所有的节点都是root用户登录的。  </p> 
<h3>1、修改每个节点的主机名，并重启。</h3> 
<p># <strong>hostnamectl set-hostname ceph-admin(ceph-1/ceph-2/ceph-client)</strong></p> 
<h3>2、配置防火墙启动端口</h3> 
<p><strong><span style="color:red">需要在每个节点上执行以下命令：</span></strong></p> 
<p># firewall-cmd--zone=public --add-port=6789/tcp –permanent</p> 
<p># firewall-cmd--zone=public --add-port=6800-7100/tcp–permanent</p> 
<p># firewall-cmd–reload</p> 
<p># firewall-cmd--zone=public --list-all</p> 
<h3>3、禁用selinux</h3> 
<p><strong><span style="color:red">需要在每个节点上执行以下命令：</span></strong></p> 
<p># setenforce 0</p> 
<p># vim /etc/selinux/config</p> 
<p>将SELINUX设置为disabled</p> 
<h3>4、配置ceph-admin节点的hosts文件</h3> 
<p># vi /etc/hosts</p> 
<p>192.168.56.100  ceph-admin</p> 
<p>192.168.56.101  ceph-1</p> 
<p>192.168.56.102  ceph-2</p> 
<p>192.168.56.103  ceph-client</p> 
<h2>四、配置ceph-admin部署节点的无密码登录每个ceph节点</h2> 
<h3>1、在每个节点上安装一个SSH服务器</h3> 
<p><strong><span style="color:red">需要在每个节点上执行以下命令：</span></strong></p> 
<p># yum installopenssh-server -y</p> 
<h3>2、配置ceph-admin管理节点与每个ceph节点无密码的SSH访问</h3> 
<p># ssh-keygen</p> 
<h3>3、复制ceph-admin节点的密钥到每个ceph节点</h3> 
<p># ssh-copy-id root@ceph-1</p> 
<p># ssh-copy-id root@ceph-2</p> 
<p># ssh-copy-id root@ceph-client</p> 
<h3>4、测试每个节点不用密码是否可以登录</h3> 
<p># ssh root@ceph-1</p> 
<p># ssh root@ceph-2</p> 
<p># ssh root@ceph-client</p> 
<h3>5、修改ceph-admin管理节点的~/.ssh/config文件</h3> 
<p># vi .ssh/config</p> 
<p>Host ceph-admin</p> 
<p>    Hostname ceph-admin</p> 
<p>    User root   </p> 
<p>Host ceph-1</p> 
<p>    Hostname ceph-1</p> 
<p>    User root</p> 
<p>Host ceph-2</p> 
<p>    Hostname ceph-2</p> 
<p>    User root</p> 
<p>Host ceph-client</p> 
<p>    Hostname ceph-client</p> 
<p>    User root</p> 
<h2>五、yum源及ceph的安装</h2> 
<p><strong><span style="color:red">需要在每个节点（</span><span style="color:red">ceph-admin/1/2/client</span><span style="color:red">）上执行以下命令：</span></strong></p> 
<p align="left"># yumclean all</p> 
<p align="left"># rm-rf /etc/yum.repos.d/*.repo</p> 
<p align="left"># wget-O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</p> 
<p align="left"># wget-O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</p> 
<p align="left"># sed-i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</p> 
<p align="left"># sed-i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</p> 
<p align="left"># sed-i 's/$releasever/7/g' /etc/yum.repos.d/CentOS-Base.repo</p> 
<h3>1、增加ceph的源</h3> 
<p># vim /etc/yum.repos.d/ceph.repo</p> 
<p>添加以下内容：</p> 
<p>[ceph]</p> 
<p>name=ceph</p> 
<p>baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/</p> 
<p>gpgcheck=0</p> 
<p>[ceph-noarch]</p> 
<p>name=cephnoarch</p> 
<p>baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/</p> 
<p>gpgcheck=0</p> 
<h3>2、安装ceph客户端</h3> 
<p># yum makecache</p> 
<p># yum install ceph ceph-radosgw rdate -y</p> 
<h2>六、配置NTP</h2> 
<p>我们把NTP Server放在ceph-admin节点上，其余三个ceph-1/2/client节点都是NTP Client，目的是从根本上解决时间同步问题。</p> 
<h3>1、 在ceph-admin节点上</h3> 
<h4>a、修改<code>/etc/ntp.conf</code>，注释掉默认的四个server，添加三行配置如下：</h4> 
<p align="left"># vim /etc/ntp.conf</p> 
<p align="left"> </p> 
<p align="left">###comment following lines:</p> 
<p align="left">#server 0.centos.pool.ntp.org iburst</p> 
<p align="left">#server 1.centos.pool.ntp.org iburst</p> 
<p align="left">#server 2.centos.pool.ntp.org iburst</p> 
<p align="left">#server 3.centos.pool.ntp.org iburst</p> 
<p align="left">###add following lines:</p> 
<p align="left">server 127.127.1.0 minpoll 4</p> 
<p align="left">fudge 127.127.1.0 stratum 0</p> 
<p align="left">#这一行需要根据client的IP范围设置。</p> 
<p align="left">restrict 192.168.56.0 mask 255.255.255.0 nomodify notrap </p> 
<h4>b、修改<code>/etc/ntp/step-tickers</code>文件如下：</h4> 
<p># vim /etc/ntp/step-tickers</p> 
<p># List of NTP servers used by the ntpdateservice.</p> 
<p># 0.centos.pool.ntp.org</p> 
<p>127.127.1.0</p> 
<h4>c、重启ntp服务，并查看server端是否运行正常，正常的标准就是<code>ntpq-p</code>指令的最下面一行是<code>*</code>：</h4> 
<p># systemctl enable ntpd</p> 
<p># systemctl restart ntpd</p> 
<p># ntpq -p</p> 
<div> 
 <p>remote           refid      st t when poll reach   delay  offset  jitter</p> 
</div> 
<p>*LOCAL(0)        .LOCL.           0 l    -  16    1    0.000   0.000   0.000</p> 
<p>至此，NTP Server端已经配置完毕，下面开始配置Client端。</p> 
<h3>2、 在ceph-1/ceph-2/ceph-client三个节点上</h3> 
<h4>a、修改<code>/etc/ntp.conf</code>，注释掉四行server，添加一行server指向ceph-admin:</h4> 
<p># vim /etc/ntp/conf</p> 
<p> </p> 
<p>#server 0.centos.pool.ntp.org.iburst</p> 
<p>#server 1.centos.pool.ntp.org.iburst</p> 
<p>#server 2.centos.pool.ntp.org.iburst</p> 
<p>#server 3.centos.pool.ntp.org.iburst</p> 
<p> </p> 
<p>server 192.168.56.100</p> 
<h4>b、重启ntp服务并观察client是否正确连接到server端，同样正确连接的标准是<code>ntpq-p</code>的最下面一行以<code>*</code>号开头:</h4> 
<p># systemctl enable ntpd</p> 
<p># systemctl restart ntpd</p> 
<p># ntpq -p</p> 
<div> 
 <p>remote           refid      st t when poll reach   delay  offset  jitter</p> 
</div> 
<p>*ceph-admin     .LOCL.      1 u    1  64    1    0.329   0.023   0.000</p> 
<h2>七．部署Ceph</h2> 
<p>在部署节点(<code>ceph-admin</code>)安装ceph-deploy，<strong>下文的部署节点统一指ceph-admin</strong>:</p> 
<h3>1、 在ceph-admin节点上安装ceph部署工具，并检查版本号</h3> 
<p># yum -yinstall ceph-deploy</p> 
<p># ceph-deploy–version</p> 
<p># ceph -v</p> 
<h3>2、在部署节点（ceph-admin）创建部署目录</h3> 
<p># mkdir cluster</p> 
<p># cd /cluster</p> 
<h3>3、创建以ceph-admin为监控节点的集群</h3> 
<p># ceph-deploynew ceph-admin</p> 
<h3>4、完成后，查看目录内容</h3> 
<p># ls</p> 
<p>ceph.conf    ceph.deploy-ceph.log     ceph.mon.keyring</p> 
<h3>5、编辑admin-node节点的ceph配置文件，把下面的配置放入ceph.conf中</h3> 
<p># vim ceph.conf</p> 
<p>osd pool default size = 2//根据具体的osd数量，这里是2</p> 
<h3>6、初始化mon节点并收集keyring</h3> 
<p># ceph-deploy mon create-initial</p> 
<p>…….</p> 
<p>…….</p> 
<p># ls</p> 
<p>ceph.bootstrap-mds.keyring  ceph.bootstrap-rgw.keyring  ceph.conf   ceph.mon.keyring</p> 
<p>ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log</p> 
<h3>7、把ceph-admin节点的配置文件与keyring同步至其它节点：</h3> 
<p># <span style="background:rgb(250,250,252)">ceph-deploy adminceph-admin ceph-1 ceph-2 ceph-client</span></p> 
<h3>8、查看集群状态</h3> 
<p># ceph -s</p> 
<p>   cluster19f4be8e-20ef-4ebb-964c-7c31e8df0059</p> 
<p>   <span style="color:red">health HEALTH_ERR</span></p> 
<p>   monmap e1: 1 mons at {ceph-admin=192.168.56.100:6789/0}</p> 
<p>          election epoch 4, quorum 0,1,2 ceph-1,ceph-2,ceph-client</p> 
<p>   osdmap e37: 0 osds: 0up, 0 in</p> 
<p>    pgmap v499: 64 pgs, 1 pools, 606 MB data, 160 objects</p> 
<p>            6950 MB used, 8030 MB / 15805 MBavail</p> 
<p>                  <span style="color:red">64 creatring</span></p> 
<h3>9、开始部署OSD</h3> 
<p># ceph-deploy --overwrite-conf osd prepareceph-2:/dev/sdb ceph-2:/dev/sdc ceph-2:/dev/sdd ceph-3:/dev/sdb ceph-3:/dev/sdcceph-3:/dev/sdd  --zap-disk</p> 
<p># ceph-deploy --overwrite-conf osd activateceph-2:/dev/sdb ceph-2:/dev/sdc ceph-2:/dev/sdd ceph-3:/dev/sdb ceph-3:/dev/sdcceph-3:/dev/sdd</p> 
<h3>10、查看集群状态</h3> 
<p># ceph -s</p> 
<p>cluster19f4be8e-20ef-4ebb-964c-7c31e8df0059</p> 
<p>   <span style="color:red">health HEALTH_WARN</span></p> 
<p>   monmap e1: 1 mons at {ceph-admin=192.168.56.100:6789/0}</p> 
<p>          election epoch 4, quorum 0,1,2 ceph-1,ceph-2,ceph-client</p> 
<p>   osdmap e37: 6 osds: 6 up, 6 in</p> 
<p>    pgmap v499: 64 pgs, 1 pools, 606 MB data, 160 objects</p> 
<p>            6950 MB used, 8030 MB / 15805 MBavail</p> 
<p>                  <span style="color:red">64 active+clean</span></p> 
<h3>11、增加rbd池的PG，去除WARN</h3> 
<p># ceph osd pool set rbd pg_num 128</p> 
<p># ceph osd pool set rbd pgp_num 128</p> 
<p># ceph -s</p> 
<p>cluster19f4be8e-20ef-4ebb-964c-7c31e8df0059</p> 
<p>   <span style="color:red">health HEALTH_OK</span></p> 
<p>   monmap e1: 1 mons at {ceph-admin=192.168.56.100:6789/0}</p> 
<p>          election epoch 4, quorum 0,1,2 ceph-1,ceph-2,ceph-client</p> 
<p>   osdmap e37: 6 osds: 6 up, 6 in</p> 
<p>    pgmap v499: 64 pgs, 1 pools, 606 MB data, 160 objects</p> 
<p>            6950 MB used, 8030 MB / 15805 MBavail</p> 
<p>                  <span style="color:red">128 active+clean</span></p> 
<h3>12、添加一个元数据服务器</h3> 
<p># ceph-deploy mds create ceph-1</p> 
<h3>13、再次查看集群状态</h3> 
<p># ceph -s</p> 
<p>cluster19f4be8e-20ef-4ebb-964c-7c31e8df0059</p> 
<p>   <span style="color:red">health HEALTH_OK</span></p> 
<p>   monmap e1: 1 mons at {ceph-admin=192.168.56.100:6789/0}</p> 
<p>          election epoch 4, quorum 0,1,2 ceph-1,ceph-2,ceph-3</p> 
<p>   <span style="color:red">mdsmap e4: 1/1/1 up {0=ceph-1=up:active}</span></p> 
<p>   osdmap e37: 6 osds: 6 up, 6 in</p> 
<p>    pgmap v499: 64 pgs, 1 pools, 606 MB data, 160 objects</p> 
<p>            6950 MB used, 8030 MB / 15805 MBavail</p> 
<p>                  <span style="color:red">128 active+clean</span></p> 
<h2>八、验证</h2> 
<p>使用Ceph块存储验证集群功能是否可用。</p> 
<h3>1、在ceph-admin上安装ceph客户端</h3> 
<p># ceph-deploy install ceph-client</p> 
<h3>2、在ceph-client上创建块设备映像</h3> 
<p># rbd create test --size 4096 --image-format 2 --image-feature  layering</p> 
<h3>3、查看创建的映像</h3> 
<p># rbd ls</p> 
<h3>4、将ceph提供的块设备映射到ceph-client</h3> 
<p># rbd map test --pool rbd --name client.admin</p> 
<h3>5、创建文件系统</h3> 
<p># mkfs.xfs  /dev/rbd/rbd/test</p> 
<h3>6、挂载文件系统</h3> 
<p align="left"># mkdir /test</p> 
<p align="left"># mount /dev/rbd/rbd/test /test</p> 
<h3>7、使用dd测试</h3> 
<p align="left"># cd /test</p> 
<p align="left"># dd if=/dev/zero of=ceshi bs=1M count=10</p> 
<h3>8、在随便一台osd节点上，查看是否有数据写入</h3> 
<p># ceph -w</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/738d4b42513c0355c8ea51bbd8a8e680/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Wireshark 捕获过滤器的语法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b4b0cea51ae3927f809d9753be1e6960/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">将字符串插入另一个字符串的指定位置</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>