<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>文本匹配模型实验报告-text2vec - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="文本匹配模型实验报告-text2vec" />
<meta property="og:description" content="文本匹配模型实验报告-text2vec 尽管基于BERT的模型在NLP诸多下游任务中取得了成功，直接从BERT导出的句向量表示往往被约束在一个很小的区域内，表现出很高的相似度，因而难以直接用于文本语义匹配。为解决BERT原生句子表示这种“坍缩”现象，Su[21]提出了一种排序Loss的句向量表示模型——CoSENT，
通过在目标领域的监督语料上Fine-tune，使模型生成的句子表示与下游任务的数据分布更加适配。
在句子语义匹配（STS）任务的实验结果显示，同等设置下CoSENT相比此前的Sentence-BERT大幅提升了5%。
1. 背景 句向量表示学习在自然语言处理（NLP）领域占据重要地位，许多NLP任务的成功离不开训练优质的句子表示向量。特别是在文本语义匹配（Semantic Textual Similarity）、文本向量检索（Dense Text Retrieval）等任务上，
模型通过计算两个句子编码后的Embedding在表示空间的相似度来衡量这两个句子语义上的相关程度，从而决定其匹配分数。
尽管基于BERT的模型在诸多NLP任务上取得了不错的性能（通过有监督的Fine-tune），但其自身导出的句向量（不经过Fine-tune，对所有词向量求平均）质量较低，甚至比不上Glove的结果，因而难以反映出两个句子的语义相似度[1][2][3][4]。
我们在研究的过程中进一步分析了BERT导出的句向量所具有的特性，证实了以下两点：
BERT对所有的句子都倾向于编码到一个较小的空间区域内，这使得大多数的句子对都具有较高的相似度分数，即使是那些语义上完全无关的句子对（如下图所示），我们将此称为BERT句子表示的“坍缩（Collapse）”现象。 左：BERT表示空间的坍缩问题（横坐标是人工标注的相似度分数，纵坐标是模型预测的余弦相似度）；右：经过CoSENT方法Fine-tune之后
BERT句向量表示的坍缩和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，坍缩现象可以在一定程度上得到缓解（如下图蓝色曲线所示）。 计算句向量时移除Top-K高频词后的性能变化
学习句向量的方案大致上可以分为无监督和有监督两大类，其中有监督句向量比较主流的方案是Facebook提出的InferSent，
而后的Sentence-BERT进一步在BERT上肯定了它的有效性。然而，不管是InferSent还是Sentence-BERT，它们都存在训练和预测不一致的问题，而如果直接优化预测目标cos值，效果往往特别差。
为了解决句向量方案这种训练与预测不一致的问题，分析了直接优化cos值无效的原因，并参考SimCSE的监督方法，分析了CoSENT的rank loss，该loss可以直接优化两个文本比较的cos值。实验显示，CoSENT在第一轮的收敛效果比Sentence-BERT高35%，最终效果上比Sentence-BERT高5%。
2. 研究现状和相关工作 2.1 句子表征学习 句子表征学习是一个很经典的任务，分为以下三类方法：
有监督的句子表征学习方法：早期的工作[5]发现自然语言推理（Natural Language Inference，NLI）任务对语义匹配任务有较大的帮助，训练过程常常融合了两个NLI的数据集SNLI和MNLI，文本表征使用BiLSTM编码器。InferSent模型用了siamese结构，两个句子共用一个encoder，分别得到u和v的文本向量表达，然后用3种计算方式，向量拼接([u,v])，相乘(u * v)，相减(|u-v|)(为了保证对称性取绝对值），来帮助后面的全连接层提取向量间的交互信息，最后跟一个3分类的分类器。Sentence-BERT[1]借鉴了InferSent的框架，只是encoder部分替换成了BERT模型。自监督的Sentence-level预训练：有监督数据标注成本高，研究者们开始寻找无监督的训练方式。BERT提出了NSP的任务，可以算作是一种自监督的句子级预训练目标。尽管之后的工作指出NSP相比于MLM其实没有太大帮助。Cross-Thought[7]、CMLM[8]是两种思想类似的预训练目标，他们把一段文章切成多个短句，然后通过相邻句子的编码去恢复当前句子中被Mask的Token。相比于MLM，额外添加了上下文其他句子的编码对Token恢复的帮助，因此更适合句子级别的训练。SLM[9]通过将原本连贯的若干个短句打乱顺序（通过改变Position Id实现），然后通过预测正确的句子顺序进行自监督预训练。无监督的句子表示迁移：预训练模型现已被普遍使用，然而BERT的NSP任务得到的表示表现更不好，大多数同学也没有资源去进行自监督预训练，因此将预训练模型的表示迁移到任务才是更有效的方式。BERT-flow[2]：CMU&amp;字节AI Lab的工作，通过在BERT之上学习一个可逆的Flow变换，可以将BERT表示空间映射到规范化的标准高斯空间，然后在高斯空间进行相似度匹配。BERT-whitening[10]：苏剑林提出对BERT表征进行白化操作（均值变为0，协方差变为单位矩阵）就能在STS上达到媲美BERT-flow的效果。SimCSE[11]：陈丹琦组在2021年4月份公开的工作，他们同样使用基于对比学习的训练框架，使用Dropout的数据增强方法，在维基百科语料上Fine-tune BERT。 2.2 Sentence-BERT模型 当前有监督的句子表征模型Sentence-BERT，表现出在句向量表示和文本匹配任务上SOTA的效果，证明了其有效性。Sentence-BERT的训练过程是把（u, v, |u - v|）拼接起来后接分类层，而预测过程，是跟普通的句向量模型一样，先计算mean pooling后的句向量，然后拿向量算cos得到相似度值。
Sentence-BERT的训练：
Sentence-BERT的预测：
Sentence-BERT模型为啥有效？ 我们根据消融实验分析：
|u - v|的作用，匹配数据集中，正样本对的文本相似度是远大于负样本对的，对于初始的BERT模型，其正样本对|u - v|差值也是小于负样本对|u - v|差值，可以看出正样本对的u - v分布是一个半径较小的球面附近，
而负样本对的u - v分布在一个较大的球面附近，也就是说，初始模型的u - v本身就有聚类倾向，我们只需要强化这种聚类倾向，使正样本对的u - v更小，负样本对的u - v更大。
BERT-flow和BERT-whitening这类BERT后处理模型，就是用无监督方法强化这种聚类倾向的方法。而监督方法的直接做法是u - v后面接一个全连接层的分类器，但交叉熵的分类器是基于内积计算的，它没法区分两个分布在不同球面的类别，所以这里加上绝对值变成|u - v|，将球面变成锥形，此时就可以用分类器来做分类了。u, v拼接的作用，从BERT-flow的工作可以知道，BERT句向量具备具备丰富的语义信息，但是句向量所在空间受到词频的影响，具备非平滑，各向异性的特点，这种特点导致未经过微调的“BERT&#43;CLS”句向量模型直接应用在语义相似计算任务上的效果甚至不如简单的GloVe句向量，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/a68ada8a7021619b2c08d1d2cb3a312b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-20T19:50:13+08:00" />
<meta property="article:modified_time" content="2023-06-20T19:50:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">文本匹配模型实验报告-text2vec</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="text2vec_0"></a>文本匹配模型实验报告-text2vec</h2> 
<p>尽管基于BERT的模型在NLP诸多下游任务中取得了成功，直接从BERT导出的句向量表示往往被约束在一个很小的区域内，表现出很高的相似度，因而难以直接用于文本语义匹配。为解决BERT原生句子表示这种“坍缩”现象，Su[21]提出了一种排序Loss的句向量表示模型——CoSENT，<br> 通过在目标领域的监督语料上Fine-tune，使模型生成的句子表示与下游任务的数据分布更加适配。</p> 
<p>在句子语义匹配（STS）任务的实验结果显示，同等设置下CoSENT相比此前的Sentence-BERT大幅提升了5%。</p> 
<h3><a id="1__7"></a>1. 背景</h3> 
<p>句向量表示学习在自然语言处理（NLP）领域占据重要地位，许多NLP任务的成功离不开训练优质的句子表示向量。特别是在文本语义匹配（Semantic Textual Similarity）、文本向量检索（Dense Text Retrieval）等任务上，<br> 模型通过计算两个句子编码后的Embedding在表示空间的相似度来衡量这两个句子语义上的相关程度，从而决定其匹配分数。</p> 
<p>尽管基于BERT的模型在诸多NLP任务上取得了不错的性能（通过有监督的Fine-tune），但其自身导出的句向量（不经过Fine-tune，对所有词向量求平均）质量较低，甚至比不上Glove的结果，因而难以反映出两个句子的语义相似度[1][2][3][4]。<br> 我们在研究的过程中进一步分析了BERT导出的句向量所具有的特性，证实了以下两点：</p> 
<ol><li>BERT对所有的句子都倾向于编码到一个较小的空间区域内，这使得大多数的句子对都具有较高的相似度分数，即使是那些语义上完全无关的句子对（如下图所示），我们将此称为BERT句子表示的“坍缩（Collapse）”现象。</li></ol> 
<img src="https://images2.imgbox.com/98/ca/qg5msspD_o.png" height="300px"> 
<p><em>左：BERT表示空间的坍缩问题（横坐标是人工标注的相似度分数，纵坐标是模型预测的余弦相似度）；右：经过CoSENT方法Fine-tune之后</em></p> 
<ol start="2"><li>BERT句向量表示的坍缩和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，坍缩现象可以在一定程度上得到缓解（如下图蓝色曲线所示）。</li></ol> 
<img src="https://images2.imgbox.com/9e/13/kykyfEvi_o.png" height="300px"> 
<p><em>计算句向量时移除Top-K高频词后的性能变化</em></p> 
<p>学习句向量的方案大致上可以分为无监督和有监督两大类，其中有监督句向量比较主流的方案是Facebook提出的<a href="https://arxiv.org/abs/1705.02364" rel="nofollow">InferSent</a>，<br> 而后的<a href="https://arxiv.org/abs/1908.10084" rel="nofollow">Sentence-BERT</a>进一步在BERT上肯定了它的有效性。然而，不管是InferSent还是Sentence-BERT，它们都存在训练和预测不一致的问题，而如果直接优化预测目标cos值，效果往往特别差。</p> 
<p>为了解决句向量方案这种训练与预测不一致的问题，分析了直接优化cos值无效的原因，并参考SimCSE的监督方法，分析了CoSENT的rank loss，该loss可以直接优化两个文本比较的cos值。实验显示，CoSENT在第一轮的收敛效果比Sentence-BERT高35%，最终效果上比Sentence-BERT高5%。</p> 
<h3><a id="2__31"></a>2. 研究现状和相关工作</h3> 
<h4><a id="21__33"></a>2.1 句子表征学习</h4> 
<p>句子表征学习是一个很经典的任务，分为以下三类方法：</p> 
<ol><li>有监督的句子表征学习方法：早期的工作[5]发现自然语言推理（Natural Language Inference，NLI）任务对语义匹配任务有较大的帮助，训练过程常常融合了两个NLI的数据集SNLI和MNLI，文本表征使用BiLSTM编码器。InferSent模型用了siamese结构，两个句子共用一个encoder，分别得到u和v的文本向量表达，然后用3种计算方式，向量拼接([u,v])，相乘(u * v)，相减(|u-v|)(为了保证对称性取绝对值），来帮助后面的全连接层提取向量间的交互信息，最后跟一个3分类的分类器。Sentence-BERT[1]借鉴了InferSent的框架，只是encoder部分替换成了BERT模型。</li><li>自监督的Sentence-level预训练：有监督数据标注成本高，研究者们开始寻找无监督的训练方式。BERT提出了NSP的任务，可以算作是一种自监督的句子级预训练目标。尽管之后的工作指出NSP相比于MLM其实没有太大帮助。Cross-Thought[7]、CMLM[8]是两种思想类似的预训练目标，他们把一段文章切成多个短句，然后通过相邻句子的编码去恢复当前句子中被Mask的Token。相比于MLM，额外添加了上下文其他句子的编码对Token恢复的帮助，因此更适合句子级别的训练。SLM[9]通过将原本连贯的若干个短句打乱顺序（通过改变Position Id实现），然后通过预测正确的句子顺序进行自监督预训练。</li><li>无监督的句子表示迁移：预训练模型现已被普遍使用，然而BERT的NSP任务得到的表示表现更不好，大多数同学也没有资源去进行自监督预训练，因此将预训练模型的表示迁移到任务才是更有效的方式。BERT-flow[2]：CMU&amp;字节AI Lab的工作，通过在BERT之上学习一个可逆的Flow变换，可以将BERT表示空间映射到规范化的标准高斯空间，然后在高斯空间进行相似度匹配。BERT-whitening[10]：苏剑林提出对BERT表征进行白化操作（均值变为0，协方差变为单位矩阵）就能在STS上达到媲美BERT-flow的效果。SimCSE[11]：陈丹琦组在2021年4月份公开的工作，他们同样使用基于对比学习的训练框架，使用Dropout的数据增强方法，在维基百科语料上Fine-tune BERT。</li></ol> 
<h4><a id="22_SentenceBERT_41"></a>2.2 Sentence-BERT模型</h4> 
<p>当前有监督的句子表征模型Sentence-BERT，表现出在句向量表示和文本匹配任务上SOTA的效果，证明了其有效性。Sentence-BERT的训练过程是把（u, v, |u - v|）拼接起来后接分类层，而预测过程，是跟普通的句向量模型一样，先计算mean pooling后的句向量，然后拿向量算cos得到相似度值。</p> 
<p>Sentence-BERT的训练：</p> 
<img src="https://images2.imgbox.com/d8/74/VwxJRFB1_o.png" width="300"> 
<p>Sentence-BERT的预测：</p> 
<img src="https://images2.imgbox.com/9b/63/tbOPgRaZ_o.png" width="300"> 
<h5><a id="SentenceBERT_52"></a>Sentence-BERT模型为啥有效？</h5> 
<p>我们根据消融实验分析：</p> 
<img src="https://images2.imgbox.com/5f/da/3xy7bZbq_o.png" width="400"> 
<ol><li>|u - v|的作用，匹配数据集中，正样本对的文本相似度是远大于负样本对的，对于初始的BERT模型，其正样本对|u - v|差值也是小于负样本对|u - v|差值，可以看出正样本对的u - v分布是一个半径较小的球面附近，<br> 而负样本对的u - v分布在一个较大的球面附近，也就是说，初始模型的u - v本身就有聚类倾向，我们只需要强化这种聚类倾向，使正样本对的u - v更小，负样本对的u - v更大。<br> BERT-flow和BERT-whitening这类BERT后处理模型，就是用无监督方法强化这种聚类倾向的方法。而监督方法的直接做法是u - v后面接一个全连接层的分类器，但交叉熵的分类器是基于内积计算的，它没法区分两个分布在不同球面的类别，所以这里<strong>加上绝对值变成|u - v|，将球面变成锥形</strong>，此时就可以用分类器来做分类了。</li><li>u, v拼接的作用，从BERT-flow的工作可以知道，BERT句向量具备具备丰富的语义信息，但是句向量所在空间受到词频的影响，具备<strong>非平滑，各向异性</strong>的特点，这种特点导致未经过微调的“BERT+CLS”句向量模型直接应用在语义相似计算任务上的效果甚至不如简单的GloVe句向量，<br> 而|u - v|只是向量的相对差距，无法明显改善这种各向异性。而在u, v拼接之后接全连接层，利用了全连接层的类别向量是随机初始化的，相当于给了u，v一个随机的优化方向，迫使他们各自“散开”，远离当前的各向异性状态。</li></ol> 
<h3><a id="3_CoSENT_63"></a>3. CoSENT模型介绍</h3> 
<h4><a id="31__65"></a>3.1 基本思路</h4> 
<p>目标：在一个类似BERT的预训练语言模型基础上，监督训练一个句向量表征模型，使模型能够在文本语义匹配任务上表现最好。<br> 其中，可以利用的标注数据是常见的句子对形式，格式是“（句子1，句子2，标签）”，按照训练encoder的思路，两个句子经过encoder后分别得到向量u，v，由于预测阶段是计算的余弦相似度cos(u，v)，所以思路是<strong>设计基于cos(u, v)的损失函数，让正样本对的相似度尽可能大、负样本对的相似度尽可能小</strong>。<br> 如SimCSE的监督方法的损失函数：</p> 
<img src="https://images2.imgbox.com/cb/44/sceKy7Eu_o.png" width="400"> 
<ul><li>输入样本是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          , 
         
         
         
           x 
          
          
          
            i 
           
          
            + 
           
          
         
        
          , 
         
         
         
           x 
          
          
          
            i 
           
          
            − 
           
          
         
        
          ) 
         
        
       
         (x^{i}, x^{i+}, x^{i-}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0747em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，其中 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
          
          
            i 
           
          
            + 
           
          
         
        
       
         x^{i+} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span></span></span></span></span> 是与 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
       
         x^{i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span> 蕴含关系，而 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
          
          
            i 
           
          
            − 
           
          
         
        
       
         x^{i-} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span></span></span></span></span> 是与 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
       
         x^{i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span> 矛盾关系</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           h 
          
         
           i 
          
         
        
       
         h^{i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span> 是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
       
         x^{i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span> 的句子embedding</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          N 
         
        
       
         N 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> 是 batch size</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          s 
         
        
          i 
         
        
          m 
         
        
       
         sim 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">im</span></span></span></span></span> 是求余弦相似性（cosine similarity）</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          t 
         
        
       
         t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span> 是温度系数，作为超参数，取值为 0.05</li></ul> 
<h4><a id="32_cos_81"></a>3.2 基于cos的排序损失函数</h4> 
<p>我们记正样本对 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          i 
         
        
       
         , 
        
        
        
          x 
         
         
         
           i 
          
         
           + 
          
         
        
       
         ) 
        
       
      
        (x^{i}, x^{i+}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0747em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ，负样本对 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          i 
         
        
       
         , 
        
        
        
          x 
         
         
         
           i 
          
         
           − 
          
         
        
       
         ) 
        
       
      
        (x^{i}, x^{i-}) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0747em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，目标是希望对于任意的正样本对和负样本对都有如下关系：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          cos 
         
        
          ⁡ 
         
        
          ( 
         
         
         
           h 
          
         
           i 
          
         
        
          , 
         
         
         
           h 
          
          
          
            i 
           
          
            + 
           
          
         
        
          ) 
         
        
          &gt; 
         
        
          cos 
         
        
          ⁡ 
         
        
          ( 
         
         
         
           h 
          
         
           i 
          
         
        
          , 
         
         
         
           h 
          
          
          
            i 
           
          
            − 
           
          
         
        
          ) 
         
        
       
         \cos(h^i, h^{i+}) &gt; \cos(h^i, h^{i-}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.1247em; vertical-align: -0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.1247em; vertical-align: -0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8747em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></p> 
<p>至于正样本对的相似度比负样本对大多少，就是越多越好。所以可以设该cos结果的差值为损失，则优化该损失。借鉴Circle Loss和SimCSE的监督损失函数，<br> 设计下面基于cos的排序损失函数：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          log 
         
        
          ⁡ 
         
         
         
           ( 
          
         
           1 
          
         
           + 
          
          
          
            ∑ 
           
           
           
             cos 
            
           
             ⁡ 
            
           
             ( 
            
            
            
              h 
             
            
              i 
             
            
           
             , 
            
            
            
              h 
             
             
             
               i 
              
             
               + 
              
             
            
           
             ) 
            
           
             &gt; 
            
           
             cos 
            
           
             ⁡ 
            
           
             ( 
            
            
            
              h 
             
            
              i 
             
            
           
             , 
            
            
            
              h 
             
             
             
               i 
              
             
               − 
              
             
            
           
             ) 
            
           
          
          
          
            e 
           
           
           
             ( 
            
           
             cos 
            
           
             ⁡ 
            
           
             ( 
            
            
            
              h 
             
            
              i 
             
            
           
             , 
            
            
            
              h 
             
             
             
               i 
              
             
               − 
              
             
            
           
             ) 
            
           
             − 
            
           
             cos 
            
           
             ⁡ 
            
           
             ( 
            
            
            
              h 
             
            
              i 
             
            
           
             , 
            
            
            
              h 
             
             
             
               i 
              
             
               + 
              
             
            
           
             ) 
            
           
             ) 
            
           
             / 
            
           
             t 
            
           
          
         
           ) 
          
         
        
       
         \log\left( 1 + \sum_{\cos(h^i, h^{i+}) &gt; \cos(h^i, h^{i-})} e^{(\cos(h^i, h^{i-}) - \cos(h^i, h^{i+}))/t}\right) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 3.6em; vertical-align: -1.55em;"></span><span class="mop">lo<span style="margin-right: 0.0139em;">g</span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.05em;"><span class="" style="top: -4.05em;"><span class="pstrut" style="height: 5.6em;"></span><span class="" style="width: 0.875em; height: 3.6em;"> 
              <svg width="0.875em" height="3.600em" viewbox="0 0 875 3600"> 
               <path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"></path> 
              </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.55em;"><span class=""></span></span></span></span></span></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.05em;"><span class="" style="top: -1.804em; margin-left: 0em;"><span class="pstrut" style="height: 3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">c</span><span class="mtight">o</span><span class="mtight">s</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7571em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7571em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mrel mtight">&gt;</span><span class="mop mtight"><span class="mtight">c</span><span class="mtight">o</span><span class="mtight">s</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7571em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7571em;"><span class="" style="top: -2.786em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.05em;"><span class="pstrut" style="height: 3.05em;"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.521em;"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 1.0445em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mop mtight"><span class="mtight">c</span><span class="mtight">o</span><span class="mtight">s</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9021em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9021em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mop mtight"><span class="mtight">c</span><span class="mtight">o</span><span class="mtight">s</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9021em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9021em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mclose mtight">))</span><span class="mord mtight">/</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.05em;"><span class="" style="top: -4.05em;"><span class="pstrut" style="height: 5.6em;"></span><span class="" style="width: 0.875em; height: 3.6em;"> 
              <svg width="0.875em" height="3.600em" viewbox="0 0 875 3600"> 
               <path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"></path> 
              </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.55em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>CoSENT的损失函数</p> 
<ul><li>正样本对是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          , 
         
         
         
           x 
          
          
          
            i 
           
          
            + 
           
          
         
        
          ) 
         
        
       
         (x^{i}, x^{i+}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0747em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> ，负样本对是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          , 
         
         
         
           x 
          
          
          
            i 
           
          
            − 
           
          
         
        
          ) 
         
        
       
         (x^{i}, x^{i-}) 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0747em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>。</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           h 
          
         
           i 
          
         
        
       
         h^i 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span> 是 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           x 
          
         
           i 
          
         
        
       
         x^{i} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8247em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span> 句子向量。</li><li><span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          t 
         
        
       
         t 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span> 表示 temperature，是温度系数，超参数。</li></ul> 
<p>模型结构：</p> 
<img src="https://images2.imgbox.com/5d/1d/bplYl3Y0_o.png" width="300"> 
<p><em>训练与预测同模型结构</em></p> 
<h4><a id="33__105"></a>3.3 融合监督和无监督信号</h4> 
<p>除了有监督训练以外，我们还可以进一步融合监督信号的策略：<br> 先做有监督再无监督（sup-unsup）：先使用有监督损失训练模型，再使用SimCSE的无监督的方法进行表示迁移也是可以的，具体效果下面有分析，大家可以自行实验，可以在领域迁移学习快速应用。</p> 
<h3><a id="4__111"></a>4. 实验分析</h3> 
<p>我们主要在文本语义匹配（Semantic Textual Similarity，STS）任务上进行了实验，主要是STS-B数据集，为STS benchmark，来自于SemEval2017评测赛，<br> 该数据集中的样本均包含两个短文本text1和text2，以及人工标注的位于0～5之间的分数，代表text1和text2语义上的匹配程度（5表示最匹配，即“两句话表达的是同一个语义”；<br> 0表示最不匹配，即“两句话表达的语义完全不相关”）。</p> 
<p>下面给出了两条样本作为示例：</p> 
<table><thead><tr><th align="left">text1</th><th align="left">text2</th><th align="left">score</th></tr></thead><tbody><tr><td align="left">A black and white photo of an old train station.</td><td align="left">A black and white photo of a motorcycle laying on the ground.</td><td align="left">0.2</td></tr><tr><td align="left">a woman is dancing in the rain .</td><td align="left">a woman dances in the rain out side .</td><td align="left">5.0</td></tr></tbody></table> 
<p>中文STS-B数据集，是英文STS-B数据集的中文翻译版本，示例case如下：</p> 
<table><thead><tr><th align="left">text1</th><th align="left">text2</th><th align="left">score</th></tr></thead><tbody><tr><td align="left">一个男人在玩电子键盘。</td><td align="left">一个人在吹长笛。</td><td align="left">1</td></tr><tr><td align="left">女人剥土豆。</td><td align="left">一个女人在剥土豆。</td><td align="left">5</td></tr></tbody></table> 
<p>在测试时，为了跟之前的文本匹配工作保持一致，选择了斯皮尔曼相关系数（Spearman correlation）作为评测指标，<br> 它将用于衡量两组值（模型预测的余弦相似度和人工标注的语义相似度）之间的相关性，结果将位于[-1, 1]之间，仅当两组值完全正相关时取到1。<br> 对于每个数据集，我们将其测试样本全部融合计算该指标。考虑到简洁性，会在表格中报告乘以100倍的结果。</p> 
<h4><a id="41__135"></a>4.1 英文匹配数据集</h4> 
<table><thead><tr><th align="left">Arch</th><th align="left">Backbone</th><th align="left">Model Name</th><th align="center">English-STS-B</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">glove</td><td align="left">Avg_word_embeddings_glove_6B_300d</td><td align="center">61.77</td></tr><tr><td align="left">BERT</td><td align="left">bert-base-uncased</td><td align="left">BERT-base-cls</td><td align="center">20.29</td></tr><tr><td align="left">BERT</td><td align="left">bert-base-uncased</td><td align="left">BERT-base-first_last_avg</td><td align="center">59.04</td></tr><tr><td align="left">BERT</td><td align="left">bert-base-uncased</td><td align="left">BERT-base-first_last_avg-whiten(NLI)</td><td align="center">63.65</td></tr><tr><td align="left">SBERT</td><td align="left">sentence-transformers/bert-base-nli-mean-tokens</td><td align="left">SBERT-base-nli-cls</td><td align="center">73.65</td></tr><tr><td align="left">SBERT</td><td align="left">sentence-transformers/bert-base-nli-mean-tokens</td><td align="left">SBERT-base-nli-first_last_avg</td><td align="center">77.96</td></tr><tr><td align="left">CoSENT</td><td align="left">bert-base-uncased</td><td align="left">CoSENT-base-first_last_avg</td><td align="center">69.93</td></tr><tr><td align="left">CoSENT</td><td align="left">sentence-transformers/bert-base-nli-mean-tokens</td><td align="left">CoSENT-base-nli-first_last_avg</td><td align="center">79.68</td></tr></tbody></table> 
<p><strong>英文数据集的实验结果</strong></p> 
<p>在英文匹配任务实验中，我们基于预训练的BERT在STS数据上进行Fine-tune。<br> 在有监督实验中，我们没有使用额外的SNLI和MNLI训练数据，仅使用了STSb的训练数据，CoSENT在backbone为bert-base-uncased和bert-base-nli-mean-tokens下，实现结果得分均超过了基线。</p> 
<p>结果显示，CoSENT方法在完全一致的设置下超过Sentence-BERT，达到了2%的相对性能提升。</p> 
<h4><a id="42__155"></a>4.2 中文匹配数据集</h4> 
<table><thead><tr><th align="left">Arch</th><th align="left">Backbone</th><th align="left">Model Name</th><th align="center">ATEC</th><th align="center">BQ</th><th align="center">LCQMC</th><th align="center">PAWSX</th><th align="center">STS-B</th><th align="center">Avg</th></tr></thead><tbody><tr><td align="left">SBERT</td><td align="left">bert-base-chinese</td><td align="left">SBERT-bert-base</td><td align="center">46.36</td><td align="center">70.36</td><td align="center">78.72</td><td align="center">46.86</td><td align="center">66.41</td><td align="center">61.74</td></tr><tr><td align="left">SBERT</td><td align="left">hfl/chinese-macbert-base</td><td align="left">SBERT-macbert-base</td><td align="center">47.28</td><td align="center">68.63</td><td align="center">79.42</td><td align="center">55.59</td><td align="center">64.82</td><td align="center">63.15</td></tr><tr><td align="left">SBERT</td><td align="left">hfl/chinese-roberta-wwm-ext</td><td align="left">SBERT-roberta-ext</td><td align="center">48.29</td><td align="center">69.99</td><td align="center">79.22</td><td align="center">44.10</td><td align="center">72.42</td><td align="center">62.80</td></tr><tr><td align="left">CoSENT</td><td align="left">bert-base-chinese</td><td align="left">CoSENT-bert-base</td><td align="center">49.74</td><td align="center">72.38</td><td align="center">78.69</td><td align="center">60.00</td><td align="center">79.27</td><td align="center">68.01</td></tr><tr><td align="left">CoSENT</td><td align="left">hfl/chinese-macbert-base</td><td align="left">CoSENT-macbert-base</td><td align="center">50.39</td><td align="center">72.93</td><td align="center">79.17</td><td align="center">60.86</td><td align="center">79.30</td><td align="center">68.53</td></tr><tr><td align="left">CoSENT</td><td align="left">hfl/chinese-roberta-wwm-ext</td><td align="left">CoSENT-roberta-ext</td><td align="center">50.81</td><td align="center">71.45</td><td align="center">79.31</td><td align="center">61.56</td><td align="center">79.96</td><td align="center">68.61</td></tr></tbody></table> 
<p><strong>中文数据集的实验结果</strong></p> 
<p>在中文匹配任务实验中，我们在五个数据集中做了实验，包括：ATEC、BQ、LCQMC、PAWSX、STS-B，并且报告了五个数据集的平均结果，结果显示，<br> CoSENT方法在在相同的MacBERT预训练模型下Fine-tune，其得分超过Sentence-BERT，达到了5%的相对性能提升。</p> 
<h4><a id="43_BackBone_173"></a>4.3 BackBone选择的实验分析</h4> 
<p>我们对比了CoSENT在不同的BackBone下的中文STS-B的实验结果，包括<code>bert-base-chinese</code>、<code>hfl/chinese-macbert-base</code>、<code>nghuyong/ernie-3.0-base-zh</code>等多种中文预训练模型。</p> 
<table><thead><tr><th align="center">backbone</th><th align="center">Chinese-STS-B (spearman, test)</th></tr></thead><tbody><tr><td align="center">bert-base-chinese</td><td align="center">0.7927</td></tr><tr><td align="center">hfl/chinese-bert-wwm-ext</td><td align="center">0.7635</td></tr><tr><td align="center">hfl/chinese-roberta-wwm-ext</td><td align="center">0.7996</td></tr><tr><td align="center">hfl/chinese-macbert-base</td><td align="center">0.7930</td></tr><tr><td align="center">hfl/chinese-macbert-large</td><td align="center">0.7495</td></tr><tr><td align="center">nghuyong/ernie-3.0-nano-zh</td><td align="center">0.6677</td></tr><tr><td align="center">nghuyong/ernie-3.0-base-zh</td><td align="center"><strong>0.8153</strong></td></tr><tr><td align="center">nghuyong/ernie-3.0-xbase-zh</td><td align="center">0.7827</td></tr></tbody></table> 
<p>在中文STS-B的实验中，我们发现CoSENT在不同的BackBone下的实验结果相差不大，这说明CoSENT方法对于不同的BackBone都有很好的适应性，<br> 该实验显示同等参数量模型size下，Backbone最佳是<code>nghuyong/ernie-3.0-base-zh</code>。</p> 
<h4><a id="44_Pooling_194"></a>4.4 Pooling策略的实验分析</h4> 
<p>我们对比了CoSENT使用不同pooling策略的实验结果，包括<code>MEAN</code>、<code>CLS</code>、<code>FIRST_LAST_AVG</code>等多种pooling策略，其他实验设置是一样的：<br> Backbone为<code>nghuyong/ernie-3.0-base-zh</code>，训练集为Chinese-STS-B，batch size为64，t为0.05。</p> 
<table><thead><tr><th align="center">pooling</th><th align="center">Chinese-STS-B (spearman, test)</th></tr></thead><tbody><tr><td align="center">CLS</td><td align="center">0.8020</td></tr><tr><td align="center">POOLER</td><td align="center">0.7379</td></tr><tr><td align="center">FIRST_LAST_AVG</td><td align="center">0.7931</td></tr><tr><td align="center">MEAN</td><td align="center">0.8153</td></tr></tbody></table> 
<p>在中文STS-B的实验中，我们发现<code>MEAN</code> pooling 效果最好，但和<code>CLS</code>、<code>FIRST_LAST_AVG</code>相差不大，我们还发现个有趣的现象，用<code>FIRST_LAST_AVG</code>训练，再用<code>MEAN</code>预测，效果损失很小，或者换过来也一样，<br> pooling策略对中文匹配影响较小。pooling策略最佳选择<code>MEAN</code>。</p> 
<h4><a id="45_Temperature_211"></a>4.5 Temperature超参的实验分析</h4> 
<p>在实验中，我们发现对比学习损失函数中的温度超参数（t）对于结果有很大影响。从下面CoSENT模型的分析实验中可以看到，当t值在0.01到0.05之间时会得到最优结果。</p> 
<p>这个现象再次证明了BERT表示的坍缩问题，因为在句子表示都很接近的情况下，t过大会使句子间相似度更平滑，编码器很难学到知识。而t如果过小，任务就太过简单，所以需要调整到一个合适的范围内。</p> 
<p><em>不同超参数t下的性能</em></p> 
<table><thead><tr><th align="center">temperature</th><th align="center">Chinese-STS-B (spearman, test)</th><th align="center">first-epoch-spearman(dev)</th><th align="center">best_epoch_num</th></tr></thead><tbody><tr><td align="center">0.2</td><td align="center">0.7711</td><td align="center">0.8158</td><td align="center">2</td></tr><tr><td align="center">0.1</td><td align="center">0.7945</td><td align="center">0.8291</td><td align="center">5</td></tr><tr><td align="center">0.05</td><td align="center">0.8051</td><td align="center">0.8277</td><td align="center">2</td></tr><tr><td align="center">0.03</td><td align="center">0.8061</td><td align="center">0.8255</td><td align="center">3</td></tr><tr><td align="center">0.025</td><td align="center">0.8065</td><td align="center">0.8181</td><td align="center">2</td></tr><tr><td align="center">0.0125</td><td align="center">0.8105</td><td align="center">0.7982</td><td align="center">9</td></tr><tr><td align="center">0.01</td><td align="center">0.8127</td><td align="center">0.7838</td><td align="center">9</td></tr><tr><td align="center">0.005</td><td align="center">0.7846</td><td align="center">0.6399</td><td align="center">4</td></tr></tbody></table> 
<p>在中文STS-B的实验中，温度超参数（t）最佳的是0.01，需要9个epoch训练，而设置<strong>t为0.05时，仅需要2个epoch即可达到最佳性能</strong>，<br> 而且模型收敛速度更快，第一个epoch的dev结果就有0.8277，相较t=0.01的0.7838有0.04的提升。另外，Sentence-BERT第一个epoch的dev结果只有0.4630。</p> 
<h4><a id="46_Batch_size_237"></a>4.6 Batch size超参的实验分析</h4> 
<p>NLP的transformer框架下的模型，Batch size会对结果有影响，因此我们也对比了不同Batch size下在Chinese-STS-B数据集CoSENT模型的表现。</p> 
<p><em>不同Batch size下的性能</em></p> 
<table><thead><tr><th align="center">batch size</th><th align="center">Chinese-STS-B (spearman, test)</th><th align="center">first-epoch-spearman(dev)</th><th align="center">best_epoch_num</th></tr></thead><tbody><tr><td align="center">16</td><td align="center">0.7891</td><td align="center">0.8295</td><td align="center">5</td></tr><tr><td align="center">32</td><td align="center">0.7957</td><td align="center">0.8339</td><td align="center">4</td></tr><tr><td align="center">64</td><td align="center">0.8051</td><td align="center">0.8306</td><td align="center">2</td></tr><tr><td align="center">128</td><td align="center">0.8011</td><td align="center">0.8251</td><td align="center">2</td></tr><tr><td align="center">192</td><td align="center">0.8018</td><td align="center">0.8191</td><td align="center">7</td></tr></tbody></table> 
<p>实验结果，可以看到batch size和spearman得分两者基本是成正比的，但提升很有限。该实验显示最佳batch size是64。</p> 
<h4><a id="47__257"></a>4.7 融合无监督信号的实验分析</h4> 
<p>无监督句子表征的模型有较大突破，为了提升模型的表征效果，我们希望能在有监督模型之后融合无监督信号，下面对比实验了Whitening、SimCSE等无监督方法。</p> 
<table><thead><tr><th align="center">arch</th><th align="center">backbone</th><th align="center">model</th><th align="center">Chinese-STS-B (spearman)</th></tr></thead><tbody><tr><td align="center">SBERT</td><td align="center">bert-base-chinese</td><td align="center">SBERT-bert-chinese-finetune-ChineseSTS</td><td align="center">0.7723</td></tr><tr><td align="center">RoFormer-Sim</td><td align="center">RoFormer-base-chinese</td><td align="center">chinese_roformer-sim-char-ft_L-12_H-768_A-12</td><td align="center">0.7827</td></tr><tr><td align="center">SimBERT</td><td align="center">bert-base-chinese</td><td align="center">chinese_simbert_L-12_H-768_A-12</td><td align="center">0.7098</td></tr><tr><td align="center">SimBERT</td><td align="center">chinese_simbert_L-12_H-768_A-12</td><td align="center">SimBERT-base-chinese-SimCSE-cls-unsup</td><td align="center">0.7562</td></tr><tr><td align="center">SimBERT</td><td align="center">chinese_simbert_L-12_H-768_A-12</td><td align="center">SimBERT-base-chinese-SimCSE-first-last-avg-unsup</td><td align="center">0.7264</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-SimCSE-cls-unsup</td><td align="center">0.6699</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-SimCSE-cls-sup</td><td align="center">0.7613</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-mean_pooling</td><td align="center">0.5473</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-first_last_avg</td><td align="center">0.5446</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-first_last_avg-whiten(768)</td><td align="center">0.6808</td></tr><tr><td align="center">BERT</td><td align="center">bert-base-chinese</td><td align="center">BERT-base-chinese-sup-finetune-ChineseSTS</td><td align="center">0.7755</td></tr><tr><td align="center">CoSENT</td><td align="center">bert-base-chinese</td><td align="center">CoSENT-bert-base-chinese-first_last_avg</td><td align="center">0.7942</td></tr><tr><td align="center">CoSENT</td><td align="center">hfl/chinese-macbert-base</td><td align="center">CoSENT-macbert-base-chinese-first_last_avg</td><td align="center">0.8051</td></tr><tr><td align="center">CoSENT</td><td align="center">hfl/chinese-macbert-base</td><td align="center">CoSENT-macbert-base-chinese-first_last_avg-whiten(768)</td><td align="center">0.7642</td></tr><tr><td align="center">CoSENT</td><td align="center">hfl/chinese-macbert-base</td><td align="center">CoSENT-macbert-base-chinese-first_last_avg-whiten(384)</td><td align="center">0.7708</td></tr><tr><td align="center">CoSENT</td><td align="center">hfl/chinese-macbert-base</td><td align="center">CoSENT-macbert-base-chinese-first_last_avg-simcse</td><td align="center">0.8133</td></tr></tbody></table> 
<p>从实验结果中可以看，后接whitening操作使模型效果降低了0.035，而接SimCSE模型效果提升0.008。</p> 
<p>表明在监督模型后直接硬接无监督训练，提升不大，基于SimCSE无监督训练会有小幅提升。</p> 
<h3><a id="5_Release_Model_287"></a>5. Release Model</h3> 
<p>我们基于以上实验结果，按最优参数训练了文本表征的CoSENT模型，在中文匹配评测集上取得了SOTA效果，并具备s2s(sentence to sentence)和s2p(sentence to paraphrase)的文本相似度计算、相似文本检索能力。</p> 
<p><strong>训练参数</strong></p> 
<ul><li>arch: CoSENT</li><li>backbone: nghuyong/ernie-3.0-base-zh</li><li>pooling: MEAN</li><li>temperature: 0.05</li><li>batch_size: 64</li><li>max_seq_length: 256</li></ul> 
<p>评测结果：</p> 
<table><thead><tr><th align="left">Arch</th><th align="left">BackBone</th><th align="left">Model</th><th align="center">ATEC</th><th align="center">BQ</th><th align="center">LCQMC</th><th align="center">PAWSX</th><th align="center">STS-B</th><th align="center">SOHU-dd</th><th align="center">SOHU-dc</th><th align="center">Avg</th><th align="center">QPS</th></tr></thead><tbody><tr><td align="left">Word2Vec</td><td align="left">word2vec</td><td align="left"><a href="https://ai.tencent.com/ailab/nlp/en/download.html" rel="nofollow">w2v-light-tencent-chinese</a></td><td align="center">20.00</td><td align="center">31.49</td><td align="center">59.46</td><td align="center">2.57</td><td align="center">55.78</td><td align="center">55.04</td><td align="center">20.70</td><td align="center">35.03</td><td align="center">23769</td></tr><tr><td align="left">SBERT</td><td align="left">xlm-roberta-base</td><td align="left"><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" rel="nofollow">sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</a></td><td align="center">18.42</td><td align="center">38.52</td><td align="center">63.96</td><td align="center">10.14</td><td align="center">78.90</td><td align="center">63.01</td><td align="center">52.28</td><td align="center">46.46</td><td align="center">3138</td></tr><tr><td align="left">Instructor</td><td align="left">hfl/chinese-roberta-wwm-ext</td><td align="left"><a href="https://huggingface.co/moka-ai/m3e-base" rel="nofollow">moka-ai/m3e-base</a></td><td align="center">41.27</td><td align="center">63.81</td><td align="center">74.87</td><td align="center">12.20</td><td align="center">76.96</td><td align="center">75.83</td><td align="center">60.55</td><td align="center">57.93</td><td align="center">2980</td></tr><tr><td align="left">CoSENT</td><td align="left">hfl/chinese-macbert-base</td><td align="left"><a href="https://huggingface.co/shibing624/text2vec-base-chinese" rel="nofollow">shibing624/text2vec-base-chinese</a></td><td align="center">31.93</td><td align="center">42.67</td><td align="center">70.16</td><td align="center">17.21</td><td align="center">79.30</td><td align="center">70.27</td><td align="center">50.42</td><td align="center">51.61</td><td align="center">3008</td></tr><tr><td align="left">CoSENT</td><td align="left">hfl/chinese-lert-large</td><td align="left"><a href="https://huggingface.co/GanymedeNil/text2vec-large-chinese" rel="nofollow">GanymedeNil/text2vec-large-chinese</a></td><td align="center">32.61</td><td align="center">44.59</td><td align="center">69.30</td><td align="center">14.51</td><td align="center">79.44</td><td align="center">73.01</td><td align="center">59.04</td><td align="center">53.12</td><td align="center">2092</td></tr><tr><td align="left">CoSENT</td><td align="left">nghuyong/ernie-3.0-base-zh</td><td align="left"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-sentence" rel="nofollow">shibing624/text2vec-base-chinese-sentence</a></td><td align="center">43.37</td><td align="center">61.43</td><td align="center">73.48</td><td align="center">38.90</td><td align="center">78.25</td><td align="center">70.60</td><td align="center">53.08</td><td align="center">59.87</td><td align="center">3089</td></tr><tr><td align="left">CoSENT</td><td align="left">nghuyong/ernie-3.0-base-zh</td><td align="left"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase" rel="nofollow">shibing624/text2vec-base-chinese-paraphrase</a></td><td align="center">44.89</td><td align="center">63.58</td><td align="center">74.24</td><td align="center">40.90</td><td align="center">78.93</td><td align="center">76.70</td><td align="center">63.30</td><td align="center"><strong>63.08</strong></td><td align="center">3066</td></tr></tbody></table> 
<ul><li><code>shibing624/text2vec-base-chinese</code>模型，是用CoSENT方法训练，基于<code>hfl/chinese-macbert-base</code>在中文STS-B数据训练得到，并在中文STS-B测试集评估达到较好效果，模型文件已经上传HF model hub，中文通用语义匹配任务推荐使用</li><li><code>shibing624/text2vec-base-chinese-sentence</code>模型，是用CoSENT方法训练，基于<code>nghuyong/ernie-3.0-base-zh</code>用人工挑选后的中文STS数据集训练得到，并在中文各NLI测试集评估达到较好效果，模型文件已经上传HF model hub，中文s2s语义匹配任务推荐使用</li><li><code>shibing624/text2vec-base-chinese-paraphrase</code>模型，是用CoSENT方法训练，基于<code>nghuyong/ernie-3.0-base-zh</code>用人工挑选后的中文STS数据集，并加入了s2p数据，强化了其长文本的表征能力，并在中文各NLI测试集评估达到SOTA，模型文件已经上传HF model hub，中文s2p语义匹配任务推荐使用</li><li>为测评模型的鲁棒性，加入了未训练过的SOHU测试集，用于测试模型的泛化能力</li></ul> 
<h3><a id="6__320"></a>6. 总结</h3> 
<p>在此工作中，我们分析了BERT句向量表示空间坍缩的原因，并分析了基于排序loss的句子表示CoSENT模型的优势。CoSENT在有监督训练的实验中表现出了优秀的性能，在中英文数据集上都超越了基线模型，表现出模型对句子表征的有效性。</p> 
<p>目前，相关代码已经放Github上：<a href="https://github.com/shibing624/text2vec">shibing624/text2vec</a>，欢迎大家使用。</p> 
<h3><a id="_326"></a>参考文献</h3> 
<ul><li>[1] Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.” Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.</li><li>[2] Li, Bohan, et al. “On the Sentence Embeddings from Pre-trained Language Models.” Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.</li><li>[3] Gao, Jun, et al. “Representation Degeneration Problem in Training Natural Language Generation Models.” International Conference on Learning Representations. 2018.</li><li>[4] Wang, Lingxiao, et al. “Improving Neural Language Generation with Spectrum Control.” International Conference on Learning Representations. 2019.</li><li>[5] Conneau, Alexis, et al. “Supervised Learning of Universal Sentence Representations from Natural Language Inference Data.” Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017.</li><li>[6] Cer, Daniel, et al. “Universal Sentence Encoder for English.” Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2018.</li><li>[7] Wang, Shuohang, et al. “Cross-Thought for Sentence Encoder Pre-training.” Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.</li><li>[8] Yang, Ziyi, et al. “Universal Sentence Representation Learning with Conditional Masked Language Model.” arXiv preprint arXiv:2012.14388 (2020).</li><li>[9] Lee, Haejun, et al. “SLM: Learning a Discourse Language Representation with Sentence Unshuffling.” Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.</li><li>[10] Su, Jianlin, et al. “Whitening sentence representations for better semantics and faster retrieval.” arXiv preprint arXiv:2103.15316 (2021).</li><li>[11] Gao, Tianyu, Xingcheng Yao, and Danqi Chen. “SimCSE: Simple Contrastive Learning of Sentence Embeddings.” arXiv preprint arXiv:2104.08821 (2021).</li><li>[12] Wu, Xing, et al. “Conditional bert contextual augmentation.” International Conference on Computational Science. Springer, Cham, 2019.</li><li>[13] Zhou, Wangchunshu, et al. “BERT-based lexical substitution.” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.</li><li>[14] He, Kaiming, et al. “Momentum contrast for unsupervised visual representation learning.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</li><li>[15] Chen, Ting, et al. “A simple framework for contrastive learning of visual representations.” International conference on machine learning. PMLR, 2020.</li><li>[16] Zhang, Yan, et al. “An Unsupervised Sentence Embedding Method by Mutual Information Maximization.” Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.</li><li>[17] Fang, Hongchao, et al. “Cert: Contrastive self-supervised learning for language understanding.” arXiv preprint arXiv:2005.12766 (2020).</li><li>[18] Carlsson, Fredrik, et al. “Semantic re-tuning with contrastive tension.” International Conference on Learning Representations. 2021.</li><li>[19] Giorgi, John M., et al. “Declutr: Deep contrastive learning for unsupervised textual representations.” arXiv preprint arXiv:2006.03659 (2020).</li><li>[20] Wu, Zhuofeng, et al. “CLEAR: Contrastive Learning for Sentence Representation.” arXiv preprint arXiv:2012.15466(2020).</li><li>[21] 苏剑林. (Su. 06, 2022). 《CoSENT（一）：比Sentence-BERT更有效的句向量方案 》[Blog post]. Retrieved from https://kexue.fm/archives/8847</li></ul>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8327b534c9e7e2df4099a4196a442343/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">postman｜接口测试 ｜ pre-request script 场景应用</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f37ca65d28222fc487d14e6342819fe7/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">java虚拟机可以创建多少对象</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>