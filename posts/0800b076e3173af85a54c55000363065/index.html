<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>DataStream API（输出算子） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="DataStream API（输出算子）" />
<meta property="og:description" content="源算子 源算子
转换算子 转换算子
输出算子 1.连接到外部系统 连接外部系统是计算机科学和信息技术领域中常见的一个任务，通常涉及到与外部数据源或服务进行交互。具体的方法和工具会根据不同的应用场景和需求而有所不同。以下是一些常见的连接外部系统的方法：
应用程序接口（API）：许多外部系统都提供API，以便其他系统可以与其进行交互。通过API，可以使用各种编程语言和工具来获取或交换数据。常见的API包括REST API和SOAP API。数据集成工具：这些工具可以帮助将数据从各种来源（包括数据库、文件、API等）集成到一个中央位置。一些流行的数据集成工具包括Talend、Apache NiFi和 Informatica PowerCenter。连接器或适配器：这些是专用的软件组件，用于连接到特定的外部系统。例如，一些CRM系统提供连接器，使其他应用程序可以与CRM系统中的数据进行交互。编程语言和框架：许多编程语言和框架可用于连接到外部系统。例如，Python是一种流行的语言，可用于通过API或Web scraping技术连接到外部系统。第三方服务：许多第三方服务提供连接外部系统的功能，例如OAuth、SAML和OpenID Connect等身份验证协议。这些服务可以与许多不同的系统进行交互，并提供安全的身份验证和授权机制。 连接外部系统时需要考虑许多因素，例如安全性、性能、可靠性和兼容性。因此，选择适当的工具和技术，并仔细规划和测试连接过程是非常重要的。
2.输出到文件 要将数据流输出到文件，您可以使用 Flink 的 File Sink。以下是一个示例代码片段
import org.apache.flink.api.common.io.OutputFormat; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.sink.RichSinkFunction; import org.apache.flink.util.IOUtils; import java.io.BufferedWriter; import java.io.FileWriter; import java.io.IOException; // 创建 StreamExecutionEnvironment StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 创建数据流 DataStream&lt;MyData&gt; dataStream = ... // 创建数据流 // 创建自定义的输出格式，用于将数据写入文件 class MyOutputFormat implements OutputFormat&lt;MyData&gt; { private String filePrefix = &#34;output&#34;; private int count = 0; private String suffix = &#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0800b076e3173af85a54c55000363065/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-23T19:45:12+08:00" />
<meta property="article:modified_time" content="2024-01-23T19:45:12+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">DataStream API（输出算子）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <h3>源算子</h3> 
<p><a class="link-info" href="https://blog.csdn.net/2301_77578187/article/details/135771782?spm=1001.2014.3001.5502" title="源算子">源算子</a></p> 
<h3>转换算子</h3> 
<p><a class="link-info" href="https://blog.csdn.net/2301_77578187/article/details/135772752?spm=1001.2014.3001.5502" title="转换算子">转换算子</a></p> 
<h3>输出算子</h3> 
<p><img alt="" height="143" src="https://images2.imgbox.com/29/d4/67GzZrc8_o.png" width="1025"></p> 
<h4 style="background-color:transparent;">1.<span style="color:#000000;">连接到外部系统 </span></h4> 
<p></p> 
<p>        连接外部系统是计算机科学和信息技术领域中常见的一个任务，通常涉及到与外部数据源或服务进行交互。具体的方法和工具会根据不同的应用场景和需求而有所不同。以下是一些常见的连接外部系统的方法：</p> 
<ol><li>应用程序接口（API）：许多外部系统都提供API，以便其他系统可以与其进行交互。通过API，可以使用各种编程语言和工具来获取或交换数据。常见的API包括REST API和SOAP API。</li><li>数据集成工具：这些工具可以帮助将数据从各种来源（包括数据库、文件、API等）集成到一个中央位置。一些流行的数据集成工具包括Talend、Apache NiFi和 Informatica PowerCenter。</li><li>连接器或适配器：这些是专用的软件组件，用于连接到特定的外部系统。例如，一些CRM系统提供连接器，使其他应用程序可以与CRM系统中的数据进行交互。</li><li>编程语言和框架：许多编程语言和框架可用于连接到外部系统。例如，Python是一种流行的语言，可用于通过API或Web scraping技术连接到外部系统。</li><li>第三方服务：许多第三方服务提供连接外部系统的功能，例如OAuth、SAML和OpenID Connect等身份验证协议。这些服务可以与许多不同的系统进行交互，并提供安全的身份验证和授权机制。</li></ol> 
<p>        连接外部系统时需要考虑许多因素，例如安全性、性能、可靠性和兼容性。因此，选择适当的工具和技术，并仔细规划和测试连接过程是非常重要的。</p> 
<h4>2.<span style="color:#000000;">输出到文件 </span></h4> 
<p>要将数据流输出到文件，您可以使用 Flink 的 File Sink。以下是一个示例代码片段</p> 
<pre><code class="language-Scala">import org.apache.flink.api.common.io.OutputFormat;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;  
import org.apache.flink.util.IOUtils;  
  
import java.io.BufferedWriter;  
import java.io.FileWriter;  
import java.io.IOException;  
  
// 创建 StreamExecutionEnvironment  
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
// 创建数据流  
DataStream&lt;MyData&gt; dataStream = ... // 创建数据流  
  
// 创建自定义的输出格式，用于将数据写入文件  
class MyOutputFormat implements OutputFormat&lt;MyData&gt; {  
    private String filePrefix = "output";  
    private int count = 0;  
    private String suffix = ".txt";  
    private String fieldDelimiter = "\t";  
    private BufferedWriter writer;  
  
    @Override  
    public void open(Configuration parameters) throws IOException {  
        filePrefix = parameters.getString("filePrefix", filePrefix);  
        suffix = parameters.getString("suffix", suffix);  
        fieldDelimiter = parameters.getString("fieldDelimiter", fieldDelimiter);  
        count = 0;  
        writer = new BufferedWriter(new FileWriter(filePrefix + count + suffix));  
    }  
  
    @Override  
    public void writeRecord(MyData record) throws IOException {  
        writer.write(record.getField1() + fieldDelimiter + record.getField2());  
        writer.newLine();  
    }  
  
    @Override  
    public void close() throws IOException {  
        writer.close();  
    }  
}  
  
// 创建 File Sink 并将其添加到数据流中  
dataStream.addSink(new RichSinkFunction&lt;MyData&gt;() {  
    private MyOutputFormat outputFormat;  
  
    @Override  
    public void open(Configuration parameters) throws Exception {  
        outputFormat = new MyOutputFormat();  
    }  
  
    @Override  
    public void invoke(MyData value, Context context) throws Exception {  
        outputFormat.writeRecord(value);  
    }  
});  
  
// 执行 Flink 作业并等待结果  
env.execute("Flink File Sink Example");</code></pre> 
<p>        在上面的示例中，我们首先创建了一个 StreamExecutionEnvironment 和一个数据流。接下来，我们定义了一个自定义的 OutputFormat 类，用于将数据写入文件。在 MyOutputFormat 类中，我们实现了 open() 方法来打开文件并设置文件名、字段分隔符等参数，writeRecord() 方法来写入数据到文件，以及 close() 方法来关闭文件。然后，我们创建了一个 RichSinkFunction，并使用 MyOutputFormat 作为序列化器。最后，我们将 Sink 添加到数据流中并执行 Flink 作业。请注意，在打开 OutputFormat 时，我们传递了作业的参数，以便在打开文件时使用。 </p> 
<h4>3.<span style="color:#000000;">输出到 </span><span style="color:#000000;">Kafka</span></h4> 
<p>要将数据流输出到 Kafka，您可以使用 Flink 的 Kafka Producer Sink。以下是一个示例代码片段</p> 
<p> </p> 
<pre><code class="language-Scala">import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;  
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer010;  
import org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;  
  
import java.util.Properties;  
  
// 创建 StreamExecutionEnvironment  
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
// 创建数据流  
DataStream&lt;MyData&gt; dataStream = ... // 创建数据流  
  
// 配置 Kafka 连接选项  
Properties properties = new Properties();  
properties.setProperty("bootstrap.servers", "localhost:9092");  
properties.setProperty("group.id", "test");  
  
// 创建 Kafka Producer Sink 并将其添加到数据流中  
dataStream.addSink(new FlinkKafkaProducer010&lt;MyData&gt;("topic", new MyDataSerializationSchema(), properties));  
  
// 执行 Flink 作业  
env.execute("Flink Kafka Sink Example");</code></pre> 
<p>         在上面的示例中，我们首先创建了一个 StreamExecutionEnvironment 和一个数据流。接下来，我们配置了 Kafka 连接选项，包括 Kafka Broker 的地址和消费者组的 ID。然后，我们使用 FlinkKafkaProducer010 类创建一个 Kafka Producer Sink，并设置要写入的 Kafka 主题、序列化器和 Kafka 连接属性。在本例中，我们自定义了一个 MyDataSerializationSchema 类来实现 KafkaSerializationSchema 接口，用于将 MyData 对象序列化为字节数组。最后，我们将 Sink 添加到数据流中并执行 Flink 作业。</p> 
<h4><span style="color:#000000;">4.输出到 </span><span style="color:#000000;">Redis </span></h4> 
<p>要将数据流输出到 Redis，您可以使用 Flink 的 Redis Sink。以下是一个示例代码片段</p> 
<pre><code class="language-Scala">import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.connectors.redis.RedisSink;  
import org.apache.flink.streaming.connectors.redis.common.config.RedisConfig;  
import org.apache.flink.streaming.connectors.redis.common.config.RedisConfigBuilder;  
import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommand;  
import org.apache.flink.streaming.connectors.redis.common.mapper.RedisCommandDescription;  
import org.apache.flink.streaming.connectors.redis.common.mapper.RedisMapper;  
import org.apache.flink.streaming.connectors.redis.common.mapper.StringRedisMapper;  
  
// 创建 StreamExecutionEnvironment  
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
// 创建数据流  
DataStream&lt;MyData&gt; dataStream = ... // 创建数据流  
  
// 配置 Redis 连接选项  
RedisConfig redisConfig = new RedisConfigBuilder()  
    .setHost("localhost")  
    .setPort(6379)  
    build();  
  
// 创建 Redis Sink 并将其添加到数据流中  
dataStream.addSink(RedisSink.&lt;String, MyData&gt;builder()  
    // 定义 Redis Mapper，将 MyData 对象转换为 Redis 命令和参数  
    .setMapper(new StringRedisMapper&lt;MyData&gt;() {  
        @Override  
        public String getCommandDescription() {  
            return RedisCommandDescription.&lt;MyData&gt;builder()  
                // 设置 Redis 命令为 "SET"  
                .setCommand(RedisCommand.&lt;MyData&gt;set())  
                // 设置键名为 "key"  
                .setKey("key")  
                // 设置值为 MyData 对象的 col1 字段  
                .setValue(myData -&gt; "value:" + myData.col1)  
                build();  
        }  
    })  
    // 配置 Redis 连接选项  
    .setRedisConfig(redisConfig)  
    build());  
  
// 执行 Flink 作业  
env.execute("Flink Redis Sink Example");</code></pre> 
<p>        在上面的示例中，我们首先创建了一个 StreamExecutionEnvironment，然后创建了一个数据流。接下来，我们配置了 Redis 连接选项，包括主机和端口。然后，我们使用 RedisSink 的 builder() 方法创建一个 Redis Sink，并设置 Redis Mapper。在 Redis Mapper 中，我们将 MyData 对象转换为 Redis 命令和参数。在本例中，我们使用了 StringRedisMapper，将 MyData 对象的 col1 字段作为值。最后，我们配置 Redis 连接选项并将 Sink 添加到数据流中。最后，我们执行 Flink 作业并等待结果。 </p> 
<h4><span style="color:#000000;">5.输出到 </span><span style="color:#000000;">Elasticsearch </span></h4> 
<p>要将数据流输出到 Elasticsearch，您可以使用 Flink 的 Elasticsearch Sink。以下是一个示例代码片段</p> 
<pre><code class="language-Scala">import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction;  
import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSink;  
import org.apache.flink.streaming.connectors.elasticsearch.common.config.ESClusterConfig;  
import org.apache.flink.streaming.connectors.elasticsearch.common.config.ESClusterConfigBuilder;  
import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSinkFunctionImpl;  
  
// 创建 StreamExecutionEnvironment  
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
// 创建数据流  
DataStream&lt;MyData&gt; dataStream = ... // 创建数据流  
  
// 配置 Elasticsearch 连接选项  
ESClusterConfig esClusterConfig = new ESClusterConfigBuilder()  
    .setClusterName("my-cluster")  
    .setHost("localhost")  
    .setPort(9200)  
    .build();  
  
// 创建 Elasticsearch Sink 并将其添加到数据流中  
dataStream.addSink(ElasticsearchSink.sink(esClusterConfig, new ElasticsearchSinkFunctionImpl&lt;MyData&gt;() {  
    @Override  
    public void sink(MyData value, OutputCollector&lt;Void&gt; out) {  
        out.collect(new MapRecord&lt;&gt;(Map.&lt;String, Object&gt;of("field1", value.col1, "field2", value.col2)));  
    }  
}));  
  
// 执行 Flink 作业  
env.execute("Flink Elasticsearch Sink Example");</code></pre> 
<p>         在上面的示例中，我们首先创建了一个 StreamExecutionEnvironment，然后创建了一个数据流。接下来，我们配置了 Elasticsearch 连接选项，包括集群名称、主机和端口。然后，我们使用 ElasticsearchSink 将数据流写入 Elasticsearch。在 ElasticsearchSink 的构造函数中，我们传递了 Elasticsearch 连接选项和 ElasticsearchSinkFunction。在 ElasticsearchSinkFunction 中，我们将 MyData 对象转换为 MapRecord，其中包含要写入 Elasticsearch 的字段和值。最后，我们执行 Flink 作业并等待结果。</p> 
<h4>6.<span style="color:#000000;">输出到 </span><span style="color:#000000;">MySQL</span><span style="color:#000000;">（</span><span style="color:#000000;">JDBC</span><span style="color:#000000;">）</span></h4> 
<p>要将数据流输出到 MySQL 数据库，您需要使用 JDBC（Java Database Connectivity）连接器。以下是一个示例代码片段</p> 
<pre><code class="language-Scala">import org.apache.flink.api.common.functions.RuntimeContext;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.connectors.jdbc.JdbcConnectionOptions;  
import org.apache.flink.streaming.connectors.jdbc.JdbcSink;  
import org.apache.flink.streaming.connectors.jdbc.JdbcStatementBuilder;  
import org.apache.flink.streaming.connectors.jdbc.JdbcStatementBuilderImpl;  
import org.apache.flink.streaming.connectors.jdbc.JdbcType;  
  
// 创建 StreamExecutionEnvironment  
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
// 创建数据流  
DataStream&lt;MyData&gt; dataStream = ... // 创建数据流  
  
// 配置 JDBC 连接选项  
JdbcConnectionOptions jdbcOptions = new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()  
    .withUrl("jdbc:mysql://localhost:3306/mydatabase")  
    .withDriverName("com.mysql.jdbc.Driver")  
    .withUsername("username")  
    .withPassword("password")  
    .build();  
  
// 创建 JDBC Sink 并将其添加到数据流中  
dataStream.addSink(JdbcSink.sink(  
    "INSERT INTO mytable (col1, col2) VALUES (?, ?)", // SQL 语句模板，使用占位符表示参数  
    (JdbcStatementBuilder&lt;MyData&gt;) (ps, value) -&gt; { // 参数转换函数，将 MyData 对象转换为 JDBC 参数  
        ps.setString(1, value.col1);  
        ps.setInt(2, value.col2);  
    },  
    JdbcType.STRING, // JDBC 参数类型，用于类型转换和占位符替换  
    jdbcOptions));  
  
// 执行 Flink 作业  
env.execute("Flink JDBC Sink Example");</code></pre> 
<p>        在上面的示例中，我们首先创建了一个 StreamExecutionEnvironment，然后创建了一个数据流。接下来，我们配置了 JDBC 连接选项，包括 JDBC URL、驱动程序名称、用户名和密码。然后，我们使用 JdbcSink 将数据流写入 MySQL 数据库。在 JdbcSink 的构造函数中，我们提供了 SQL 语句模板和参数转换函数。在参数转换函数中，我们将 MyData 对象转换为 JDBC 参数。最后，我们执行 Flink 作业并等待结果。</p> 
<h4><span style="color:#000000;">7.自定义 </span><span style="color:#000000;">Sink </span><span style="color:#000000;">输出 </span></h4> 
<p>        Flink 是一个流处理框架，它提供了强大的功能来处理实时数据流。与 Source 类似，Flink 也提供了 SinkFunction 接口和 RichSinkFunction 抽象类，用于将数据写入外部存储。通过实现这些接口，开发人员可以自定义写入任何外部存储的方式，如数据库、文件系统、消息队列等。</p> 
<p>        通过简单地调用 DataStream 的 addSink() 方法，并将自定义的 SinkFunction 传递给该方法，开发人员可以轻松地将数据写入外部存储。这样，开发人员可以更加灵活地处理数据流，并将处理后的结果存储在所需的位置。</p> 
<p>        Flink 的 SinkFunction 接口定义了将数据写入外部存储的基本操作。开发人员需要实现该接口，并覆盖其中的一些方法，如 open()、invoke() 和 close()。RichSinkFunction 是 SinkFunction 的一个扩展，它提供了更多的功能和灵活性，如处理异常和延迟触发等。</p> 
<p>        Flink 的 SinkFunction 和 RichSinkFunction 抽象类为开发人员提供了灵活的数据写入方式，使得开发人员可以轻松地将数据流写入到任何外部存储中。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3904b8056f809132b6c61b943bbe937f/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">hadoop集群主节点挂了standby没有自动切换处理</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f3704d3f0fa95cb930703fb43cc3dae8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">DataStream API（转换算子）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>