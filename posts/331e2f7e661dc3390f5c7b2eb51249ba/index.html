<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ALTER TABLE 分区操作-动态增加一级，多级分区，动态删除分区 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ALTER TABLE 分区操作-动态增加一级，多级分区，动态删除分区" />
<meta property="og:description" content="1.8.5.6 ALTER TABLE 分区操作 alter 分区操作包括增加分区和删除分区操作，这种分区操作在Spark3.x之后被支持，spark2.4版本不支持，并且使用时，必须在spark配置中加入spark.sql.extensions属性，其值为：org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions，在添加分区时还支持分区转换，语法如下：
添加分区语法：ALTER TABLE ... ADD PARTITION FIELD删除分区语法：ALTER TABLE ... DROP PARTITION FIELD 具体操作如下：
创建表mytbl，并插入数据 val spark: SparkSession = SparkSession.builder().master(&#34;local&#34;).appName(&#34;SparkOperateIceberg&#34;) //指定hadoop catalog，catalog名称为hadoop_prod .config(&#34;spark.sql.catalog.hadoop_prod&#34;, &#34;org.apache.iceberg.spark.SparkCatalog&#34;) .config(&#34;spark.sql.catalog.hadoop_prod.type&#34;, &#34;hadoop&#34;) .config(&#34;spark.sql.catalog.hadoop_prod.warehouse&#34;, &#34;hdfs://mycluster/sparkoperateiceberg&#34;) .config(&#34;spark.sql.extensions&#34;,&#34;org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions&#34;) .getOrCreate() //1.创建普通表 spark.sql( &#34;&#34;&#34; | create table hadoop_prod.default.mytbl(id int,name string,loc string,ts timestamp) using iceberg &#34;&#34;&#34;.stripMargin) //2.向表中插入数据，并查询 spark.sql( &#34;&#34;&#34; |insert into hadoop_prod.default.mytbl values |(1,&#39;zs&#39;,&#34;beijing&#34;,cast(1608469830 as timestamp)), |(3,&#39;ww&#39;,&#34;shanghai&#34;,cast(1603096230 as timestamp)) &#34;&#34;&#34;.stripMargin) spark.sql(&#34;select * from hadoop_prod.default.mytbl&#34;).show() 在HDFS中数据存储和结果如下：
将表loc列添加为分区列，并插入数据，查询 //3.将 loc 列添加成分区,必须添加 config(&#34;" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/331e2f7e661dc3390f5c7b2eb51249ba/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-11T16:41:00+08:00" />
<meta property="article:modified_time" content="2022-11-11T16:41:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ALTER TABLE 分区操作-动态增加一级，多级分区，动态删除分区</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown" style="font-size: 16px;"> 
 <h2 id="1856-alter-table-分区操作">1.8.5.6 <strong>ALTER TABLE 分区操作</strong></h2> 
 <p>alter 分区操作包括增加分区和删除分区操作，这种分区操作在Spark3.x之后被支持，spark2.4版本不支持，并且使用时，必须在spark配置中加入spark.sql.extensions属性，其值为：org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions，在添加分区时还支持分区转换，语法如下：</p> 
 <ul><li>添加分区语法：ALTER TABLE ... ADD PARTITION FIELD</li><li>删除分区语法：ALTER TABLE ... DROP PARTITION FIELD</li></ul> 
 <p>具体操作如下：</p> 
 <ol><li><strong>创建表mytbl，并插入数据</strong></li></ol> 
 <pre class="has"><code class="language-scala">val spark: SparkSession = SparkSession.builder().master("local").appName("SparkOperateIceberg")
  //指定hadoop catalog，catalog名称为hadoop_prod
  .config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")
  .config("spark.sql.catalog.hadoop_prod.type", "hadoop")
  .config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://mycluster/sparkoperateiceberg")
  .config("spark.sql.extensions","org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
  .getOrCreate()

//1.创建普通表
spark.sql(
  """
    | create table hadoop_prod.default.mytbl(id int,name string,loc string,ts timestamp) using iceberg
  """.stripMargin)
//2.向表中插入数据，并查询
spark.sql(
  """
    |insert into hadoop_prod.default.mytbl values
    |(1,'zs',"beijing",cast(1608469830 as timestamp)),
    |(3,'ww',"shanghai",cast(1603096230 as timestamp))
  """.stripMargin)
spark.sql("select * from hadoop_prod.default.mytbl").show()</code></pre> 
 <p><em><strong>在HDFS中数据存储和结果如下：</strong></em></p> 
 <p><img src="https://images2.imgbox.com/3f/06/ejnAKE8m_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/ce/69/6SI0QB0D_o.png" alt="" style="outline: none;"></p> 
 <ol><li><strong>将表loc列添加为分区列，并插入数据，查询</strong></li></ol> 
 <pre class="has"><code class="language-scala">//3.将 loc 列添加成分区,必须添加 config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") 配置
spark.sql(
  """
    |alter table hadoop_prod.default.mytbl add partition field loc
  """.stripMargin)

//4.向表 mytbl中继续插入数据，之前数据没有分区，之后数据有分区
spark.sql(
  """
    |insert into hadoop_prod.default.mytbl values
    |(5,'tq',"hangzhou",cast(1608279630 as timestamp)),
    |(2,'ls',"shandong",cast(1634559630 as timestamp))
  """.stripMargin )
spark.sql("select * from hadoop_prod.default.mytbl").show()</code></pre> 
 <p><em><strong>在HDFS中数据存储和结果如下：</strong></em></p> 
 <p><img src="https://images2.imgbox.com/9c/dd/SdbkN8H8_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/d0/ff/3R0uLdsx_o.png" alt="" style="outline: none;"></p> 
 <p><strong>注意：<mark>添加分区字段是元数据操作，不会改变现有的表数据，新数据将使用新分区写入数据，现有数据将继续保留在原有的布局中</mark>。</strong></p> 
 <blockquote> 
  <p>我的测试：</p> 
 </blockquote> 
 <p><mark>测试代码：</mark></p> 
 <pre class="has"><code class="language-scala">package com.shujia.spark.iceberg

import org.apache.spark.sql.SparkSession

object AlterTablePartition {
  def main(args: Array[String]): Unit = {

    /**
     * alter 分区操作包括增加分区和删除分区操作，这种分区操作在Spark3.x之后被支持，spark2.4版本不支持，并且使用时，
     * 必须在spark配置中加入spark.sql.extensions属性，
     * 其值为：org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions，在添加分区时还支持分区转换，语法如下：
     *     添加分区语法：ALTER TABLE ... ADD PARTITION FIELD
     *    删除分区语法：ALTER TABLE ... DROP PARTITION FIELD
     *
     */

    val spark: SparkSession = SparkSession
      .builder()
      .appName("SparkOperateIceberg")
      //指定hive catalog, catalog名称为hive_prod
      .config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.hive_prod.type", "hive")
      .config("spark.sql.catalog.hive_prod.uri", "thrift://master:9083")
      .config("iceberg.engine.hive.enabled", "true")
      // 将 loc 列添加成分区,必须添加
      .config("spark.sql.extensions","org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()


    //1.创建普通表
    spark.sql(
      """
        | create table if not exists hive_prod.iceberg.repartition1
        | (id int,name string,loc string,ts timestamp) using iceberg
        |
  """.stripMargin)
    //2.向表中插入数据，并查询
    spark.sql(
      """
        |insert into hive_prod.iceberg.repartition1 values
        |(1,'zs',"beijing",cast(1608469830 as timestamp)),
        |(3,'ww',"shanghai",cast(1603096230 as timestamp))
        |
      """.stripMargin)

    spark.sql("select * from hive_prod.iceberg.repartition1").show()


    //3.将 loc 列添加成分区,必须添加 config("spark.sql.extensions",
    // "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") 配置
    spark.sql(
      """
        |alter table hive_prod.iceberg.repartition1 add partition field loc
        |
  """.stripMargin)

    //4.向表 mytbl中继续插入数据，之前数据没有分区，之后数据有分区
    spark.sql(
      """
        |insert into hive_prod.iceberg.repartition1 values
        |(5,'tq',"hangzhou",cast(1608279630 as timestamp)),
        |(6,'xx',"hangzhou",cast(1608279631 as timestamp)),
        |(2,'ls',"shandong",cast(1634559632 as timestamp))
        |
      """.stripMargin )

    spark.sql("select * from hive_prod.iceberg.repartition1").show()

    //spark 提交任务的命令
    //spark-submit --master yarn --class com.shujia.spark.iceberg.AlterTablePartition spark-1.0.jar
  }
}</code></pre> 
 <p><img src="https://images2.imgbox.com/e0/06/45Avpzjo_o.png" alt="" style="outline: none;"></p> 
 <ol><li><strong>将ts列进行转换作为分区列，插入数据并查询</strong></li></ol> 
 <pre class="has"><code class="language-scala">//5.将 ts 列通过分区转换添加为分区列
spark.sql(
  """
    |alter table hadoop_prod.default.mytbl add partition field years(ts)
  """.stripMargin)

//6.向表 mytbl中继续插入数据，之前数据没有分区，之后数据有分区
spark.sql(
  """
    |insert into hadoop_prod.default.mytbl values
    |(4,'ml',"beijing",cast(1639920630 as timestamp)),
    |(6,'gb',"tianjin",cast(1576843830 as timestamp))
  """.stripMargin )
spark.sql("select * from hadoop_prod.default.mytbl").show()</code></pre> 
 <p><em><strong>在HDFS中数据存储和结果如下：</strong></em></p> 
 <p><img src="https://images2.imgbox.com/90/de/lj3nBbEL_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/59/6e/WtUmEv7p_o.png" alt="" style="outline: none;"></p> 
 <blockquote> 
  <p><mark>我的测试，在一级分区的基础上再次添加分区</mark></p> 
 </blockquote> 
 <p>测试代码：</p> 
 <pre class="has"><code class="language-scala">package com.shujia.spark.iceberg

import org.apache.spark.sql.SparkSession

object AlterTable2Partitions {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession = SparkSession
      .builder()
      .appName("SparkOperateIceberg")
      //指定hive catalog, catalog名称为hive_prod
      .config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.hive_prod.type", "hive")
      .config("spark.sql.catalog.hive_prod.uri", "thrift://master:9083")
      .config("iceberg.engine.hive.enabled", "true")
      // 将 loc 列添加成分区,必须添加
      .config("spark.sql.extensions","org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()


    //5.将 ts 列通过分区转换添加为分区列
    spark.sql(
      """
        |alter table hive_prod.iceberg.repartition1 add partition field years(ts)
  """.stripMargin)

    //6.向表 mytbl中继续插入数据，之前数据没有分区，之后数据有分区
    spark.sql(
      """
        |insert into hive_prod.iceberg.repartition1 values
        |(4,'ml',"beijing",cast(1639920630 as timestamp)),
        |(4,'mm',"beijing",cast(1639920639 as timestamp)),
        |(6,'gb',"tianjin",cast(1576843830 as timestamp))
        |
      """.stripMargin )


    spark.sql("select * from hive_prod.iceberg.repartition1").show()

    //spark 提交任务的命令
    //spark-submit --master yarn --class com.shujia.spark.iceberg.AlterTable2Partitions spark-1.0.jar
  }
}</code></pre> 
 <p><img src="https://images2.imgbox.com/92/f5/mANTMuAs_o.png" alt="" style="outline: none;"></p> 
 <ol><li><strong>删除分区loc</strong></li></ol> 
 <pre class="has"><code class="language-scala">//7.删除表 mytbl 中的loc分区
spark.sql(
  """
    |alter table hadoop_prod.default.mytbl drop partition field loc
  """.stripMargin)
//8.继续向表 mytbl 中插入数据，并查询
spark.sql(
  """
    |insert into hadoop_prod.default.mytbl values
    |(4,'ml',"beijing",cast(1639920630 as timestamp)),
    |(6,'gb',"tianjin",cast(1576843830 as timestamp))
  """.stripMargin )
spark.sql("select * from hadoop_prod.default.mytbl").show()</code></pre> 
 <p><em><strong>在HDFS中数据存储和结果如下：</strong></em></p> 
 <p><img src="https://images2.imgbox.com/7a/ae/9PzrL4M9_o.png" alt="" style="outline: none;"></p> 
 <p><strong>注意：由于表中还有ts分区转换之后对应的分区，所以继续插入的数据loc分区为null</strong></p> 
 <p><img src="https://images2.imgbox.com/d1/e2/SBXHJgzK_o.png" alt="" style="outline: none;"></p> 
 <blockquote> 
  <p><mark>我的测试</mark></p> 
 </blockquote> 
 <p>测试代码：</p> 
 <pre class="has"><code class="language-scala">package com.shujia.spark.iceberg

import org.apache.spark.sql.SparkSession

object DeleteTablePartition {
  def main(args: Array[String]): Unit = {

    /**
     *
     * 删除一个分区
     *
     */
    val spark: SparkSession = SparkSession
      .builder()
      .appName("SparkOperateIceberg")
      //指定hive catalog, catalog名称为hive_prod
      .config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.hive_prod.type", "hive")
      .config("spark.sql.catalog.hive_prod.uri", "thrift://master:9083")
      .config("iceberg.engine.hive.enabled", "true")
      // 将 loc 列添加成分区,必须添加
      .config("spark.sql.extensions","org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()

    //7.删除表 mytbl 中的loc分区
    spark.sql(
      """
        |alter table hive_prod.iceberg.repartition1 drop partition field loc
  """.stripMargin)
    //8.继续向表 mytbl 中插入数据，并查询
    spark.sql(
      """
        |insert into hive_prod.iceberg.repartition1 values
        |(4,'ml',"beijing",cast(1639920630 as timestamp)),
        |(6,'gb',"tianjin",cast(1576843830 as timestamp))
        |
  """.stripMargin )

    spark.sql("select * from hive_prod.iceberg.repartition1").show()
    //spark 提交任务的命令
    //spark-submit --master yarn --class com.shujia.spark.iceberg.DeleteTablePartition spark-1.0.jar
  }
}</code></pre> 
 <p><img src="https://images2.imgbox.com/47/a5/R5NwUrcl_o.png" alt="" style="outline: none;"></p> 
 <ol><li><strong>删除分区years(ts)</strong></li></ol> 
 <pre class="has"><code class="language-scala">//9.删除表 mytbl 中的years(ts) 分区
spark.sql(
  """
    |alter table hadoop_prod.default.mytbl drop partition field years(ts)
  """.stripMargin)
//10.继续向表 mytbl 中插入数据，并查询
spark.sql(
  """
    |insert into hadoop_prod.default.mytbl values
    |(5,'tq',"hangzhou",cast(1608279630 as timestamp)),
    |(2,'ls',"shandong",cast(1634559630 as timestamp))
  """.stripMargin )
spark.sql("select * from hadoop_prod.default.mytbl").show()</code></pre> 
 <p><em><strong>在HDFS中数据存储和结果如下：</strong></em></p> 
 <p><img src="https://images2.imgbox.com/2b/0d/CLrb9Mek_o.png" alt="" style="outline: none;"></p> 
 <p><img src="https://images2.imgbox.com/dd/39/zarn5NRr_o.png" alt="" style="outline: none;"></p> 
 <blockquote> 
  <p><mark>我的测试：</mark></p> 
 </blockquote> 
 <p>测试代码：</p> 
 <pre class="has"><code class="language-scala">package com.shujia.spark.iceberg

import org.apache.spark.sql.SparkSession

object DeleteTable2Partitions {
  def main(args: Array[String]): Unit = {

    /**
     *
     * 删除一个分区之后再次删除一个分区
     *
     */
    val spark: SparkSession = SparkSession
      .builder()
      .appName("SparkOperateIceberg")
      //指定hive catalog, catalog名称为hive_prod
      .config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog")
      .config("spark.sql.catalog.hive_prod.type", "hive")
      .config("spark.sql.catalog.hive_prod.uri", "thrift://master:9083")
      .config("iceberg.engine.hive.enabled", "true")
      // 将 loc 列添加成分区,必须添加
      .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
      .enableHiveSupport()
      .getOrCreate()

    //9.删除表 mytbl 中的years(ts) 分区
    spark.sql(
      """
        |alter table hive_prod.iceberg.repartition1 drop partition field years(ts)
  """.stripMargin)
    //10.继续向表 mytbl 中插入数据，并查询
    spark.sql(
      """
        |insert into hive_prod.iceberg.repartition1 values
        |(5,'tq',"hangzhou",cast(1608279630 as timestamp)),
        |(2,'ls',"shandong",cast(1634559630 as timestamp))
  """.stripMargin )
    spark.sql("select * from hive_prod.iceberg.repartition1").show()


    //spark 提交任务的命令
    //spark-submit --master yarn --class com.shujia.spark.iceberg.DeleteTable2Partitions spark-1.0.jar
  }
  }</code></pre> 
 <p><img src="https://images2.imgbox.com/2e/6c/1WlIKmTA_o.png" alt="" style="outline: none;"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e48d2059c1db80ecf3201db6839b4863/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Redis 升级更新</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d07f82b4c07c0f61dc085669de8a3852/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">phpStudy backdoor 2019后门漏洞复现过程记录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>