<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>中文NLP笔记 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="中文NLP笔记" />
<meta property="og:description" content="1.中文自然语言处理的一般流程 图片发自简书App
中文NLP一般流程
1. 获取语料
语料，是NLP任务所研究的内容
通常用一个文本集合作为语料库（Corpus）
来源：
已有语料
积累的文档
下载语料
搜狗语料、人民日报语料
抓取语料
2. 语料预处理
1.语料清洗
留下有用的，删掉噪音数据
常见的数据清洗方式 :
人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。
2.分词
将文本分成词语
常见的分词算法 :
基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法
3.词性标注
给词语打词类标签，如形容词、动词、名词等
在情感分析、知识推理等任务中需要
常见的词性标注方法
基于规则
基于统计
如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。
4.去停用词
去掉对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等
3.特征工程
把分词表示成计算机能够计算的类型，一般为向量
常用的表示模型 :
词袋模型（Bag of Word, BOW)
TF-IDF 词向量
One-hot Word2Vec 4. 特征选择
选择合适的、表达能力强的特征
常见的特征选择方法 :
有 DF、 MI、 IG、 CHI、WLLR、WFO
5. 模型训练
机器学习模型 :
KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等
深度学习模型
CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN
注意过拟合、欠拟合问题" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/be216aba24abcce6312edfc90b741898/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-16T02:13:04+08:00" />
<meta property="article:modified_time" content="2021-07-16T02:13:04+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">中文NLP笔记</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="1_0"></a>1.中文自然语言处理的一般流程</h2> 
<p><img src="https://images2.imgbox.com/5d/5e/XqMhum7v_o.jpg" alt="在这里插入图片描述"></p> 
<p>图片发自简书App</p> 
<p>中文NLP一般流程</p> 
<p><strong>1. 获取语料</strong></p> 
<p>语料，是NLP任务所研究的内容</p> 
<p>通常用一个文本集合作为语料库（Corpus）</p> 
<p>来源：</p> 
<blockquote> 
 <p>已有语料</p> 
 <p>积累的文档</p> 
 <p>下载语料</p> 
 <p>搜狗语料、人民日报语料</p> 
 <p>抓取语料</p> 
</blockquote> 
<p><strong>2. 语料预处理</strong></p> 
<ul><li> <p>1.语料清洗</p> <p>留下有用的，删掉噪音数据</p> </li></ul> 
<p>常见的数据清洗方式 :</p> 
<blockquote> 
 <p>人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。</p> 
</blockquote> 
<ul><li> <p>2.分词</p> <p>将文本分成词语</p> </li></ul> 
<p>常见的分词算法 :</p> 
<blockquote> 
 <p>基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法</p> 
</blockquote> 
<ul><li> <p>3.词性标注</p> <p>给词语打词类标签，如形容词、动词、名词等</p> <p>在情感分析、知识推理等任务中需要</p> </li></ul> 
<p>常见的词性标注方法</p> 
<blockquote> 
 <p>基于规则</p> 
 <p>基于统计</p> 
 <p>如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。</p> 
</blockquote> 
<ul><li> <p>4.去停用词</p> <p>去掉对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等</p> </li></ul> 
<p><strong>3.特征工程</strong><br> 把分词表示成计算机能够计算的类型，一般为向量</p> 
<p>常用的表示模型 :</p> 
<blockquote> 
 <p>词袋模型（Bag of Word, BOW)</p> 
 <pre><code>TF-IDF  
</code></pre> 
 <p>词向量</p> 
 <pre><code>One-hot 

Word2Vec
</code></pre> 
</blockquote> 
<p><strong>4. 特征选择</strong></p> 
<p>选择合适的、表达能力强的特征</p> 
<p>常见的特征选择方法 :</p> 
<blockquote> 
 <p>有 DF、 MI、 IG、 CHI、WLLR、WFO</p> 
</blockquote> 
<p><strong>5. 模型训练</strong></p> 
<p>机器学习模型 :</p> 
<blockquote> 
 <p>KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等</p> 
</blockquote> 
<p>深度学习模型</p> 
<blockquote> 
 <p>CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN</p> 
</blockquote> 
<p><em>注意过拟合、欠拟合问题</em></p> 
<p>过拟合：在训练集上表现很好，但是在测试集上表现很差。</p> 
<pre><code>常见的解决方法有：
</code></pre> 
<p>增大数据的训练量；</p> 
<p>增加正则化项，如 L1 正则和 L2 正则；</p> 
<p>特征选取不合理，人工筛选特征和使用特征选择算法；</p> 
<p>采用 Dropout 方法等。</p> 
<p>欠拟合：就是模型不能够很好地拟合数据</p> 
<pre><code>常见的解决方法有：
</code></pre> 
<p>添加其他特征项；</p> 
<p>增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；</p> 
<p>减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</p> 
<p>注意梯度消失和梯度爆炸问题</p> 
<p><strong>6. 评价指标</strong></p> 
<p>错误率、精度、准确率、精确度、召回率、F1 衡量。</p> 
<p>ROC 曲线、AUC 曲线</p> 
<p><strong>7. 模型上线应用</strong></p> 
<p>第一就是线下训练模型，然后将模型做线上部署</p> 
<p>第二种就是在线训练，在线训练完成之后把模型 pickle 持久化</p> 
<h2><a id="2_jieba_140"></a>2.中文分词的工具 jieba</h2> 
<p><img src="https://images2.imgbox.com/af/2d/M4lqNMfz_o.jpg" alt="在这里插入图片描述"><br> 中文分词的工具有：</p> 
<p>中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等</p> 
<hr> 
<p>其中 jieba 分词可以做下面这些事情：</p> 
<p><strong>1. 精确分词</strong></p> 
<p>试图将句子最精确地切开</p> 
<p><strong>2. 全模式</strong></p> 
<p>把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义</p> 
<p><strong>3. 搜索引擎模式</strong></p> 
<p>在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词</p> 
<p><strong>4. 用 lcut 生成 list</strong></p> 
<p>jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合</p> 
<p><strong>5. 获取词性</strong></p> 
<p>jieba.posseg 模块实现词性标注</p> 
<p><strong>6. 获取分词结果中词列表的 top n</strong></p> 
<p><strong>7. 自定义添加词和字典</strong></p> 
<p>使用默认分词，是识别不出一句话中的新词，需要添加新词到字典</p> 
<p><strong>8. 还可以做：</strong></p> 
<p>关键词提取、自动摘要、依存句法分析、情感分析等任务</p> 
<h2><a id="3__180"></a>3. 关键词提取的几个方法</h2> 
<p><img src="https://images2.imgbox.com/48/ff/nzhkI4YI_o.jpg" alt="在这里插入图片描述"><br> 提取 ，意思是从文本里面把意义最相关的词语抽取出来。</p> 
<p>在文献检索、自动文摘、文本聚类/分类等任务中有重要的应用</p> 
<hr> 
<p>主要有2种提取方法</p> 
<p><strong>1. 关键词分配</strong></p> 
<p>在一个已有的关键词库中匹配几个词语作为这篇文档的关键词。</p> 
<p><strong>2. 关键词提取</strong></p> 
<p>通过算法分析，提取文档中一些词语作为关键词。</p> 
<hr> 
<p>其中第二种，关键词提取的常用算法有以下几个</p> 
<p><strong>1. 基于 TF-IDF 算法进行关键词提取</strong></p> 
<p>TF-IDF ：用于反映一个词对于某篇文档的重要性。过滤掉常见的词语，保留重要的词语</p> 
<p>如果某个词在一篇文档中出现的频率高，则TF 高；并且在其他文档中很少出现，则 IDF 高，TF-IDF 就是将二者相乘为 TF * IDF， 这样这个词具有很好的类别区分能力。</p> 
<p>在 jieba 用以下代码实现</p> 
<pre><code> jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) 
</code></pre> 
<p><strong>2. 基于 TextRank 算法进行关键词提取</strong></p> 
<p>由 PageRank 改进而来，将文本中的词看作图中的节点，通过边相互连接，权重高的节点作为关键词。</p> 
<p>在 jieba 用以下代码实现</p> 
<pre><code>jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 
</code></pre> 
<p><strong>3. 基于 LDA 主题模型进行关键词提取</strong></p> 
<p>一般步骤为：文件加载 -&gt; jieba 分词 -&gt; 去停用词 -&gt; 构建词袋模型 -&gt; LDA 模型训练 -&gt; 结果可视化。</p> 
<p><strong>4. 基于 pyhanlp 进行关键词提取</strong></p> 
<p>可以用 HanLP 的 TextRankKeyword 实现</p> 
<pre><code>from pyhanlp import *
result = HanLP.extractKeyword(sentence, 20)
print(result)
</code></pre> 
<h2><a id="4___239"></a>4. 文本数据可视化 的几个方法</h2> 
<p>文本数据可视化 有下面三种</p> 
<ol><li>基于文本内容的可视化</li></ol> 
<p>基于词频的可视化和基于词汇分布的可视化</p> 
<p>常用的有词云、分布图和 Document Cards</p> 
<ol start="2"><li>基于文本关系的可视化</li></ol> 
<p>研究文本内外关系，帮助人们理解文本内容和发现规律</p> 
<p>常用的可视化形式有树状图、节点连接的网络图、力导向图、叠式图和 Word Tree 等</p> 
<ol start="3"><li>基于多层面信息的可视化</li></ol> 
<p>研究如何结合信息的多个方面，帮助用户更深层次理解文本</p> 
<p>常用的有地理热力图、ThemeRiver、SparkClouds、TextFlow 和基于矩阵视图的情感分析可视化等</p> 
<hr> 
<p>代码举例</p> 
<ol><li>词云</li></ol> 
<pre><code>wordcloud=WordCloud(font_path=simhei,background_color="white",max_font_size=80) 
</code></pre> 
<ol start="2"><li>关系图</li></ol> 
<p>用连线图来表示事物相互关系的一种方法。</p> 
<p>安装 Matplotlib、NetworkX</p> 
<pre><code>DG = nx.DiGraph()
DG.add_nodes_from(nodes)
DG.add_edges_from(weights
nx.draw(DG,with_labels=True, node_size=1000, node_color = colors) 
</code></pre> 
<ol start="3"><li>地理热力图</li></ol> 
<p>通过分词得到城市名称后，将地理名词通过转换成经纬度</p> 
<p>使用 Folium 库进行热力图绘制地图</p> 
<pre><code>map_osm = folium.Map(location=[35,110],zoom_start=5)   
HeatMap(data1).add_to(map_osm)  
</code></pre> 
<h2><a id="_294"></a>中文情感分析</h2> 
<p><img src="https://images2.imgbox.com/bf/ba/we8qbAtT_o.jpg" alt="在这里插入图片描述"><br> 图片发自简书App</p> 
<p>中文情感分析</p> 
<p>什么是情感分析</p> 
<p>即分析主体对某一客体的主观喜恶和评价</p> 
<p>由两个方面来衡量</p> 
<p>情感倾向方向</p> 
<p>情感倾向度</p> 
<p>情感分析的方法主要分为两类</p> 
<p>基于情感词典的方法</p> 
<p>需要用到标注好的情感词典</p> 
<p>基于机器学习的方法</p> 
<p>需要大量的人工标注的语料作为训练集，提取文本特征，构建分类器，进行情感的分类。</p> 
<p>分析粒度可以是词语、句子、段落或篇章</p> 
<p>段落篇章级</p> 
<p>如电影评论的分析</p> 
<p>需要构建电影行业自己的情感词典，这样效果会比通用情感词典更好；</p> 
<p>也可以通过人工标注大量电影评论来构建分类器</p> 
<p>也可以通过聚合篇章中所有的句子的情感倾向来计算得出</p> 
<p>句子级</p> 
<p>大多通过计算句子里包含的所有情感词的值来得到</p> 
<p>中文情感分析的一些难点</p> 
<p>句子是由词语根据一定规则构成的，应该把词语的依存关系纳入到情感的计算过程中去</p> 
<p>不同的依存关系，进行情感计算是不一样的</p> 
<blockquote> 
 <p>转载于：https://cloud.tencent.com/developer/user/1000059/activities</p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3c62aad33fac0ae3ce83d06aaa8a0d84/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Pandas常用操</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/26a7302d598dc65ef1fa084ced315002/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">计算机主板一直滴滴响,主板报警声大全_主板一直滴滴滴短响含义详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>