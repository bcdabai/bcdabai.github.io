<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LangChain简介 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LangChain简介" />
<meta property="og:description" content="LangChain是什么 LangChain是基于大语言模型，提供很多工具组件，来构建语言模型应用的开发框架。我们可以用来开发聊天机器人、生成式问题回答（GQA）、摘要等应用。
框架的核心思想是，基于大语言模型，将不同的组件链接在一起，创建高级的应用。组件链由来自不同模块的组件构成，通常有下列的组件：
Prompt模板：提示模板是不同类型提示的模板。例如“聊天机器人”风格的模板，问答等。LLMs：像GPT-3，BLOOM等大型语言模型。Agents：代理使用LLMs来决定应该采取哪些操作。可以使用像网页搜索或计算器等工具，所有的工具都被打包成一个操作的逻辑循环。Memory：短期记忆，长期记忆。 Prompt模版组件 输入到LLMs的Prompt通常会以不同的方式结构化，以便我们能够获得不同的结果。对于问答，我们可以将用户的问题重新格式化，以适应不同的问答风格，例如传统的问答形式、答案的项目列表，甚至是与给定问题相关的问题摘要。
使用LangChain创建Prompts 首先安装 langchain库
pip install langchain 导入PromptTemplate模块，然后创建模版
from langchain import PromptTemplate template = &#34;&#34;&#34;Question: {question} Answer: &#34;&#34;&#34; prompt = PromptTemplate( template=template, input_variables=[&#39;question&#39;] ) # user question question = &#34;Which NFL team won the Super Bowl in the 2010 season?&#34; 最终基于模版和问题创建得到的Prompt如下：
Question: Which NFL team won the Super Bowl in the 2010 season? Answer: Hugging Face Hub LLM huggging face类似github，上面提供了大量的免费预训练模型，LangChain也提供了对应的组件，可以访问huggging face上模型接口来进行问答。
在LangChain中，提供了Hugging Face模型访问组件，首先我们需要一个Hugging Face账户和API密钥[1]。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/05f6c67babb1736d992362298ffbbe5a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-17T21:08:21+08:00" />
<meta property="article:modified_time" content="2023-12-17T21:08:21+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LangChain简介</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="LangChain_0"></a>LangChain是什么</h2> 
<p>LangChain是基于大语言模型，提供很多工具组件，来构建语言模型应用的开发框架。我们可以用来开发聊天机器人、生成式问题回答（GQA）、摘要等应用。</p> 
<p>框架的核心思想是，基于大语言模型，将不同的组件链接在一起，创建高级的应用。组件链由来自不同模块的组件构成，通常有下列的组件：</p> 
<ul><li>Prompt模板：提示模板是不同类型提示的模板。例如“聊天机器人”风格的模板，问答等。</li><li>LLMs：像GPT-3，BLOOM等大型语言模型。</li><li>Agents：代理使用LLMs来决定应该采取哪些操作。可以使用像网页搜索或计算器等工具，所有的工具都被打包成一个操作的逻辑循环。</li><li>Memory：短期记忆，长期记忆。</li></ul> 
<h2><a id="Prompt_10"></a>Prompt模版组件</h2> 
<p>输入到LLMs的Prompt通常会以不同的方式结构化，以便我们能够获得不同的结果。对于问答，我们可以将用户的问题重新格式化，以适应不同的问答风格，例如传统的问答形式、答案的项目列表，甚至是与给定问题相关的问题摘要。</p> 
<h3><a id="LangChainPrompts_14"></a>使用LangChain创建Prompts</h3> 
<p>首先安装 langchain库</p> 
<pre><code class="prism language-python">pip install langchain
</code></pre> 
<p>导入PromptTemplate模块，然后创建模版</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> PromptTemplate

template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Question: {question}

Answer: """</span>
prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>
        template<span class="token operator">=</span>template<span class="token punctuation">,</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'question'</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># user question</span>
question <span class="token operator">=</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span>
</code></pre> 
<p>最终基于模版和问题创建得到的Prompt如下：</p> 
<pre><code class="prism language-python">Question<span class="token punctuation">:</span> Which NFL team won the Super Bowl <span class="token keyword">in</span> the <span class="token number">2010</span> season? Answer<span class="token punctuation">:</span>
</code></pre> 
<h3><a id="Hugging_Face_Hub_LLM_40"></a>Hugging Face Hub LLM</h3> 
<p><a href="https://huggingface.co/models" rel="nofollow">huggging face</a>类似github，上面提供了大量的免费预训练模型，LangChain也提供了对应的组件，可以访问huggging face上模型接口来进行问答。</p> 
<p>在LangChain中，提供了Hugging Face模型访问组件，首先我们需要一个Hugging Face账户和API密钥[1]。</p> 
<p>一旦你拥有API密钥，我们可以将其添加到HUGGINGFACEHUB_API_TOKEN环境变量中。我们可以使用Python完成此操作，代码如下</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HUGGINGFACEHUB_API_TOKEN'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'HF_API_KEY'</span>
</code></pre> 
<p>接着安装hugging face库</p> 
<pre><code class="prism language-python">pip install huggingface_hub
</code></pre> 
<p>接下来使用模型google/flan-t5-x1进行文本生成，最终完整的代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> HuggingFaceHub<span class="token punctuation">,</span> LLMChain

<span class="token comment"># initialize Hub LLM</span>
hub_llm <span class="token operator">=</span> HuggingFaceHub<span class="token punctuation">(</span>
        repo_id<span class="token operator">=</span><span class="token string">'google/flan-t5-xl'</span><span class="token punctuation">,</span>
    model_kwargs<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'temperature'</span><span class="token punctuation">:</span><span class="token number">1e-10</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>

<span class="token comment"># create prompt template &gt; LLM chain</span>
llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>hub_llm
<span class="token punctuation">)</span>

<span class="token comment"># ask the user question about NFL 2010</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>使用huggingface api[2]最终得到一个输出答案。</p> 
<h3><a id="_77"></a>问多个问题</h3> 
<p>如果我们想提问多个问题，可以尝试两种方法：</p> 
<ul><li>使用generate方法逐一回答所有问题。</li><li>将所有问题放入单个Prompt，这个需要看所使用的LLM的支持情况确定。</li></ul> 
<p>首先，让我们看看如何使用generate方法：</p> 
<pre><code class="prism language-python">qs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Who was the 12th person on the moon?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"How many eyes does a blade of grass have?"</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
res <span class="token operator">=</span> llm_chain<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>qs<span class="token punctuation">)</span>
</code></pre> 
<p>除了第一个问题的答案，其余的问题结果外都不太好。如果模型无法准确回答单个问题，将所有查询都放在一个Prompt中也不太可能起作用。作为实验，我们可以试一试。</p> 
<pre><code class="prism language-python">multi_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Answer the following questions one at a time.

Questions:
{questions}

Answers:
"""</span>
long_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>template<span class="token operator">=</span>multi_template<span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"questions"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>long_prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>flan_t5
<span class="token punctuation">)</span>

qs_str <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?\n"</span> <span class="token operator">+</span>
    <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?\n"</span> <span class="token operator">+</span>
    <span class="token string">"Who was the 12th person on the moon?"</span> <span class="token operator">+</span>
    <span class="token string">"How many eyes does a blade of grass have?"</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>qs_str<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="OpenAI_LLMs_122"></a>OpenAI LLMs</h3> 
<p>使用OpenAI接口，提前注册好账户，获得key</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'OPENAI_API_TOKEN'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'OPENAI_API_KEY'</span>
</code></pre> 
<p>接着安装OpenAI的接口库</p> 
<pre><code class="prism language-python">pip install openai
</code></pre> 
<p>现在我们可以使用OpenAI的接口来生成文本。例子中使用text-davinci-003模型。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAI
davinci <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">'text-davinci-003'</span><span class="token punctuation">)</span>
</code></pre> 
<p>此外还可以通过Azure来使用</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> AzureOpenAI

llm <span class="token operator">=</span> AzureOpenAI<span class="token punctuation">(</span>
    deployment_name<span class="token operator">=</span><span class="token string">"your-azure-deployment"</span><span class="token punctuation">,</span> 
    model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>我们将使用与之前Hugging Face示例相同的简单问题-答案Prompt模板。唯一的变化是使用的模型。</p> 
<pre><code class="prism language-python">llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>davinci
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>同样，我们可以使用generate方法发起多个提问</p> 
<pre><code class="prism language-python">qs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Who was the 12th person on the moon?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"How many eyes does a blade of grass have?"</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
llm_chain<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>qs<span class="token punctuation">)</span>
</code></pre> 
<p>我们的大多数结果都是正确的或者有一定的真实性。这个模型无疑比google/flan-t5-xl模型更好地运行。像之前一样，让我们尝试一次把所有的问题都输入到模型中。</p> 
<pre><code class="prism language-python">llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>long_prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>davinci
<span class="token punctuation">)</span>

qs_str <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?\n"</span> <span class="token operator">+</span>
    <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?\n"</span> <span class="token operator">+</span>
    <span class="token string">"Who was the 12th person on the moon?"</span> <span class="token operator">+</span>
    <span class="token string">"How many eyes does a blade of grass have?"</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>qs_str<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2><a id="_188"></a>总结</h2> 
<p>这就是我们对LangChain的简单介绍——一个允许我们基于LLMs构建更高级应用程序的库，如OpenAI的GPT-3模型或通过Hugging Face提供的模型。</p> 
<h2><a id="_191"></a>参考</h2> 
<p>[1].Hugging Face API key<br> ,https://zhuanlan.zhihu.com/p/671128034<br> [2].hugging face api quicktour rhttps://huggingface.co/docs/api-inference/quicktour</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/91b7e1e89458d0c77b8bc0aa6186944c/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">事务的传播行为</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/9a9ced41f71012b17edbc83917a8c1f6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">配置Nginx解决跨域问题</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>