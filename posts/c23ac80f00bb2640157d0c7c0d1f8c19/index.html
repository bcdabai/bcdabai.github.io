<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>spark&#43;mongodb &#43; quartz - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="spark&#43;mongodb &#43; quartz" />
<meta property="og:description" content="由于业务数据量大，使用普通查询统计已不能满足需求，所以使用spark&#43;mongodb进行聚合统计，两种方案
1使用quartz调度spark，定时进行业务数据统计
2使用crontab调度spark，定时进行业务数据统计
为了便于管理最终使用方案1
quartz调度代码这里就不展示了，只写spark&#43;mongodb调用
1 引入maven依赖
&lt;dependency&gt;
&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
&lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
&lt;version&gt;1.6.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;
&lt;artifactId&gt;mongo-spark-connector_2.10&lt;/artifactId&gt;
&lt;version&gt;1.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
&lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;
&lt;version&gt;1.6.2&lt;/version&gt;
&lt;/dependency&gt; 2 第一种方式：使用mongodb做聚合，spark将数据转换成document，然后保存
public class UserFlowCount
{
public static void main(String args[]) throws IOException
{
//spark mongodb 连接配置
SparkConf sc = new SparkConf()
.setMaster(&#34;spark://10.1.12.4228:2000&#34;) //spark master 地址
.setAppName(&#34;MongoSparkConnectorTour&#34;) //应用名称
.set(&#34;spark.mongodb.input.uri&#34;,
&#34;mongodb://bssd:123456@10.1.123.321:34234/bss.AccountDeductionDetailRecord_bak&#34;)//mongodb input 连接
.set(&#34;spark.mongodb.output.uri&#34;,
&#34;mongodb://bssdb:123456@10.1.123.12:234234/bss.myCollection&#34;)//mongodb output 连接
.set(&#34;spark.mongodb.input.partitioner&#34;, &#34;MongoPaginateBySizePartitioner&#34;)
.set(&#34;spark.cores.max&#34;, &#34;12&#34;).set(&#34;spark.driver.port&#34;, &#34;20898&#34;)
.set(&#34;spark.fileserver.port&#34;, &#34;20896&#34;).set(&#34;spark.broadcast.port&#34;, &#34;20895&#34;)
.set(&#34;spark.blockManager.port&#34;, &#34;20894&#34;).set(&#34;spark.executor.port&#34;, &#34;20893&#34;)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/c23ac80f00bb2640157d0c7c0d1f8c19/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2016-11-21T11:36:38+08:00" />
<meta property="article:modified_time" content="2016-11-21T11:36:38+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">spark&#43;mongodb &#43; quartz</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>由于业务数据量大，使用普通查询统计已不能满足需求，所以使用spark+mongodb进行聚合统计，两种方案</p> 
<p>1使用quartz调度spark，定时进行业务数据统计</p> 
<p>2使用crontab调度spark，定时进行业务数据统计<br> </p> 
<p><br> </p> 
<p> 为了便于管理最终使用方案1</p> 
<p> quartz调度代码这里就不展示了，只写spark+mongodb调用</p> 
<p>1 引入maven依赖</p> 
<p>          &lt;dependency&gt;<br> <span style="white-space:pre"></span>&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br> <span style="white-space:pre"></span>&lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;<br> <span style="white-space:pre"></span>&lt;version&gt;1.6.2&lt;/version&gt;<br> <span style="white-space:pre"></span>&lt;/dependency&gt;<br> <span style="white-space:pre"></span>&lt;dependency&gt;<br> <span style="white-space:pre"></span>&lt;groupId&gt;org.mongodb.spark&lt;/groupId&gt;<br> <span style="white-space:pre"></span>&lt;artifactId&gt;mongo-spark-connector_2.10&lt;/artifactId&gt;<br> <span style="white-space:pre"></span>&lt;version&gt;1.0.0&lt;/version&gt;<br> <span style="white-space:pre"></span>&lt;/dependency&gt;<br> <span style="white-space:pre"></span>&lt;dependency&gt;<br> <span style="white-space:pre"></span>&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br> <span style="white-space:pre"></span>&lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;<br> <span style="white-space:pre"></span>&lt;version&gt;1.6.2&lt;/version&gt;<br> <span style="white-space:pre"></span>&lt;/dependency&gt; </p> 
<p><br> </p> 
<p>2 </p> 
<p>第一种方式：使用mongodb做聚合，spark将数据转换成document，然后保存</p> 
<p>public class UserFlowCount<br> {<!-- --><br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>public static void main(String args[]) throws IOException<br> <span style="white-space:pre"></span>{<!-- --><br> <span style="white-space:pre"></span>//spark mongodb 连接配置<br> <span style="white-space:pre"></span>SparkConf sc = new SparkConf()<br> <span style="white-space:pre"></span>.setMaster("spark://10.1.12.4228:2000") //spark master 地址<br> <span style="white-space:pre"></span>.setAppName("MongoSparkConnectorTour") //应用名称<br> <span style="white-space:pre"></span>.set("spark.mongodb.input.uri",<br> <span style="white-space:pre"></span>"mongodb://bssd:123456@10.1.123.321:34234/bss.AccountDeductionDetailRecord_bak")//mongodb input 连接<br> <span style="white-space:pre"></span>.set("spark.mongodb.output.uri",<br> <span style="white-space:pre"></span>"mongodb://bssdb:123456@10.1.123.12:234234/bss.myCollection")//mongodb output 连接<br> <span style="white-space:pre"></span>.set("spark.mongodb.input.partitioner", "MongoPaginateBySizePartitioner")<br> <span style="white-space:pre"></span>.set("spark.cores.max", "12").set("spark.driver.port", "20898")<br> <span style="white-space:pre"></span>.set("spark.fileserver.port", "20896").set("spark.broadcast.port", "20895")<br> <span style="white-space:pre"></span>.set("spark.blockManager.port", "20894").set("spark.executor.port", "20893")<br> <span style="white-space:pre"></span>.set("spark.replClassServer.port", "20892");<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>//依赖的jar<br> <span style="white-space:pre"></span>String[] jars = new String[] {<!-- --><br> <span style="white-space:pre"></span>"D:\\maven\\org\\mongodb\\mongo-java-driver\\3.2.2\\mongo-java-driver-3.2.2.jar",<br> <span style="white-space:pre"></span>"D:\\maven\\org\\mongodb\\spark\\mongo-spark-connector_2.10\\1.0.0\\mongo-spark-connector_2.10-1.0.0.jar",<br> <span style="white-space:pre"></span>"E:\\DEV-PLATFORM\\ucl-platform\\code\\demo\\spark\\target\\spark-example-0.0.1-SNAPSHOT.jar"};<br> <br> <br> <span style="white-space:pre"></span>sc.setJars(jars);<br> <br> <br> <span style="white-space:pre"></span>JavaSparkContext jsc = new JavaSparkContext(sc); // Create a Java Spark<br> <span style="white-space:pre"></span><br>         //设置mongodb 读配置<br> <span style="white-space:pre"></span>Map&lt;String, String&gt; readOverrides = new HashMap&lt;String, String&gt;();<br> <span style="white-space:pre"></span>readOverrides.put("collection", "AccountDeductionDetailRecord_bak");// 读取的collection<br> <span style="white-space:pre"></span>readOverrides.put("readPreference.name", "secondaryPreferred");//读从节点优先<br> <span style="white-space:pre"></span>ReadConfig readConfig = ReadConfig.create(jsc).withOptions(readOverrides);<br> <br> <br> <span style="white-space:pre"></span>//读取collection，过滤条件{ $match: { createDate : { $gte : 1466956800000,$lte : 1467043200000} } }<br> <span style="white-space:pre"></span>JavaRDD&lt;Document&gt; customRdd = MongoSpark.load(jsc, readConfig).withPipeline(<br> <span style="white-space:pre"></span>java.util.Collections.singletonList(Document<br> <span style="white-space:pre"></span>.parse("{ $match: { createDate : { $gte : 1466956800000,$lte : 1467043200000} } }")));<br> <br> <br> <span style="white-space:pre"></span>//根据userName 聚合flowSize<br> <span style="white-space:pre"></span>JavaPairRDD&lt;String, Long&gt; result = customRdd.mapToPair(<br> <span style="white-space:pre"></span>new PairFunction&lt;Document, String, Long&gt;() {<!-- --><br> <br> <br> <span style="white-space:pre"></span>public Tuple2&lt;String, Long&gt; call(Document arg0) throws Exception<br> <span style="white-space:pre"></span>{<!-- --><br> <span style="white-space:pre"></span>// TODO Auto-generated method stub<br> <br> <br> <span style="white-space:pre"></span>return new Tuple2(arg0.getString("userName"), arg0.getLong("flowSize"));<br> <span style="white-space:pre"></span>}<br> <br> <br> <span style="white-space:pre"></span>}).cache();</p> 
<p>//如果连起来写会出现执行时间特别长，还未弄清楚原因</p> 
<p>//为何需要缓存当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p> 
<p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; word-wrap:break-word"> //Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执//行你的算子操作。这种方式的性能是很差的。</p> 
<p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; word-wrap:break-word"> //因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行//算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作</p> 
<p>result  = result .reduceByKey(new Function2&lt;Long, Long, Long&gt;() {<!-- --><br> <br> <br> <span style="white-space:pre"></span>public Long call(Long arg0, Long arg1) throws Exception<br> <span style="white-space:pre"></span>{<!-- --><br> <span style="white-space:pre"></span>// TODO Auto-generated method stub<br> <span style="white-space:pre"></span>return arg0 + arg1;<br> <span style="white-space:pre"></span>}<br> <br> <br> <span style="white-space:pre"></span>});<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>//将聚合结果转化为Document<br> <span style="white-space:pre"></span>JavaRDD&lt;Document&gt; saveR = result.map(new Function&lt;Tuple2&lt;String,Long&gt;,Document&gt;(){<!-- --><br> <br> <br> <span style="white-space:pre"></span>public Document call(Tuple2&lt;String, Long&gt; arg0) throws Exception<br> <span style="white-space:pre"></span>{<!-- --><br> <span style="white-space:pre"></span>// TODO Auto-generated method stub<br> <span style="white-space:pre"></span>return Document.parse("{userName:\""+arg0._1+"\",flowSize:"+arg0._2+"}");<br> <span style="white-space:pre"></span>}<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>});<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>//将结果保存回mongodb<br> <span style="white-space:pre"></span>Map&lt;String, String&gt; writeOverrides = new HashMap&lt;String, String&gt;();<br> <span style="white-space:pre"></span>writeOverrides.put("collection", "spark");<br> <span style="white-space:pre"></span>writeOverrides.put("writeConcern.w", "majority");<br> <span style="white-space:pre"></span>WriteConfig writeConfig = WriteConfig.create(jsc).withOptions(writeOverrides);<br> <span style="white-space:pre"></span>MongoSpark.save(saveR,writeConfig);<br> <br> <br> <span style="white-space:pre"></span>System.out.println("success");<br> <span style="white-space:pre"></span>//System.out.println(customRdd.first().toJson());<br> <span style="white-space:pre"></span>}   <br> </p> 
<p><br> </p> 
<p> 第2中方式，直接读取数据，用spark的sql做聚合</p> 
<p>public class CdrCount<br> {<!-- --><br> <span style="white-space:pre"></span>public static void main(String args[]) throws IOException<br> <span style="white-space:pre"></span>{<!-- --><br> <span style="white-space:pre"></span>SparkConf sc = new SparkConf().setMaster("spark://10.1.123.123:20898").setAppName("MongoSparkConnectorTour")<br> <span style="white-space:pre"></span>.set("spark.mongodb.input.uri", "mongodb://bssdb:123456@10.2.12.146:2000/bss.AccountDeductionDetailRecord_bak")<br> <span style="white-space:pre"></span>.set("spark.mongodb.output.uri", "mongodb://bssdb:123456@10.2.13.146:2000/bss.myCollection")<br> <span style="white-space:pre"></span>.set("spark.mongodb.input.partitioner", "MongoPaginateBySizePartitioner")<br> <span style="white-space:pre"></span>.set("spark.cores.max", "12")<br> <span style="white-space:pre"></span>.set("spark.driver.port","20898")<br> <span style="white-space:pre"></span>.set("spark.fileserver.port","20896")<br> <span style="white-space:pre"></span>.set("spark.broadcast.port","20895")<br> <span style="white-space:pre"></span>.set("spark.blockManager.port", "20894")<br> <span style="white-space:pre"></span>.set("spark.executor.port", "20893")<br> <span style="white-space:pre"></span>.set("spark.replClassServer.port", "20892");<br> <span style="white-space:pre"></span>String[] jars = new String[] {<!-- --><br> <span style="white-space:pre"></span>"D:\\maven\\org\\mongodb\\mongo-java-driver\\3.2.2\\mongo-java-driver-3.2.2.jar",<br> <span style="white-space:pre"></span>"D:\\maven\\org\\mongodb\\spark\\mongo-spark-connector_2.10\\1.0.0\\mongo-spark-connector_2.10-1.0.0.jar" };<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>sc.setJars(jars);<br> <br> <br> <span style="white-space:pre"></span>JavaSparkContext jsc = new JavaSparkContext(sc); // Create a Java Spark // Context<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>SQLContext sqlContext = SQLContext.getOrCreate(jsc.sc());<br> <span style="white-space:pre"></span>Map&lt;String, String&gt; readOverrides = new HashMap&lt;String, String&gt;();<br> <span style="white-space:pre"></span>readOverrides.put("collection", "AccountDeductionDetailRecord_bak");<br> <span style="white-space:pre"></span>readOverrides.put("readPreference.name", "secondaryPreferred");<br> <span style="white-space:pre"></span>ReadConfig readConfig = ReadConfig.create(jsc).withOptions(readOverrides);<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>//定义DataFrame<br> <span style="white-space:pre"></span>DataFrame df = MongoSpark.load(jsc,readConfig).toDF();<br> <span style="white-space:pre"></span>//过滤数据<br> <span style="white-space:pre"></span>df = df.filter(df.col("createDate").between(1466956800000L, 1467043200000L));<br> <span style="white-space:pre"></span>//映射表<br> <span style="white-space:pre"></span>df.registerTempTable("flowLog");<br>         //聚合sql<br> <span style="white-space:pre"></span>DataFrame centenarians = sqlContext.sql("SELECT userName,sum(flowSize) FROM flowLog group by userName");<span style="white-space:pre"></span><br> <span style="white-space:pre"></span>//mongodb保存<br> <span style="white-space:pre"></span>MongoSpark.save(centenarians.write().option("collection", "hundredClub").mode("append"));<br> <span style="white-space:pre"></span><br> <span style="white-space:pre"></span>}<br> <br> <br> }<br> </p> 
<p><br> </p> 
<p>注意：1 、work节点函数入口为main函数，如果没有main函数不能进入</p> 
<p>            2 、JavaSparkContext 设置成全局变量,在入口处初始化一次，切记不可以频繁初始化，如果频繁初始化会造成老年代占用内存过大，GC频繁回收，而且spark使用堆外内存导致机器内存过高被OOM-KILL杀掉。</p> 
<p>                  如果设置驱动节点内存最大值，又导致堆内存溢出。</p> 
<p>                 将需要使用的参数通过main函数的String[] args传入，如果是对象可以序列化成Json串传入(项目中使用dubbo接口去查询其他模块对象)，在解析即可。</p> 
<p>           3、spark连接mongos和单点可以正常运行，主从未测试</p> 
<p>           4 、javaRdd执行isEmpty(),take()、count()、collect()会从<span style="color:rgb(54,46,43); font-family:Arial; font-size:14px">从远程集群上获取数据到本地，非常慢不建议使用</span></p> 
<p><span style="color:rgb(54,46,43); font-family:Arial; font-size:14px"><br> </span></p> 
<p><span style="color:rgb(54,46,43); font-family:Arial; font-size:14px"><br> </span></p> 
<p><span style="color:rgb(54,46,43); font-family:Arial; font-size:14px"><span style="color:rgb(51,51,51); font-family:Helvetica,Tahoma,Arial,sans-serif; font-size:14px">弹性分布式数据集是Apache Spark的核心理念。它是由数据组成的不可变分布式集合，其主要进行两个操作：transformation和action。Transformation是类似在RDD上做 filter()、map()或union() 以生成另一个RDD的操作，而action则是count()、first()、take(n)、collect() 等促发一个计算并返回值到Master或者稳定存储系统的操作。Transformations一般都是lazy的，直到action执行后才会被执行。Spark Master/Driver会保存RDD上的Transformations。这样一来，如果某个RDD丢失（也就是salves宕掉），它可以快速和便捷地转换到集群中存活的主机上。这也就是RDD的弹性所在。</span><br> </span></p> 
<div> 
 <br> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/df63ffec8a5bf726919bf31a9561a976/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">web之html，css，javascript 简要概述</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/2d984fdbdf183f73798791fbd1e49299/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">redis 事件</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>