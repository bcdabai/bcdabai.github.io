<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>自编码器原理及使用Pytorch框架实现（AutoEncoder） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="自编码器原理及使用Pytorch框架实现（AutoEncoder）" />
<meta property="og:description" content="目录
1.自编码器产生背景
2.自编码器原理
（1）一般的神经网络结构
（2）自编码器
3.自编码器的实现
（1）网络结构
（2）代码实现
1.自编码器产生背景 像我们目前所进行的图像分类，目标识别，图像分割等都是基于有监督学习来的，所以对于海量的数据需要进行人工的标注。但是随着时代的发展和人工智能不断的火起来，对于数据量的需求已经不是想象中的样子了，数据量的需求已经远远超出人们的认知。面对海量的数据集，有没有一种办法就是能够从中学习到数据的分布P（x）的算法呢？
提示：而解决上面的算法计算无监督学习。
自编码器 自编码器（autoencoder, AE）是一类在半监督学习和非监督学习中使用的人工神经网络（Artificial Neural Networks, ANNs），其功能是通过将输入信息作为学习目标，对输入信息进行表征学习（representation learning）自编码器原理 包含编码器（encoder）和解码器（decoder）两部分 。按学习范式，自编码器可以被分为收缩自编码器（contractive autoencoder）、正则自编码器（regularized autoencoder）和变分自编码器（Variational AutoEncoder, VAE），其中前两者是判别模型、后者是生成模型 。按构筑类型，自编码器可以是前馈结构或递归结构的神经网络。自编码器应用场景 自编码器具有一般意义上表征学习算法的功能，被应用于降维（dimensionality reduction）和异常值检测（anomaly detection） 。包含卷积层构筑的自编码器可被应用于计算机视觉问题，包括图像降噪（image denoising） 、神经风格迁移（neural style transfer）等 。 2.自编码器原理 （1）一般的神经网络结构 能否利用神经网络的强大非线性表达能力去学习到低维的数据表示呢？但是这样也会引入一个问题就是，训练神经网络都是在有标签的清况下，对于一个无监督的学习，是没有标签的，只有输入的数据本身x.
（2）自编码器 我们希望从编码器到解码器的最后输出近似等于原来的输入，所以自编码器的优化目标如下：
3.自编码器的实现 （1）网络结构 （2）代码实现 本文的代码下载：GitHub - KeepTryingTo/Pytorch-GAN: 使用Pytorch实现GAN 的过程
参考书籍和链接
《TensorFlow深度学习》
自编码器相关知识点介绍" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/fc6e0924332c9b192f07db3e30adbbd6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-12T13:32:59+08:00" />
<meta property="article:modified_time" content="2023-05-12T13:32:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">自编码器原理及使用Pytorch框架实现（AutoEncoder）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF-toc" style="margin-left:0px;"><a href="#1.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF" rel="nofollow">1.自编码器产生背景</a></p> 
<p id="2.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;"><a href="#2.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%8E%9F%E7%90%86" rel="nofollow">2.自编码器原理</a></p> 
<p id="%EF%BC%881%EF%BC%89%E4%B8%80%E8%88%AC%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#%EF%BC%881%EF%BC%89%E4%B8%80%E8%88%AC%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">（1）一般的神经网络结构</a></p> 
<p id="%EF%BC%882%EF%BC%89%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8-toc" style="margin-left:40px;"><a href="#%EF%BC%882%EF%BC%89%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8" rel="nofollow">（2）自编码器</a></p> 
<p id="3.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0-toc" style="margin-left:0px;"><a href="#3.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0" rel="nofollow">3.自编码器的实现</a></p> 
<p id="%EF%BC%881%EF%BC%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;"><a href="#%EF%BC%881%EF%BC%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">（1）网络结构</a></p> 
<p id="%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:40px;"><a href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="nofollow">（2）代码实现</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p></p> 
<h2 id="1.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF">1.自编码器产生背景</h2> 
<blockquote> 
 <p>        <strong>像我们目前所进行的图像分类，目标识别，图像分割等都是基于有监督学习来的，所以对于海量的数据需要进行人工的标注。但是随着时代的发展和人工智能不断的火起来，对于数据量的需求已经不是想象中的样子了，数据量的需求已经远远超出人们的认知。面对海量的数据集，有没有一种办法就是能够从中学习到数据的分布P（x）的算法呢？</strong></p> 
</blockquote> 
<blockquote> 
 <p>        提示：而解决上面的算法计算无监督学习。</p> 
 <ul><li>自编码器 
   <ul><li>自编码器（autoencoder, AE）是一类在<a href="https://baike.baidu.com/item/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/9075473?fromModule=lemma_inlink" rel="nofollow" title="半监督学习">半监督学习</a>和<a href="https://baike.baidu.com/item/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/16588789?fromModule=lemma_inlink" rel="nofollow" title="非监督学习">非监督学习</a>中使用的<a href="https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/382460?fromModule=lemma_inlink" rel="nofollow" title="人工神经网络">人工神经网络</a>（Artificial Neural Networks, ANNs），其功能是通过将输入信息作为学习目标，对输入信息进行<a href="https://baike.baidu.com/item/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/2140515?fromModule=lemma_inlink" rel="nofollow" title="表征学习">表征学习</a>（representation learning）</li></ul></li><li>自<a href="https://baike.baidu.com/item/%E7%BC%96%E7%A0%81%E5%99%A8/6029803?fromModule=lemma_inlink" rel="nofollow" title="编码器">编码器</a>原理 
   <ul><li>包含编码器（encoder）和<a href="https://baike.baidu.com/item/%E8%A7%A3%E7%A0%81%E5%99%A8/84366?fromModule=lemma_inlink" rel="nofollow" title="解码器">解码器</a>（<a href="https://baike.baidu.com/item/decoder/7881831?fromModule=lemma_inlink" rel="nofollow" title="decoder">decoder</a>）两部分  。按学习范式，自编码器可以被分为<a href="https://baike.baidu.com/item/%E6%94%B6%E7%BC%A9%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/22768373?fromModule=lemma_inlink" rel="nofollow" title="收缩自编码器">收缩自编码器</a>（contractive autoencoder）、<a href="https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/22768165?fromModule=lemma_inlink" rel="nofollow" title="正则自编码器">正则自编码器</a>（regularized autoencoder）和<a href="https://baike.baidu.com/item/%E5%8F%98%E5%88%86/53607500?fromModule=lemma_inlink" rel="nofollow" title="变分">变分</a>自编码器（Variational AutoEncoder, VAE），其中前两者是<a href="https://baike.baidu.com/item/%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/16224017?fromModule=lemma_inlink" rel="nofollow" title="判别模型">判别模型</a>、后者是<a href="https://baike.baidu.com/item/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/6563656?fromModule=lemma_inlink" rel="nofollow" title="生成模型">生成模型</a>  。按构筑类型，自编码器可以是<a href="https://baike.baidu.com/item/%E5%89%8D%E9%A6%88/141922?fromModule=lemma_inlink" rel="nofollow" title="前馈">前馈</a>结构或<a href="https://baike.baidu.com/item/%E9%80%92%E5%BD%92%E7%BB%93%E6%9E%84/54410208?fromModule=lemma_inlink" rel="nofollow" title="递归结构">递归结构</a>的神经网络。</li></ul></li><li>自编码器应用场景 
   <ul><li>自编码器具有一般意义上表征学习算法的功能，被应用于<a href="https://baike.baidu.com/item/%E9%99%8D%E7%BB%B4/12737059?fromModule=lemma_inlink" rel="nofollow" title="降维">降维</a>（dimensionality reduction）和异常值检测（anomaly detection） 。包含<a href="https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF%E5%B1%82/22701737?fromModule=lemma_inlink" rel="nofollow" title="卷积层">卷积层</a>构筑的自编码器可被应用于<a href="https://baike.baidu.com/item/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2803351?fromModule=lemma_inlink" rel="nofollow" title="计算机视觉">计算机视觉</a>问题，包括<a href="https://baike.baidu.com/item/%E5%9B%BE%E5%83%8F%E9%99%8D%E5%99%AA/4136566?fromModule=lemma_inlink" rel="nofollow" title="图像降噪">图像降噪</a>（image denoising） 、神经风格迁移（neural style transfer）等  。</li></ul></li></ul> 
</blockquote> 
<p><img alt="" height="605" src="https://images2.imgbox.com/86/e6/eb6cwauE_o.png" width="1084"></p> 
<p></p> 
<p></p> 
<h2 id="2.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%8E%9F%E7%90%86">2.自编码器原理</h2> 
<h3 id="%EF%BC%881%EF%BC%89%E4%B8%80%E8%88%AC%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">（1）一般的神经网络结构</h3> 
<p class="img-center"><img alt="" height="307" src="https://images2.imgbox.com/4b/5d/T1yk4zeF_o.png" width="523"></p> 
<blockquote> 
 <p>     <img alt="" height="385" src="https://images2.imgbox.com/8c/8d/PJKt1dV5_o.png" width="963"></p> 
</blockquote> 
<blockquote> 
 <p>        <strong>能否利用神经网络的强大非线性表达能力去学习到低维的数据表示呢？但是这样也会引入一个问题就是，训练神经网络都是在有标签的清况下，对于一个无监督的学习，是没有标签的，只有输入的数据本身x.</strong></p> 
</blockquote> 
<p></p> 
<h3 id="%EF%BC%882%EF%BC%89%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">（2）自编码器</h3> 
<p class="img-center"><img alt="" height="196" src="https://images2.imgbox.com/76/45/A5cxjpRc_o.png" width="468"></p> 
<blockquote> 
 <p> <img alt="" height="348" src="https://images2.imgbox.com/75/ad/XpqbjjnJ_o.png" width="969"></p> 
</blockquote> 
<blockquote> 
 <p>        我们希望从编码器到解码器的最后输出近似等于原来的输入，所以自编码器的优化目标如下：</p> 
 <p><img alt="" height="179" src="https://images2.imgbox.com/e8/96/2M9ood98_o.png" width="948"></p> 
</blockquote> 
<p></p> 
<h2 id="3.%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%9A%84%E5%AE%9E%E7%8E%B0">3.自编码器的实现</h2> 
<h3 id="%EF%BC%881%EF%BC%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">（1）网络结构</h3> 
<p><img alt="" height="688" src="https://images2.imgbox.com/9a/2a/6LU4VqIl_o.png" width="1084"></p> 
<p><img alt="" height="515" src="https://images2.imgbox.com/e4/87/iPzbYGOA_o.png" width="948"></p> 
<h3 id="%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">（2）代码实现</h3> 
<blockquote> 
 <p><strong>本文的代码下载：<a href="https://github.com/KeepTryingTo/Pytorch-GAN" title="GitHub - KeepTryingTo/Pytorch-GAN: 使用Pytorch实现GAN 的过程">GitHub - KeepTryingTo/Pytorch-GAN: 使用Pytorch实现GAN 的过程</a></strong></p> 
</blockquote> 
<p></p> 
<p><strong>参考书籍和链接</strong></p> 
<blockquote> 
 <p><strong>《TensorFlow深度学习》</strong></p> 
 <p><strong><a class="link-info" href="https://baike.baidu.com/item/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/23686966" rel="nofollow" title="自编码器相关知识点介绍">自编码器相关知识点介绍</a></strong></p> 
</blockquote> 
<p></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/8122d80b56cd0bea03264394b514ba47/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">第一个个人网站</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8ca99a83195b802aad30fce9ff4c2cc3/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">ArcGIS Pro坐标系统</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>