<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习PyTorch（三）循环神经网络 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习PyTorch（三）循环神经网络" />
<meta property="og:description" content="文章目录 基础RNN_pytorch实现 LSTM(Long Short Term Memory networks)LSTM_pytorch实现 GRU(Gated Recurrent Unit) RNN的应用用RNN做图像分类mnistpytorch实现 RNN做时间序列（回归）pytorch实现 自然语言处理词嵌入pytorch实现 Skip-Gram模型N-Gram模型pytorch实现 LSTM做词性预测pytorch实现 基础 没有办法找到非常前面的信息，存在长时依赖问题。
RNN_pytorch实现 使用RnnCell
import torch from torch.autograd import Variable from torch import nn #定义一个单步的rnn rnn_single=nn.RNNCell(input_size=100,hidden_size=200) print(rnn_single.weight_hh) &#34;&#34;&#34; Parameter containing: 1.00000e-02 * 6.2260 -5.3805 3.5870 ... -2.2162 6.2760 1.6760 -5.1878 -4.6751 -5.5926 ... -1.8942 0.1589 1.0725 3.3236 -3.2726 5.5399 ... 3.3193 0.2117 1.1730 ... ⋱ ... 2.4032 -3.4415 5.1036 ... -2.2035 -0.1900 -6.4016 5.2031 -1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5d0c5e69e8b60f0660812ad83c5bee15/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-08-09T21:30:59+08:00" />
<meta property="article:modified_time" content="2019-08-09T21:30:59+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习PyTorch（三）循环神经网络</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-tomorrow-night">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p></p> 
<div class="toc"> 
 <h4>文章目录</h4> 
 <ul><li><a href="#_1" rel="nofollow">基础</a></li><li><ul><li><ul><li><a href="#RNN_pytorch_26" rel="nofollow">RNN_pytorch实现</a></li></ul> 
   </li><li><a href="#LSTMLong_Short_Term_Memory_networks_139" rel="nofollow">LSTM(Long Short Term Memory networks)</a></li><li><ul><li><a href="#LSTM_pytorch_153" rel="nofollow">LSTM_pytorch实现</a></li></ul> 
   </li><li><a href="#GRUGated_Recurrent_Unit_194" rel="nofollow">GRU(Gated Recurrent Unit)</a></li></ul> 
  </li><li><a href="#RNN_230" rel="nofollow">RNN的应用</a></li><li><ul><li><a href="#RNNmnist_236" rel="nofollow">用RNN做图像分类mnist</a></li><li><ul><li><a href="#pytorch_248" rel="nofollow">pytorch实现</a></li></ul> 
   </li><li><a href="#RNN_297" rel="nofollow">RNN做时间序列（回归）</a></li><li><ul><li><a href="#pytorch_307" rel="nofollow">pytorch实现</a></li></ul> 
   </li><li><a href="#_434" rel="nofollow">自然语言处理</a></li><li><ul><li><a href="#_437" rel="nofollow">词嵌入</a></li><li><ul><li><a href="#pytorch_444" rel="nofollow">pytorch实现</a></li></ul> 
    </li><li><a href="#SkipGram_497" rel="nofollow">Skip-Gram模型</a></li><li><a href="#NGram_511" rel="nofollow">N-Gram模型</a></li><li><ul><li><a href="#pytorch_520" rel="nofollow">pytorch实现</a></li></ul> 
    </li><li><a href="#LSTM_743" rel="nofollow">LSTM做词性预测</a></li><li><ul><li><a href="#pytorch_749" rel="nofollow">pytorch实现</a></li></ul> 
   </li></ul> 
  </li></ul> 
 </li></ul> 
</div> 
<p></p> 
<h2><a id="_1"></a>基础</h2> 
<p><img src="https://images2.imgbox.com/6a/79/N1JgRS5A_o.png" width="80%" alt=""></p> 
<p><img src="https://images2.imgbox.com/1f/a5/sv5JvdA6_o.png" width="80%" alt=""></p> 
<p><img src="https://images2.imgbox.com/3e/74/aK6hn5nn_o.png" width="80%" alt=""></p> 
<p><img src="https://images2.imgbox.com/9d/0f/WnqEGFME_o.png" width="80%" alt=""></p> 
<p><img src="https://images2.imgbox.com/86/83/JYjtbPTD_o.png" width="80%" alt=""><br> 没有办法找到非常前面的信息，存在<strong>长时依赖问题</strong>。</p> 
<p><img src="https://images2.imgbox.com/9d/8c/QBRoKAvd_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/ae/f4/tcJaFifN_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/86/91/VmiyEFgl_o.png" width="80%" alt=""></p> 
<h4><a id="RNN_pytorch_26"></a>RNN_pytorch实现</h4> 
<p>使用<code>RnnCell</code></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

<span class="token comment">#定义一个单步的rnn</span>
rnn_single<span class="token operator">=</span>nn<span class="token punctuation">.</span>RNNCell<span class="token punctuation">(</span>input_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rnn_single<span class="token punctuation">.</span>weight_hh<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Parameter containing:
1.00000e-02 *
6.2260 -5.3805 3.5870 ... -2.2162 6.2760 1.6760
-5.1878 -4.6751 -5.5926 ... -1.8942 0.1589 1.0725
3.3236 -3.2726 5.5399 ... 3.3193 0.2117 1.1730
... ⋱ ...
2.4032 -3.4415 5.1036 ... -2.2035 -0.1900 -6.4016
5.2031 -1.5793 -0.0623 ... 0.3424 6.9412 6.3707
-5.4495 4.5280 2.1774 ... 1.8767 2.4968 5.3403
[torch.FloatTensor of size 200x200]
"""</span>

<span class="token comment">#构造一个序列，长为6，batch是5，特征是100</span>
x<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#这是rnn的输入格式(seq_len,batch,feature(input size))</span>

<span class="token comment">#定义初始的记忆状态</span>
h_t<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zero<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#传入rnn</span>
out<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    h_t<span class="token operator">=</span>rnn_single<span class="token punctuation">(</span>x<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>h_t<span class="token punctuation">)</span>
    out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h_t<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>h_t<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Variable containing:
0.0136 0.3723 0.1704 ... 0.4306 -0.7909 -0.5306
-0.2681 -0.6261 -0.3926 ... 0.1752 0.5739 -0.2061
-0.4918 -0.7611 0.2787 ... 0.0854 -0.3899 0.0092
0.6050 0.1852 -0.4261 ... -0.7220 0.6809 0.1825
-0.6851 0.7273 0.5396 ... -0.7969 0.6133 -0.0852
[torch.FloatTensor of size 5x200]
"""</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#6</span>
out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token comment">#torch.Size([5, 200])</span>
<span class="token triple-quoted-string string">"""
经过rnn后，隐藏状态的值已经被改变了，因为网络记忆了序列中的信息，同时输出6个结果
"""</span>
</code></pre> 
<p>使用<code>Rnn</code></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

rnn_seq<span class="token operator">=</span>nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token comment">#(input_size,hdden_size)</span>

<span class="token comment">#访问其中的参数</span>
rnn_seq<span class="token punctuation">.</span>weight_hh_10
<span class="token triple-quoted-string string">"""
Parameter containing:
1.00000e-02 *
1.0998 -1.5018 -1.4337 ... 3.8385 -0.8958 -1.6781
5.3302 -5.4654 5.5568 ... 4.7399 5.4110 3.6170
1.0788 -0.6620 5.7689 ... -5.0747 -2.9066 0.6152
... ⋱ ...
-5.6921 0.1843 -0.0803 ... -4.5852 5.6194 -1.4734
4.4306 6.9795 -1.5736 ... 3.4236 -0.3441 3.1397
7.0349 -1.6120 -4.2840 ... -5.5676 6.8897 6.1968
[torch.FloatTensor of size 200x200]
"""</span>

x<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

out<span class="token punctuation">,</span>h_t<span class="token operator">=</span>rnn_seq<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment">#默认使用的券0隐藏状态</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>h_t<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Variable containing:
( 0 ,.,.) =
0.2012 0.0517 0.0570 ... 0.2316 0.3615 -0.1247
0.5307 0.4147 0.7881 ... -0.4138 -0.1444 0.3602
0.0882 0.4307 0.3939 ... 0.3244 -0.4629 -0.2315
0.2868 0.7400 0.6534 ... 0.6631 0.2624 -0.0162
0.0841 0.6274 0.1840 ... 0.5800 0.8780 0.4301
[torch.FloatTensor of size 1x5x200]
"""</span>
<span class="token builtin">len</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token comment">#6</span>
<span class="token triple-quoted-string string">"""
这里的h_t是网络最后的隐藏状态，网络也输出了6个结果，h_t是最后的结果
"""</span>

<span class="token comment">#自己定义初始的隐藏状态</span>
h_0<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#(num_layers * num_direction, batch, hidden_size)</span>

out<span class="token punctuation">,</span>h_t<span class="token operator">=</span>rnn_seq<span class="token punctuation">(</span>x<span class="token punctuation">,</span>h_0<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>h_t<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Variable containing:
( 0 ,.,.) =
0.2091 0.0353 0.0625 ... 0.2340 0.3734 -0.1307
0.5498 0.4221 0.7877 ... -0.4143 -0.1209 0.3335
0.0757 0.4204 0.3826 ... 0.3187 -0.4626 -0.2336
0.3106 0.7355 0.6436 ... 0.6611 0.2587 -0.0338
0.1025 0.6350 0.1943 ... 0.5720 0.8749 0.4525
[torch.FloatTensor of size 1x5x200]
"""</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment">#torch.Size([6, 5, 200]),(seq, batch, feature)</span>
</code></pre> 
<p>一般使用<code>Rnn</code>，避免手动写循环，非常方便。<br> 如果不特别说明，选择使用默认的全0初始化隐藏状态 。</p> 
<h3><a id="LSTMLong_Short_Term_Memory_networks_139"></a>LSTM(Long Short Term Memory networks)</h3> 
<p>能在一定程度上解决长时依赖问题。<br> 由三个门来控制，分别是输入门，遗忘门和输出门。<br> 输入门和输出门限制着输入和输出的大小，遗忘门控制记忆的保留。<br> 对于一个任务，遗忘门能够自己学习到该保留多少以前的记忆，不需要人为进行干扰，所以具备长时记忆的功能。<br> <img src="https://images2.imgbox.com/35/3e/cVnrbgtS_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/9d/a8/IeWD4qU0_o.png" width="80%" alt=""><br> 1.<strong>f</strong>是衰减系数，表示保留过去的多少信息。<br> 2.<strong>i</strong>表示保留接受的新信息的多少 ，<strong>c_</strong> 表示新接受的信息。<br> 3.将前面得到的两个衰减系数的分别乘上过去的信息和现在的新信息来确定t时刻真正的记忆状态<strong>c</strong><br> 4.<strong>o</strong>是一个系数，表示t时刻到底输出多少的记忆状态c，得到真正的模型输出<strong>h</strong></p> 
<h4><a id="LSTM_pytorch_153"></a>LSTM_pytorch实现</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

lstm_seq<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span><span class="token number">100</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment">#(in_size,hidden_size,num_layers)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lstm_seq<span class="token punctuation">.</span>weight_hh_10<span class="token punctuation">)</span><span class="token comment">#第一层的h_t权重</span>

<span class="token triple-quoted-string string">"""
Parameter containing:
1.00000e-02 *
3.8420 5.7387 6.1351 ... 1.2680 0.9890 1.3037
-4.2301 6.8294 -4.8627 ... -6.4147 4.3015 8.4103
9.4411 5.0195 9.8620 ... -1.6096 9.2516 -0.6941
... ⋱ ...
1.2930 -1.3300 -0.9311 ... -6.0891 -0.7164 3.9578
9.0435 2.4674 9.4107 ... -3.3822 -3.9773 -3.0685
-4.2039 -8.2992 -3.3605 ... 2.2875 8.2163 -9.3277
[torch.FloatTensor of size 400x100]
"""</span>
lstm_input<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#(seq_len.batch,in_size)</span>
out<span class="token punctuation">,</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span>c<span class="token punctuation">)</span><span class="token operator">=</span>lstm_seq<span class="token punctuation">(</span>lstm_input<span class="token punctuation">)</span><span class="token comment">#使用默认的全0的隐藏状态</span>
<span class="token triple-quoted-string string">"""
LSTM输出的隐藏状态有两个，h,c.就是图中每个cell之间的两个箭头。
这两个隐藏状态的大小都是相同的（num_layers*direction,batch,feature）
"""</span>
h<span class="token punctuation">.</span>shape<span class="token comment">#两层(2,3,100)</span>
c<span class="token punctuation">.</span>shape<span class="token comment">#torch.Size([2, 3, 100])</span>
out<span class="token punctuation">.</span>shape<span class="token comment">#(10,3,100)</span>

<span class="token comment">#当不使用默认的隐藏状态时</span>
h_init <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
c_init <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
out<span class="token punctuation">,</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span>c<span class="token punctuation">)</span><span class="token operator">=</span>lstm_seq<span class="token punctuation">(</span>lstm_input<span class="token punctuation">,</span><span class="token punctuation">(</span>h_init<span class="token punctuation">,</span>c_init<span class="token punctuation">)</span><span class="token punctuation">)</span>

h<span class="token punctuation">.</span>shape<span class="token comment">#(2,3,100)</span>
c<span class="token punctuation">.</span>shape<span class="token comment">#torch.Size([2, 3, 100])</span>
out<span class="token punctuation">.</span>shape<span class="token comment">#(10,3,100)</span>
</code></pre> 
<h3><a id="GRUGated_Recurrent_Unit_194"></a>GRU(Gated Recurrent Unit)</h3> 
<p>2014年由Cho提出。<br> GRU和LSTM最大的不同在于GRU将遗忘门和输入门合成为一个更新门，同时网络不再额外给出记忆状态c,而是将输出结果作为记忆状态不断往后传。<br> 重置门（reset gate）和更新门（update gate）。同时在这个结构中，把细胞状态和隐藏状态进行了合并。最后模型比标准的 LSTM 结构要简单，而且这个结构后来也非常流行。</p> 
<p><img src="https://images2.imgbox.com/75/e7/heDe6Apl_o.png" width="60%" alt=""></p> 
<p><img src="https://images2.imgbox.com/c5/15/vGu1WQ6Y_o.png" width="90%" alt=""></p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

gru_seq<span class="token operator">=</span>nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token comment">#(input_size,outputz-size)</span>
gru_input<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#(seq_len,batch,inpput_size)</span>

out<span class="token punctuation">,</span>h<span class="token operator">=</span>gru_seq<span class="token punctuation">(</span>gru_input<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>gru_seq<span class="token punctuation">.</span>weight_hh_10<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Parameter containing:
0.0766 -0.0548 -0.2008 ... -0.0250 -0.1819 0.1453
-0.1676 0.1622 0.0417 ... 0.1905 -0.0071 -0.1038
0.0444 -0.1516 0.2194 ... -0.0009 0.0771 0.0476
... ⋱ ...
0.1698 -0.1707 0.0340 ... -0.1315 0.1278 0.0946
0.1936 0.1369 -0.0694 ... -0.0667 0.0429 0.1322
0.0870 -0.1884 0.1732 ... -0.1423 -0.1723 0.2147
[torch.FloatTensor of size 60x20]
"""</span>
h<span class="token punctuation">.</span>shape<span class="token comment">#torch.Size([1, 32, 20])</span>
out<span class="token punctuation">.</span>shape<span class="token comment">#torch.Size([3, 32, 20])</span>
</code></pre> 
<h2><a id="RNN_230"></a>RNN的应用</h2> 
<p><a href="https://blog.csdn.net/yangyang_yangqi/article/details/84585998">LSTM代码具体解释</a></p> 
<p><a href="https://blog.csdn.net/xijuezhu8128/article/details/86590435">PyTorch 高维矩阵转置 Transpose 和 Permute</a><br> <a href="https://blog.csdn.net/qq_21210467/article/details/81415300">x.view(x.size(0), -1)的解释</a><br> <a href="https://www.jianshu.com/p/c1025b1290fa" rel="nofollow">python常用简写</a></p> 
<h3><a id="RNNmnist_236"></a>用RNN做图像分类mnist</h3> 
<p><img src="https://images2.imgbox.com/4f/9a/5Y4pmVfN_o.png" width="90%" alt=""></p> 
<p><img src="https://images2.imgbox.com/13/e2/ZenPARRb_o.png" width="90%" alt=""><br> <img src="https://images2.imgbox.com/02/17/S0eB7ANX_o.png" width="90%" alt=""></p> 
<p><img src="https://images2.imgbox.com/a8/54/N0AzCMwz_o.png" width="80%" alt=""><br> 结果比用cnn差一点，因为这个数据集主要是视觉信息</p> 
<h4><a id="pytorch_248"></a>pytorch实现</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms <span class="token keyword">as</span>  tfs
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> MNIST

<span class="token keyword">from</span> utils <span class="token keyword">import</span> train
<span class="token comment">#定义数据</span>
data_tf<span class="token operator">=</span>tfs<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    tfs<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tfs<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#标准化</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

train_set<span class="token operator">=</span>MNIST<span class="token punctuation">(</span><span class="token string">'./data'</span><span class="token punctuation">,</span>train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>tranform<span class="token operator">=</span>tfs<span class="token punctuation">)</span>
test_set<span class="token operator">=</span>MNIST<span class="token punctuation">(</span><span class="token string">'./data'</span><span class="token punctuation">,</span>train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>tranform<span class="token operator">=</span>tfs<span class="token punctuation">)</span>
train_data<span class="token operator">=</span>DataLoader<span class="token punctuation">(</span>train_set<span class="token punctuation">,</span>batch<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
test_data<span class="token operator">=</span>DataLoader<span class="token punctuation">(</span>test_set<span class="token punctuation">,</span>batch<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token comment">#定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">rnn_classify</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_feature<span class="token operator">=</span><span class="token number">28</span><span class="token punctuation">,</span>hidden_feature<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>num_class<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>rnn_classify<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>in_feature<span class="token punctuation">,</span>hidden_feature<span class="token punctuation">,</span>num_layers<span class="token punctuation">)</span><span class="token comment">#使用两层lstm</span>
        <span class="token comment">#将最后一个rnn的输出使用全连接得到最后的分类结果</span>
        self<span class="token punctuation">.</span>classifier<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_feature<span class="token punctuation">,</span>num_class<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        x大小为（batch，1，28，28），所以我们需要将其转换成RNN的输入形式，即（28，batch，28）
        """</span>
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#去掉（batch，1，28，28）的1</span>
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#将最后一维放到第一维，变成（28,batch,28）</span>
        out<span class="token punctuation">,</span>_<span class="token operator">=</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment">#使用默认的隐藏状态，得到的out是(28，batch，hidden_feature)</span>
        out<span class="token operator">=</span>out<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment">#取序列中的最后一个，大小为(batch,hidden_feature)</span>
        out<span class="token operator">=</span>self<span class="token punctuation">.</span>classifier<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token comment">#得到分类结果</span>
        <span class="token keyword">return</span> out

net<span class="token operator">=</span>rnn_classify<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adadelta<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment">#开始训练</span>
train<span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_data<span class="token punctuation">,</span>test_data<span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>optimizer<span class="token punctuation">,</span>criterion<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="RNN_297"></a>RNN做时间序列（回归）</h3> 
<p>图像分类并不是RNN擅长的应用场景，RNN更加擅长处理<strong>时间序列</strong>关系的数据。<br> 时间序列的数据：具有时间特性的数据，比如股票价格、机票价格。随着时间的变化数据会不断发生变化，而且变化依赖于时间。<br> 举例：根据历史飞机流量数据，预测未来飞机流量<br> <img src="https://images2.imgbox.com/cd/6d/0kHV0eak_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/8a/54/E3rEXkte_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/f8/5a/Io8i62QT_o.png" width="80%" alt=""></p> 
<h4><a id="pytorch_307"></a>pytorch实现</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token comment">#%matplotlib inline</span>

<span class="token keyword">import</span>  torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable

data_csv<span class="token operator">=</span>pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./data.csv'</span><span class="token punctuation">,</span>usecols<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>data_csv<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
进行预处理，将数据中的na的数据去掉，然后将数据标准化到0~1之间
"""</span>
<span class="token comment">#数据预处理</span>
data_csv<span class="token operator">=</span>data_csv<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#去掉na(np.nan),清除缺失值</span>
dataset<span class="token operator">=</span>data_csv<span class="token punctuation">.</span>values
dataset<span class="token operator">=</span>dataset<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token comment">#转换成float32字符</span>
max_value<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
min_value<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
scalar<span class="token operator">=</span>max_value<span class="token operator">-</span>min_value
dataset<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token operator">/</span>scalar<span class="token punctuation">,</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
进行数据集的闯进啊，想通过前两个月的流量预测当月的流量。
则将前两个月的流量当作输入，当月的流量当作输出。
同时需要将数据集分为训练集和测试集。
简单地将前几年的数据作为训练集，后面两年的数据作为测试集
"""</span>
<span class="token keyword">def</span> <span class="token function">create_dataset</span><span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>look_back<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    dataX<span class="token punctuation">,</span>dataY<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token operator">-</span>look_back<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a<span class="token operator">=</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">(</span>i<span class="token operator">+</span>look_back<span class="token punctuation">)</span><span class="token punctuation">]</span>
        dataX<span class="token punctuation">.</span>append<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
        dataY<span class="token punctuation">.</span>append<span class="token punctuation">(</span>dataset<span class="token punctuation">[</span>i<span class="token operator">+</span>look_back<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>dataX<span class="token punctuation">)</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>dataY<span class="token punctuation">)</span>

<span class="token comment">#创建好输入输出</span>
data_X<span class="token punctuation">,</span>data_Y<span class="token operator">=</span>create_dataset<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>

<span class="token comment">#划分训练集和数据集.70%作为训练集</span>
train_size<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_X<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.7</span><span class="token punctuation">)</span>
test_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data_X<span class="token punctuation">)</span><span class="token operator">-</span>train_size
train_X<span class="token operator">=</span>data_X<span class="token punctuation">[</span><span class="token punctuation">:</span>train_size<span class="token punctuation">]</span>
train_Y<span class="token operator">=</span>data_Y<span class="token punctuation">[</span><span class="token punctuation">:</span>train_size<span class="token punctuation">]</span>
test_X<span class="token operator">=</span>data_X<span class="token punctuation">[</span>train_size<span class="token punctuation">:</span><span class="token punctuation">]</span>
test_Y<span class="token operator">=</span>data_Y<span class="token punctuation">[</span>train_size<span class="token punctuation">:</span><span class="token punctuation">]</span>

<span class="token triple-quoted-string string">"""
将数据改变一下形状，RNN需要读入的数据为(seq_len,batch,input_size).
所以需要重新改变一下数据的维度，这里只有一个序列，所以batch是1.
而输入的feature就是我们希望依据的几个月份。
这里我们定的是两个月份，所以feature=2
"""</span>

train_X<span class="token operator">=</span>train_X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
train_Y<span class="token operator">=</span>train_Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
test_X <span class="token operator">=</span> test_X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>

train_X<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>
train_Y<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>train_Y<span class="token punctuation">)</span>
test_X<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>test_X<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
定义模型
第一部分是一个两层的RNN，每一步模型接受两个月的输入作为特征，得到一个输出特征。
接着通过一个线性层将RNN的输出回归到流量的具体数值。
这里我们需要用view来重新排列，因为nn.Linear不接受三维的输入，
所以我们先将前两维合并在一起，然后通过线性层之后再将其分开，最后输出结果。
"""</span>
<span class="token comment">#定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">lstm_reg</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>lstm_reg<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>hidden_size<span class="token punctuation">,</span>num_layers<span class="token punctuation">)</span><span class="token comment">#rnn</span>
        self<span class="token punctuation">.</span>reg<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token comment">#回归</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span>_<span class="token operator">=</span>self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        s<span class="token punctuation">,</span>b<span class="token punctuation">,</span>h<span class="token operator">=</span>x<span class="token punctuation">.</span>shape<span class="token comment">#(seq_len,batch,hidden_size)</span>
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>s<span class="token operator">*</span>b<span class="token punctuation">,</span>h<span class="token punctuation">)</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>reg<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>s<span class="token punctuation">,</span>b<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

net<span class="token operator">=</span>lstm_reg<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion<span class="token operator">=</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#均方误差</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
定义好网络结构，输入的维度为2，因为我们选用两个月的流量作为输入。
隐藏层的维度可以任意指定，这里选择4
"""</span>
<span class="token comment">#开始训练</span>
<span class="token keyword">for</span> e <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    var_x<span class="token operator">=</span>Variable<span class="token punctuation">(</span>train_X<span class="token punctuation">)</span>
    var_y<span class="token operator">=</span>Variable<span class="token punctuation">(</span>train_Y<span class="token punctuation">)</span>
    <span class="token comment">#前向传播</span>
    out<span class="token operator">=</span>net<span class="token punctuation">(</span>var_x<span class="token punctuation">)</span>
    loss<span class="token operator">=</span>criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span>var_y<span class="token punctuation">)</span>
    <span class="token comment">#反向传播</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">100</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token comment">#每100次输出结果</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:{},Loss:{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment">#训练完后，用训练好的模型取预测后面的结果</span>
net<span class="token operator">=</span>net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#转换成测试模式</span>

data_X<span class="token operator">=</span>data_X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
data_X<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data_X<span class="token punctuation">)</span>
var_data<span class="token operator">=</span>Variable<span class="token punctuation">(</span>data_X<span class="token punctuation">)</span>
pre_test<span class="token operator">=</span>net<span class="token punctuation">(</span>var_data<span class="token punctuation">)</span><span class="token comment">#测试集的预测结果</span>

<span class="token comment">#改变输出的格式</span>
pre_test<span class="token operator">=</span>pre_test<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">#画出实际结果和预测的结果</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>pre_test<span class="token punctuation">,</span><span class="token string">'r'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'prediction'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span><span class="token string">'b'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'real'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>lagend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span>
</code></pre> 
<h3><a id="_434"></a>自然语言处理</h3> 
<p>是RNN一个非常热门的方向。<br> 有词嵌入，n-gram，序列lstm三个经典模型</p> 
<h4><a id="_437"></a>词嵌入</h4> 
<p><img src="https://images2.imgbox.com/69/1a/xG4OHZBs_o.png" width="40%" alt=""><br> <img src="https://images2.imgbox.com/ae/e0/EMdIYwpZ_o.png" width="480%" alt=""><br> <img src="https://images2.imgbox.com/70/22/X9N6BVnB_o.png" width="480%" alt=""></p> 
<h5><a id="pytorch_444"></a>pytorch实现</h5> 
<p>在pytorch中使用<code>torch.nn.Embedding(m,n)</code>即可，m表示单词的总数，n表示词嵌入的维度。<br> 词嵌入相当于一个巨大的矩阵，矩阵的每一行表示一个单词。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable

<span class="token comment">#定义词嵌入</span>
embeds<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token comment">#两个单词，维度为4</span>

<span class="token comment">#得到词嵌入矩阵</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>embeds<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Parameter containing:
-1.3426 0.7316 -0.2437 0.4925 -0.0191
-0.8326 0.3367 0.2135 0.5059 0.8326
[torch.FloatTensor of size 2x5]
"""</span>

<span class="token triple-quoted-string string">"""
通过weitht得到整个词嵌入的矩阵。
注意，该矩阵是一个可以改变的parameter，在网络的训练中会不断更新，
同时词嵌入的数值可以直接进行修改。
比如我们可以读入一个预训练好的词嵌入等
"""</span>
<span class="token comment">#直接手动修改词嵌入的值</span>
embeds<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token operator">=</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>embeds<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Parameter containing:
1 1 1 1 1
1 1 1 1 1
[torch.FloatTensor of size 2x5]
"""</span>

<span class="token triple-quoted-string string">"""
如果要访问其中一个单词的词向量，我么可以直接调用定义好的词嵌入。
但输入必须传入一个Variable,且类型是LongTensor
"""</span>
<span class="token comment">#访问第50个词的词向量</span>
embeds<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span>
single_word_embed<span class="token operator">=</span>embeds<span class="token punctuation">(</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>single_word_embed<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
Variable containing:
-1.4954 -1.8475 0.2913 -0.9674 -2.1250 -0.5783 -0.6717 0.5638 0.7038 0.4437
[torch.FloatTensor of size 1x10]
"""</span>
</code></pre> 
<h4><a id="SkipGram_497"></a>Skip-Gram模型</h4> 
<p>词嵌入是用来定义词向量的相似性。<br> 如何得到词嵌入?<br> 如果一个词嵌入100维，显然不可能人为去赋值，所以为了得到词向量，需要Skip-Gram。<br> Skip-Gram模型是<a href="https://arxiv.org/pdf/1301.3781.pdf" rel="nofollow">word2vec</a>的网络架构。<br> <img src="https://images2.imgbox.com/6c/23/jKrDgwl9_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/eb/bd/6DrU3ZUF_o.png" width="70%" alt=""><br> <img src="https://images2.imgbox.com/ba/75/8C2PpDlu_o.png" width="70%" alt=""></p> 
<h4><a id="NGram_511"></a>N-Gram模型</h4> 
<p>词嵌入如何来训练语言模型<br> <img src="https://images2.imgbox.com/3f/fa/48L1lAq0_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/c1/2c/xoxmi0uv_o.png" width="80%" alt=""></p> 
<p><img src="https://images2.imgbox.com/a1/a8/stWOcbbP_o.png" width="80%" alt=""></p> 
<h5><a id="pytorch_520"></a>pytorch实现</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

CONTEXT_SIZE<span class="token operator">=</span><span class="token number">2</span><span class="token comment">#依据的单词数,希望由前面两个单词来预测这个单词</span>
EMBEDDING_DIM<span class="token operator">=</span><span class="token number">10</span><span class="token comment">#词向量的维度</span>
<span class="token comment">#我们使用莎士比亚的诗</span>
test_sentence<span class="token operator">=</span><span class="token triple-quoted-string string">"""When forty winters shall besiege thy brow,
And dig deep trenches in thy beauty's field,
Thy youth's proud livery so gazed on now,
Will be a totter'd weed of small worth held:
Then being asked, where all thy beauty lies,
Where all the treasure of thy lusty days;
To say, within thine own deep sunken eyes,
Were an all-eating shame, and thriftless praise.
How much more praise deserv'd thy beauty's use,
If thou couldst answer 'This fair child of mine
Shall sum my count, and make my old excuse,'
Proving his beauty by succession thine!
This were to be new made when thou art old,
And see thy blood warm when thou feel'st it cold."""</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">"""
建立训练集，遍历整个语料库，将单词三个分组，前两个作为输入，最后一个作为预测的结果
"""</span>
trigram<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token punctuation">(</span>test_sentence<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>test_sentence<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>test_sentence<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
         <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_sentence<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token comment">#总的数据量</span>
<span class="token builtin">len</span><span class="token punctuation">(</span>trigram<span class="token punctuation">)</span><span class="token comment">#113</span>

<span class="token comment">#取出一个数看看</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>trigram<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#(('When', 'forty'), 'winters')</span>

<span class="token comment">#建立每个词与数字的编码，据此构建词嵌入</span>
vocb<span class="token operator">=</span><span class="token builtin">set</span><span class="token punctuation">(</span>test_sentence<span class="token punctuation">)</span><span class="token comment">#使用set将重复的元素去掉</span>
word_to_idx<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>word<span class="token punctuation">:</span>i <span class="token keyword">for</span> i<span class="token punctuation">,</span>word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>vocb<span class="token punctuation">)</span><span class="token punctuation">}</span>
idx_to_word<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>word_to_idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span><span class="token punctuation">:</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> word_to_idx<span class="token punctuation">}</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
{"'This": 94,
'And': 71,
'How': 18,
'If': 49,
'Proving': 78,
'Shall': 48,
'Then': 33,
'This': 68,
'Thy': 75,
'To': 81,
'Were': 61,
'When': 14,
'Where': 95,
'Will': 27,
'a': 21,
'all': 53,
'all-eating': 3,
'an': 15,
'and': 23,
'answer': 80,
'art': 70,
'asked,': 69,
'be': 29,
'beauty': 16,
"beauty's": 40,
'being': 79,
'besiege': 55,
'blood': 11,
'brow,': 1,
'by': 59,
'child': 8,
'cold.': 32,
'couldst': 26,
'count,': 77,
'days;': 43,
'deep': 62,
"deserv'd": 41,
'dig': 64,
"excuse,'": 86,
'eyes,': 84,
'fair': 56,
"feel'st": 44,
'field,': 9,
'forty': 46,
'gazed': 93,
'held:': 12,
'his': 89,
'in': 45,
'it': 34,
'lies,': 57,
'livery': 28,
'lusty': 65,
'made': 54,
'make': 42,
'mine': 13,
'more': 83,
'much': 30,
'my': 50,
'new': 92,
'now,': 25,
'of': 47,
'old': 22,
'old,': 19,
'on': 74,
'own': 20,
'praise': 38,
'praise.': 96,
'proud': 5,
'say,': 63,
'see': 58,
'shall': 87,
'shame,': 90,
'small': 31,
'so': 67,
'succession': 36,
'sum': 10,
'sunken': 60,
'the': 73,
'thine': 24,
'thine!': 0,
'thou': 51,
'thriftless': 72,
'thy': 76,
'to': 85,
"totter'd": 2,
'treasure': 17,
'trenches': 39,
'use,': 35,
'warm': 66,
'weed': 91,
'were': 82,
'when': 7,
'where': 37,
'winters': 88,
'within': 4,
'worth': 52,
"youth's": 6}
"""</span>

<span class="token triple-quoted-string string">"""
从上面可看出每个词都对应一个数字，且这里的单词都各不相同
接下来定义模型，模型输入就是前面两个词，输出就是预测单词的概率
"""</span>
<span class="token comment">#定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">n_gram</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>vocab_size<span class="token punctuation">,</span>context_size<span class="token operator">=</span>CONTEXT_SIZE<span class="token punctuation">,</span>n_dim<span class="token operator">=</span>EMBEDDING_DIM<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>n_gram<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embeds<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span>n_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>classify<span class="token operator">=</span>nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>context_size<span class="token operator">*</span>n_dim<span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span>vocab_size<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        voc_embed<span class="token operator">=</span>self<span class="token punctuation">.</span>embeds<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment">#得到词嵌入</span>
        voc_embed<span class="token operator">=</span>voc_embed<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#将两个词向量拼在一起</span>
        out<span class="token operator">=</span>self<span class="token punctuation">.</span>classify<span class="token punctuation">(</span>voc_embed<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

<span class="token triple-quoted-string string">"""
最后输出条件概率，相当于是一个分类问题，可以使用交叉熵来方便地衡量误差
"""</span>
net<span class="token operator">=</span>n_gram<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
criterion<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span>weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> e <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token operator">=</span><span class="token number">0</span>
    <span class="token keyword">for</span> word<span class="token punctuation">,</span>label <span class="token keyword">in</span> trigram<span class="token punctuation">:</span><span class="token comment">#使用前100个作为数据集</span>
        word<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span>word_to_idx<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> word<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#将前两个词作为输入,（（2个单词），1个单词）</span>
        label<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>word_to_idx<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#前向传播</span>
        out<span class="token operator">=</span>net<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        loss<span class="token operator">=</span>criterion<span class="token punctuation">(</span>out<span class="token punctuation">,</span>label<span class="token punctuation">)</span>
        train_loss<span class="token operator">+=</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token comment">#反向传播</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">20</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch:{},Loss:{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>train_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>trigram<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net<span class="token operator">=</span>net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#测试结果</span>
word<span class="token punctuation">,</span>label<span class="token operator">=</span>trigram<span class="token punctuation">[</span><span class="token number">19</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input:{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label:{}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
word<span class="token operator">=</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>word_to_idx<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> word<span class="token punctuation">)</span><span class="token punctuation">)</span>
out<span class="token operator">=</span>net<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
pred_label_idx<span class="token operator">=</span>out<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
predict_word<span class="token operator">=</span>idx_to_word<span class="token punctuation">[</span>pred_label_idx<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'real word is {}，predicted word is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span>predict_word<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
input: ('so', 'gazed')
label: on
real word is on, predicted word is on
"""</span>

word<span class="token punctuation">,</span> label <span class="token operator">=</span> trigram<span class="token punctuation">[</span><span class="token number">75</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input: {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'label: {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
word <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span>word_to_idx<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> word<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> net<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
pred_label_idx <span class="token operator">=</span> out<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
predict_word <span class="token operator">=</span> idx_to_word<span class="token punctuation">[</span>pred_label_idx<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'real word is {}, predicted word is {}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>label<span class="token punctuation">,</span> predict_word<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
input: ("'This", 'fair')
label: child
real word is child, predicted word is child
"""</span>
<span class="token triple-quoted-string string">"""
这里样本过少，容易发生过拟合现象
"""</span>
</code></pre> 
<h4><a id="LSTM_743"></a>LSTM做词性预测</h4> 
<p><img src="https://images2.imgbox.com/e5/f7/Y9jIfPXl_o.png" width="80%" alt=""><br> <img src="https://images2.imgbox.com/bb/e0/Qpc1VabG_o.png" width="80%" alt=""><br> 接着把这个单词和其前面几个单词构成序列，可以对这些单词构建新的词嵌入，最后输出结果是单词的词性，也就是根据前面几个词的词性进行分类。入上图右</p> 
<h5><a id="pytorch_749"></a>pytorch实现</h5> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable

<span class="token comment">#使用下面简单的训练集</span>
training_data<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"The dog ate the apple"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">"DET"</span><span class="token punctuation">,</span><span class="token string">"NN"</span><span class="token punctuation">,</span><span class="token string">"V"</span><span class="token punctuation">,</span><span class="token string">"DET"</span><span class="token punctuation">,</span><span class="token string">"NN"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
               <span class="token punctuation">(</span><span class="token string">"Everybody read that book"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token string">"NN"</span><span class="token punctuation">,</span> <span class="token string">"V"</span><span class="token punctuation">,</span> <span class="token string">"DET"</span><span class="token punctuation">,</span> <span class="token string">"NN"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token comment">#对单词和标签进行编码</span>
word_to_idx<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
tag_to_idx<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

<span class="token keyword">for</span> context<span class="token punctuation">,</span>tag <span class="token keyword">in</span> training_data<span class="token punctuation">:</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> context<span class="token punctuation">:</span>
        <span class="token keyword">if</span> word<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> word_to_idx<span class="token punctuation">:</span>
            word_to_idx<span class="token punctuation">[</span>word<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">)</span>
    <span class="token keyword">for</span> label <span class="token keyword">in</span> tag<span class="token punctuation">:</span>
        <span class="token keyword">if</span> label<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">not</span> <span class="token keyword">in</span> tag_to_idx<span class="token punctuation">:</span>
            tag_to_idx<span class="token punctuation">[</span>label<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>tag_to_idx<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
{'apple': 3,
'ate': 2,
'book': 7,
'dog': 1,
'everybody': 4,
'read': 5,
'that': 6,
'the': 0}
"""</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tag_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
{'det': 0, 'nn': 1, 'v': 2}
"""</span>

<span class="token comment">#然后我们对字母进行编码</span>
alphabet<span class="token operator">=</span><span class="token string">'abcdefghijklmnopqrstuvwxyz'</span>
char_to_idx<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>alphabet<span class="token punctuation">)</span><span class="token punctuation">:</span>
    char_to_idx<span class="token punctuation">[</span>alphabet<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">=</span>i
<span class="token keyword">print</span><span class="token punctuation">(</span>char_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
{'a': 0,
'b': 1,
'c': 2,
'd': 3,
'e': 4,
'f': 5,
'g': 6,
'h': 7,
'i': 8,
'j': 9,
'k': 10,
'l': 11,
'm': 12,
'n': 13,
'o': 14,
'p': 15,
'q': 16,
'r': 17,
's': 18,
't': 19,
'u': 20,
'v': 21,
'w': 22,
'x': 23,
'y': 24,
'z': 25}
"""</span>

<span class="token comment">#构建训练数据</span>
<span class="token keyword">def</span> <span class="token function">make_sequence</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>dic<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#字符编码</span>
    idx<span class="token operator">=</span><span class="token punctuation">[</span>dic<span class="token punctuation">[</span>i<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> x<span class="token punctuation">]</span>
    idx<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>idx<span class="token punctuation">)</span>
    <span class="token keyword">return</span> idx

make_sequence<span class="token punctuation">(</span><span class="token string">'apple'</span><span class="token punctuation">,</span>char_to_idx<span class="token punctuation">)</span><span class="token comment">#apple在字母表中对应的标签</span>
<span class="token triple-quoted-string string">"""
0
15
15
11
4
[torch.LongTensor of size 5]
"""</span>

training_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
<span class="token triple-quoted-string string">"""
['Everybody', 'read', 'that', 'book']
"""</span>

make_sequence<span class="token punctuation">(</span>training_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>word_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
4
5
6
7
[torch.LongTensor of size 4]
"""</span>

<span class="token comment">#构建单个字符的lstm模型</span>
<span class="token keyword">class</span> <span class="token class-name">char_lstm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>n_char<span class="token punctuation">,</span>char_dim<span class="token punctuation">,</span>char_hidden<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>char_lstm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>char_embed<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_char<span class="token punctuation">.</span>char_dim<span class="token punctuation">)</span><span class="token comment">#(字符个数，每个字符维数)</span>
        self<span class="token punctuation">.</span>lstm<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>char_dim<span class="token punctuation">.</span>char_hidden<span class="token punctuation">)</span><span class="token comment">#(input_size,hiddem_size,num_layers)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token operator">=</span>self<span class="token punctuation">.</span>char_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out<span class="token punctuation">,</span>_<span class="token operator">=</span>self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token comment">#(batch,hidden_size)</span>

<span class="token comment">#构建词性分类的lstm模型</span>
<span class="token keyword">class</span> <span class="token class-name">lstm_tagger</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>n_word<span class="token punctuation">,</span>n_char<span class="token punctuation">,</span>char_dim<span class="token punctuation">,</span>word_dim<span class="token punctuation">,</span>
                 char_hidden<span class="token punctuation">,</span>word_hidden<span class="token punctuation">,</span>n_tag<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>lstm_tagger<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>word_embed<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>n_word<span class="token punctuation">,</span>word_dim<span class="token punctuation">)</span><span class="token comment">#(单词个数，每个单词维数)</span>
        self<span class="token punctuation">.</span>char_lstm<span class="token operator">=</span>char_lstm<span class="token punctuation">(</span>n_char<span class="token punctuation">,</span>char_dim<span class="token punctuation">,</span>char_hidden<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>word_lstm<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>word_dim<span class="token operator">+</span>char_hidden<span class="token punctuation">,</span>word_hidden<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>classify<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>word_hidden<span class="token punctuation">,</span>n_tag<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>
        char<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> w <span class="token keyword">in</span> word<span class="token punctuation">:</span><span class="token comment">#对每个单词做字符的lstm</span>
            char_list<span class="token operator">=</span>make_sequence<span class="token punctuation">(</span>w<span class="token punctuation">,</span>char_to_idx<span class="token punctuation">)</span><span class="token comment">#对应字母表</span>
            char_list<span class="token operator">=</span>char_list<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#(seq_len,batch,char_dim)满足lstm输入条件</span>
            char_info<span class="token operator">=</span>self<span class="token punctuation">.</span>char_lstm<span class="token punctuation">(</span>Variable<span class="token punctuation">(</span>char_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#(batch,char_hidden)</span>
            char<span class="token punctuation">.</span>append<span class="token punctuation">(</span>char_info<span class="token punctuation">)</span><span class="token comment">#(len(word)个)</span>
        char<span class="token operator">=</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>char<span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#(seq, batch, feature)</span>

        x<span class="token operator">=</span>self<span class="token punctuation">.</span>word_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment">#(batch,seq_len,word_dim)</span>
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment">#(seq_len,batch,word_dim)改变顺序</span>
        x<span class="token operator">=</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>char<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment">#沿着特征通道将没歌词的词嵌入和字符lstm输出的结果拼接在一起</span>
        x<span class="token punctuation">,</span>_<span class="token operator">=</span>self<span class="token punctuation">.</span>word_lstm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        s<span class="token punctuation">,</span>b<span class="token punctuation">,</span>h<span class="token operator">=</span>x<span class="token punctuation">.</span>shape
        x<span class="token operator">=</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>s<span class="token operator">*</span>b<span class="token punctuation">,</span>h<span class="token punctuation">)</span>
        out<span class="token operator">=</span>self<span class="token punctuation">.</span>classify<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

net<span class="token operator">=</span>lstm_tagger<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>char_to_idx<span class="token punctuation">)</span><span class="token punctuation">,</span>char_dim<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>word_dim<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
                 char_hidden<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>word_hidden<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>n_tag<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>tag_to_idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
criterion<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment">#开始训练</span>
<span class="token keyword">for</span> e <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token operator">=</span><span class="token number">0</span>
    <span class="token keyword">for</span> word<span class="token punctuation">,</span>tag <span class="token keyword">in</span> training_data<span class="token punctuation">:</span>
        word_list<span class="token operator">=</span>make_sequence<span class="token punctuation">(</span>word<span class="token punctuation">,</span>word_to_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#(batch,seq_len,word_dim)</span>
        tag<span class="token operator">=</span>make_sequence<span class="token punctuation">(</span>tag<span class="token punctuation">,</span>tag_to_idx<span class="token punctuation">)</span>
        word<span class="token operator">=</span>Variable<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        tag<span class="token operator">=</span>Variable<span class="token punctuation">(</span>tag<span class="token punctuation">)</span>
        <span class="token comment">#前向传播</span>
        out<span class="token operator">=</span>net<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        loss<span class="token operator">=</span>criterion<span class="token punctuation">(</span>word<span class="token punctuation">,</span>tag<span class="token punctuation">)</span>
        train_loss<span class="token operator">+=</span>loss
        <span class="token comment">#反向传播</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span><span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">%</span><span class="token number">50</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Epoch:{},Loss:{:.5f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>e<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>train_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>training_data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net<span class="token operator">=</span>net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

test_sent<span class="token operator">=</span><span class="token string">'Everybody ate the apple'</span>
test<span class="token operator">=</span>make_sequence<span class="token punctuation">(</span>test_sent<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>word_to_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#(batch,seq_len,word_dim)</span>
out<span class="token operator">=</span>net<span class="token punctuation">(</span>Variable<span class="token punctuation">(</span>test<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token comment">#(n_tag,_)</span>
<span class="token triple-quoted-string string">"""
Variable containing:
-1.2148 1.9048 -0.6570
-0.9272 -0.4441 1.4009
1.6425 -0.7751 -1.1553
-0.6121 1.6036 -1.1280
[torch.FloatTensor of size 4x3]
"""</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tag_to_idx<span class="token punctuation">)</span>
<span class="token triple-quoted-string string">"""
{'det': 0, 'nn': 1, 'v': 2}
"""</span>
<span class="token triple-quoted-string string">"""
因为最后一层的线性层没有使用softmax,所以数值不太像个概率，但是每一行数值最大的就表示该类。
"""</span>
</code></pre>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/33b4cd5db9c12f8c77e5135e3c11db64/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">MicroPhoneInput 自动判定音源录入&#43;百度音频录入的问题</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/f6da8ef03b98fe88b3ce40f88e6a536d/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">pwn（无system函数下的栈溢出漏洞利用 | 【puts函数】</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>