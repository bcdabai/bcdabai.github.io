<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>C&#43;&#43;实现yolov5的OpenVINO部署 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="C&#43;&#43;实现yolov5的OpenVINO部署" />
<meta property="og:description" content="点击上方“计算机视觉工坊”，选择“星标”
干货第一时间送达
[GiantPandaCV导语] 本文介绍了一种使用c&#43;&#43;实现的,使用OpenVINO部署yolov5的方法。此方法在2020年9月结束的极市开发者榜单中取得后厨老鼠识别赛题第四名。2020年12月,注意到yolov5有了许多变化,对部署流程重新进行了测试,并进行了整理。希望能给需要的朋友一些参考,节省一些踩坑的时间。
模型训练 1. 首先获取yolov5工程 git clone https://github.com/ultralytics/yolov5.git 本文编辑的时间是2020年12月3日,官方最新的releases是v3.1,在v3.0的版本中,官网有如下的声明
August 13, 2020**: v3.0 release(https://github.com/ultralytics/yolov5/releases/tag/v3.0): nn.Hardswish() activations, data autodownload, native AMP.
yolov5训练获得的原始的模型以.pt文件方式存储,要转换为OpenVINO的.xml和.bin的模型存储方式,需要经历两次转换.
两次转换所用到的工具无法同时支持nn.Hardswish()函数的转换,v3.0版本时需要切换到v2.0版本替换掉nn.Hardswish()函数才能够完成两次模型转换,当时要完成模型转换非常的麻烦.
在v3.1版本的yolov5中用于进行pt模型转onnx模型的程序对nn.Hardswish()进行了兼容,模型转换过程大为化简.
2. 训练准备 yolov5官方的指南: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data
描述信息准备 在yolov5的文件夹下/yolov5/models/目录下可以找到以下文件
yolov5s.yaml
yolov5m.yaml
yolov5l.yaml
这三个文件分别对应s(小尺寸模型),ｍ(中尺寸模型)和l(大尺寸模型)的结构描述信息
其中为了实现自己的训练常常需要更改以下两个参数
nc
需要识别的类别数量,yolov5原始的默认类别数量为80
anchors
通过kmeans等算法根据自己的数据集得出合适的锚框． 这里需要注意:yolov5内部实现了锚框的自动计算训练过程默认使用自适应锚框计算.
经过实际测试，自己通过kmeans算法得到的锚框在特定数据集上能取得更好的性能
在3.执行训练中将提到禁止自动锚框计算的方法.
数据准备 参考官方指南的
Create Labels
Organize Directories
部分的数据要求
注意标注格式是class x_center y_center width height,其中x_center y_center width height均是根据图像尺寸归一化的0到1之间的数值.
3. 执行训练 python ~/src_repo/yolov5/train.py --batch 16 --epochs 10 --data ~/src_repo/rat.yaml --cfg ~/src_repo/yolov5/models/yolov5s." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/313b5d2abf2634add158dd2f88776353/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-01-09T21:37:05+08:00" />
<meta property="article:modified_time" content="2021-01-09T21:37:05+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">C&#43;&#43;实现yolov5的OpenVINO部署</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align: center">点击上方“计算机视觉工坊”，选择“星标”</p> 
 <p style="text-align: center">干货第一时间送达</p> 
 <p style="text-align: center"><img src="https://images2.imgbox.com/37/a8/DeacP1Zs_o.png" width="100%"></p> 
 <blockquote> 
  <p>[GiantPandaCV导语] <strong>本文介绍了一种使用c++实现的,使用OpenVINO部署yolov5的方法。此方法在2020年9月结束的极市开发者榜单中取得后厨老鼠识别赛题第四名。2020年12月,注意到yolov5有了许多变化,对部署流程重新进行了测试,并进行了整理。希望能给需要的朋友一些参考,节省一些踩坑的时间</strong>。</p> 
 </blockquote> 
 <h3><strong>模型训练</strong></h3> 
 <h4>1. 首先获取yolov5工程</h4> 
 <pre class="has"><code class="language-php">git clone https://github.com/ultralytics/yolov5.git

</code></pre> 
 <p>本文编辑的时间是2020年12月3日,官方最新的releases是v3.1,在v3.0的版本中,官网有如下的声明</p> 
 <blockquote> 
  <ul><li><p style="text-align: left">August 13, 2020**: v3.0 release(<code>https://github.com/ultralytics/yolov5/releases/tag/v3.0</code>): nn.Hardswish() activations, data autodownload, native AMP.</p></li></ul> 
 </blockquote> 
 <p>yolov5训练获得的原始的模型以.pt文件方式存储,要转换为OpenVINO的.xml和.bin的模型存储方式,需要经历两次转换.</p> 
 <p>两次转换所用到的工具无法同时支持nn.Hardswish()函数的转换,v3.0版本时需要切换到v2.0版本替换掉nn.Hardswish()函数才能够完成两次模型转换,当时要完成模型转换非常的麻烦.</p> 
 <p>在v3.1版本的yolov5中用于进行pt模型转onnx模型的程序对nn.Hardswish()进行了兼容,模型转换过程大为化简.</p> 
 <h4>2. 训练准备</h4> 
 <p style="text-align: left">yolov5官方的指南: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data</p> 
 <h5>描述信息准备</h5> 
 <p>在yolov5的文件夹下/yolov5/models/目录下可以找到以下文件</p> 
 <blockquote> 
  <p>yolov5s.yaml</p> 
  <p>yolov5m.yaml</p> 
  <p>yolov5l.yaml</p> 
 </blockquote> 
 <p>这三个文件分别对应s(小尺寸模型),ｍ(中尺寸模型)和l(大尺寸模型)的结构描述信息</p> 
 <p>其中为了实现自己的训练常常需要更改以下两个参数</p> 
 <ul><li><p>nc</p><p>需要识别的类别数量,yolov5原始的默认类别数量为80</p></li><li><p>anchors</p><p>通过kmeans等算法根据自己的数据集得出合适的锚框． 这里需要注意:yolov5内部实现了锚框的自动计算训练过程默认使用自适应锚框计算.</p><p>经过实际测试，自己通过kmeans算法得到的锚框在特定数据集上能取得更好的性能</p><p>在3.执行训练中将提到禁止自动锚框计算的方法.</p></li></ul> 
 <h5><strong>数据准备</strong></h5> 
 <p>参考官方指南的</p> 
 <ul><li><p style="text-align: left">Create Labels</p></li><li><p style="text-align: left">Organize Directories</p></li></ul> 
 <p>部分的数据要求</p> 
 <p>注意标注格式是class x_center y_center width height,其中x_center y_center width height均是根据图像尺寸归一化的0到1之间的数值.</p> 
 <h4>3. 执行训练</h4> 
 <pre class="has"><code class="language-php">python ~/src_repo/yolov5/train.py --batch 16 --epochs 10 --data ~/src_repo/rat.yaml --cfg ~/src_repo/yolov5/models/yolov5s.yaml --weights ""

</code></pre> 
 <p>其中</p> 
 <ul><li><p style="text-align: left">--data　参数后面需要填充的是训练数据的说明文件．其中需要说明训练集，测试集，种类数目和种类名称等信息，具体格式可以参考yolov5/data/coco.yaml.</p></li><li><p style="text-align: left">--cfg　为在训练准备阶段完成的模型结构描述文件.</p></li><li><p style="text-align: left">--weights　后面跟预训练模型的路径,如果是""则重新训练一个模型.推荐使用预训练模型继续训练,不使用该参数则默认使用预训练模型.</p></li><li><p style="text-align: left">--noautoanchor　该参数可选，使用该参数则禁止自适应anchor计算，使用--cfg文件中提供的原始锚框.</p></li></ul> 
 <h3><strong>模型转换</strong></h3> 
 <p>经过训练,模型的原始存储格式为.pt格式，为了实现OpenVINO部署，需要首先转换为.onnx的存储格式，之后再转化为OpenVINO需要的.xml和.bin的存储格式.</p> 
 <h4>1. pt格式转onnx格式</h4> 
 <p style="text-align: left">这一步的转换主要由yolov5/models/export.py脚本实现.</p> 
 <p style="text-align: left">可以参考yolov5提供的简单教程:https://github.com/ultralytics/yolov5/issues/251</p> 
 <p style="text-align: left">使用该教程中的方法可以获取onnx模型,但直接按照官方方式获取的onnx模型其中存在OpenVINO模型转换中不支持的运算,因此,使用该脚本之前需要进行一些更改:</p> 
 <ul><li><p style="text-align: left">opset_version</p></li></ul> 
 <p style="text-align: left">在/yolov5/models/export.py中</p> 
 <pre class="has"><code class="language-php">torch.onnx.export(model, img, f, verbose=False, opset_version=12, input_names=['images'],
                          output_names=['classes', 'boxes'] if y is None else ['output'])

</code></pre> 
 <p>opset_version=12,将导致后面的OpenVINO模型装换时遇到未支持的运算 因此设置为opset_version=10.</p> 
 <ul><li><p style="text-align: left">Detect layer export</p></li></ul> 
 <pre class="has"><code class="language-php">model.model[-1].export = True  

</code></pre> 
 <p>设置为True则Detect层(包含nms,锚框计算等)不会输出到模型中.</p> 
 <p>设置为False包含Detect层的模型无法通过onnx到OpenVINO格式模型的转换.</p> 
 <p>需要执行如下指令:</p> 
 <pre class="has"><code class="language-php">python ./models/export.py --weight .pt文件路径 --img 640 --batch 1

</code></pre> 
 <p>需要注意的是在填入的.pt文件路径不存在时,该程序会自动下载官方预训练的模型作为转换的原始模型,转换完成则获得onnx格式的模型.</p> 
 <p>转换完成后可以使用Netron:https://github.com/lutzroeder/netron.git 进行可视化.对于陌生的模型,该可视化工具对模型结构的认识有很大的帮助.</p> 
 <p style="text-align: left"><img src="https://images2.imgbox.com/4e/f0/LlmB974L_o.png"></p> 
 <figcaption> 
  net.jpg 
 </figcaption> 
 <h4>2. onnx格式转换OpenVINO的xml和bin格式</h4> 
 <p>OpenVINO是一个功能丰富的跨平台边缘加速工具箱,本文用到了其中的模型优化工具和推理引擎两部分内容.</p> 
 <p style="text-align: left">OpenVINO的安装配置可以参考https://docs.openvinotoolkit.org/2019_R2/_docs_install_guides_installing_openvino_linux.html ,本文的所有实现基于2020.4版本,为确保可用,建议下载2020.4版本的OpenVINO.</p> 
 <p>安装完成后在~/.bashrc文件中添加如下内容,用于在终端启动时配置环境变量.</p> 
 <pre class="has"><code class="language-php">source /opt/intel/openvino/bin/setupvars.sh
source /opt/intel/openvino/opencv/setupvars.sh

</code></pre> 
 <p>安装完成后运行如下脚本实现onnx模型到xml bin模型的转换.</p> 
 <pre class="has"><code class="language-php">python /opt/intel/openvino/deployment_tools/model_optimizer/mo_onnx.py --input_model .onnx文件路径  --output_dir 期望模型输出的路径

</code></pre> 
 <p>运行成功之后会获得.xml和.bin文件,xml和bin是OpenVINO中的模型存储方式,后续将基于bin和xml文件进行部署.该模型转换工具还有定点化等模型优化功能,有兴趣可以自己试试.</p> 
 <h3><strong>使用OpenVINO进行推理部署</strong></h3> 
 <p style="text-align: left">OpenVINO除了模型优化工具外,还提供了一套运行时推理引擎.</p> 
 <p style="text-align: left">想使用OpenVINO的模型进行推理部署,有两种方式,第一种方式是使用OpenVINO原生的sdk,另外一种方式是使用支持OpenVINO的opencv(比如OpenVINO自带的opencv)进行部署,本文对原生sdk的部署方式进行介绍.</p> 
 <p style="text-align: left">OpenVINO提供了相对丰富的例程,本文中实现的yolov5的部署参考了/opt/intel/openvino/deployment_tools/inference_engine/demos/object_detection_demo_yolov3_async文件夹中yolov3的实现方式.</p> 
 <h4>1. 推理引擎的初始化</h4> 
 <p>首先需要进行推理引擎的初始化,此部分代码封装在detector.cpp的init函数.</p> 
 <p>主要流程如下:</p> 
 <pre class="has"><code class="language-php">Core ie;
//读入xml文件,该函数会在xml文件的目录下自动读取相应的bin文件,无需手动指定
auto cnnNetwork = ie.ReadNetwork(_xml_path); 
//从模型中获取输入数据的格式信息
InputsDataMap inputInfo(cnnNetwork.getInputsInfo());
InputInfo::Ptr&amp; input = inputInfo.begin()-&gt;second;
_input_name = inputInfo.begin()-&gt;first;
input-&gt;setPrecision(Precision::FP32);
input-&gt;getInputData()-&gt;setLayout(Layout::NCHW);
ICNNNetwork::InputShapes inputShapes = cnnNetwork.getInputShapes();
SizeVector&amp; inSizeVector = inputShapes.begin()-&gt;second;
cnnNetwork.reshape(inputShapes);
//从模型中获取推断结果的格式
_outputinfo = OutputsDataMap(cnnNetwork.getOutputsInfo());
for (auto &amp;output : _outputinfo) {
    output.second-&gt;setPrecision(Precision::FP32);
}
//获取可执行网络,这里的CPU指的是推断运行的器件,可选的还有"GPU",这里的GPU指的是intel芯片内部的核显
//配置好核显所需的GPU运行环境,使用GPU模式进行的推理速度上有很大提升,这里先拿CPU部署后面会提到GPU环境的配置方式
_network =  ie.LoadNetwork(cnnNetwork, "CPU");

</code></pre> 
 <h4>2. 数据准备</h4> 
 <p>为了适配网络的输入数据格式要求,需要对原始的opencv读取的Mat数据进行预处理.</p> 
 <ul><li><p style="text-align: left">resize</p></li></ul> 
 <p>最简单的方式是将输入图像直接resize到640*640尺寸,此种方式会造成部分物体失真变形,识别准确率会受到部分影响,简单起见,在demo代码里使用了该方式.</p> 
 <p>在竞赛代码中,为了追求正确率,图像缩放的时候需要按图像原始比例将图像的长或宽缩放到640.假设长被放大到640,宽按照长的变换比例无法达到640,则在图像的两边填充黑边确保输入图像总尺寸为640*640.竞赛代码中使用了该种缩放方式,需要注意的是如果使用该种缩放方式,在获取结果时需要将结果转换为在原始图像中的坐标.</p> 
 <ul><li><p style="text-align: left">颜色通道转换</p></li></ul> 
 <p>鉴于opencv和pytorch的颜色通道差异,opencv是BGR通道,pytorch是RGB,在输入网络之前,需要进行通道转换.</p> 
 <ul><li><p style="text-align: left">推断请求和blob填充</p></li></ul> 
 <pre class="has"><code class="language-php">InferRequest::Ptr infer_request = _network.CreateInferRequestPtr();
Blob::Ptr frameBlob = infer_request-&gt;GetBlob(_input_name);
InferenceEngine::LockedMemory&lt;void&gt; blobMapped = InferenceEngine::as&lt;InferenceEngine::MemoryBlob&gt;(frameBlob)-&gt;wmap();
float* blob_data = blobMapped.as&lt;float*&gt;();
//nchw
for(size_t row =0;row&lt;640;row++){
    for(size_t col=0;col&lt;640;col++){
        for(size_t ch =0;ch&lt;3;ch++){
            //将图像转换为浮点型填入模型
            blob_data[img_size*ch + row*640 + col] = float(inframe.at&lt;Vec3b&gt;(row,col)[ch])/255.0f;
        }
    }
}


</code></pre> 
 <h4>3. 推断执行与解析</h4> 
 <ul><li><p style="text-align: left">推断执行</p></li></ul> 
 <pre class="has"><code class="language-php">infer_request-&gt;Infer();


</code></pre> 
 <ul><li><p style="text-align: left">获取推断结果</p></li></ul> 
 <p>从Netron的可视化结果可知</p> 
 <p><img src="https://images2.imgbox.com/e5/8b/jNwOR9vp_o.png"></p> 
 <figcaption> 
  output.png 
 </figcaption> 
 <p>网络只包含到输出三个检测头的部分，三个检测头分别对应80,40,和20的栅格尺寸,因此需要对三种尺寸的检测头输出结果依次解析,具体的解析过程在parse_yolov5函数中进行了实现:</p> 
 <pre class="has"><code class="language-php">//获取各层结果
vector&lt;Rect&gt; origin_rect;                     //保存原始的框信息
vector&lt;float&gt; origin_rect_cof;            //保存框对应的置信度信息
int s[3] = {80,40,20};
int i=0;
for (auto &amp;output : _outputinfo) {
    auto output_name = output.first;
    Blob::Ptr blob = infer_request-&gt;GetBlob(output_name);
    parse_yolov5(blob,s[i],_cof_threshold,origin_rect,origin_rect_cof);
    ++i;
}


</code></pre> 
 <ul><li><p style="text-align: left">对检测头的内容进行解析</p></li></ul> 
 <p>这部分主要是使用c++将yolov5代码中的detect层内容重新实现一下,主要代码实现如下:</p> 
 <pre class="has"><code class="language-php">//注意此处的阈值是框和物体prob乘积的阈值
bool Detector::parse_yolov5(const Blob::Ptr &amp;blob,int net_grid,float cof_threshold,
    vector&lt;Rect&gt;&amp; o_rect,vector&lt;float&gt;&amp; o_rect_cof){
    vector&lt;int&gt; anchors = get_anchors(net_grid);
    LockedMemory&lt;const void&gt; blobMapped = as&lt;MemoryBlob&gt;(blob)-&gt;rmap();
    const float *output_blob = blobMapped.as&lt;float *&gt;();
    //80个类是85,一个类是6,n个类是n+5
    //int item_size = 6;
    int item_size = 85;
    size_t anchor_n = 3;
    for(int n=0;n&lt;anchor_n;++n)
        for(int i=0;i&lt;net_grid;++i)
            for(int j=0;j&lt;net_grid;++j)
            {
                double box_prob = output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j *item_size+ 4];
                box_prob = sigmoid(box_prob);
                //框置信度不满足则整体置信度不满足
                if(box_prob &lt; cof_threshold)
                    continue;
                
                //注意此处输出为中心点坐标,需要转化为角点坐标
                double x = output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j*item_size + 0];
                double y = output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j*item_size + 1];
                double w = output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j*item_size + 2];
                double h = output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j *item_size+ 3];
               
                double max_prob = 0;
                int idx=0;
                for(int t=5;t&lt;85;++t){
                    double tp= output_blob[n*net_grid*net_grid*item_size + i*net_grid*item_size + j *item_size+ t];
                    tp = sigmoid(tp);
                    if(tp &gt; max_prob){
                        max_prob = tp;
                        idx = t;
                    }
                }
                float cof = box_prob * max_prob;                
                //对于边框置信度小于阈值的边框,不关心其他数值,不进行计算减少计算量
                if(cof &lt; cof_threshold)
                    continue;

                x = (sigmoid(x)*2 - 0.5 + j)*640.0f/net_grid;
                y = (sigmoid(y)*2 - 0.5 + i)*640.0f/net_grid;
                w = pow(sigmoid(w)*2,2) * anchors[n*2];
                h = pow(sigmoid(h)*2,2) * anchors[n*2 + 1];

                double r_x = x - w/2;
                double r_y = y - h/2;
                Rect rect = Rect(round(r_x),round(r_y),round(w),round(h));
                o_rect.push_back(rect);
                o_rect_cof.push_back(cof);
            }
    if(o_rect.size() == 0) return false;
    else return true;
}


</code></pre> 
 <p>这一部分最艰难的是搞清楚输出数据的排列方式,一开始我也试了很多次,最后才得到了正确的输出.</p> 
 <p>需要注意的一点是,按照输出排列方式读取的数值不是最终我们需要的结果,需要进行一些计算来进行转换,</p> 
 <p>转换的依据可以参考yolov5/models/yolo.py中forward函数的实现.</p> 
 <p>注意这里有一个参数cof_threshold,其计算方式是框置信度乘以物品置信度,如果识别效果不佳,则需要对该数值进行调整.</p> 
 <ul><li><p style="text-align: left">NMS获取最终结果</p></li></ul> 
 <p>经过以上步骤,原始的框信息存储在origin_rect变量中,还需要通过NMS去除同一个物体多余的框.</p> 
 <p>OpenVNIO自带的opencv提供了NMS的一种实现,因而直接进行调用.</p> 
 <pre class="has"><code class="language-php"> vector&lt;int&gt; final_id;
    dnn::NMSBoxes(origin_rect,origin_rect_cof,_cof_threshold,_nms_area_threshold,final_id);
    //根据final_id获取最终结果
    for(int i=0;i&lt;final_id.size();++i){
        Rect resize_rect= origin_rect[final_id[i]];
        detected_objects.push_back(Object{
            origin_rect_cof[final_id[i]],
            "",resize_rect
        });
    }


</code></pre> 
 <p>其中origin_rect为原始矩形,origin_rect_cof为矩形对应的置信度,_cof_threshold为置信度(框置信度乘以物品置信度)阈值,_nms_area_threshold是重叠百分比多少则算为一个物体的阈值,final_id为目标矩形在origin_rect中的下标.</p> 
 <h4>4. 性能测试</h4> 
 <p>计时实现如下:</p> 
 <pre class="has"><code class="language-php">auto start = chrono::high_resolution_clock::now();
auto end = chrono::high_resolution_clock::now();
std::chrono::duration&lt;double&gt; diff = end - start;
cout&lt;&lt;"use "&lt;&lt;diff.count()&lt;&lt;" s" &lt;&lt; endl;


</code></pre> 
 <p>原始的未经优化的CPU运行的yolov5,推理时间在240ms左右,测试平台为intel corei7 6700hq.</p> 
 <p>检测结果如下:</p> 
 <p><img src="https://images2.imgbox.com/4b/6b/WHepqFTv_o.png"></p> 
 <figcaption> 
  检测结果 
 </figcaption> 
 <h3><strong>推理加速</strong></h3> 
 <ul><li><p style="text-align: left">使用核显GPU进行计算</p></li></ul> 
 <p>将</p> 
 <pre class="has"><code class="language-php">_network =  ie.LoadNetwork(cnnNetwork, "CPU");


</code></pre> 
 <p>改为</p> 
 <pre class="has"><code class="language-php">_network =  ie.LoadNetwork(cnnNetwork, "GPU");


</code></pre> 
 <p>如果OpenVINO环境配置设置无误程序应该可以直接运行.</p> 
 <p>检测环境是否配置无误的方法是运行:</p> 
 <p>/opt/intel/openvino/deployment_tools/demo中的./demo_security_barrier_camera.sh</p> 
 <p>若成功运行则cpu环境正常.</p> 
 <p>./demo_security_barrier_camera.sh -d GPU 运行正常则gpu环境运行正常.</p> 
 <ul><li><p style="text-align: left">使用openmp进行并行化</p></li></ul> 
 <p>在推理之外的数据预处理和解析中存在大量循环,这些循环都可以利用openmp进行并行优化.</p> 
 <ul><li><p style="text-align: left">模型优化如定点化为int8类型</p></li></ul> 
 <p>在模型转换时通过设置参数可以实现模型的定点化.</p> 
 <h3><strong>git项目使用</strong></h3> 
 <p style="text-align: left">项目地址:https://github.com/fb029ed/yolov5_cpp_openvino</p> 
 <ul><li><p style="text-align: left">demo部分完成了yolov5原始模型的部署</p></li></ul> 
 <p style="text-align: left">使用方法为依次执行</p> 
 <pre class="has"><code class="language-php">cd ./demo
mkdir build 
cd build
cmake ..
make 
./detect_test


</code></pre> 
 <ul><li><p style="text-align: left">cvmart_competition部分为开发者榜单竞赛的参赛代码,不能直接运行仅供参考</p></li></ul> 
 <p style="text-align: left">本文仅做学术分享，如有侵权，请联系删文。</p> 
 <p style="text-align: left"><strong>下载1</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>深度学习</strong>，即可下载深度学习算法、3D深度学习、深度学习框架、目标检测、GAN等相关内容近30本pdf书籍。</p> 
 <p style="text-align: left"><strong>下载2</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>计算机视觉</strong>，即可下载计算机视觉相关17本pdf书籍，包含计算机视觉算法、Python视觉实战、Opencv3.0学习等。</p> 
 <p style="text-align: left"><strong>下载3</strong></p> 
 <p style="text-align: left">在「计算机视觉工坊」公众号后台回复：<strong>SLAM</strong>，即可下载独家SLAM相关视频课程，包含视觉SLAM、激光SLAM精品课程。</p> 
 <h3></h3><p style="text-align: center"><strong>重磅！计算机视觉工坊</strong><strong>-学习</strong><strong>交流群</strong><strong>已成立</strong></p><p style="text-align: left">扫码添加小助手微信，可申请加入3D视觉工坊-学术论文写作与投稿 微信交流群，旨在<strong>交流顶会、顶刊、SCI、EI等写作与投稿事宜。</strong></p><p style="text-align: left"><strong>同时</strong>也可申请加入我们的细分方向交流群，目前主要有<strong>ORB-SLAM系列源码学习、</strong><strong>3D视觉</strong>、<strong>CV&amp;深度学习</strong>、<strong>SLAM</strong>、<strong>三维重建</strong>、<strong>点云后处理</strong>、<strong>自动驾驶、CV入门、三维测量、VR/AR、3D人脸识别、医疗影像、缺陷检测、行人重识别、目标跟踪、视觉产品落地、视觉竞赛、车牌识别、硬件选型、深度估计、<strong>学术交流、</strong>求职交流</strong>等微信群，请扫描下面微信号加群，备注：”研究方向+学校/公司+昵称“，例如：”3D视觉 + 上海交大 + 静静“。请按照格式备注，否则不予通过。添加成功后会根据研究方向邀请进去相关微信群。<strong>原创投稿</strong>也请联系。</p><p><img src="https://images2.imgbox.com/cb/5a/IwyrnD1s_o.png" title="3D视觉工坊小助理微信.jpg.jpg"></p><p>▲长按加微信群或投稿</p><p><img src="https://images2.imgbox.com/00/02/xHdKRDOt_o.png"></p><p>▲长按关注公众号</p> 
 <h3></h3><p style="text-align: left"><strong>3D视觉从入门到精通知识星球</strong>：针对3D视觉领域的<strong>知识点汇总、入门进阶学习路线、最新paper分享、疑问解答</strong>四个方面进行深耕，更有各类大厂的算法工程人员进行技术指导。与此同时，星球将联合知名企业发布3D视觉相关算法开发岗位以及项目对接信息，打造成集技术与就业为一体的铁杆粉丝聚集区，近2000星球成员为创造更好的AI世界共同进步，知识星球入口：</p><p style="text-align: center">学习3D视觉核心技术，扫描查看介绍，3天内无条件退款</p> 
 <h3></h3><p style="text-align: center"><img src="https://images2.imgbox.com/a5/42/NdmuT2fK_o.png"></p><p style="text-align: center"> 圈里有高质量教程资料、可答疑解惑、助你高效解决问题</p><p style="text-align: right"><strong>觉得有用，麻烦给个赞和在看~</strong><strong>  </strong><strong><img src="https://images2.imgbox.com/91/9d/pIu9nemy_o.gif"></strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e2b563c3f648a53c1ad2bd597e19324b/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何在电脑上安装多个版本的CUDA</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/43c3f7dc8fda7f3633bd12ba7521c348/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">nlp-知识图谱简介</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>