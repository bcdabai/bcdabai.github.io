<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Pytorch:cifar10官方例程代码复盘，详解 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Pytorch:cifar10官方例程代码复盘，详解" />
<meta property="og:description" content="笔者暑假跑了一个简单的cifar10分类来进行深度学习入门，当时不求甚解，很多地方没有理解，于是最近就将全部代码仔细复盘了一下，期间踩了些坑，同时写下自己的理解，不正确之处还请大佬斧正。
先附上官方例程的代码。
https://pytorch.org/tutorials/
以下是我的代码，对官方例程进行了一些修改，在代码中每一句都有标注。
import torch from torch import nn from torchvision import models import torchvision import torchvision.transforms as transforms import torch.utils.data as Data import torch.optim as optim import torch.nn.functional as F device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else&#34;cpu&#34;) #调用cuda，两种方式，一种是张量.cuda()，一种是张量.to(device) transform = transforms.Compose( #数据预处理，张量化与归一化，在torchvision.datasets时调用 [transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))] ) #下载与载入torchvision中提供的cifar10数据集，root为数据集下载路径，可以使用相对路径也可以使用绝对路径 dataset = torchvision.datasets.CIFAR10(root=&#39;/home/ericzhao/下载&#39;,train=True,transform= transform,target_transform=None,download=True) trainloader = Data.DataLoader(dataset,batch_size=4,shuffle=False,num_workers=2) #训练集的载入，batch_size对训练效果有影响，后面再讨论 testset = torchvision.datasets.CIFAR10(root = &#39;/home/ericzhao/下载&#39;,train=False,transform = transform,download=True) testloader = Data.DataLoader(testset,batch_size = 4,shuffle=False,num_workers=2 ) #因为cifar10是一个十分类数据集，所以定义它的十个类别 classes = (&#39;plane&#39;,&#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) #自定义一个网络，继承父类nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/eb228d6fcc737bf1152570af54e893d0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-20T10:54:13+08:00" />
<meta property="article:modified_time" content="2020-09-20T10:54:13+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch:cifar10官方例程代码复盘，详解</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>笔者暑假跑了一个简单的cifar10分类来进行深度学习入门，当时不求甚解，很多地方没有理解，于是最近就将全部代码仔细复盘了一下，期间踩了些坑，同时写下自己的理解，不正确之处还请大佬斧正。<br> 　　先附上官方例程的代码。<br> 　　https://pytorch.org/tutorials/<br> 以下是我的代码，对官方例程进行了一些修改，在代码中每一句都有标注。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> models
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda:0"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>  
<span class="token comment">#调用cuda，两种方式，一种是张量.cuda()，一种是张量.to(device)</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>      <span class="token comment">#数据预处理，张量化与归一化，在torchvision.datasets时调用</span>
   <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment">#下载与载入torchvision中提供的cifar10数据集，root为数据集下载路径，可以使用相对路径也可以使用绝对路径</span>
dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'/home/ericzhao/下载'</span><span class="token punctuation">,</span>train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>transform<span class="token operator">=</span> transform<span class="token punctuation">,</span>target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
trainloader <span class="token operator">=</span>  Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment">#训练集的载入，batch_size对训练效果有影响，后面再讨论</span>
testset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root <span class="token operator">=</span> <span class="token string">'/home/ericzhao/下载'</span><span class="token punctuation">,</span>train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>transform <span class="token operator">=</span> transform<span class="token punctuation">,</span>download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
testloader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>num_workers<span class="token operator">=</span><span class="token number">2</span> <span class="token punctuation">)</span>

<span class="token comment">#因为cifar10是一个十分类数据集，所以定义它的十个类别</span>
classes <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'plane'</span><span class="token punctuation">,</span><span class="token string">'car'</span><span class="token punctuation">,</span> <span class="token string">'bird'</span><span class="token punctuation">,</span> <span class="token string">'cat'</span><span class="token punctuation">,</span>
<span class="token string">'deer'</span><span class="token punctuation">,</span> <span class="token string">'dog'</span><span class="token punctuation">,</span> <span class="token string">'frog'</span><span class="token punctuation">,</span> <span class="token string">'horse'</span><span class="token punctuation">,</span> <span class="token string">'ship'</span><span class="token punctuation">,</span> <span class="token string">'truck'</span><span class="token punctuation">)</span>

<span class="token comment">#自定义一个网络，继承父类nn.Module</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
       <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment">#继承父类的构造函数</span>
       self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment">#由于输入是一个32*32*3的图像，所以通道数为3。卷积核数为6，卷积核大小我改成了3*3</span>
<span class="token comment">#经过第一个卷积，此时输入为6*30*30</span>
       self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment">#池化层，下采样，步长为2,2*2进行采样，所以此时输出为6*15*15</span>
       self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment">#卷积层，输出为16*13*13</span>
        

       self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span> <span class="token number">6</span> <span class="token operator">*</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>   <span class="token comment">#全连接层，输入为16*6*6，输出为120</span>
       self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>      <span class="token comment">#全连接层，输入120，输出84</span>
       self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>     <span class="token comment">#全连接层，因为是10分类，所以最后输出10</span>

   <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>       <span class="token comment">#定义前向函数，就是层与层之间的连接关系</span>
       x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># --&gt;卷积--&gt;激活--&gt;池化，x=15*15*6</span>
       x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment">#--&gt;卷积--&gt;激活--&gt;池化，x=  6*6*16 ，下采样，13/2=6.5，取6</span>
       x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token operator">*</span> <span class="token number">6</span><span class="token operator">*</span> <span class="token number">6</span><span class="token punctuation">)</span>     <span class="token comment">#将输入拉成16*6*6列，因此为1行的二维张量</span>
       x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment">#全连接层1+激活</span>
       x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>       <span class="token comment">#全连接层2+激活</span>
       x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                    <span class="token comment">#全连接层3</span>
       <span class="token keyword">return</span> x                          <span class="token comment">#返回输出</span>

resnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment">#调用自定义Net类</span>
resnet<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment">#调用cuda</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#调用loss函数</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>resnet<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.003</span><span class="token punctuation">,</span>momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>  <span class="token comment">#调用优化器</span>

<span class="token comment">#训练部分</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

   running_loss <span class="token operator">=</span> <span class="token number">0.0</span>   <span class="token comment">#先给loss置0</span>
   <span class="token keyword">for</span> i<span class="token punctuation">,</span>data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment">#trainloader的数据下标给i，数据赋给data</span>

       inputs<span class="token punctuation">,</span>labels <span class="token operator">=</span> data   <span class="token comment">#cifar10的数据集带有标签，所以数据给inputs，标签值给labels，由于batch_size=4，所以inputs 的torch.size =([4,3,32,32])</span>

       inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
       labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment">#调用GPU</span>
       optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#优化器参数清0</span>

       outputs <span class="token operator">=</span> resnet<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  <span class="token comment">#将图像数据输入resnet网络中，得到输出，由上面得知，此时outputs维度为[4,10]</span>
       loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>  <span class="token comment">#将输出和标签值输入loss计算函数，计算loss</span>
      <span class="token comment"># 关于nn.CrossEntropyLoss()计算公式：https://blog.csdn.net/geter_CS/article/details/84857220</span>
       loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment">#反向传播</span>
       optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment">#参数优化</span>

       running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
       <span class="token keyword">if</span> i<span class="token operator">%</span><span class="token number">200</span> <span class="token operator">==</span><span class="token number">199</span><span class="token punctuation">:</span>                       <span class="token comment">#每200次输出一下平均loss</span>
           <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'[%d,%5d] loss: %.3f'</span><span class="token operator">%</span>
                     <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span>i <span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>running_loss<span class="token operator">/</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
           running_loss <span class="token operator">=</span> <span class="token number">0.0</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span>


<span class="token comment">#测试部分</span>
correct <span class="token operator">=</span> <span class="token number">0</span>
total <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment">#torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。</span>
   <span class="token keyword">for</span> data <span class="token keyword">in</span> testloader<span class="token punctuation">:</span>             <span class="token comment">#将测试数据集的图像和标签分别赋值给imgaes和labels</span>
       images<span class="token punctuation">,</span> labels <span class="token operator">=</span> data
       images<span class="token punctuation">,</span>labels <span class="token operator">=</span> images<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
       outputs <span class="token operator">=</span> resnet<span class="token punctuation">(</span>images<span class="token punctuation">)</span>     <span class="token comment">#outputs的size，[4,16*6*6]</span>



       _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment">#把输出张量里的每一行的最大值筛出来，赋给predicted</span>
       total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>               <span class="token comment">#计算输入的总数量</span>

       correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment">#判断相等，相等求和</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy of the network on the 10000 test images: %d %%'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>
           <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>接下来说说踩的坑<br> 一开始，为了图省事，我用的是 torchivision.models.resnet50，就是预加载模型。代码很简单:</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torchvision
net <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>models<span class="token punctuation">.</span>resnet50<span class="token punctuation">(</span>pretained <span class="token operator">=</span> true<span class="token punctuation">)</span>
</code></pre> 
<p>但是在加载了预训练模型后，我本以为的极好的训练效果并没有出现，与之相反，收敛速度很慢，训练速度也很慢，2个epoch结束，loss=2，acc只有20不到。于是我又换回了官方例程里的简单两层卷积网络，跑了两个epoch，acc达到54%。<br> 我一开始的理解，resnet模型过深，cifar10的输入图像分辨率只有32*32，在几次maxpool采样后，跑到后面，一个像素点可能都没有了，所以效果不好。<br> 但是我在后来调整了batch_size为128后，收敛速度变快，训练速度变快，采用resenet50网络预训练模型后，2个epoch，acc到了76，而官方例程里的代码，由于只有2个卷积层深度，所以batch_size较小，收敛速度与效果也不错。<br> 结论：一般来说，在合理的范围之内，越大的 batch size 使下降方向越准确，震荡越小；batch size 如果过大，则可能会出现局部最优的情况。小的 bath size 引入的随机性更大，难以达到收敛，极少数情况下可能会效果变好。<br> 目前，用resenet50，优化器采用SGD，调整学习率为0.002，在官方例程的代码基础下，20个epoch，最好ACC能够达到85%。这次主要目的为学习，不追求高acc，所以就到这里了。</p> 
<p>参考文章：<br> 《pytorch损失函数之nn.CrossEntropyLoss()、nn.NLLLoss()》：<br> https://blog.csdn.net/geter_CS/article/details/84857220<br> 《深度学习中Batch size对训练效果的影响》：https://blog.csdn.net/juronghui/article/details/78612653</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/c96eb104e635fe0152ef41eec330bdd5/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">2020氧化工艺实操考试视频及氧化工艺在线考试</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7947b6497b5356ff79010eb0c357ef85/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">开会，就是浪费生命</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>