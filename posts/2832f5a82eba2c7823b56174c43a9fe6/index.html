<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>深度学习中的backbone、head、neck等名词解释 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="深度学习中的backbone、head、neck等名词解释" />
<meta property="og:description" content="backbone： 翻译为主干网络，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。
这些网络经常使用的是resnet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。
head： head是获取网络输出内容的网络，利用之前提取的特征，head利用这些特征，做出预测。
neck: 是放在backbone和head之间的，是为了更好的利用backbone提取的特征。
bottleneck: 瓶颈的意思，通常指的是网网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多，就像脖子一样，变细了。经常设置的参数 bottle_num=256，指的是网络输出的数据的维度是256 ，可是输入进来的可能是1024维度的。
GAP： 在设计的网络中经常能够看到gap这个层，即Global Average Pool全局平均池化，就是将某个通道的特征取平均值，经常使用AdaptativeAvgpoold(1),在pytorch中，这个代表自适应性全局平均池化，即将某个通道的特征取平均值。
self.gap = nn.AdaptiveAvgPool2d(1) Embedding: 深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）
pretext task和downstream task： 用于预训练的任务被称为前置/代理任务(pretext task)，用于微调的任务被称为下游任务(downstream task)
temperature parameters 在论文中经常能看到这个温度参数的身影，那么他都有什么用处呢？比如经常看到下面这样的式子：
里面的beta就是temperature parameter，起到平滑softmax输出结果的作用，举例子如下：
import torch x = torch.tensor([1.0,2.0,3.0]) y = torch.softmax(x,0) print(y) x1 = x / 2 # beta 为2 y = torch.softmax(x1,0) print(y) x2 = x/0.5 # beta 为0.5 y = torch.softmax(x2,0) print(y) 输出结果如下：
tensor([0.0900, 0.2447, 0.6652]) tensor([0.1863, 0.3072, 0.5065]) tensor([0." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2832f5a82eba2c7823b56174c43a9fe6/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-25T18:04:32+08:00" />
<meta property="article:modified_time" content="2022-10-25T18:04:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习中的backbone、head、neck等名词解释</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h4><a id="backbone_0"></a>backbone：</h4> 
<p>翻译为主干网络，大多时候指的是提取特征的网络，其作用就是提取图片中的信息，供后面的网络使用。<br> 这些网络经常使用的是resnet、VGG等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为backbone的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的backbone模型已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。</p> 
<h4><a id="head_4"></a>head：</h4> 
<p>head是获取网络输出内容的网络，利用之前提取的特征，head利用这些特征，做出预测。</p> 
<h4><a id="neck_6"></a>neck:</h4> 
<p>是放在backbone和head之间的，是为了更好的利用backbone提取的特征。</p> 
<h4><a id="bottleneck_8"></a>bottleneck:</h4> 
<p>瓶颈的意思，通常指的是网网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多，就像脖子一样，变细了。经常设置的参数 bottle_num=256，指的是网络输出的数据的维度是256 ，可是输入进来的可能是1024维度的。</p> 
<h4><a id="GAP_10"></a>GAP：</h4> 
<p>在设计的网络中经常能够看到gap这个层，即Global Average Pool全局平均池化，就是将某个通道的特征取平均值，经常使用AdaptativeAvgpoold(1),在pytorch中，这个代表自适应性全局平均池化，即将某个通道的特征取平均值。</p> 
<pre><code class="prism language-python"> self<span class="token punctuation">.</span>gap <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<h4><a id="Embedding_16"></a>Embedding:</h4> 
<p>深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”（vector），这一过程一般也称为“嵌入”（embedding）</p> 
<h4><a id="pretext_taskdownstream_task_18"></a>pretext task和downstream task：</h4> 
<p>用于预训练的任务被称为前置/代理任务(pretext task)，用于微调的任务被称为下游任务(downstream task)</p> 
<h4><a id="temperature_parameters_22"></a>temperature parameters</h4> 
<p>在论文中经常能看到这个温度参数的身影，那么他都有什么用处呢？比如经常看到下面这样的式子：<br> <img src="https://images2.imgbox.com/6a/b4/c3epLwnb_o.png" alt="在这里插入图片描述"><br> 里面的beta就是temperature parameter，起到平滑softmax输出结果的作用，举例子如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
 
x1 <span class="token operator">=</span> x <span class="token operator">/</span> <span class="token number">2</span>  <span class="token comment"># beta 为2</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x1<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
 
x2 <span class="token operator">=</span> x<span class="token operator">/</span><span class="token number">0.5</span>  <span class="token comment"># beta 为0.5</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x2<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>

</code></pre> 
<p>输出结果如下：</p> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0900</span><span class="token punctuation">,</span> <span class="token number">0.2447</span><span class="token punctuation">,</span> <span class="token number">0.6652</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.1863</span><span class="token punctuation">,</span> <span class="token number">0.3072</span><span class="token punctuation">,</span> <span class="token number">0.5065</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0159</span><span class="token punctuation">,</span> <span class="token number">0.1173</span><span class="token punctuation">,</span> <span class="token number">0.8668</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>当beta&gt;1的时候，可以将输出结果变得平滑，当beta&lt;1的时候，可以让输出结果变得差异更大一下，更尖锐一些。如果beta比较大，则分类的crossentropy损失会很大，可以在不同的迭代次数里，使用不同的beta数值，有点类似于学习率的效果。</p> 
<h4><a id="Warm_up_52"></a>Warm up</h4> 
<p>Warm up指的是用一个小的学习率先训练几个epoch，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。</p> 
<h4><a id="end_to_end_54"></a>end to end</h4> 
<p>在论文中经常能遇到end to end这样的描述，那么到底什么是端到端呢？其实就是给了一个输入，我们就给出一个输出，不管其中的过程多么复杂，但只要给了一个输入，机会对应一个输出。比如分类问题，你输入了一张图片，肯呢个网络有特征提取，全链接分类，概率计算什么的，但是跳出算法问题，单从结果来看，就是给了一张输入，输出了一个预测结果。End-To-End的方案，即输入一张图，输出最终想要的结果，算法细节和学习过程全部丢给了神经网络。</p> 
<h4><a id="domain_adaptation_domain_generalization__56"></a>domain adaptation 和domain generalization 域适应和域泛化</h4> 
<p>域适应中，常见的设置是源域D_S完全已知，目标域D_T有或无标签。域适应方法试着将源域知识迁移到目标域。第二种场景可以视为domain generalization域泛化。这种更常见因为将模型应用到完全未知的领域，正因为没有见过，所以没有任何模型更新和微调。这种泛化问题就是一种开集问题，由于所需预测类别较多，所以比较头疼 。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/210d30f12cbd994067aabc55da975c69/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">如何清除（登录）缓存</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/8cdfa5b976b80704b64ca5b54705d5b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">使用mybatis-plus批量插入遇到的两个问题记录</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>