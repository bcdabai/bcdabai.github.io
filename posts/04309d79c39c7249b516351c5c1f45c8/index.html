<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>初识GPU编程 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="初识GPU编程" />
<meta property="og:description" content="初识GPU编程 基本概念 CPU和主存被称为主机（Host），GPU和显存（显卡内存）被称为设备（Device），CPU无法直接读显存，GPU无法直接读主存，通过Bus相互通信。
检查是否安装了CUDA
$conda install cudatoolkit 安装 Numba库
$ conda install numba 检查CUDA与Numba是否安装成功
from numba import cuda print(cuda.gpus) cuda会独占一张卡，默认0号，可以使用
CUDA_VISIBLE_DEVICES=&#39;5&#39; python example.py 环境变量设置选择某张卡
GPU程序和CPU程序的区别 CPU程序是顺序执行的，一般需要：
初始化CPU计算得到计算结果 CUDA编程中，GPU流程：
初始化， 将必要数据拷贝到GPU设别的显存CPU调用GPU函数，启动GPU多核同时计算CPU和GPU异步计算将GPU计算结果拷贝回主机，得到计算结果 GPU程序示例
from numba import cuda def cpu_print(): print(&#34;print by cpu.&#34;) @cuda.jit def gpu_print(): # GPU核函数 print(&#34;print by gpu.&#34;) def main(): gpu_print[1, 2]() cuda.synchronize() cpu_print() if __name__ == &#34;__main__&#34;: main() 不同于从传统的Python CPU代码
使用 from numba import cuda 引入cuda库在GPU函数上添加@cuda.jit装饰符，GPU函数又叫核函数主函数调用GPU核函数，添加[1, 2]，告诉GPU以多大的并行粒度同时计算。gpu_print[1, 2]()表示同时开启两个线程并行执行，函数回被执行2次。GPU启动异步：CPU不会等GPU执行完，必要时用cuda.synchronize()等待执行。 Thread层次结构 GPU需要定义执行配置。并行执行2次还是8次。需要明白CUDA的Thread层次结构。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/04309d79c39c7249b516351c5c1f45c8/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-08T15:44:22+08:00" />
<meta property="article:modified_time" content="2022-05-08T15:44:22+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">初识GPU编程</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="GPU_0"></a>初识GPU编程</h2> 
<h3><a id="_2"></a>基本概念</h3> 
<p>CPU和主存被称为主机（Host），GPU和显存（显卡内存）被称为设备（Device），CPU无法直接读显存，GPU无法直接读主存，通过Bus相互通信。</p> 
<p><img src="https://images2.imgbox.com/51/41/eZWLfDgP_o.png" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-ysiCWmT5-1651995724963)(image-20220507193517306.png)]"></p> 
<p>检查是否安装了CUDA</p> 
<pre><code class="prism language-shell"><span class="token variable">$conda</span> <span class="token function">install</span> cudatoolkit
</code></pre> 
<p>安装 Numba库</p> 
<pre><code class="prism language-shell">$ conda <span class="token function">install</span> numba
</code></pre> 
<p>检查CUDA与Numba是否安装成功</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> numba <span class="token keyword">import</span> cuda
<span class="token keyword">print</span><span class="token punctuation">(</span>cuda<span class="token punctuation">.</span>gpus<span class="token punctuation">)</span>
</code></pre> 
<p>cuda会独占一张卡，默认0号，可以使用</p> 
<pre><code class="prism language-python">CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token string">'5'</span> python example<span class="token punctuation">.</span>py
</code></pre> 
<p>环境变量设置选择某张卡</p> 
<h3><a id="GPUCPU_36"></a>GPU程序和CPU程序的区别</h3> 
<p>CPU程序是顺序执行的，一般需要：</p> 
<ol><li>初始化</li><li>CPU计算</li><li>得到计算结果</li></ol> 
<p>CUDA编程中，GPU流程：</p> 
<ol><li>初始化， 将必要数据拷贝到GPU设别的显存</li><li>CPU调用GPU函数，启动GPU多核同时计算</li><li>CPU和GPU异步计算</li><li>将GPU计算结果拷贝回主机，得到计算结果</li></ol> 
<p>GPU程序示例</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> numba <span class="token keyword">import</span> cuda

<span class="token keyword">def</span> <span class="token function">cpu_print</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"print by cpu."</span><span class="token punctuation">)</span>

<span class="token decorator annotation punctuation">@cuda<span class="token punctuation">.</span>jit</span>
<span class="token keyword">def</span> <span class="token function">gpu_print</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># GPU核函数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"print by gpu."</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    gpu_print<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>
    cpu_print<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>不同于从传统的Python CPU代码</p> 
<ul><li>使用 <code>from numba import cuda</code> 引入cuda库</li><li>在GPU函数上添加<code>@cuda.jit</code>装饰符，GPU函数又叫<strong>核函数</strong></li><li>主函数调用GPU核函数，添加<code>[1, 2]</code>，告诉GPU以多大的并行粒度同时计算。<code>gpu_print[1, 2]()</code>表示同时开启两个线程并行执行，函数回被执行2次。</li><li>GPU启动异步：CPU不会等GPU执行完，必要时用<code>cuda.synchronize()</code>等待执行。</li></ul> 
<h3><a id="Thread_80"></a>Thread层次结构</h3> 
<p>GPU需要定义执行配置。并行执行2次还是8次。需要明白CUDA的Thread层次结构。<br> <img src="https://images2.imgbox.com/de/9f/Yhqgdfmu_o.png" alt="在这里插入图片描述"></p> 
<p>CUDA将核函数所定义的运算称为<strong>线程</strong>，多个线程构成一个块，多个块组成网格(Grid)。两个block，每个block有4个thread，代码改为<code>gpu_print[2, 4]()</code></p> 
<p>线程是编程上的软件概念。多个block运行的grid运行在一个GPU显卡上. CUDA提供了一系列内置变量, 以记录Thread和Block的大小及索引下标.以<code>[2, 4]</code>配置为例: <code>blockDim.x</code>变量表示Block的大小是4, 每个block有4个, 即每个Block有4个线程, <code>threadIdx.x</code>变量是从0到<code>blockDim.x - 1</code>的索引下标, 记录这是第几个线程; <code>gridDim.x</code>变量表示Grid的大小是2, 即每个Grid有2个Block.</p> 
<p>问题简化为8个小学生参加本此计算任务, 可以将这8个小学生分为2组, 每组4人. 整个Grid有2个Block, 即<code>gridDim.x</code>为2, 每组有4人, <code>blockDim.x</code>为4. 现在, 让第2个小学生对1和2加和, 如何定义1号学生呢?1 + 0 * blockDim.x</p> 
<p>某个Thread在整个Grid中的位置编号是: <code>threadIdx.x + blockIdx.x * blockDim.x</code></p> 
<pre><code class="prism language-python">!apt<span class="token operator">-</span>get install nvidia<span class="token operator">-</span>cuda<span class="token operator">-</span>toolkit
!pip3 install numba

<span class="token keyword">import</span> os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'NUMBAPRO_LIBDEVICE'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"/usr/lib/nvidia-cuda-toolkit/libdevice"</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'NUMBAPRO_NVVM'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"/usr/lib/x86_64-linux-gnu/libnvvm.so"</span>

<span class="token keyword">from</span> numba <span class="token keyword">import</span> cuda
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> time

<span class="token decorator annotation punctuation">@cuda<span class="token punctuation">.</span>jit</span>
<span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    data<span class="token punctuation">[</span>cuda<span class="token punctuation">.</span>blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">,</span> cuda<span class="token punctuation">.</span>threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">]</span> <span class="token operator">=</span> cuda<span class="token punctuation">.</span>blockIdx<span class="token punctuation">.</span>x

numBlocks <span class="token operator">=</span> <span class="token number">5</span>
threadsPerBlock <span class="token operator">=</span> <span class="token number">10</span>

data <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>numBlocks<span class="token punctuation">,</span> threadsPerBlock<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>

hello<span class="token punctuation">[</span>numBlocks<span class="token punctuation">,</span> threadsPerBlock<span class="token punctuation">]</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre> 
<h3><a id="Block__120"></a>Block 大小设置</h3> 
<p><code>[gridDim, blockDim]</code>的参考, 需要根据当前硬件设置block大小<code>blockDim</code>. 最好是32, 128, 256倍数, block最好不要超过1024</p> 
<p>Grid的大小<code>gridDim</code>, 需要执行的总次数, 除以<code>blockDim</code>, 向上取整.</p> 
<h3><a id="_126"></a>内存分配</h3> 
<p>计算时, 需要将数据从主存拷贝到显存上, 也就是将数据从主机端拷贝到设备端. CUDA能自动将数据从主机和设备间相互拷贝.</p> 
<pre><code class="prism language-python"><span class="token decorator annotation punctuation">@cuda<span class="token punctuation">.</span>jit</span>
<span class="token keyword">def</span> <span class="token function">gpu_add</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> result<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># a, b为输入向量，result为输出向量</span>
    <span class="token comment"># 所有向量都是n维</span>
    <span class="token comment"># 得到当前thread的编号</span>
    idx <span class="token operator">=</span> cuda<span class="token punctuation">.</span>threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> cuda<span class="token punctuation">.</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> cuda<span class="token punctuation">.</span>blockIdx<span class="token punctuation">.</span>x
    <span class="token keyword">if</span> idx <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>
        result<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
</code></pre> 
<p>解决极简单的问题, cuda 并不一定不 numpy快, 比如numpy.add, 已经优化到了极致.</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> numba <span class="token keyword">import</span> cuda
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> math
<span class="token keyword">from</span> time <span class="token keyword">import</span> time

<span class="token decorator annotation punctuation">@cuda<span class="token punctuation">.</span>jit</span>
<span class="token keyword">def</span> <span class="token function">gpu_add</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> result<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>
    idx <span class="token operator">=</span> cuda<span class="token punctuation">.</span>threadIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> cuda<span class="token punctuation">.</span>blockDim<span class="token punctuation">.</span>x <span class="token operator">*</span> cuda<span class="token punctuation">.</span>blockIdx<span class="token punctuation">.</span>x
    <span class="token keyword">if</span> idx <span class="token operator">&lt;</span> n<span class="token punctuation">:</span>
        result<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> a<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+</span> b<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    n <span class="token operator">=</span> <span class="token number">20000000</span>
    x <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> x

    gpu_result <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
    cpu_result <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n<span class="token punctuation">)</span>

    threads_per_block <span class="token operator">=</span> <span class="token number">1024</span>
    blocks_per_grid <span class="token operator">=</span> math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>n <span class="token operator">/</span> threads_per_block<span class="token punctuation">)</span>
    start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    gpu_add<span class="token punctuation">[</span>blocks_per_grid<span class="token punctuation">,</span> threads_per_block<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> gpu_result<span class="token punctuation">,</span> n<span class="token punctuation">)</span>
    cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"gpu vector add time "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>
    start <span class="token operator">=</span> time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    cpu_result <span class="token operator">=</span> np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"cpu vector add time "</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>cpu_result<span class="token punctuation">,</span> gpu_result<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"result correct"</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>上面代码使用了cuda默认的统一内存管理机制, 没有对数据拷贝优化, 统一内存系统是当GPU运行到某块数据发现不再设备端, 再去主机端将数据拷贝过来.</li><li>这份代码没有做流水线优化, 并非同时计算2千万数据. 一般分批次流水: 一边对某批数据计算, 一边将下一批数据从主存拷贝. 计算占用cuda核心, 数据拷贝占用总线.</li></ul> 
<p>cuda方便的内存模型缺点是计算速度慢. 我们可以继续优化这个程序, 告知GPU哪些数据需要拷贝到设备, 哪些需要拷贝回主机.</p> 
<p>Numba对NumPy比较友好, 编程中一定要使用Numpy数据类型. 用到比较多的内存分配函数有:</p> 
<ul><li> <p><code>cuda.device_array()</code>: 在设备上分配空向量, 类似于numpy.empty()</p> </li><li> <p><code>cuda.to_device()</code>: 将主机的数据拷贝到设备</p> </li><li> <p><code>cuda.copy_to_host()</code>: 将设备的数据拷贝到主机</p> </li></ul> 
<h3><a id="_192"></a>高维执行配置</h3> 
<p>使用<code>threadIdx</code>和<code>blockIdx</code>等参数来描述线程Thread的编号. 实际上, CUDA允许这两个变量最多为三维. 一维, 二维和三维的大小配置可以适应向量, 矩阵和张量等不同的场景.</p> 
<p><img src="https://images2.imgbox.com/ce/43/hxx0cEfL_o.png" alt="在这里插入图片描述"></p> 
<p>一个二维的执行配置, 每个BLOCK有(3, 4)个Thread, 每个Grid有(2, 3)个Block. 二维块大小为(Dx, Dy), 某个线程号(x, y)的公式(x + yDx); 三维块大小为(Dx, Dy, Dz), 某个线程号(x, y, z)的公式为(x + yDx + zDxDy).</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e6bd25463d68beeb5f2375ce680df023/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">性能测试实战分享1---TPS上升后下降</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/d9ecb21088594d769ed9dffd5a649a43/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">python 文件读写with open模式r,r&#43; w,w&#43; a,a&#43;区别详解</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>