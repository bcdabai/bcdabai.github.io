<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>解决mmdetection训练过程loss为nan的问题 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="解决mmdetection训练过程loss为nan的问题" />
<meta property="og:description" content="我之前遇到多次loss为nan，一次是由于数据标注出现问题，换不同的模型参数均出现此问题，因此需要仔细检查数据格式；有一次是换了个neck的结构，loss变为nan，后面通过将学习率调为原来的1/10（根据实际情况调整），就没有出现了；还有一次是注释掉fp16训练即可。下面参考了官方文档给出的解决方案以及自身经验，应该可以解决大部分问题。
检查数据的标注是否正常， 长或宽为 0 的框可能会导致回归 loss 变为 nan，一些小尺寸（宽度或高度小于 1）的框在数据增强（例如，instaboost）后也会导致此问题。 因此，可以检查标注并过滤掉那些特别小甚至面积为 0 的框，并关闭一些可能会导致 0 面积框出现数据增强。
降低学习率：由于某些原因，例如 batch size 大小的变化， 导致当前学习率可能太大。 您可以降低为可以稳定训练模型的值。
optimizer = dict(type=&#39;AdamW&#39;, lr=0.0001, betas=(0.9, 0.999), weight_decay=0.05, # 0.001 paramwise_cfg=dict(custom_keys={&#39;absolute_pos_embed&#39;: dict(decay_mult=0.), &#39;relative_position_bias_table&#39;: dict(decay_mult=0.), &#39;norm&#39;: dict(decay_mult=0.)})) 延长 warm up 的时间：一些模型在训练初始时对学习率很敏感，您可以把 warmup_iters 从 500 更改为 1000 或 2000。
lr_config = dict( policy=&#39;step&#39;, warmup=&#39;linear&#39;, warmup_iters=500, # 修改这里 Epoch [1][500/xxxx]之前的学习率的意思 warmup_ratio=0.001, step=[8, 11]) 添加 gradient clipping: 一些模型需要梯度裁剪来稳定训练过程。 默认的 grad_clip 是 None, 你可以在 config 设置 optimizer_config=dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2)) 如果你的 config 没有继承任何包含 optimizer_config=dict(grad_clip=None), 你可以直接设置optimizer_config=dict(grad_clip=dict(max_norm=35, norm_type=2))." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/bd6acb4b293edfd455a2aa611dfdeae0/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-12-08T19:04:25+08:00" />
<meta property="article:modified_time" content="2021-12-08T19:04:25+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">解决mmdetection训练过程loss为nan的问题</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>我之前遇到多次loss为nan，一次是由于数据标注出现问题，换不同的模型参数均出现此问题，因此需要<strong>仔细检查数据格式</strong>；有一次是换了个neck的结构，loss变为nan，后面通过<strong>将学习率调为原来的1/10</strong>（根据实际情况调整），就没有出现了；还有一次是<strong>注释掉fp16训练</strong>即可。下面参考了官方文档给出的解决方案以及自身经验，应该可以解决大部分问题。</p> 
<ol><li> <p><strong>检查数据的标注是否正常</strong>， 长或宽为 0 的框可能会导致回归 loss 变为 nan，一些小尺寸（宽度或高度小于 1）的框在数据增强（例如，instaboost）后也会导致此问题。 因此，可以检查标注并过滤掉那些特别小甚至面积为 0 的框，并关闭一些可能会导致 0 面积框出现数据增强。</p> </li><li> <p><strong>降低学习率</strong>：由于某些原因，例如 batch size 大小的变化， 导致当前学习率可能太大。 您可以降低为可以稳定训练模型的值。</p> <pre><code class="prism language-python">optimizer <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AdamW'</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token comment"># 0.001</span>
                 paramwise_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>custom_keys<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'absolute_pos_embed'</span><span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>decay_mult<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 <span class="token string">'relative_position_bias_table'</span><span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>decay_mult<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 <span class="token string">'norm'</span><span class="token punctuation">:</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>decay_mult<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>延长 warm up 的时间</strong>：一些模型在训练初始时对学习率很敏感，您可以把 <code>warmup_iters</code> 从 500 更改为 1000 或 2000。</p> <pre><code class="prism language-python">lr_config <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>
    policy<span class="token operator">=</span><span class="token string">'step'</span><span class="token punctuation">,</span>
    warmup<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span>
    warmup_iters<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>  <span class="token comment"># 修改这里 Epoch [1][500/xxxx]之前的学习率的意思</span>
    warmup_ratio<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span>
    step<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>添加 gradient clipping</strong>: 一些模型需要梯度裁剪来稳定训练过程。 默认的 <code>grad_clip</code> 是 <code>None</code>, 你可以在 config 设置 <code>optimizer_config=dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))</code> 如果你的 config 没有继承任何包含 <code>optimizer_config=dict(grad_clip=None)</code>, 你可以直接设置<code>optimizer_config=dict(grad_clip=dict(max_norm=35, norm_type=2))</code>.</p> <pre><code class="prism language-python">optimizer_config <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>grad_clip<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>max_norm<span class="token operator">=</span><span class="token number">35</span><span class="token punctuation">,</span> norm_type<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> </li><li> <p><strong>不使用fp16训练</strong>: 如果用了fp16训练，可尝试将其注释掉。</p> <pre><code class="prism language-python"><span class="token comment"># fp16 = dict(loss_scale=512.)</span>
</code></pre> </li></ol> 
<p>参考：</p> 
<p><a href="https://mmdetection.readthedocs.io/zh_CN/latest/faq.html?highlight=nan#training" rel="nofollow">常见问题解答 — MMDetection 2.19.0 文档</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/587e857e0d95ab2fa5359b3b892dd267/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">关于在阅读论文中常见的简写说明</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b85071b7e6a19b7f7d7628bf9548d8c2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">vue强制刷新后跳转首页</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>