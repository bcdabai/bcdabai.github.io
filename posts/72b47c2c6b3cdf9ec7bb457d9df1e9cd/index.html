<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>因果论文精读与总结（一） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="因果论文精读与总结（一）" />
<meta property="og:description" content="本文重点梳理从因果树到因果森林、再到广义随机森林，正交随机森林，其中不乏涉及相关论文的精读推导.
1. 因果树 定义：处理效应的均方误差 公式： 核心点：&#34;诚实&#34;估计，honest approach定义：①原来的树方法，使用训练样本训练出模型以后，我们用训练样本上各个子集的样本均值做为估计值，然后使用该估计值在测试集（test set）上计算MSE来判断模型的好坏；②修改后的计算方法，将训练样本切割成两部分，一部分仍是训练样本（train set），另一部分是估计样本（estimate set），即在训练样本上训练模型，模型训练好以后放到估计样本上计算估计值，最后使用该估计值在测试集上计算MSE来判断模型的好坏。文章亮点： 改为诚实方法: 修改了MSE的表达式, 标准的Rubin因果框架假设样本个体间不存在相关性（SUTVA），保住了此假设.修改了均方误差的计算方法。 ​变成了​ 式中的p代表的是处理组在子集中所占的比例， 1−p是对照组所占的比例了. 加快收敛速度.原因二是无法观测到反事实结果，减掉 τ² 能够让我们回避无法直接观测到τ这个问题. 如右图所示. 诚实预估下的分裂准则： 优点：
诚实树和子采样保证了估计的结果具有渐近正态性与一致性
相对于uplift树的优势在于采用了诚实的方法. 有效避免过拟合. 2. 因果森林 提出一种非参的方法，相比传统的非参可以破除“维度诅咒”的问题。其次，是在决策树、随机森林的基础上提出的因果森林，可以有效的提高样本的泛化能力“诚实树”的保证，处理效应的结果估计是无偏的.除了无偏，作者最后证明因果森林得到的处理效应是渐近正态的。 3. 广义随机森林 泛化性.假设 θ(x) 是我们感兴趣的参数， v(x) 是我们不感兴趣的参数（nuisance parameter）； O为观测结果；那么广义随机森林解决的是以下局部矩方程求得我们感兴趣的 θ(x) ,使得
跟因果森林一样：GRF的分裂标准是treatment效应估计的MSE最小，且采用诚实的方法.结果预测：广义随机森林先对样本X与各棵子树计算相似度 ，再加权求得估计值. 树分裂准则：最小化感兴趣的参数的误差等价于最大化两个子节点的异质性. 4. 梯度提升树算法
4. 正交随机森林 但GRF需要假定unconfoundedness，对Y和T进行局部中心化后，对中心化后的结果进行估计（本质上其实是DML＋GRF），可以在存在confounding的情况下识别treatment异质性，去除偏差。 5. 机器学习能为因果做些什么？ 总结一句话，就是帮助解决因果推断中的“异质性”问题.
在了解所有的异质性处理效应之后，我们可以根据收益/成本最大化的目的，针对不同的群体实施treatment。通过数据驱动的方式识别异质性因果的差异，而不需要预先分析计划。传统的计量经济学在分析异质性的时候，往往需要多次尝试，以了解异质性处理效应的结果。尤其是在连续变量切分的时候，我们往往无从下手。比如年龄段的cohort分析如何切分？房屋的大小，多大是大，多小是小？其中阈值难以知道。而机器学习，采用数据驱动的方式，省去了预先计划分析异质性。 6. QA 1.什么是维数诅咒？
增加维度而不增加训练样本的数量导致分类器性能的降低。不断增加维度，训练数据量需要以指数级增长，以保持相同的训练样本覆盖范围并避免过度拟合。
2.广义随机森林中的得分函数跟工具变量有何关系？
7. 因果的文章总结： 《Recursive Partitioning for Heterogeneous Causal Effects∗》Athey and Imbens (2016)
《Estimation and Inference of Heterogeneous Treatment Effects using Random Forests∗》 Wager and Athey (2018)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/72b47c2c6b3cdf9ec7bb457d9df1e9cd/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-07T22:29:39+08:00" />
<meta property="article:modified_time" content="2023-08-07T22:29:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">因果论文精读与总结（一）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p></p> 
<p>本文重点梳理从因果树到因果森林、再到广义随机森林，正交随机森林，其中不乏涉及相关论文的精读推导.</p> 
<h4 id="uZLhs">1. 因果树</h4> 
<ol><li>定义：处理效应的均方误差 
  <ol><li id="u258e73e0">公式： <p class="img-center"><img alt="" height="144" id="u93ab9444" src="https://images2.imgbox.com/94/3c/gJa2lSze_o.png" width="1200"></p> </li><li id="ud9b35c95">核心点："诚实"估计，honest approach</li><li>定义：①原来的树方法，使用训练样本训练出模型以后，我们用<strong>训练样本</strong>上各个子集的样本均值做为估计值，然后使用该估计值在测试集（test set）上计算MSE来判断模型的好坏；②修改后的计算方法，将训练样本切割成两部分，一部分仍是训练样本（train set），另一部分是估计样本（estimate set），即在训练样本上训练模型，模型训练好以后放到<strong>估计样本</strong>上计算估计值，最后使用该估计值在测试集上计算MSE来判断模型的好坏。</li><li>文章亮点： 
    <ol><li>改为诚实方法: 修改了MSE的表达式, 标准的Rubin因果框架假设样本个体间不存在相关性（SUTVA），保住了此假设.</li><li>修改了均方误差的计算方法。 <img alt="" height="37" id="ue0ede093" src="https://images2.imgbox.com/25/87/2gTcp4K6_o.png" width="195">​变成了<img alt="" height="30" id="u2388a0ab" src="https://images2.imgbox.com/60/b8/wvM0MmgZ_o.png" width="236">​ <p>式中的p代表的是处理组在子集中所占的比例， 1−p是对照组所占的比例了. </p> 
      <ol><li id="u3a0ea18f">加快收敛速度.</li><li>原因二是无法观测到反事实结果，减掉 τ² 能够让我们回避无法直接观测到τ这个问题. 如右图所示. <p class="img-center"><img alt="" height="67" id="ue81acf70" src="https://images2.imgbox.com/7e/94/KfvydlCa_o.png" width="171"></p> </li><li>诚实预估下的分裂准则： <p class="img-center"><img alt="image.png" height="53" src="https://images2.imgbox.com/69/25/ErZ59WJp_o.png" width="416"></p> </li></ol></li><li> <p>优点：</p> 
      <ol><li> <p>诚实树和子采样保证了估计的结果具有渐近正态性与一致性</p> </li><li> <p>相对于uplift树的优势在于采用了诚实的方法. 有效避免过拟合.  </p> </li></ol></li></ol></li></ol></li></ol> 
<h4>2. 因果森林</h4> 
<ol><li>提出一种非参的方法，相比传统的非参可以破除“维度诅咒”的问题。</li><li>其次，是在决策树、随机森林的基础上提出的因果森林，可以有效的提高样本的泛化能力</li><li>“诚实树”的保证，处理效应的结果估计是无偏的.</li><li>除了无偏，作者最后证明因果森林得到的处理效应是渐近正态的。</li></ol> 
<p id="u4de15dd0"></p> 
<h4 id="DHaT0">3. 广义随机森林</h4> 
<p id="ufec94778">泛化性.假设 θ(x) 是我们感兴趣的参数， v(x) 是我们不感兴趣的参数（nuisance parameter）； O为观测结果；那么广义随机森林解决的是以下局部矩方程求得我们感兴趣的 θ(x) ,使得</p> 
<p class="img-center"><img alt="" height="46" id="u2283e9b4" src="https://images2.imgbox.com/63/3f/p9LVwsCj_o.png" width="241"></p> 
<ol><li id="u110318f3">跟因果森林一样：GRF的分裂标准是treatment效应估计的MSE最小，且采用诚实的方法.</li><li id="u8781cfb5">结果预测：<strong>广义随机森林先对样本X与各棵子树计算相似度 </strong> ，再加权求得估计值. <p class="img-center"><img alt="" height="209" id="ud498decf" src="https://images2.imgbox.com/f3/e3/VYmIKdIh_o.png" width="567"></p> </li><li id="u12894b09">树分裂准则：<strong>最小化感兴趣的参数的误差等价于最大化两个子节点的异质性. </strong></li></ol> 
<p class="img-center"><img alt="" height="184" id="u6a1c8de6" src="https://images2.imgbox.com/37/5c/MlD3NxfW_o.png" width="539"></p> 
<p class="img-center"><img alt="" height="297" id="uf2be2a42" src="https://images2.imgbox.com/ea/13/1fe9jtjH_o.png" width="564"></p> 
<p>4.  梯度提升树算法</p> 
<p class="img-center"><img alt="" height="510" id="u0b8cd862" src="https://images2.imgbox.com/be/e1/Fi4Fppmr_o.png" width="574"></p> 
<h4 id="hmMPA">4. 正交随机森林</h4> 
<ol><li id="u94365293">但GRF需要假定unconfoundedness，对Y和T进行局部<strong>中心化</strong>后，对中心化后的结果进行估计（本质上其实是<strong>DML＋GRF</strong>），可以在存在confounding的情况下识别treatment异质性，去除偏差。</li></ol> 
<p class="img-center"><img alt="" height="160" id="u5c030ee5" src="https://images2.imgbox.com/31/6c/vHTnzD6J_o.png" width="484"></p> 
<h4 id="Hof94">5. 机器学习能为因果做些什么？</h4> 
<p id="ue463bd9a">总结一句话，就是帮助解决因果推断中的“异质性”问题.</p> 
<ol><li id="ub0edb0e9">在了解所有的异质性处理效应之后，我们可以根据收益/成本最大化的目的，针对不同的群体实施treatment。</li><li id="ua52248c7">通过数据驱动的方式识别异质性因果的差异，而不需要预先分析计划。传统的计量经济学在分析异质性的时候，往往需要多次尝试，以了解异质性处理效应的结果。尤其是在连续变量切分的时候，我们往往无从下手。比如年龄段的cohort分析如何切分？房屋的大小，多大是大，多小是小？其中阈值难以知道。而机器学习，采用数据驱动的方式，省去了预先计划分析异质性。</li></ol> 
<h4 id="oYKZm">6. QA</h4> 
<p id="u7a5a385e">1.什么是维数诅咒？</p> 
<p id="ub58b6da6"><strong>增加维度而不增加训练样本的数量导致分类器性能的降低</strong>。不断增加维度，训练数据量需要以指数级增长，以保持相同的训练样本覆盖范围并避免过度拟合。</p> 
<p id="ue7f33633">2.广义随机森林中的得分函数跟工具变量有何关系？</p> 
<p id="u6bf5222a"></p> 
<h4 id="UCQBF">7. 因果的文章总结：</h4> 
<p id="ub69775fb">《Recursive Partitioning for Heterogeneous Causal Effects∗》<strong>Athey and Imbens (2016)</strong></p> 
<p id="u747150b1">《Estimation and Inference of Heterogeneous Treatment Effects using Random Forests∗》 <strong>Wager and Athey (2018)</strong></p> 
<p id="u3d416aff">《GENERALIZED RANDOM FORESTS》<strong> Athey, Julie and Wager (2019)</strong></p> 
<p id="u03e4722f"></p> 
<h4 id="kCYYd">8. 引用：</h4> 
<p id="uf86e6ad5"><a href="https://zhuanlan.zhihu.com/p/547052146?utm_medium=social&amp;utm_oi=986245548764061696&amp;utm_psn=1670893908161892352&amp;utm_source=ZHShareTargetIDMore" rel="nofollow" title="机器学习能为异质性因果做些什么？（一） - 知乎">机器学习能为异质性因果做些什么？（一） - 知乎</a> 东海岸的向往</p> 
<p id="u321aa123"><a href="https://zhuanlan.zhihu.com/p/115223013" rel="nofollow" title="CART与因果推断I: Athey&amp;Imbens(2016) - 知乎">CART与因果推断I: Athey&amp;Imbens(2016) - 知乎</a> 对论文因果异质性切割方法的分析，主要将诚实的算法.</p> 
<p id="ue42c72cc"><a href="https://zhuanlan.zhihu.com/p/589094281" rel="nofollow" title="因果推断笔记 | 广义随机森林GRF（Generalized Random Forests） - 知乎">因果推断笔记 | 广义随机森林GRF（Generalized Random Forests） - 知乎</a> 对广义随机森林论文的深度解读</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e5371d80971c149c83dd0c9494205348/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">PHP基础语法</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/18a41b72bcd7858fe8a54db98aba649f/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">42.利用 牛顿迭代法解非线性高维方程组（matlab程序）</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>