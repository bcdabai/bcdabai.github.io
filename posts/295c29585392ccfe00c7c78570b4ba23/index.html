<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Outfit Anyone：阿里 虚拟试衣 新技术，实现真人百变换装 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Outfit Anyone：阿里 虚拟试衣 新技术，实现真人百变换装" />
<meta property="og:description" content="本文来源 机器之心 不实际试穿，就能尝试各种服饰，虚拟试衣技术让「QQ秀」升级成了真人版，为时尚行业打开了新世界的大门。
然而，现有的虚拟试衣方法在逼真性和细节上的一致性方面还存在挑战。虽然扩散模型在创造高品质和真实感图像方面表现出众，但在虚拟试衣等特定场景中，它们在维持控制力和一致性方面还有待提高。
Outfit Anyone 利用了一种创新的双流条件扩散模型，有效地解决了这些问题，能够精确地处理服装的变形效果，实现更加逼真的试穿体验。Outfit Anyone 最大的特点是其极强的适应性和广泛的应用范围，不仅能调整以适应不同的姿势和体形，无论是动画形象还是真人，都可以一键换装。现已开放试玩。
GitHub: https://github.com/HumanAIGC/OutfitAnyone
Project: https://humanaigc.github.io/outfit-anyone/
Demo 体验 (V0.9)： Modelscope: https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary
Huggingface Demo: https://humanaigc.github.io/outfit-anyone/
主要方法：条件扩散网络
虚拟试衣任务本质是一个条件生成的任务，也就是基于给定一张服饰图片作为条件输入，控制生成服饰在人身上的试衣图片。当前的 diffusion model 在生成的可控性方面做了很多工作，比如基于 tuning-based 的方法，如 lora, dreambooth 等，可以实现通过针对某一个或几个概念的样本图片进行针对性训练，学习对应的某个 concept, 在生成的过程中可以实现对应 concept 或者物体的生成。然而这种方式以来 finetuning，计算和时间成本高，且难以扩展到多个物体的同时生成。
另外一类控制生成的方法是以 controlnet 为代表，其主要原理是通过 zero-conv 训练一个插件的网络，可以实现利用 mask，canny edge, depth 等多种信号控制最终生成图片的 layout。这种方式的最大的弊端在于控制信号与目标图像在空间上是 align 的，但服饰与控制信号和目标图像在空间分布上有较大的差异，导致无法直接使用，从而限制了其应用的拓展范围。
因此，作者提出了一种新的支持试衣功能的条件生成网络，实现服饰的形变，光照的变化，服饰新视角变化情况下的生成，同时能够保持服饰的纹理，版型，细节的一致性。
相比 lora，dreambooth 等方法的好处是，不再需要针对每个物体进行 finetuning，具有很强的泛化性，从而可以实现 zero-shot 一键试衣。
此外，为了提升试衣结果的真实性，作者提出了 refiner 网络，对服饰的细节进行提升，从而能够提升服饰的材质、色彩，使其更接近真实的试衣效果。Outfit Anyone 也支持各种复杂的服饰，多样的姿势，以及适配多种体型，使其能够满足用户多样化的试衣需求。
框架设计
近些年，虽然模型仍层出不穷，但模型设计逐渐走向同质化。主要可以分为 3 个部分：
（1）输入信号（图像 / 视频 / 文本 /timestep）转化为 embedding 参入到后续网络计算中；" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/295c29585392ccfe00c7c78570b4ba23/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-02T11:52:39+08:00" />
<meta property="article:modified_time" content="2024-01-02T11:52:39+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Outfit Anyone：阿里 虚拟试衣 新技术，实现真人百变换装</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:right;">本文来源 机器之心 </p> 
 <p>不实际试穿，就能尝试各种服饰，虚拟试衣技术让「QQ秀」升级成了真人版，为时尚行业打开了新世界的大门。</p> 
 <p>然而，现有的虚拟试衣方法在逼真性和细节上的一致性方面还存在挑战。虽然扩散模型在创造高品质和真实感图像方面表现出众，但在虚拟试衣等特定场景中，它们在维持控制力和一致性方面还有待提高。</p> 
 <p>Outfit Anyone 利用了一种创新的双流条件扩散模型，有效地解决了这些问题，能够精确地处理服装的变形效果，实现更加逼真的试穿体验。Outfit Anyone 最大的特点是其极强的适应性和广泛的应用范围，不仅能调整以适应不同的姿势和体形，无论是动画形象还是真人，都可以一键换装。现已开放试玩。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/96/8d/MzIK06er_o.jpg" alt="e7ca4c2b1ae303004d6c49af5711b94e.jpeg"></p> 
 <p style="text-align:left;">GitHub: https://github.com/HumanAIGC/OutfitAnyone</p> 
 <p style="text-align:left;">Project: https://humanaigc.github.io/outfit-anyone/</p> 
 <p>Demo 体验 (V0.9)： </p> 
 <p style="text-align:left;">Modelscope: https://modelscope.cn/studios/DAMOXR/OutfitAnyone/summary</p> 
 <p style="text-align:left;">Huggingface Demo: https://humanaigc.github.io/outfit-anyone/</p> 
 <p style="text-align:center;"><strong>主要方法：条件扩散网络</strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/cd/43/F2BZ8UHL_o.png" alt="91f59c2e57adbf18251fe098d134553d.png"></p> 
 <p>虚拟试衣任务本质是一个条件生成的任务，也就是基于给定一张服饰图片作为条件输入，控制生成服饰在人身上的试衣图片。当前的 diffusion model 在生成的可控性方面做了很多工作，比如基于 tuning-based 的方法，如 lora, dreambooth 等，可以实现通过针对某一个或几个概念的样本图片进行针对性训练，学习对应的某个 concept, 在生成的过程中可以实现对应 concept 或者物体的生成。然而这种方式以来 finetuning，计算和时间成本高，且难以扩展到多个物体的同时生成。</p> 
 <p>另外一类控制生成的方法是以 controlnet 为代表，其主要原理是通过 zero-conv 训练一个插件的网络，可以实现利用 mask，canny edge, depth 等多种信号控制最终生成图片的 layout。这种方式的最大的弊端在于控制信号与目标图像在空间上是 align 的，但服饰与控制信号和目标图像在空间分布上有较大的差异，导致无法直接使用，从而限制了其应用的拓展范围。</p> 
 <p>因此，作者提出了一种新的支持试衣功能的条件生成网络，实现服饰的形变，光照的变化，服饰新视角变化情况下的生成，同时能够保持服饰的纹理，版型，细节的一致性。</p> 
 <p>相比 lora，dreambooth 等方法的好处是，不再需要针对每个物体进行 finetuning，具有很强的泛化性，从而可以实现 zero-shot 一键试衣。</p> 
 <p>此外，为了提升试衣结果的真实性，作者提出了 refiner 网络，对服饰的细节进行提升，从而能够提升服饰的材质、色彩，使其更接近真实的试衣效果。Outfit Anyone 也支持各种复杂的服饰，多样的姿势，以及适配多种体型，使其能够满足用户多样化的试衣需求。</p> 
 <p style="text-align:center;"><strong>框架设计</strong></p> 
 <p>近些年，虽然模型仍层出不穷，但模型设计逐渐走向同质化。主要可以分为 3 个部分：</p> 
 <p>（1）输入信号（图像 / 视频 / 文本 /timestep）转化为 embedding 参入到后续网络计算中；</p> 
 <p>（2）基础计算单元：以 Convolution Block 和 Transformer Block 构成；</p> 
 <p>（3）信息交互单元则根据 embedding 之间的不同，可以通过 spatially-aligned operation 和 non-spatially aligned operation 的多种方式实现融合。</p> 
 <p>在框架设计上，研究团队遵循简洁有效的原则，按以上的基础思路，首先确定了需要何种输入信号，并根据信号的差异化采用不同的特征交互方式。</p> 
 <p>在试衣场景中，需要 3 个控制信号：</p> 
 <ul><li><p>模特控制：模型提取模特 id，姿态等控制信号，实现模特的控制。</p></li><li><p>服饰控制：服饰的平铺图、服饰的上身图、饰品（帽子、包、鞋子等）。</p></li><li><p>图像全局控制：文本描述。</p></li></ul> 
 <p>Outfit Anyone 采用了以下的控制信号植入形式：</p> 
 <ul><li><p>模特控制：利用 spatially aligned operation ，本身作为模特图抽取特征内容，与目标图像在空间对齐。</p></li><li><p>服饰控制：本身与模特图空间不能对齐，需要进行形变操作，再通过非线性的操作进行特征融合。</p></li><li><p>背景、质量等控制：利用 attention 机制实现语义层次特征与图像特征的融合。</p></li></ul> 
 <p>目前，基于 Diffusion Model 的生成模型强调生成内容在语义层面的对齐性，所以常采用以 CLIP 为代表的图像语义抽取模型进行特征提取，但这对于试衣模型需要保留所输入服饰的纹理细节矛盾。因此，现有基于 CLIP 特征的试衣模型难以准确完整的还原服饰本身的特性，采用对服饰纹理细节可还原 / 生成的网络为佳。</p> 
 <p>而针对于模特相关的控制信号，在训练时，本身是输入模特图的一种抽象信号，可作为输入模特图的一个特征通道，在同一网络中，通过 Channel 维度进行信息整合，并不需要遵循 ControlNet 的设计，额外增加网络进行处理，从而一定程度简化模型结构。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/7e/53/HO7dhHME_o.png" alt="8b0d61cd50c27c6b75037201fad43958.png"></p> 
 <p>基于以上思考，作者设计了 Outfit Anyone 的模型框架，将多种不同的输入信号，输入进两个网络流中，通过融合的方式实现可控生成。</p> 
 <p style="text-align:center;"><strong>数据</strong></p> 
 <p>作者扩充了现有的公开服饰数据集，构建了一个大规模的虚拟试衣服饰数据集。整个数据涵盖了各种类目，包含大量高质量图片。此外，为了实现高质量的服饰还原，作者充分地整理和提取了服饰相关的材质属性等信息。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/5d/50/worRpv5Y_o.png" alt="ebad6336fd2982a81a1e4cf7e711ccff.png"></p> 
 <p style="text-align:center;"><strong>效果展示</strong></p> 
 <p><strong>1. 仅需平铺图输入，且支持单件 + 上下装成套的试衣</strong></p> 
 <p>站在服饰商家的角度，需要以平铺图作为输入，避免需要上身图的额外要求。但这也在服饰上身后的自然度方面对算法提出了更高的要求。</p> 
 <p>Outfit Anyone 支持平铺图的输入，并且可同时支持单件或者上下搭配。模型根据模特姿势身材的不同，相应生成褶皱、光照等细节不同的服饰上身效果，从而实现百变的换装体验。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/b8/4c/72wI35GB_o.gif" alt="17443af1e03f08a1d046e01045eb0b7f.gif"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/87/bd/f8pK6vOM_o.gif" alt="6a96c045c2dfb036cf4e20ca6ccfdac4.gif"></p> 
 <p><strong>2. 非常规服饰试衣</strong></p> 
 <p>在时尚浪潮的前沿，除了常规版型的服饰，还有更多有创意的新奇服饰。Outfit Anyone 对这类服饰也能提供很好的支持。</p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/88/da/gwC0ytHh_o.gif" alt="e8fda1f3449216df0b617db409b39564.gif"></p> 
 <p><strong>3. 细节一致性提升，可以保证服饰细节的一致性</strong></p> 
 <p>为了使 Outfit Anyone 所生成的试衣图片达到摄影级别的质量，作者进一步基于试衣模型结构开发了 refiner。可以在保留服饰基本 ID 的基础上，显著提升服饰的材料质感，模特的皮肤真实度。</p> 
 <p><img src="https://images2.imgbox.com/f0/43/I5JOquNu_o.gif" alt="1749af4e2cb8add073196815e269886b.gif"></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/33/f7/mzm7L2Eu_o.gif" alt="03386cf95234e6332169b9b4ae036991.gif"></p> 
 <p style="text-align:center;"><strong>关注公众号【机器学习与AI生成创作】，更多精彩等你来读</strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">卧剿，6万字！30个方向130篇！CVPR 2023 最全 AIGC 论文！一口气读完</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出stable diffusion：AI作画技术背后的潜在扩散模型论文解读</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">深入浅出ControlNet，一种可控生成的AIGC绘画生成算法！</a> <br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">经典GAN不得不读：StyleGAN</a><br></strong></p> 
 <p style="text-align:center;"><img src="https://images2.imgbox.com/9d/44/6IrlO6uk_o.png" alt="a3e00e4675d2190d190a51ffcc18c34c.png"> <strong><a href="" rel="nofollow">戳我，查看GAN的系列专辑~！</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">一杯奶茶，成为AIGC+CV视觉的前沿弄潮儿！</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">最新最全100篇汇总！生成扩散模型Diffusion Models</a></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">ECCV2022 | 生成对抗网络GAN部分论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">CVPR 2022 | 25+方向、最新50篇GAN论文</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow"> ICCV 2021 | 35个主题GAN论文汇总</a><br></strong></p> 
 <p style="text-align:center;"><strong><a href="" rel="nofollow">超110篇！CVPR 2021最全GAN论文梳理</a></strong><strong><br></strong></p> 
 <p style="text-align:center;"><a href="" rel="nofollow"><strong>超100篇！CVPR 2020最全GAN论文梳理</strong></a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">拆解组新的GAN：解耦表征MixNMatch</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">StarGAN第2版：多域多样性图像生成</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 | 《可解释的机器学习》中文版</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《TensorFlow 2.0 深度学习算法实战》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">附下载 |《计算机视觉中的数学方法》分享</a></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度学习的表面缺陷检测方法综述》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《零样本图像分类综述: 十年进展》</a><br></p> 
 <p style="text-align:center;"><a href="" rel="nofollow">《基于深度神经网络的少样本学习综述》</a></p> 
 <blockquote> 
  <p>《礼记·学记》有云：独学而无友，则孤陋而寡闻</p> 
 </blockquote> 
 <p><strong>点击</strong><em><strong><a href="" rel="nofollow">一杯奶茶，成为AIGC+CV视觉的前沿弄潮儿！</a></strong></em><strong>，加入 </strong><strong>AI生成创作与计算机视觉</strong><strong> 知识星球！</strong></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/f22cbf9212cbd64ab10d4972fe9cfeff/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">一文搞懂 Kafka 中的 listeners 和 advertised.listeners 以及其他通信配置</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/5318f1f8e39dab4cf8da1a0f45ce2b1a/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Linux常用指令】用户管理</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>