<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>技术精讲 | BEVFusion: 基于统一BEV表征的多任务多传感器融合 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="技术精讲 | BEVFusion: 基于统一BEV表征的多任务多传感器融合" />
<meta property="og:description" content="观点摘要：该论文是BEV模型研究传感器融合中很重要的一个工作，其实现特征级融合的同时，利用预计算（precomputing）和间隙降低（interval reduction）在BEV模型重要的模块“视角转换”中实现了对特征聚合的BEV池化（pooling）操作大幅加速，便于BEV模型在客户端的部署。
arXiv上传于2022年5月26日论文“BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation“，来自MIT韩松团队的工作报告（注：NeurIPS会议投稿格式）。
前不久介绍过一篇BEV多传感器融合的目标检测工作：“FUTR3D，一个统一的传感器融合3D检测框架”：
https://zhuanlan.zhihu.com/p/499567644
还有CVPR 2022上的传感器融合工作 TransFusion （注：是在图像平面的特征融合，不是在BEV）：
https://zhuanlan.zhihu.com/p/485971924
将多传感器融合对于准确可靠的自动驾驶系统至关重要。最近提出的方法基于点级（point-level）融合：使用摄像头特征增强激光雷达点云。然而，摄像头到激光雷达的投影丢弃了摄像头特征的语义密度（semantic density），阻碍了此类方法的有效性，尤其是对于面向语义的任务（如3D场景分割）。
本文提出的BEVFusion是一种多任务多传感器融合框架，其统一BEV表征空间中的多模态特征，很好地保留了几何和语义信息。为实现这一点，优化BEV池化，诊断并解除视图转换中的关键效率瓶颈，将延迟减少了40倍。BEVFusion从根本上来说是任务无关的，无缝支持不同的3D感知任务，几乎没有架构的更改。
在nuScenes数据集的3D目标检测上实现1.3%的mAP和NDS提升，在BEV分割上实现了13.6%的mIoU提升，计算成本降低了1.9倍。代码将开源 https://github.com/mit-han-lab/。
自动驾驶系统配备了各种传感器。例如，Waymo的自动驾驶车辆有29个摄像头、6个雷达和5个激光雷达。不同的传感器提供互补信号：例如，摄像头捕捉丰富的语义信息，激光雷达提供精确的空间信息，而雷达提供即时速度估计。因此，多传感器融合对于准确可靠的感知具有重要意义。
来自不同传感器的数据以根本不同的方式表征：例如，摄像头在透视图中捕获数据，激光雷达在3D视图中捕获数据。为了解决这种视图差异，必须找到一种适用于多任务多模态特征融合的统一表征。由于在2D感知方面取得了巨大成功，自然的想法是将激光雷达点云投影到摄像头图像平面上，并使用2D CNN处理RGB-D数据。然而，这种激光雷达到摄像头的投影引入了严重的几何畸变，对于面向几何的任务（如3D目标识别）的效率较低。
最近的传感器融合方法遵循了另一个方向，用语义标注、CNN特征或2D图像中的虚拟点（virtual points）来增强激光雷达点云，然后应用现有基于激光雷达的检测器预测3D边框。尽管这些点级融合方法在大规模检测基准上表现出了卓越的性能，但几乎不适用于面向语义的任务，如BEV地图分割。这是因为摄像头到激光雷达的投影在语义上是有损的，而BEB Fusion就是想避免这个几何和语义的损失，建立BEV特征的融合表征，实现3D语义任务。
如图所示：对于典型的32线激光雷达扫描，只有5%的摄像头特征与激光雷达点匹配，而其他所有特征都将被删除。对于更稀疏的激光雷达（或成像雷达），这种密度差异将变得更加剧烈。
近年来，多传感器融合方法可分为提议级（proposal level）融合和点级融合方法。早期方法MV3D在3D中创建目标提议，并将其投影到图像以提取RoI特征。F-PointNet、F-ConvNet和CenterFusion都将图像提议提升到3D平截体（frustum）中。最近的工作FUTR3D和TransFusion定义了3D空间中的目标查询，并将图像特征融合到这些提议中。所有提议级融合方法都是以目标为中心的，不能简单地推广到其他任务，如BEV地图分割。另一方面，点级融合方法通常将图像语义特征绘制到前景FG激光雷达点上，并在修饰的（decorated）点云输入上做基于激光雷达的检测。因此，它们既以目标为中心，又以几何为中心。其中，PointPaint、PointAugmenting、MVP、FusionPaint和AutoAlign是（激光雷达）输入级修饰，而Deep Continuous Fusion和DeepFusion是特征级修饰。
多任务CNN在2D计算机视觉领域也得到了很好的研究，包括联合目标检测、实例分割、姿势估计和人机交互。最近同时出现的研究M2BEV和BEVFormer，联合执行3D目标检测和BEV分割。上述方法均未考虑多传感器融合。MMF同时使用摄像头和激光雷达输入进行深度图补全和目标检测，但仍然以目标为中心，不适用于BEV地图分割。
如图所示是BEVFusion流水线概览：给定不同的感知输入，首先应用特定于模态的编码器来提取其特征；将多模态特征转换为一个统一的BEV表征，其同时保留几何和语义信息；存在的视图转换效率瓶颈，可以通过预计算和间歇降低来加速BEV池化过程；然后，将基于卷积的BEV编码器应用到统一的BEV特征中，以缓解不同特征之间的局部偏准；最后，添加一些特定任务头支持不同的3D场景理解工作。
本文采用BEV作为融合的统一表征，该视图对几乎所有感知任务都很友好，因为输出空间也在BEV。更重要的是，到BEV的转换同时保持了几何结构（来自激光雷达特征）和语义密度（来自摄像头特征）。一方面，LiDAR到BEV投影将稀疏LiDAR特征沿高度维度（height dimension）展平，因此不会产生几何失真。另一方面，摄像头到BEV投影将每个摄像头特征像素投射回3D空间的一条光线中（ray casting），这可以生成密集的BEV特征图，并保留了摄像头的全部语义信息。
摄像头到BEV的变换非常重要，因为与每个摄像头图像特征像素关联的深度（depth）本质上是不明确的。根据LSS，明确预测每个像素的离散深度分布。然后，沿着摄像头光线将每个特征像素分散成D个离散点，并根据相应的深度概率重缩放（rescale）相关特征。这将生成大小为N*H*W*D的摄像头特征点云，其中N是摄像头数，（H，W）是摄像头特征图大小。此类3D特征点云沿x、y轴量化，步长为r（例如，0.4m）。用BEV池化操作来聚合每个r×r BEV网格内的所有特征，并沿z轴展平特征。
虽然简单，但BEV池化的效率和速度惊人地低，在RTX 3090 GPU上需要500毫秒以上（而模型的其余部分计算只需要100毫秒左右）。这是因为摄像头特征点云非常大，即典型的工作负载，每帧可能生成约200万个点，比激光雷达特征点云密度高两个数量级。为了消除这一效率瓶颈，建议通过预计算和间歇降低来优化BEV池化进程。
如图所示：摄像机到BEV变换（a）是在统一的BEV空间进行传感器融合的关键步骤。然而，现有的实现速度非常慢，单个场景可能需要2秒的时间。文章提出了有效的BEV池化方法（b），通过预计算使间歇降低和加快网格关联，视图转换模块（c，d）的执行速度提高了40倍。
1、预计算
BEV池化的第一步是将摄像头特征点云的每个点与BEV网格相关联。与激光雷达点云不同，摄像头特征点云的坐标是固定的（只要摄像头内参外参保持不变，通常在适当标定后）。基于此，预计算每个点的3D坐标和BEV网格索引。还有，根据网格索引对所有点进行排序，并记录每个点排名。在推理过程中，只需要根据预计算的排序对所有特征点重排序。这种缓存机制可以将网格关联的延迟从17ms减少到4ms。
2、间歇降低
网格关联后，同一BEV网格的所有点将在张量表征中连续。BEV池化的下一步是通过一些对称函数（例如，平均值、最大值和求和）聚合每个BEV网格内的特征。现有的实现方法首先计算所有点的前缀和（prefix sum），然后减去索引发生变化的边界值。然而，前缀和操作，需要在GPU进行树缩减（tree reduction），并生成许多未使用的部分和（因为只需要边界值），这两种操作都是低效的。为了加速特征聚合，文章里实现一个专门的GPU内核，直接在BEV网格并行化：为每个网格分配一个GPU线程，该线程计算其间歇和（interval sum）并将结果写回。该内核消除输出之间的依赖关系（因此不需要多级树缩减），并避免将部分和写入DRAM，从而将特征聚合的延迟从500ms减少到2ms。
3、总结
通过优化的BEV池化，摄像头到BEV的转换速度提高了40倍：延迟从500ms减少到12ms（仅为模型端到端运行时间的10%），并且可以在不同的分特征辨率之间很好地扩展。在共享BEV表征中，这是统一多模态感知特征的关键促成因素。两项并行化工作也发现了纯摄像头3D检测的这一效率瓶颈。假设均匀深度分布，或截断每个BEV网格内的点，可以近似视图transformer计算。相比之下，该技术在没有任何近似的情况下是精确的，但仍然更快。
实验结果如下：
深信科创：致力于自动驾驶工业软件
深信科创是一家专注于提供自动驾驶仿真及智慧交通解决方案的国家高新技术企业。公司基于人工智能、软件测试、数字孪生与大数据等技术，致力于自动驾驶工具链的研发，拥有一支高素质的研发团队，研发了国际领先的自动驾驶仿真及数据闭环工具链SYNKROTRON™ Oasis产品系列，能够提供高精度传感器模型、动力学模型及感知级交通环境仿真解决方案等，客户可以在仿真平台上对自动驾驶系统开展大规模的仿真测试和模型训练，提前识别自动驾驶系统缺陷、降低实车测试成本、消除场景端落地的安全隐患，加速自动驾驶技术在场景端的安全落地。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/1c1df877267016898351ac3d9c2e457b/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-06-25T17:05:32+08:00" />
<meta property="article:modified_time" content="2023-06-25T17:05:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">技术精讲 | BEVFusion: 基于统一BEV表征的多任务多传感器融合</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <blockquote> 
 <p><strong>观点摘要</strong>：该论文是BEV模型研究传感器融合中很重要的一个工作，其实现特征级融合的同时，利用预计算（precomputing）和间隙降低（interval reduction）在BEV模型重要的模块“视角转换”中实现了对特征聚合的BEV池化（pooling）操作大幅加速，便于BEV模型在客户端的部署。</p> 
 <p></p> 
 <p>arXiv上传于2022年5月26日论文“BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird’s-Eye View Representation“，来自MIT韩松团队的工作报告（注：NeurIPS会议投稿格式）。</p> 
</blockquote> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/82/54/NYGwrWsF_o.jpg"></p> 
<p>前不久介绍过一篇BEV多传感器融合的目标检测工作：“<strong>FUTR3D</strong>，一个统一的传感器融合3D检测框架”：</p> 
<p>https://zhuanlan.zhihu.com/p/499567644</p> 
<p>还有CVPR 2022上的传感器融合工作 <strong>TransFusion</strong> （注：是在图像平面的特征融合，不是在BEV）：</p> 
<p>https://zhuanlan.zhihu.com/p/485971924</p> 
<p>将多传感器融合对于准确可靠的自动驾驶系统至关重要。最近提出的方法基于<strong>点级（point-level）</strong>融合：使用摄像头特征增强激光雷达点云。然而，摄像头到激光雷达的投影丢弃了摄像头特征的语义密度（semantic density），阻碍了此类方法的有效性，尤其是对于面向语义的任务（如3D场景分割）。</p> 
<p>本文提出的BEVFusion是一种多任务多传感器融合框架，其统一BEV表征空间中的多模态特征，很好地保留了几何和语义信息。为实现这一点，优化BEV池化，诊断并解除视图转换中的关键效率瓶颈，将延迟减少了40倍。BEVFusion从根本上来说是任务无关的，无缝支持不同的3D感知任务，几乎没有架构的更改。</p> 
<p>在nuScenes数据集的3D目标检测上实现1.3%的mAP和NDS提升，在BEV分割上实现了13.6%的mIoU提升，计算成本降低了1.9倍。代码将开源 https://github.com/mit-han-lab/。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/15/96/4zFvJQ5o_o.jpg"></p> 
<p>自动驾驶系统配备了各种传感器。例如，Waymo的自动驾驶车辆有29个摄像头、6个雷达和5个激光雷达。不同的传感器提供互补信号：例如，摄像头捕捉丰富的语义信息，激光雷达提供精确的空间信息，而雷达提供即时速度估计。因此，多传感器融合对于准确可靠的感知具有重要意义。</p> 
<p>来自不同传感器的数据以根本不同的方式表征：例如，摄像头在透视图中捕获数据，激光雷达在3D视图中捕获数据。为了解决这种视图差异，必须找到一种适用于多任务多模态特征融合的统一表征。由于在2D感知方面取得了巨大成功，自然的想法是将激光雷达点云投影到摄像头图像平面上，并使用2D CNN处理RGB-D数据。然而，这种激光雷达到摄像头的投影引入了严重的几何畸变，对于面向几何的任务（如3D目标识别）的效率较低。</p> 
<p>最近的传感器融合方法遵循了另一个方向，用语义标注、CNN特征或2D图像中的虚拟点（virtual points）来增强激光雷达点云，然后应用现有基于激光雷达的检测器预测3D边框。尽管这些点级融合方法在大规模检测基准上表现出了卓越的性能，但几乎不适用于面向语义的任务，如BEV地图分割。这是因为摄像头到激光雷达的投影在语义上是有损的，而BEB Fusion就是想避免这个几何和语义的损失，建立BEV特征的融合表征，实现3D语义任务。</p> 
<p>如图所示：对于典型的32线激光雷达扫描，只有5%的摄像头特征与激光雷达点匹配，而其他所有特征都将被删除。对于更稀疏的激光雷达（或成像雷达），这种密度差异将变得更加剧烈。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3b/80/wB9Q2RlX_o.png"></p> 
<p>近年来，多传感器融合方法可分为<strong>提议级（proposal level）</strong><strong>融合</strong>和点级融合方法。早期方法MV3D在3D中创建目标提议，并将其投影到图像以提取RoI特征。F-PointNet、F-ConvNet和CenterFusion都将图像提议提升到3D平截体（frustum）中。最近的工作FUTR3D和TransFusion定义了3D空间中的目标查询，并将图像特征融合到这些提议中。所有提议级融合方法都是以目标为中心的，不能简单地推广到其他任务，如BEV地图分割。另一方面，点级融合方法通常将图像语义特征绘制到前景FG激光雷达点上，并在修饰的（decorated）点云输入上做基于激光雷达的检测。因此，它们既以目标为中心，又以几何为中心。其中，PointPaint、PointAugmenting、MVP、FusionPaint和AutoAlign是（激光雷达）输入级修饰，而Deep Continuous Fusion和DeepFusion是特征级修饰。</p> 
<p>多任务CNN在2D计算机视觉领域也得到了很好的研究，包括联合目标检测、实例分割、姿势估计和人机交互。最近同时出现的研究M2BEV和BEVFormer，联合执行3D目标检测和BEV分割。上述方法均未考虑多传感器融合。MMF同时使用摄像头和激光雷达输入进行深度图补全和目标检测，但仍然以目标为中心，不适用于BEV地图分割。</p> 
<p>如图所示是BEVFusion流水线概览：给定不同的感知输入，首先应用特定于模态的编码器来提取其特征；将多模态特征转换为一个统一的BEV表征，其同时保留几何和语义信息；存在的视图转换效率瓶颈，可以通过预计算和间歇降低来加速BEV池化过程；然后，将基于卷积的BEV编码器应用到统一的BEV特征中，以缓解不同特征之间的局部偏准；最后，添加一些特定任务头支持不同的3D场景理解工作。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/d3/b4/0E4Rl6FD_o.jpg"></p> 
<p>本文采用BEV作为融合的统一表征，该视图对几乎所有感知任务都很友好，因为输出空间也在BEV。更重要的是，到BEV的转换同时保持了几何结构（来自激光雷达特征）和语义密度（来自摄像头特征）。一方面，LiDAR到BEV投影将稀疏LiDAR特征沿高度维度（height dimension）展平，因此不会产生几何失真。另一方面，摄像头到BEV投影将每个摄像头特征像素投射回3D空间的一条光线中（ray casting），这可以生成密集的BEV特征图，并保留了摄像头的全部语义信息。</p> 
<p>摄像头到BEV的变换非常重要，因为与每个摄像头图像特征像素关联的深度（depth）本质上是不明确的。根据LSS，明确预测每个像素的离散深度分布。然后，沿着摄像头光线将每个特征像素分散成D个离散点，并根据相应的深度概率重缩放（rescale）相关特征。这将生成大小为N*H*W*D的摄像头特征点云，其中N是摄像头数，（H，W）是摄像头特征图大小。此类3D特征点云沿x、y轴量化，步长为r（例如，0.4m）。用BEV池化操作来聚合每个r×r BEV网格内的所有特征，并沿z轴展平特征。</p> 
<p>虽然简单，但BEV池化的效率和速度惊人地低，在RTX 3090 GPU上需要500毫秒以上（而模型的其余部分计算只需要100毫秒左右）。这是因为摄像头特征点云非常大，即典型的工作负载，每帧可能生成约200万个点，比激光雷达特征点云密度高两个数量级。为了消除这一效率瓶颈，建议通过预计算和间歇降低来优化BEV池化进程。</p> 
<p>如图所示：摄像机到BEV变换（a）是在统一的BEV空间进行传感器融合的关键步骤。然而，现有的实现速度非常慢，单个场景可能需要2秒的时间。文章提出了有效的BEV池化方法（b），通过预计算使间歇降低和加快网格关联，视图转换模块（c，d）的执行速度提高了40倍。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/e6/8e/uGLFujF6_o.jpg"></p> 
<p></p> 
<p><strong>1、预计算</strong></p> 
<p>BEV池化的第一步是将摄像头特征点云的每个点与BEV网格相关联。与激光雷达点云不同，摄像头特征点云的坐标是固定的（只要摄像头内参外参保持不变，通常在适当标定后）。基于此，预计算每个点的3D坐标和BEV网格索引。还有，根据网格索引对所有点进行排序，并记录每个点排名。在推理过程中，只需要根据预计算的排序对所有特征点重排序。这种缓存机制可以将网格关联的延迟从17ms减少到4ms。</p> 
<p><strong>2、间歇降低</strong></p> 
<p>网格关联后，同一BEV网格的所有点将在张量表征中连续。BEV池化的下一步是通过一些对称函数（例如，平均值、最大值和求和）聚合每个BEV网格内的特征。现有的实现方法首先计算所有点的前缀和（prefix sum），然后减去索引发生变化的边界值。然而，前缀和操作，需要在GPU进行树缩减（tree reduction），并生成许多未使用的部分和（因为只需要边界值），这两种操作都是低效的。为了加速特征聚合，文章里实现一个专门的GPU内核，直接在BEV网格并行化：为每个网格分配一个GPU线程，该线程计算其间歇和（interval sum）并将结果写回。该内核消除输出之间的依赖关系（因此不需要多级树缩减），并避免将部分和写入DRAM，从而将特征聚合的延迟从500ms减少到2ms。</p> 
<p><strong>3、总结</strong></p> 
<p>通过优化的BEV池化，摄像头到BEV的转换速度提高了40倍：延迟从500ms减少到12ms（仅为模型端到端运行时间的10%），并且可以在不同的分特征辨率之间很好地扩展。在共享BEV表征中，这是统一多模态感知特征的关键促成因素。两项并行化工作也发现了纯摄像头3D检测的这一效率瓶颈。假设均匀深度分布，或截断每个BEV网格内的点，可以近似视图transformer计算。相比之下，该技术在没有任何近似的情况下是精确的，但仍然更快。</p> 
<p><strong>实验结果如下：</strong></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/0b/fa/3Y9dIvp4_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/9a/61/fhRbuu3H_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/81/9b/8wjKSbPq_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/7b/98/1MVdcOKG_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/02/88/38dlcVdX_o.jpg"></p> 
<p></p> 
<blockquote> 
 <p><strong>深信科创：</strong><strong>致力于自动驾驶工业软件</strong></p> 
 <p></p> 
 <p>深信科创是一家专注于提供自动驾驶仿真及智慧交通解决方案的国家高新技术企业。公司基于人工智能、软件测试、数字孪生与大数据等技术，致力于自动驾驶工具链的研发，拥有一支高素质的研发团队，研发了国际领先的自动驾驶仿真及数据闭环工具链SYNKROTRON™ Oasis产品系列，能够提供高精度传感器模型、动力学模型及感知级交通环境仿真解决方案等，客户可以在仿真平台上对自动驾驶系统开展大规模的仿真测试和模型训练，提前识别自动驾驶系统缺陷、降低实车测试成本、消除场景端落地的安全隐患，加速自动驾驶技术在场景端的安全落地。</p> 
</blockquote> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a8/77/8RhO2IAp_o.png"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/57383936df080cae6cd82427f7a260ea/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">vue在移动端调用扫一扫功能</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/55e4696412c0499051ed8dd0bfa8ddaf/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">前端同比缩放页面 模拟浏览器缩放</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>