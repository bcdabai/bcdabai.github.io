<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>超参数优化 - 贝叶斯优化基础方法 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="超参数优化 - 贝叶斯优化基础方法" />
<meta property="og:description" content="目录
1. 贝叶斯优化的基本流程
2. 贝叶斯优化用于HPO
在之前的学习中我们了解了网格搜索、随机网格搜索与Halving网格搜索，无论具体每种网格搜索的思想如何变化，网格优化都是在一个大参数空间中、尽量对所有点进行验证后再返回最优损失函数值的方法，这一类方法在计算量与计算时间上有着不可避免的缺陷，因此才会有随机、Halving等试图缩短训练时间、让整体网格搜索更加适合于大型数据和大型空间的手段。然而，尽管sklearn在提高网格搜索效率方面做出了种种优化，但上述方法仍然无法在效率和精度上做到双赢，若希望更快速的进行参数搜索、并且搜索出一组泛化能力尽可能强的参数，目前的常见做法还是选用一些带有先验过程的调参工具，即一些基于贝叶斯过程调参工具。
ps：先验过程是指在观测到任何数据之前，对模型参数或未知变量的分布进行建模。它是基于先前的经验或领域知识，以及模型的先验假设，来描述参数或变量的分布。先验过程可以帮助我们在没有足够数据时对模型进行初始估计，或者在数据不足或不完整的情况下提供一种合理的先验推断。在贝叶斯统计中，先验过程与后验过程共同构成了贝叶斯推断的基础。
1. 贝叶斯优化的基本流程 首先，假设现在知道一个函数𝑓(𝑥))的表达式以及其自变量𝑥的定义域，现在，我们希望求解出𝑥的取值范围上𝑓(𝑥)的最小值，你打算如何求解这个最小值呢？面对这个问题，无论是从单纯的数学理论角度，还是从机器学习的角度，我们都已经见过好几个通俗的思路：
① 对𝑓(𝑥)求导、令其一阶导数为0来求解其最小值。要求：函数𝑓(𝑥)可微，且微分方程可以直接被求解。
② 通过梯度下降等优化方法迭代出𝑓(𝑥)的最小值。要求：函数𝑓(𝑥)可微，且函数本身为凸函数。
③ 将全域的𝑥带入𝑓(𝑥)计算出所有可能的结果，再找出最小值。要求：函数𝑓(𝑥)相对不复杂、自变量维度相对低、计算量可以承受。
当我们知道函数𝑓(𝑥)的表达式时，以上方法常常能够有效，但每个方法都有自己的前提条件。假设现在函数𝑓(𝑥)是一个平滑均匀的函数，但它异常复杂、且不可微，我们无法使用上述三种方法中的任意一种方法求解，但我们还是想求解其最小值，可以怎么办呢？由于函数异常复杂，带入任意𝑥计算的所需的时间很长，所以我们不太可能将全域𝑥都带入进行计算，但我们还是可以从中随机抽样部分观测点来观察整个函数可能存在的趋势。于是我们选择在𝑥的定义域上随机选择了4个点，并将4个点带入𝑓(𝑥)进行计算，得到了如下结果：
当我们有了4个观测值，并且知道我们的函数时相对均匀、平滑的函数，那我们可能对函数的整体分布有如下猜测： 当我们对函数整体分布有一个猜测时，这个分布上一定会存在该函数的最小值。同时，不同的人可能对函数的整体分布有不同的猜测，不同猜测下对应的最小值也是不同的。
现在，假设我们邀请了数万个人对该问题做出猜测，每个人所猜测的曲线如下图所示。不难发现，在观测点的附近，每个人猜测的函数值差距不大，但是在远离远侧点的地方，每个人猜测的函数值就高度不一致了。这也是当然的，因为观测点之间函数的分布如何完全是未知的，并且该分布离观测点越远时，我们越不确定真正的函数值在哪里，因此人们猜测的函数值的范围非常巨大。
现在，将所有猜测求均值，并将任意均值周围的潜在函数值所在的区域用色块表示，可以得到一条所有人猜测的平均曲线。不难发现，色块所覆盖的范围其实就是大家猜测的函数值的上界和下界，而任意𝑥所对应的上下界差异越大，表示人们对函数上该位置的猜测值的越不确定。因此上下界差异可以衡量人们对该观测点的置信度，色块范围越大，置信度越低。 在观测点周围，置信度总是很高的，远离观测点的地方，置信度总是很低，所以如果我们能够在置信度很低的地方补充一个实际的观测点，我们就可以很快将众人的猜测统一起来。以下图为例，当我们在置信度很低的区间内取一个实际观测值时，围绕该区间的“猜测”会立刻变得集中，该区间内的置信度会大幅升高。
当整个函数上的置信度都非常高时，我们可以说我们得出了一条与真实的𝑓(𝑥)曲线高度相似的曲线𝑓*，此时我们就可以将𝑓*的最小值当作真实𝑓(𝑥)的最小值来看待。自然，如果估计越准确，𝑓*越接近𝑓(𝑥)，则𝑓*的最小值也会越接近于𝑓(𝑥)的真实最小值。那如何才能够让𝑓*更接近𝑓(𝑥)呢？根据我们刚才提升置信度的过程，很明显——观测点越多，我们估计出的曲线会越接近真实的𝑓(𝑥)。然而，由于计算量有限，我们每次进行观测时都要非常谨慎地选择观测点。那如何选择观测点才能够最大程度地帮助我们估计出𝑓(𝑥)的最小值呢？
有非常多的方法，其中最简单的手段是使用最小值出现的频数进行判断。由于不同的人对函数的整体分布有不同的猜测，不同猜测下对应的最小值也是不同的，根据每个人猜测的函数结果，我们在𝑋轴上将定义域区间均匀划分为100个小区间，如果有某个猜测的最小值落在其中一个区间中，我们就对该区间进行计数（这个过程跟对离散型变量绘制直方图的过程完全一致）。当有数万个人进行猜测之后，我们同时也绘制了基于𝑋轴上不同区间的频数图，频数越高，说明猜测最小值在该区间内的人越多，反之则说明该猜测最小值在该区间内的人越少。该频数一定程度上反馈出最小值出现的概率，频数越高的区间，函数真正的最小值出现的概率越高。
当我们将𝑋轴上的区间划分得足够细后，绘制出的频数图可以变成概率密度曲线，曲线的最大值所对应的点是𝑓(𝑥)的最小值的概率最高，因此很明显，我们应该将曲线最大值所对应的点确认为下一个观测点。根据图像，最小值最有可能在的区间就在x=0.7左右的位置。当我们不取新的观测点时，现在𝑓(𝑥)上可以获得的可靠的最小值就是x=0.6时的点，但我们如果在x=0.7处取新的观测值，我们就很有可能找到比当前x=0.6的点还要小的𝑓𝑚𝑖𝑛。因此，我们可以就此决定，在x=0.7处进行观测。 当我们在x=0.7处取出观测值之后，我们就有了5个已知的观测点。现在，我们再让数万人根据5个已知的观测点对整体函数分布进行猜测，猜测完毕之后再计算当前最小值频数最高的区间，然后再取新的观测点对𝑓(𝑥)进行计算。当允许的计算次数被用完之后（比如，500次），整个估计也就停止了。
在这个过程当中，我们其实在不断地优化我们对目标函数𝑓(𝑥)的估计，虽然没有对𝑓(𝑥)进行全部定义域上的计算，也没有找到最终确定一定是𝑓(𝑥)分布的曲线，但是随着观测的点越来越多，我们对函数的估计是越来越准确的，因此也有越来越大的可能性可以估计出𝑓(𝑥)真正的最小值。这个优化的过程，就是贝叶斯优化。
2. 贝叶斯优化用于HPO 在贝叶斯优化的数学过程当中，我们主要执行以下几个步骤：
① 定义需要估计的𝑓(𝑥)以及𝑥的定义域
② 取出有限的n个𝑥上的值，求解出这些𝑥对应的𝑓(𝑥)（求解观测值）
③ 根据有限的观测值，对函数进行估计（该假设被称为贝叶斯优化中的先验知识），得出该估计𝑓*上的目标值（最大值或最小值）
④ 定义某种规则，以确定下一个需要计算的观测点
并持续在②~④步骤中进行循环，直到假设分布上的目标值达到我们的标准，或者所有计算资源被用完为止（例如，最多观测m次，或最多允许运行t分钟）。
以上流程又被称为序贯模型优化（SMBO），是最为经典的贝叶斯优化方法。在实际的运算过程当中，尤其是超参数优化的过程当中，有以下具体细节需要注意：
当贝叶斯优化不被用于HPO时，一般𝑓(𝑥)可以是完全的黑盒函数（black box function，也译作黑箱函数，即只知道𝑥与𝑓(𝑥)的对应关系，却丝毫不知道函数内部规律、同时也不能写出具体表达式的一类函数），因此贝叶斯优化也被认为是可以作用于黑盒函数估计的一类经典方法。但在HPO过程当中，需要定义的𝑓(𝑥)一般是交叉验证的结果/损失函数的结果，而我们往往非常清楚损失函数的表达式，只是我们不了解损失函数内部的具体规律，因此HPO中的𝑓(𝑥)不能算是严格意义上的黑盒函数。
在HPO中，自变量𝑥就是超参数空间。在上述二维图像表示中，𝑥为一维的，但在实际进行优化时，超参数空间往往是高维且极度复杂的空间。
最初的观测值数量n、以及最终可以取到的最大观测数量m都是贝叶斯优化的超参数，最大观测数量m也决定了整个贝叶斯优化的迭代次数。
在第3步中，根据有限的观测值、对函数分布进行估计的工具被称为概率代理模型（Probability Surrogate model），毕竟在数学计算中我们并不能真的邀请数万人对我们的观测点进行连线。这些概率代理模型自带某些假设，他们可以根据廖廖数个观测点估计出目标函数的分布𝑓*（包括𝑓*上每个点的取值以及该点对应的置信度）。在实际使用时，概率代理模型往往是一些强大的算法，最常见的比如高斯过程、高斯混合模型等等。传统数学推导中往往使用高斯过程，但现在最普及的优化库中基本都默认使用基于高斯混合模型的TPE过程。
在第4步中用来确定下一个观测点的规则被称为采集函数（Aquisition Function），采集函数衡量观测点对拟合𝑓*所产生的影响，并选取影响最大的点执行下一步观测，因此我们往往关注采集函数值最大的点。最常见的采集函数主要是概率增量PI（Probability of improvement，比如我们计算的频数）、期望增量（Expectation Improvement）、置信度上界（Upper Confidence Bound）、信息熵（Entropy）等等。上方gif图像当中展示了PI、UCB以及EI。其中大部分优化库中默认使用期望增量。
在HPO中使用贝叶斯优化时，我们常常会看见下面的图像，这张图像表现了贝叶斯优化的全部基本元素，我们的目标就是在采集函数指导下，让𝑓*尽量接近𝑓(𝑥)。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/0e84ac05be9c44ad3cdfa2728c952a4a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-28T10:18:51+08:00" />
<meta property="article:modified_time" content="2023-07-28T10:18:51+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">超参数优化 - 贝叶斯优化基础方法</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p>        </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%C2%A01.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B-toc" style="margin-left:80px;"><a href="#%C2%A01.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B" rel="nofollow"> 1. 贝叶斯优化的基本流程</a></p> 
<p id="%C2%A02.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%94%A8%E4%BA%8EHPO-toc" style="margin-left:80px;"><a href="#%C2%A02.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%94%A8%E4%BA%8EHPO" rel="nofollow"> 2. 贝叶斯优化用于HPO</a></p> 
<hr id="hr-toc"> 
<p></p> 
<p>        在之前的学习中我们了解了网格搜索、随机网格搜索与Halving网格搜索，无论具体每种网格搜索的思想如何变化，网格优化都是在一个大参数空间中、尽量对所有点进行验证后再返回最优损失函数值的方法，这一类方法在计算量与计算时间上有着不可避免的缺陷，因此才会有随机、Halving等试图缩短训练时间、让整体网格搜索更加适合于大型数据和大型空间的手段。然而，尽管sklearn在提高网格搜索效率方面做出了种种优化，但上述方法仍然无法在效率和精度上做到双赢，若希望更快速的进行参数搜索、并且搜索出一组泛化能力尽可能强的参数，目前的常见做法还是选用一些带有<span style="background-color:#dad5e9;">先验过程</span>的调参工具，即一些基于贝叶斯过程调参工具。</p> 
<blockquote> 
 <p>ps：先验过程是指在观测到任何数据之前，对模型参数或未知变量的分布进行建模。它是基于先前的经验或领域知识，以及模型的先验假设，来描述参数或变量的分布。先验过程可以帮助我们在没有足够数据时对模型进行初始估计，或者在数据不足或不完整的情况下提供一种合理的先验推断。在贝叶斯统计中，先验过程与后验过程共同构成了贝叶斯推断的基础。</p> 
</blockquote> 
<h4 id="%C2%A01.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"> 1. 贝叶斯优化的基本流程</h4> 
<p>        首先，假设现在知道一个函数𝑓(𝑥))的表达式以及其自变量𝑥的定义域，现在，我们希望求解出𝑥的取值范围上𝑓(𝑥)的最小值，你打算如何求解这个最小值呢？面对这个问题，无论是从单纯的数学理论角度，还是从机器学习的角度，我们都已经见过好几个通俗的思路：</p> 
<p>① 对𝑓(𝑥)求导、令其一阶导数为0来求解其最小值。要求：<span style="color:#494949;"><span style="background-color:#dad5e9;">函数𝑓(𝑥)可微，且微分方程可以直接被求解</span>。</span></p> 
<p>② 通过梯度下降等优化方法迭代出𝑓(𝑥)的最小值。要求：<span style="color:#494949;"><span style="background-color:#dad5e9;">函数𝑓(𝑥)可微，且函数本身为凸函数</span>。</span></p> 
<p>③ 将全域的𝑥带入𝑓(𝑥)计算出所有可能的结果，再找出最小值。要求：<span style="color:#494949;"><span style="background-color:#dad5e9;">函数𝑓(𝑥)相对不复杂、自变量维度相对低、计算量可以承受</span>。</span></p> 
<p><span style="color:#494949;">        </span>当我们知道函数𝑓(𝑥)的表达式时，以上方法常常能够有效，但每个方法都有自己的前提条件。假设现在函数𝑓(𝑥)是一个平滑均匀的函数，但它异常复杂、且不可微，我们无法使用上述三种方法中的任意一种方法求解，但我们还是想求解其最小值，可以怎么办呢？由于函数异常复杂，带入任意𝑥计算的所需的时间很长，所以我们不太可能将全域𝑥都带入进行计算，但我们还是可以从中随机抽样部分观测点来观察整个函数可能存在的趋势。于是我们选择在𝑥的定义域上随机选择了4个点，并将4个点带入𝑓(𝑥)进行计算，得到了如下结果：</p> 
<p class="img-center"><img alt="01" height="304" src="https://images2.imgbox.com/ea/aa/H4U4y4z4_o.png" width="676"></p> 
<p>        当我们有了4个观测值，并且知道我们的函数时相对均匀、平滑的函数，那我们可能对函数的整体分布有如下猜测： </p> 
<p class="img-center"><img alt="02" height="293" src="https://images2.imgbox.com/70/4c/y1424WdQ_o.png" width="662"></p> 
<p>         当我们对函数整体分布有一个猜测时，这个分布上一定会存在该函数的最小值。同时，不同的人可能对函数的整体分布有不同的猜测，不同猜测下对应的最小值也是不同的。</p> 
<p class="img-center"><img alt="03" height="280" src="https://images2.imgbox.com/15/16/7yOAgnke_o.png" width="649"></p> 
<p class="img-center"><img alt="04" height="282" src="https://images2.imgbox.com/db/cb/LSyMoULU_o.png" width="652"></p> 
<p>         现在，假设我们邀请了数万个人对该问题做出猜测，每个人所猜测的曲线如下图所示。不难发现，在观测点的附近，每个人猜测的函数值差距不大，但是在远离远侧点的地方，每个人猜测的函数值就高度不一致了。这也是当然的，因为观测点之间函数的分布如何完全是未知的，并且该分布离观测点越远时，我们越不确定真正的函数值在哪里，因此人们猜测的函数值的范围非常巨大。</p> 
<p class="img-center"><img alt="05" height="280" src="https://images2.imgbox.com/72/db/gBvoPiIw_o.png" width="643"></p> 
<p>        现在，将所有猜测求均值，并将任意均值周围的潜在函数值所在的区域用色块表示，可以得到一条所有人猜测的平均曲线。不难发现，色块所覆盖的范围其实就是大家猜测的函数值的上界和下界，而任意𝑥所对应的上下界差异越大，表示人们对函数上该位置的猜测值的越不确定。因此<span style="background-color:#dad5e9;">上下界差异可以衡量人们对该观测点的置信度，色块范围越大，置信度越低</span>。 </p> 
<p class="img-center"><img alt="" height="282" src="https://images2.imgbox.com/3b/25/40W9COYf_o.png" width="643"></p> 
<p>        在观测点周围，置信度总是很高的，远离观测点的地方，置信度总是很低，所以如果我们能够在置信度很低的地方补充一个实际的观测点，我们就可以很快将众人的猜测统一起来。以下图为例，当我们在置信度很低的区间内取一个实际观测值时，围绕该区间的“猜测”会立刻变得集中，该区间内的置信度会大幅升高。</p> 
<p class="img-center"><img alt="" height="412" src="https://images2.imgbox.com/01/d4/5JKS9D7I_o.png" width="1200"></p> 
<p>         当整个函数上的置信度都非常高时，我们可以说我们得出了一条与真实的𝑓(𝑥)曲线高度相似的曲线𝑓*，此时我们就可以将𝑓*的最小值当作真实𝑓(𝑥)的最小值来看待。自然，如果估计越准确，𝑓*越接近𝑓(𝑥)，则𝑓*的最小值也会越接近于𝑓(𝑥)的真实最小值。那如何才能够让𝑓*更接近𝑓(𝑥)呢？根据我们刚才提升置信度的过程，很明显——观测点越多，我们估计出的曲线会越接近真实的𝑓(𝑥)。然而，由于计算量有限，我们每次进行观测时都要非常谨慎地选择观测点。那如何选择观测点才能够最大程度地帮助我们估计出𝑓(𝑥)的最小值呢？</p> 
<p>        有非常多的方法，其中最简单的手段是使用<span style="background-color:#dad5e9;">最小值出现的频数</span>进行判断。由于不同的人对函数的整体分布有不同的猜测，不同猜测下对应的最小值也是不同的，根据每个人猜测的函数结果，我们在𝑋轴上将定义域区间均匀划分为100个小区间，如果有某个猜测的最小值落在其中一个区间中，我们就对该区间进行计数（这个过程跟对离散型变量绘制直方图的过程完全一致）。当有数万个人进行猜测之后，我们同时也绘制了基于𝑋轴上不同区间的频数图，频数越高，说明猜测最小值在该区间内的人越多，反之则说明该猜测最小值在该区间内的人越少。<span style="background-color:#dad5e9;">该频数一定程度上反馈出最小值出现的概率，频数越高的区间，函数真正的最小值出现的概率越高。</span></p> 
<p class="img-center"><img alt="" height="444" src="https://images2.imgbox.com/00/74/3Sd0vwIY_o.png" width="691"></p> 
<p>当我们将𝑋轴上的区间划分得足够细后，绘制出的频数图可以变成概率密度曲线，<span style="background-color:#dad5e9;">曲线的最大值所对应的点是𝑓(𝑥)的最小值的概率最高</span>，因此很明显，我们应该将曲线最大值所对应的点确认为下一个观测点。根据图像，最小值最有可能在的区间就在x=0.7左右的位置。当我们不取新的观测点时，现在𝑓(𝑥)上可以获得的可靠的最小值就是x=0.6时的点，但我们如果在x=0.7处取新的观测值，我们就很有可能找到比当前x=0.6的点还要小的𝑓𝑚𝑖𝑛。因此，我们可以就此决定，在x=0.7处进行观测。 </p> 
<p class="img-center"><img alt="" height="421" src="https://images2.imgbox.com/aa/9e/IipNtgwp_o.png" width="652"></p> 
<p>        当我们在x=0.7处取出观测值之后，我们就有了5个已知的观测点。现在，我们再让数万人根据5个已知的观测点对整体函数分布进行猜测，猜测完毕之后再计算当前最小值频数最高的区间，然后再取新的观测点对𝑓(𝑥)进行计算。当允许的计算次数被用完之后（比如，500次），整个估计也就停止了。</p> 
<p>        在这个过程当中，我们其实在不断地优化我们对目标函数𝑓(𝑥)的估计，虽然没有对𝑓(𝑥)进行全部定义域上的计算，也没有找到最终确定一定是𝑓(𝑥)分布的曲线，但是随着观测的点越来越多，我们对函数的估计是越来越准确的，因此也有越来越大的可能性可以估计出𝑓(𝑥)真正的最小值。<span style="background-color:#dad5e9;">这个优化的过程，就是</span><strong><span style="background-color:#dad5e9;">贝叶斯优化</span></strong>。</p> 
<p class="img-center"><img alt="" height="789" src="https://images2.imgbox.com/20/98/YTEoYAXz_o.gif" width="700"></p> 
<h4 id="%C2%A02.%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96%E7%94%A8%E4%BA%8EHPO"> 2. 贝叶斯优化用于HPO</h4> 
<p>在贝叶斯优化的数学过程当中，我们主要执行以下几个步骤：</p> 
<p>        ① 定义需要估计的𝑓(𝑥)以及𝑥的定义域</p> 
<p>        ② 取出有限的n个𝑥上的值，求解出这些𝑥对应的𝑓(𝑥)（求解观测值）</p> 
<p>        ③ 根据有限的观测值，对函数进行估计（该假设被称为贝叶斯优化中的先验知识），得出该估计𝑓*上的目标值（最大值或最小值）</p> 
<p>        ④ 定义某种规则，以确定下一个需要计算的观测点</p> 
<p>并持续在②~④步骤中进行循环，直到假设分布上的目标值达到我们的标准，或者所有计算资源被用完为止（例如，最多观测m次，或最多允许运行t分钟）。</p> 
<p>以上流程又被称为序贯模型优化（SMBO），是最为经典的贝叶斯优化方法。在实际的运算过程当中，尤其是超参数优化的过程当中，有以下具体细节需要注意：</p> 
<ul><li> <p>当贝叶斯优化不被用于HPO时，一般𝑓(𝑥)可以是完全的黑盒函数（black box function，也译作黑箱函数，即只知道𝑥与𝑓(𝑥)的对应关系，却丝毫不知道函数内部规律、同时也不能写出具体表达式的一类函数），因此贝叶斯优化也被认为是可以作用于黑盒函数估计的一类经典方法。但在HPO过程当中，需要定义的𝑓(𝑥)一般是交叉验证的结果/损失函数的结果，而我们往往非常清楚损失函数的表达式，只是我们不了解损失函数内部的具体规律，因此HPO中的𝑓(𝑥)不能算是严格意义上的黑盒函数。</p> </li><li> <p>在HPO中，自变量𝑥就是超参数空间。在上述二维图像表示中，𝑥为一维的，但在实际进行优化时，超参数空间往往是高维且极度复杂的空间。</p> </li><li> <p>最初的观测值数量n、以及最终可以取到的最大观测数量m都是贝叶斯优化的超参数，最大观测数量m也决定了整个贝叶斯优化的迭代次数。</p> </li><li> <p>在第3步中，根据有限的观测值、对函数分布进行估计的工具被称为<strong><span style="background-color:#dad5e9;">概率代理模型</span></strong>（Probability Surrogate model），毕竟在数学计算中我们并不能真的邀请数万人对我们的观测点进行连线。这些概率代理模型自带某些假设，他们可以根据廖廖数个观测点估计出目标函数的分布𝑓*（包括𝑓*上每个点的取值以及该点对应的置信度）。在实际使用时，概率代理模型往往是一些强大的算法，最常见的比如高斯过程、高斯混合模型等等。传统数学推导中往往使用高斯过程，但现在最普及的优化库中基本都默认使用基于高斯混合模型的TPE过程。</p> </li><li> <p>在第4步中用来确定下一个观测点的规则被称为<strong><span style="background-color:#dad5e9;">采集函数</span></strong>（Aquisition Function），采集函数衡量观测点对拟合𝑓*所产生的影响，并选取影响最大的点执行下一步观测，因此我们往往关注<span style="background-color:#dad5e9;">采集函数值最大的点</span>。最常见的采集函数主要是概率增量PI（Probability of improvement，比如我们计算的频数）、期望增量（Expectation Improvement）、置信度上界（Upper Confidence Bound）、信息熵（Entropy）等等。上方gif图像当中展示了PI、UCB以及EI。其中大部分优化库中默认使用期望增量。</p> </li></ul> 
<p>        在HPO中使用贝叶斯优化时，我们常常会看见下面的图像，这张图像表现了贝叶斯优化的全部基本元素，我们的目标就是在采集函数指导下，让𝑓*尽量接近𝑓(𝑥)。</p> 
<p class="img-center"><img alt="" height="331" src="https://images2.imgbox.com/49/74/ojIPZUUd_o.png" width="754"></p> 
<p></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/39c8d6bfff5c1497bc9cbbc45a37e76e/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">复制Eclipse项目</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/98e204b1fcc90e173402371faba6508b/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">SOLIDWORKS磁力配合工具</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>