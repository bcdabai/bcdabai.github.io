<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS" />
<meta property="og:description" content="本文是LLM系列文章，针对《LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS》的翻译。
Longlora:长上下文大型语言模型的高效微调 摘要1 引言2 相关工作3 LongLoRA4 实验5 结论 摘要 我们提出了LongLoRA，一种有效的微调方法，以有限的计算成本扩展预训练的大型语言模型(llm)的上下文大小。通常，训练具有长上下文大小的llm在计算上是昂贵的，需要大量的训练时间和GPU资源。例如，在上下文长度为8192的情况下进行训练，自注意力层的计算成本是在上下文长度为2048的情况下的16倍。本文从两个方面加快LLM的语境扩展。一方面，虽然在推理过程中需要密集的全局关注，但通过稀疏的局部关注可以有效地对模型进行微调。所提出的转移短注意力(S2 -Attn)有效地支持上下文扩展，从而节省大量计算，性能与使用普通注意力进行微调相似。特别的是，它可以在训练中仅用两行代码实现，而在推理中是可选的。另一方面，我们重新审视了上下文扩展的参数有效微调机制。值得注意的是，我们发现LoRA在可训练的嵌入和规范化的前提下可以很好地进行上下文扩展。LongLoRA在LLaMA2模型从7B/13B到70B的各种任务上证明了强有力的实证结果。LongLoRA在单个8× A100机器上采用LLaMA2 7B从4k上下文到100k，或LLaMA2 70B到32k。LongLoRA扩展了模型的上下文，同时保留了它们原来的架构，并且与大多数现有技术兼容，比如FlashAttention-2。此外，为了使LongLoRA实用，我们收集了一个数据集LongQA，用于监督微调。它包含超过3k长的上下文问答对。我们所有的代码、模型、数据集和演示都可以在github.com/dvlab-research/LongLoRA上获得。
1 引言 2 相关工作 3 LongLoRA 4 实验 5 结论 在这项工作中，我们提出了LongLoRA，可以有效地扩展llm的上下文长度，使其显着变大。与标准的完全微调相比，LongLoRA具有更少的GPU内存成本和训练时间，并且具有最小的精度折衷。在体系结构层面，我们建议在训练期间将短暂注意力转移到接近标准的自注意力模式。转移短暂注意力很容易实现，只需要两行代码。此外，通过转移短注意力训练的模型在推理过程中保留了原始的标准注意力结构，使大多数预先存在的基础设施和优化可重用。在训练层面，我们用可训练的归一化和嵌入弥合了LoRA和完全微调之间的差距。我们的方法可以在一台8× A100机器上将LLaMA2 7B模型扩展到100k上下文长度，将70B模型扩展到32k上下文长度。我们认为LongLoRA是一种通用的方法，可以兼容更多类型的llm和位置编码，我们计划在未来进行研究。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/2ac978ddb6ccd769aabb9e845a0bf81a/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-10T09:09:53+08:00" />
<meta property="article:modified_time" content="2023-10-10T09:09:53+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <p>本文是LLM系列文章，针对《LONGLORA: EFFICIENT FINE-TUNING OF LONGCONTEXT LARGE LANGUAGE MODELS》的翻译。<br> </p> 
<div class="toc"> 
 <h4>Longlora:长上下文大型语言模型的高效微调</h4> 
 <ul><li><a href="#_2" rel="nofollow">摘要</a></li><li><a href="#1__4" rel="nofollow">1 引言</a></li><li><a href="#2__6" rel="nofollow">2 相关工作</a></li><li><a href="#3_LongLoRA_8" rel="nofollow">3 LongLoRA</a></li><li><a href="#4__10" rel="nofollow">4 实验</a></li><li><a href="#5__12" rel="nofollow">5 结论</a></li></ul> 
</div> 
<p></p> 
<h2><a id="_2"></a>摘要</h2> 
<p>我们提出了LongLoRA，一种有效的微调方法，以有限的计算成本扩展预训练的大型语言模型(llm)的上下文大小。通常，训练具有长上下文大小的llm在计算上是昂贵的，需要大量的训练时间和GPU资源。例如，在上下文长度为8192的情况下进行训练，自注意力层的计算成本是在上下文长度为2048的情况下的16倍。本文从两个方面加快LLM的语境扩展。一方面，虽然在推理过程中需要密集的全局关注，但通过稀疏的局部关注可以有效地对模型进行微调。所提出的转移短注意力(S2 -Attn)有效地支持上下文扩展，从而节省大量计算，性能与使用普通注意力进行微调相似。特别的是，它可以在训练中仅用两行代码实现，而在推理中是可选的。另一方面，我们重新审视了上下文扩展的参数有效微调机制。值得注意的是，我们发现LoRA在可训练的嵌入和规范化的前提下可以很好地进行上下文扩展。LongLoRA在LLaMA2模型从7B/13B到70B的各种任务上证明了强有力的实证结果。LongLoRA在单个8× A100机器上采用LLaMA2 7B从4k上下文到100k，或LLaMA2 70B到32k。LongLoRA扩展了模型的上下文，同时保留了它们原来的架构，并且与大多数现有技术兼容，比如FlashAttention-2。此外，为了使LongLoRA实用，我们收集了一个数据集LongQA，用于监督微调。它包含超过3k长的上下文问答对。我们所有的代码、模型、数据集和演示都可以在<a href="http://github.com/dvlab-research/LongLoRA">github.com/dvlab-research/LongLoRA</a>上获得。</p> 
<h2><a id="1__4"></a>1 引言</h2> 
<h2><a id="2__6"></a>2 相关工作</h2> 
<h2><a id="3_LongLoRA_8"></a>3 LongLoRA</h2> 
<h2><a id="4__10"></a>4 实验</h2> 
<h2><a id="5__12"></a>5 结论</h2> 
<p>在这项工作中，我们提出了LongLoRA，可以有效地扩展llm的上下文长度，使其显着变大。与标准的完全微调相比，LongLoRA具有更少的GPU内存成本和训练时间，并且具有最小的精度折衷。在体系结构层面，我们建议在训练期间将短暂注意力转移到接近标准的自注意力模式。转移短暂注意力很容易实现，只需要两行代码。此外，通过转移短注意力训练的模型在推理过程中保留了原始的标准注意力结构，使大多数预先存在的基础设施和优化可重用。在训练层面，我们用可训练的归一化和嵌入弥合了LoRA和完全微调之间的差距。我们的方法可以在一台8× A100机器上将LLaMA2 7B模型扩展到100k上下文长度，将70B模型扩展到32k上下文长度。我们认为LongLoRA是一种通用的方法，可以兼容更多类型的llm和位置编码，我们计划在未来进行研究。</p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3ba301ea3315b74629ece97ab5bb8bd3/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">轻松上手：＜Android Studio笔记应用开发＞（二）笔记可显示Part2:定义笔记的数据结构类型</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/1b6dac89312fff56d28e8bc9fe851a66/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">springboot &#43; shiro 配置 ehcache 缓存</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>