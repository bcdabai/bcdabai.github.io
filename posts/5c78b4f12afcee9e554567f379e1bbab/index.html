<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Batch Normalization、Layer Normalization 的总结与实现 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Batch Normalization、Layer Normalization 的总结与实现" />
<meta property="og:description" content="Batch Normalization、Layer Normalization 的总结与实现 一、前言 共同点：两者都属于归一化的方法，用在不同的场景下。Batch Normalization 从公式上来看是对不同样本张量在同一个维度（通道）进行归一化，能够降低不同样本之间的差异性。Layer Normalization 是对同一个样本里面不同的通道做归一化，降低了样本间特征的差异性。
关于贝塞尔校正，就是求方差的时候，是把累积的平方差值被 n 除还是被 n -1 除。使用了贝塞尔校正就是被 n - 1 除。
二、Batch Normalization（BN） 1. 简介 BN 一般在卷积层使用。
物理意义：让每张图片的相同通道的所有像素值使用相同的的均值和方差做归一化。
作用：
使得数据分布一致。使得每一层的输入都近似标准正态分布，缓解了内部协变量偏移问题。避免梯度爆炸、梯度消失。具有一定的正则化效果，可以防止过拟合。降低模型对网络参数的影响，使网络学习更稳定，降低参数初始化对模型训练的影响（因此可以选择较大的初始学习率）。 公式：
y = x − m e a n ( X ) ( v a r ( X ) &#43; e p s ) ∗ γ &#43; β y = \frac{x - mean(X)}{\sqrt(var(X) &#43; eps)} * \gamma&#43; \beta y=( ​var(X)&#43;eps)x−mean(X)​∗γ&#43;β" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/5c78b4f12afcee9e554567f379e1bbab/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-09-30T10:50:32+08:00" />
<meta property="article:modified_time" content="2023-09-30T10:50:32+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Batch Normalization、Layer Normalization 的总结与实现</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h2><a id="Batch_NormalizationLayer_Normalization__0"></a>Batch Normalization、Layer Normalization 的总结与实现</h2> 
<h3><a id="_1"></a>一、前言</h3> 
<p>  共同点：两者都属于归一化的方法，用在不同的场景下。Batch Normalization 从公式上来看是对不同样本张量在同一个维度（通道）进行归一化，能够降低不同样本之间的差异性。Layer Normalization 是对同一个样本里面不同的通道做归一化，降低了样本间特征的差异性。<br>   关于贝塞尔校正，就是求方差的时候，是把累积的平方差值被 n 除还是被 n -1 除。使用了贝塞尔校正就是被 n - 1 除。</p> 
<h3><a id="Batch_NormalizationBN_5"></a>二、Batch Normalization（BN）</h3> 
<h4><a id="1__6"></a>1. 简介</h4> 
<p>  BN 一般在卷积层使用。<br>   物理意义：让每张图片的相同通道的所有像素值使用相同的的均值和方差做归一化。<br>   作用：</p> 
<ul><li>使得数据分布一致。使得每一层的输入都近似标准正态分布，缓解了内部协变量偏移问题。</li><li>避免梯度爆炸、梯度消失。</li><li>具有一定的正则化效果，可以防止过拟合。</li><li>降低模型对网络参数的影响，使网络学习更稳定，降低参数初始化对模型训练的影响（因此可以选择较大的初始学习率）。</li></ul> 
<p>  公式：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
         
          
          
            x 
           
          
            − 
           
          
            m 
           
          
            e 
           
          
            a 
           
          
            n 
           
          
            ( 
           
          
            X 
           
          
            ) 
           
          
          
           
           
             ( 
            
           
          
            v 
           
          
            a 
           
          
            r 
           
          
            ( 
           
          
            X 
           
          
            ) 
           
          
            + 
           
          
            e 
           
          
            p 
           
          
            s 
           
          
            ) 
           
          
         
        
          ∗ 
         
        
          γ 
         
        
          + 
         
        
          β 
         
        
       
         y = \frac{x - mean(X)}{\sqrt(var(X) + eps)} * \gamma+ \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.557em; vertical-align: -1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.175em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.935em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mopen" style="padding-left: 1em;">(</span></span><span class="" style="top: -2.895em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
                   <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
                    <path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.305em;"><span class=""></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">an</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.13em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span></span><br>   ps：假设输入为 [N, C, H, W]，要计算的 x 处于第 i（i &lt; C）个通道上，则 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         e 
        
       
         a 
        
       
         n 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
      
        mean(X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">an</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> 是指将 N 个数据的第 i 个通道的所有像素值求均值，即一共要对 N * H * W 个数据求均值， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
      
        \gamma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        \beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span> 的维度均为 [1, C, 1, 1]，以对应 C 个通道。</p> 
<p><img src="https://images2.imgbox.com/64/2d/4T4r0ziE_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2__20"></a>2. 实现</h4> 
<p>  关于是否使用贝塞尔校正：<br>   numpy.std() ：无法设置是否使用贝塞尔校正，公式中未使用贝塞尔校正。<br>   torch.std()：unbiased 为 True 时使用贝塞尔校正，默认为 True。<br>   torch 调用：</p> 
<pre><code># 唯一需要输入的参数 num_features 是指输入的通道数 C
# torch 中 BatchNorm2d 的实现未使用贝塞尔校正
# affine 指定仿射参数（即公式中的 γ 和 β）是否可学习，若不可学习，γ 为 1，β 为 0
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)

# 调用 API
inputx = torch.randn(3, 3, 3, 3)
batch_norm_op = nn.BatchNorm2d(num_features=3)
bn_y = batch_norm_op(inputx)

# 手动实现
eps = 1e-5
mean = torch.mean(inputx, axis=(0, 2, 3), keepdims=True)
std = torch.std(inputx, axis=(0, 2, 3), keepdims=True,unbiased=False)
verify_bn_y = (inputx - mean) / torch.sqrt(std ** 2 + eps)
</code></pre> 
<pre><code># x：[N, C, H, W]
y = torch.mean(x, axis=(1))
# y 形状为 [N, H, W]，y[n0, h0, w0] = x[n0, :, h0, w0] 的所有元素求均值，即可以理解为按指定的轴取值然后求均值，axis 指定多个轴也是同理
</code></pre> 
<h3><a id="Layer_NormalizationLN_48"></a>二、Layer Normalization（LN）</h3> 
<h4><a id="1__49"></a>1. 简介</h4> 
<p>  作用与 BN 相同。<br>   公式：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
        
          y 
         
        
          = 
         
         
          
          
            x 
           
          
            − 
           
          
            m 
           
          
            e 
           
          
            a 
           
          
            n 
           
          
            ( 
           
          
            X 
           
          
            ) 
           
          
          
           
           
             ( 
            
           
          
            v 
           
          
            a 
           
          
            r 
           
          
            ( 
           
          
            X 
           
          
            ) 
           
          
            + 
           
          
            e 
           
          
            p 
           
          
            s 
           
          
            ) 
           
          
         
        
          ∗ 
         
        
          γ 
         
        
          + 
         
        
          β 
         
        
       
         y = \frac{x - mean(X)}{\sqrt(var(X) + eps)} * \gamma+ \beta 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 2.557em; vertical-align: -1.13em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em;"><span class="" style="top: -2.175em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.935em;"><span class="svg-align" style="top: -3.2em;"><span class="pstrut" style="height: 3.2em;"></span><span class="mopen" style="padding-left: 1em;">(</span></span><span class="" style="top: -2.895em;"><span class="pstrut" style="height: 3.2em;"></span><span class="hide-tail" style="min-width: 1.02em; height: 1.28em;"> 
                   <svg width="400em" height="1.28em" viewbox="0 0 400000 1296" preserveaspectratio="xMinYMin slice"> 
                    <path d="M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z"></path> 
                   </svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.305em;"><span class=""></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">p</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">an</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.13em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span></span><br>   ps：假设输入为 [N, C, H, W]， <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         e 
        
       
         a 
        
       
         n 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
      
        mean(X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">an</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         v 
        
       
         a 
        
       
         r 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
      
        var(X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> 分别计算指定通道上的均值和方差，且指定通道只能是最后一个或几个。在 CV 中，Layer Normalization 一般指定最后三个通道，即 [C, H, W] 这三个通道，此时，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
      
        y 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">y</span></span></span></span></span> 的形状为 [N]，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         m 
        
       
         e 
        
       
         a 
        
       
         n 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
      
        mean(X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">an</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> 一共需要对 C * H * W 个数据求均值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         v 
        
       
         a 
        
       
         r 
        
       
         ( 
        
       
         X 
        
       
         ) 
        
       
      
        var(X) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mclose">)</span></span></span></span></span> 同理。 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         γ 
        
       
      
        \gamma 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0556em;">γ</span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         β 
        
       
      
        \beta 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord mathnormal" style="margin-right: 0.0528em;">β</span></span></span></span></span> 的形状和指定通道相同，CV 中为 [C, H, W]。</p> 
<p><img src="https://images2.imgbox.com/5e/62/uiYpCWhd_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2__56"></a>2. 实现</h4> 
<p>  torch 调用：</p> 
<pre><code># 唯一需要输入的参数 normalized_shape 指定通道
# 未采用贝塞尔校正
# elementwise_affine 指定仿射参数（即公式中的 γ 和 β）是否可学习，若不可学习，γ 为 1，β 为 0
torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)

# 调用 API
inputx = torch.randn(2, 4, 3, 4)
layer_norm = nn.LayerNorm([4, 3, 4])
ln_y = layer_norm(inputx)

# 手动实现
eps = 1e-5
mean = torch.mean(inputx, axis=(1, 2, 3), keepdims=True)
std = torch.std(inputx, axis=(1, 2, 3), keepdims=True,unbiased=False)
verify_ln_y = (inputx - mean) / torch.sqrt(std ** 2 + eps)
</code></pre> 
<h3><a id="_76"></a>三、其它</h3> 
<p>  另外两种归一化方法，和以上两种方法类似，可以类比。</p> 
<h4><a id="1_Group_NormalizationGN_78"></a>1. Group Normalization（GN）</h4> 
<p>  顾图思意：</p> 
<p><img src="https://images2.imgbox.com/73/4e/IXJNqyS5_o.png" alt="在这里插入图片描述"></p> 
<h4><a id="2_Instance_NormalizationIN_83"></a>2. Instance Normalization（IN）</h4> 
<p>  顾图思意：<br> <img src="https://images2.imgbox.com/7d/2a/bTjD6TfR_o.png" alt="在这里插入图片描述"></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/52d94fedaaa9af60b2de32ecb8986f11/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">[c&#43;&#43;] [1043] 最高的分数</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/babc8ccd549995cc20152a8c5bb301b2/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">C中的编译和链接</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>