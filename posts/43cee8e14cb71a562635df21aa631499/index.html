<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>【9种】ElasticSearch分词器详解，一文get！！！| 博学谷狂野架构师 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="【9种】ElasticSearch分词器详解，一文get！！！| 博学谷狂野架构师" />
<meta property="og:description" content="ElasticSearch 分词器 作者: 博学谷狂野架构师GitHub：GitHub地址 （有我精心准备的130本电子书PDF） 只分享干货、不吹水，让我们一起加油！😄
概述 分词器的主要作用将用户输入的一段文本，按照一定逻辑，分析成多个词语的一种工具
什么是分词器 顾名思义，文本分析就是把全文本转换成一系列单词（term/token）的过程，也叫分词。在 ES 中，Analysis 是通过分词器（Analyzer） 来实现的，可使用 ES 内置的分析器或者按需定制化分析器。
举一个分词简单的例子：比如你输入 Mastering Elasticsearch，会自动帮你分成两个单词，一个是 mastering，另一个是 elasticsearch，可以看出单词也被转化成了小写的。
分词器的构成 分词器是专门处理分词的组件，分词器由以下三部分组成：
组成部分 character filter 接收原字符流，通过添加、删除或者替换操作改变原字符流
例如：去除文本中的html标签，或者将罗马数字转换成阿拉伯数字等。一个字符过滤器可以有零个或者多个
tokenizer 简单的说就是将一整段文本拆分成一个个的词。
例如拆分英文，通过空格能将句子拆分成一个个的词，但是对于中文来说，无法使用这种方式来实现。在一个分词器中,有且只有一个tokenizeer
token filters 将切分的单词添加、删除或者改变
例如将所有英文单词小写，或者将英文中的停词a删除等，在token filters中，不允许将token(分出的词)的position或者offset改变。同时，在一个分词器中，可以有零个或者多个token filters.
分词顺序 同时 Analyzer 三个部分也是有顺序的，从图中可以看出，从上到下依次经过 Character Filters，Tokenizer 以及 Token Filters，这个顺序比较好理解，一个文本进来肯定要先对文本数据进行处理，再去分词，最后对分词的结果进行过滤。
索引和搜索分词 文本分词会发生在两个地方：
创建索引：当索引文档字符类型为text时，在建立索引时将会对该字段进行分词。搜索：当对一个text类型的字段进行全文检索时，会对用户输入的文本进行分词。 配置分词器 默认ES使用standard analyzer，如果默认的分词器无法符合你的要求，可以自己配置
分词器测试 可以通过_analyzerAPI来测试分词的效果。
COPY# 过滤html 标签 POST _analyze { &#34;tokenizer&#34;:&#34;keyword&#34;, #原样输出 &#34;char_filter&#34;:[&#34;html_strip&#34;], # 过滤html标签 &#34;text&#34;:&#34;&lt;b&gt;hello world&lt;b&gt;&#34; # 输入的文本 } 指定分词器 使用地方 分词器的使用地方有两个：" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/43cee8e14cb71a562635df21aa631499/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-08T16:23:34+08:00" />
<meta property="article:modified_time" content="2023-05-08T16:23:34+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【9种】ElasticSearch分词器详解，一文get！！！| 博学谷狂野架构师</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-light">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <h3 id="elasticsearch-分词器">ElasticSearch 分词器</h3> 
<p><img src="https://images2.imgbox.com/4c/5b/Vp9yapeS_o.png" alt="img"></p> 
<ul><li><strong>作者:</strong> 博学谷狂野架构师</li><li><strong>GitHub：</strong><a href="https://i.tulcn.cn/clWnsM" rel="nofollow">GitHub地址</a> （有我精心准备的130本电子书PDF） 
  <blockquote> 
   <p>只分享干货、不吹水，让我们一起加油！😄</p> 
  </blockquote> </li></ul> 
<h4 id="概述">概述</h4> 
<blockquote> 
 <p>分词器的主要作用将用户输入的一段文本，按照一定逻辑，分析成多个词语的一种工具</p> 
</blockquote> 
<h5 id="什么是分词器">什么是分词器</h5> 
<blockquote> 
 <p>顾名思义，文本分析就是<strong>把全文本转换成一系列单词（term/token）的过程</strong>，也叫<strong>分词</strong>。在 ES 中，Analysis 是通过<strong>分词器（Analyzer）</strong> 来实现的，可使用 ES 内置的分析器或者按需定制化分析器。</p> 
</blockquote> 
<p> 举一个分词简单的例子：比如你输入 <code>Mastering Elasticsearch</code>，会自动帮你分成两个单词，一个是 <code>mastering</code>，另一个是 <code>elasticsearch</code>，可以看出单词也被转化成了小写的。</p> 
<p><img src="https://images2.imgbox.com/59/aa/mkS47CbY_o.png" alt="图片"></p> 
<h5 id="分词器的构成">分词器的构成</h5> 
<blockquote> 
 <p>分词器是专门处理分词的组件，分词器由以下三部分组成：</p> 
</blockquote> 
<h6 id="组成部分">组成部分</h6> 
<h6 id="character-filter">character filter</h6> 
<blockquote> 
 <p>接收原字符流，通过添加、删除或者替换操作改变原字符流</p> 
</blockquote> 
<p> 例如：去除文本中的html标签，或者将罗马数字转换成阿拉伯数字等。一个字符过滤器可以有<code>零个或者多个</code></p> 
<h6 id="tokenizer">tokenizer</h6> 
<blockquote> 
 <p>简单的说就是将一整段文本拆分成一个个的词。</p> 
</blockquote> 
<p> 例如拆分英文，通过空格能将句子拆分成一个个的词，但是对于中文来说，无法使用这种方式来实现。在一个分词器中,<code>有且只有一个</code>tokenizeer</p> 
<h6 id="token-filters">token filters</h6> 
<blockquote> 
 <p>将切分的单词添加、删除或者改变</p> 
</blockquote> 
<p> 例如将所有英文单词小写，或者将英文中的停词<code>a</code>删除等，在<code>token filters</code>中，不允许将<code>token(分出的词)</code>的<code>position</code>或者<code>offset</code>改变。同时，在一个分词器中，可以有零个或者多个<code>token filters</code>.</p> 
<h6 id="分词顺序">分词顺序</h6> 
<p><img src="https://images2.imgbox.com/ac/b3/XSN89RbN_o.png" alt="图片"></p> 
<p> 同时 Analyzer 三个部分也是有顺序的，从图中可以看出，从上到下依次经过 <code>Character Filters</code>，<code>Tokenizer</code> 以及 <code>Token Filters</code>，这个顺序比较好理解，一个文本进来肯定要先对文本数据进行处理，再去分词，最后对分词的结果进行过滤。</p> 
<h6 id="索引和搜索分词">索引和搜索分词</h6> 
<blockquote> 
 <p>文本分词会发生在两个地方：</p> 
</blockquote> 
<ul><li><code>创建索引</code>：当索引文档字符类型为<code>text</code>时，在建立索引时将会对该字段进行分词。</li><li><code>搜索</code>：当对一个<code>text</code>类型的字段进行全文检索时，会对用户输入的文本进行分词。</li></ul> 
<h6 id="配置分词器">配置分词器</h6> 
<blockquote> 
 <p>默认ES使用<code>standard analyzer</code>，如果默认的分词器无法符合你的要求，可以自己配置</p> 
</blockquote> 
<h6 id="分词器测试">分词器测试</h6> 
<blockquote> 
 <p>可以通过<code>_analyzer</code>API来测试分词的效果。</p> 
</blockquote> 
<pre><code>COPY# 过滤html 标签
POST _analyze
{
    "tokenizer":"keyword", #原样输出
    "char_filter":["html_strip"], # 过滤html标签
    "text":"&lt;b&gt;hello world&lt;b&gt;"  # 输入的文本
}</code></pre> 
<p><img src="https://images2.imgbox.com/ee/75/9pJ3IDU3_o.png" alt="image-20220808135231869"></p> 
<h5 id="指定分词器">指定分词器</h5> 
<h6 id="使用地方">使用地方</h6> 
<blockquote> 
 <p>分词器的使用地方有两个：</p> 
</blockquote> 
<ul><li>创建索引时</li><li>进行搜索时</li></ul> 
<h6 id="创建索引时指定分词器">创建索引时指定分词器</h6> 
<blockquote> 
 <p>如果设置手动设置了分词器，ES将按照下面顺序来确定使用哪个分词器：</p> 
</blockquote> 
<ul><li>先判断字段是否有设置分词器，如果有，则使用字段属性上的分词器设置</li><li>如果设置了<code>analysis.analyzer.default</code>，则使用该设置的分词器</li><li>如果上面两个都未设置，则使用默认的<code>standard</code>分词器</li></ul> 
<h6 id="字段指定分词器">字段指定分词器</h6> 
<blockquote> 
 <p>为title属性指定分词器</p> 
</blockquote> 
<pre><code>COPYPUT my_index
{
  "mappings": {
    "properties": {
      "title":{
        "type":"text",
        "analyzer": "whitespace"
      }
    }
  }
}</code></pre> 
<h6 id="设置默认分词器">设置默认分词器</h6> 
<pre><code>COPYPUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default":{
          "type":"simple"
        }
      }
    }
  }
}</code></pre> 
<h6 id="搜索时如何确定分词器">搜索时如何确定分词器</h6> 
<blockquote> 
 <p>在搜索时，通过下面参数依次检查搜索时使用的分词器：</p> 
</blockquote> 
<ul><li>搜索时指定<code>analyzer</code>参数</li><li>创建mapping时指定字段的<code>search_analyzer</code>属性</li><li>创建索引时指定<code>setting</code>的<code>analysis.analyzer.default_search</code></li><li>查看创建索引时字段指定的<code>analyzer</code>属性</li><li>如果上面几种都未设置，则使用默认的<code>standard</code>分词器。</li></ul> 
<h6 id="指定analyzer">指定analyzer</h6> 
<blockquote> 
 <p>搜索时指定analyzer查询参数</p> 
</blockquote> 
<pre><code>COPYGET my_index/_search
{
  "query": {
    "match": {
      "message": {
        "query": "Quick foxes",
        "analyzer": "stop"
      }
    }
  }
}</code></pre> 
<h6 id="指定字段analyzer">指定字段analyzer</h6> 
<pre><code>COPYPUT my_index
{
  "mappings": {
    "properties": {
      "title":{
        "type":"text",
        "analyzer": "whitespace",
        "search_analyzer": "simple"
      }
    }
  }
}</code></pre> 
<h6 id="指定默认default_seach">指定默认default_seach</h6> 
<pre><code>COPYPUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default":{
          "type":"simple"
        },
        "default_seach":{
          "type":"whitespace"
        }
      }
    }
  }
}</code></pre> 
<h5 id="内置分词器">内置分词器</h5> 
<blockquote> 
 <p>es在索引文档时，会通过各种类型 <code>Analyzer</code> 对text类型字段做分析，</p> 
</blockquote> 
<p> 不同的 <code>Analyzer</code> 会有不同的分词结果，内置的分词器有以下几种，基本上内置的 <code>Analyzer</code> 包括 <code>Language Analyzers</code> 在内，对中文的分词都不够友好，中文分词需要安装其它 <code>Analyzer</code></p> 
<table><thead><tr><th>分析器</th><th>描述</th><th>分词对象</th><th>结果</th></tr></thead><tbody><tr><td>standard</td><td>标准分析器是默认的分析器，如果没有指定，则使用该分析器。它提供了基于文法的标记化(基于 Unicode 文本分割算法，如 Unicode 标准附件 # 29所规定) ，并且对大多数语言都有效。</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog’s, bone ]</td></tr><tr><td>simple</td><td>简单分析器将文本分解为任何非字母字符的标记，如数字、空格、连字符和撇号、放弃非字母字符，并将大写字母更改为小写字母。</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]</td></tr><tr><td>whitespace</td><td>空格分析器在遇到空白字符时将文本分解为术语</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog’s, bone. ]</td></tr><tr><td>stop</td><td>停止分析器与简单分析器相同，但增加了删除停止字的支持。默认使用的是 <code>_english_</code> 停止词。</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]</td></tr><tr><td>keyword</td><td>不分词，把整个字段当做一个整体返回</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.]</td></tr><tr><td>pattern</td><td>模式分析器使用正则表达式将文本拆分为术语。正则表达式应该匹配令牌分隔符，而不是令牌本身。正则表达式默认为 <code>w+</code> (或所有非单词字符)。</td><td>The 2 QUICK Brown-Foxes jumped over the lazy dog’s bone.</td><td>[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]</td></tr><tr><td>多种西语系 arabic, armenian, basque, bengali, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english等等</td><td>一组旨在分析特定语言文本的分析程序。</td><td></td><td></td></tr></tbody></table> 
<h5 id="中文扩展分析器">中文扩展分析器</h5> 
<blockquote> 
 <p>中文分词器最简单的是ik分词器，还有jieba分词，哈工大分词器等</p> 
</blockquote> 
<table><thead><tr><th>分词器</th><th>描述</th><th>分词对象</th><th>结果</th></tr></thead><tbody><tr><td>ik_smart</td><td>ik分词器中的简单分词器，支持自定义字典，远程字典</td><td>学如逆水行舟，不进则退</td><td>[学如逆水行舟,不进则退]</td></tr><tr><td>ik_max_word</td><td>ik_分词器的全量分词器，支持自定义字典，远程字典</td><td>学如逆水行舟，不进则退</td><td>[学如逆水行舟,学如逆水,逆水行舟,逆水,行舟,不进则退,不进,则,退]</td></tr></tbody></table> 
<h4 id="词语分词">词语分词</h4> 
<h5 id="标准分词器（standard-tokenizer）">标准分词器（Standard Tokenizer）</h5> 
<blockquote> 
 <p>根据standardUnicode文本分段算法的定义，将文本划分为多个单词边界的上的术语</p> 
</blockquote> 
<p> 它是 ES <strong>默认的分词器</strong>，它会对输入的文本<strong>按词的方式进行切分</strong>，切分好以后会进行<strong>转小写</strong>处理，<strong>默认的 stopwords 是关闭的</strong>。</p> 
<p><img src="https://images2.imgbox.com/49/7f/JbPAvx49_o.png" alt="图片"></p> 
<h6 id="使用案例">使用案例</h6> 
<blockquote> 
 <p>下面使用 Kibana 看一下它是怎么样进行工作的</p> 
</blockquote> 
<h6 id="原始内容"><strong>原始内容</strong></h6> 
<pre><code>COPYIn 2020, Java is the best language in the world.</code></pre> 
<h6 id="测试分词">测试分词</h6> 
<blockquote> 
 <p>在 Kibana 的开发工具（Dev Tools）中指定 Analyzer 为 <code>standard</code>，并输入文本 <code>In 2020, Java is the best language in the world.</code>，然后我们运行一下：</p> 
</blockquote> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer": "standard"
}</code></pre> 
<p><img src="https://images2.imgbox.com/7b/de/twAEPjeB_o.png" alt="image-20220808105014779"></p> 
<p> 可以看出是按照空格、非字母的方式对输入的文本进行了转换，比如对 <code>Java</code> 做了转小写，对一些停用词也没有去掉，比如 <code>in</code>，其中 <code>token</code> 为分词结果；<code>start_offset</code> 为起始偏移；<code>end_offset</code> 为结束偏移；<code>position</code> 为分词位置。</p> 
<h6 id="可配置项">可配置项</h6> 
<table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>max_token_length</td><td>最大令牌长度。如果看到令牌超过此长度，则将其max_token_length间隔分割。默认为255。</td></tr><tr><td>stopwords</td><td>预定义的停用词列表，例如english或包含停用词列表的数组。默认为none。</td></tr><tr><td>stopwords_path</td><td>包含停用词的文件的路径。</td></tr></tbody></table> 
<pre><code>COPY{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_english_analyzer": {
                    "type": "standard",
                    "max_token_length": 5,
                    "stopwords": "_english_"
                }
            }
        }
    }
}</code></pre> 
<h5 id="简单分词器（letter-tokenizer）">简单分词器（Letter Tokenizer）</h5> 
<blockquote> 
 <p>当simple分析器遇到非字母的字符时，它会将文本划分为多个术语，它小写所有术语，对于中文和亚洲很多国家的语言来说是无用的</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/c1/da/DwH004yG_o.png" alt="图片"></p> 
<p> 它只包括了 <code>Lower Case</code> 的 <code>Tokenizer</code>，它会按照<strong>非字母切分</strong>，<strong>非字母的会被去除</strong>，最后对切分好的做<strong>转小写</strong>处理，然后接着用刚才的输入文本，分词器换成 <code>simple</code> 来进行分词，运行结果如下：</p> 
<h6 id="使用案例-1">使用案例</h6> 
<h6 id="原始内容-1">原始内容</h6> 
<pre><code>COPYIn 2020, Java is the best language in the world.</code></pre> 
<h6 id="测试分词-1">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer": "simple"
}</code></pre> 
<p><img src="https://images2.imgbox.com/9f/05/CFRRQBz6_o.png" alt="image-20220808105857762"></p> 
<h5 id="空白分词器（whitespace-tokenizer）">空白分词器（Whitespace Tokenizer）</h5> 
<blockquote> 
 <p>它非常简单，根据名称也可以看出是<strong>按照空格进行切分</strong>的</p> 
</blockquote> 
<p> 该whitespace分析仪将文本分为方面每当遇到任何空白字符，和上面的分词器不同，空白分词器默认并不会将内容转换为小写。</p> 
<p><img src="https://images2.imgbox.com/f6/3e/OBdSdAqe_o.png" alt="图片"></p> 
<h6 id="使用案例-2">使用案例</h6> 
<h6 id="原始内容-2">原始内容</h6> 
<pre><code>COPYIn 2020, Java is the best language in the world.</code></pre> 
<h6 id="测试分词-2">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer": "whitespace"
}</code></pre> 
<p><img src="https://images2.imgbox.com/45/84/5LXDxcWL_o.png" alt="image-20220808110235685"></p> 
<h5 id="电子邮件分词器（uax-url-email-tokenizer）">电子邮件分词器（UAX URL Email Tokenizer）</h5> 
<blockquote> 
 <p>此分词器主要是针对email和url地址进行关键内容的标记。</p> 
</blockquote> 
<h6 id="使用案例-3">使用案例</h6> 
<h6 id="原始内容-3">原始内容</h6> 
<pre><code>COPY"Email me at john.smith@global-international.com"</code></pre> 
<h6 id="测试分词-3">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"Email me at john.smith@global-international.com",
  "tokenizer": "uax_url_email"
}</code></pre> 
<h6 id="可配置项-1">可配置项</h6> 
<blockquote> 
 <p><code>max_token_length</code>最大令牌长度。如果看到令牌超过此长度，则将其max_token_length间隔分割。默认为255</p> 
</blockquote> 
<pre><code>COPY{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_english_analyzer": {
                    "type": "standard",
                    "max_token_length": 5
                }
            }
        }
    }
}</code></pre> 
<h5 id="经典分词器（classic-tokenizer）">经典分词器（Classic Tokenizer）</h5> 
<blockquote> 
 <p>可对首字母缩写词，公司名称，电子邮件地址和互联网主机名进行特殊处理，但是，这些规则并不总是有效，并且此关键词生成器不适用于英语以外的大多数其他语言</p> 
</blockquote> 
<h6 id="特点">特点</h6> 
<ul><li>它最多将标点符号拆分为单词，删除标点符号，但是，不带空格的点被认为是查询关键词的一部分</li><li>此分词器可以将邮件地址和URL地址识别为查询的term（词条）</li></ul> 
<h6 id="使用案例-4">使用案例</h6> 
<h6 id="原始内容-4">原始内容</h6> 
<pre><code>COPY"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."</code></pre> 
<h6 id="测试分词-4">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.",
  "analyzer": "classic"
}</code></pre> 
<p><img src="https://images2.imgbox.com/a5/b7/9DTudvRS_o.png" alt="image-20220808111755030"></p> 
<h6 id="可配置项-2">可配置项</h6> 
<blockquote> 
 <p><code>max_token_length</code>最大令牌长度。如果看到令牌超过此长度，则将其max_token_length间隔分割。默认为255。</p> 
</blockquote> 
<pre><code>COPY{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "my_tokenizer"
                }
            },
            "tokenizer": {
                "my_tokenizer": {
                    "type": "classic",
                    "max_token_length": 5
                }
            }
        }
    }
}</code></pre> 
<h4 id="结构化文本分词">结构化文本分词</h4> 
<h5 id="关键词分词器（keyword-tokenizer）">关键词分词器（Keyword Tokenizer）</h5> 
<blockquote> 
 <p>它其实不做分词处理，只是将输入作为 Term 输出</p> 
</blockquote> 
<p> 关键词分词器其实是执行了一个空操作的分析，它将任何输入的文本作为一个单一的关键词输出。</p> 
<p><img src="https://images2.imgbox.com/9a/d4/Rzrkd6dU_o.png" alt="图片"></p> 
<h6 id="使用案例-5">使用案例</h6> 
<h6 id="原始内容-5">原始内容</h6> 
<pre><code>COPY"In 2020, Java is the best language in the world."</code></pre> 
<h6 id="测试分词-5">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer": "keyword"
}</code></pre> 
<blockquote> 
 <p>会发现前后内容根本没有发生改变，这也是这个分词器的作用，有些时候我们针对一个需要分词查询的字段进行查询的时候，可能并不希望查询条件被分词，这个时候就可以使用这个分词器，整个查询条件作为一个关键词使用</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/23/9f/xIIVWknQ_o.png" alt="image-20220808112235674"></p> 
<h5 id="正则分词器（pattern-tokenizer）">正则分词器（Pattern Tokenizer）</h5> 
<blockquote> 
 <p>模式标记器使用 Java正则表达式。使用JAVA的正则表达式进行词语的拆分。</p> 
</blockquote> 
<p> 它可以通过<strong>正则表达式的方式进行分词</strong>，默认是用 <code>\W+</code> 进行分割的，也就是非字母的符合进行切分的。</p> 
<p><img src="https://images2.imgbox.com/c5/6c/eGNY2Mbk_o.png" alt="图片"></p> 
<h6 id="使用案例-6">使用案例</h6> 
<h6 id="原始内容-6">原始内容</h6> 
<pre><code>COPY"In 2020, Java is the best language in the world."</code></pre> 
<h6 id="测试分词-6">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer": "patter"
}</code></pre> 
<p><img src="https://images2.imgbox.com/f0/c0/mZJHeMMl_o.png" alt="image-20220808112737629"></p> 
<h6 id="可配置项-3">可配置项</h6> 
<blockquote> 
 <p>正则分词器有以下的选项</p> 
</blockquote> 
<table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>pattern</td><td>正则表达式</td></tr><tr><td>flags</td><td>正则表达式标识</td></tr><tr><td>lowercase</td><td>是否使用小写词汇</td></tr><tr><td>stopwords</td><td>停止词的列表。</td></tr><tr><td>stopwords_path</td><td>定义停止词文件的路径。</td></tr></tbody></table> 
<pre><code>COPY{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_email_analyzer": {
                    "type": "pattern",
                    "pattern": "\\W|_",
                    "lowercase": true
                }
            }
        }
    }
}</code></pre> 
<h5 id="路径分词器（path-tokenizer）">路径分词器（Path Tokenizer）</h5> 
<blockquote> 
 <p>可以对文件系统的路径样式的请求进行拆分，返回被拆分各个层级内容。</p> 
</blockquote> 
<h6 id="使用案例-7">使用案例</h6> 
<h6 id="原始内容-7">原始内容</h6> 
<pre><code>COPY"/one/two/three"</code></pre> 
<h6 id="测试分词-7">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"/one/two/three",
  "tokenizer":"path_hierarchy"
}</code></pre> 
<p><img src="https://images2.imgbox.com/88/45/qK2SzN91_o.png" alt="image-20220808113500717"></p> 
<h6 id="可配置项-4">可配置项</h6> 
<table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>delimiter</td><td>用作路径分隔符的字符</td></tr><tr><td>replacement</td><td>用于定界符的可选替换字符</td></tr><tr><td>buffer_size</td><td>单次读取到术语缓冲区中的字符数。默认为1024。术语缓冲区将以该大小增长，直到所有文本都被消耗完为止。建议不要更改此设置。</td></tr><tr><td>reverse</td><td>正向还是反向获取关键词</td></tr><tr><td>skip</td><td>要忽略的内容</td></tr></tbody></table> 
<pre><code>COPY{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "tokenizer": "my_tokenizer"
                }
            },
            "tokenizer": {
                "my_tokenizer": {
                    "type": "path_hierarchy",
                    "delimiter": "-",
                    "replacement": "/",
                    "skip": 2
                }
            }
        }
    }
}</code></pre> 
<h5 id="语言分词（language-analyzer）">语言分词（Language Analyzer）</h5> 
<blockquote> 
 <p>ES 为不同国家语言的输入提供了 <code>Language Analyzer</code> 分词器，在里面可以指定不同的语言</p> 
</blockquote> 
<h5 id="支持语种">支持语种</h5> 
<blockquote> 
 <p>支持如下语种：</p> 
</blockquote> 
<table><thead><tr><th>关键字</th><th>语种</th></tr></thead><tbody><tr><td>arabic</td><td>美 /ˈærəbɪk/ 阿拉伯语</td></tr><tr><td>armenian</td><td>美 /ɑːrˈmiːniən/ 亚美尼亚语</td></tr><tr><td>basque</td><td>美 /bæsk,bɑːsk/ 巴斯克语</td></tr><tr><td>bengali</td><td>美 /beŋˈɡɑːli/ 孟加拉语</td></tr><tr><td>brazilian</td><td>美 /brəˈzɪliən/ 巴西语</td></tr><tr><td>bulgarian</td><td>美 /bʌlˈɡeriən/ 保加利亚语</td></tr><tr><td>catalan</td><td>美 /ˈkætəlæn/ 加泰罗尼亚语</td></tr><tr><td>cjk</td><td>中日韩统一表意文字</td></tr><tr><td>czech</td><td>美 /tʃek/ 捷克语</td></tr><tr><td>danish</td><td>美 /ˈdeɪnɪʃ/ 丹麦语</td></tr><tr><td>dutch</td><td>美 /dʌtʃ/ 荷兰语</td></tr><tr><td>english</td><td>美 /ˈɪŋɡlɪʃ/ 英语</td></tr><tr><td>estonian</td><td>美 /eˈstoʊniən/ 爱沙尼亚语</td></tr><tr><td>finnish</td><td>美 /ˈfɪnɪʃ/ 芬兰语</td></tr><tr><td>french</td><td>美 /frentʃ/ 法语</td></tr><tr><td>galician</td><td>美 /ɡəˈlɪʃn/ 加里西亚语</td></tr><tr><td>german</td><td>美 /ˈdʒɜːrmən/ 德语</td></tr><tr><td>greek</td><td>美 /ɡriːk/ 希腊语</td></tr><tr><td>hindi</td><td>美 /ˈhɪndi/ 北印度语</td></tr><tr><td>hungarian</td><td>美 /hʌŋˈɡeriən/ 匈牙利语</td></tr><tr><td>indonesian</td><td>美 /ˌɪndəˈniːʒn/ 印度尼西亚语</td></tr><tr><td>irish</td><td>美 /ˈaɪrɪʃ/ 爱尔兰语</td></tr><tr><td>italian</td><td>美 /ɪˈtæliən/ 意大利语</td></tr><tr><td>latvian</td><td>美 /ˈlætviən/ 拉脱维亚语</td></tr><tr><td>lithuanian</td><td>美 /ˌlɪθuˈeɪniən/ 立陶宛语</td></tr><tr><td>norwegian</td><td>美 /nɔːrˈwiːdʒən/ 挪威语</td></tr><tr><td>persian</td><td>/‘pɜːrʒən/ 波斯语</td></tr><tr><td>portuguese</td><td>美 /ˌpɔːrtʃʊˈɡiːz/ 葡萄牙语</td></tr><tr><td>romanian</td><td>美 /ro’menɪən/ 罗马尼亚语</td></tr><tr><td>russian</td><td>美 /ˈrʌʃn/ 俄语</td></tr><tr><td>sorani</td><td>索拉尼语</td></tr><tr><td>spanish</td><td>美 /ˈspænɪʃ/ 西班牙语</td></tr><tr><td>swedish</td><td>美 /ˈswiːdɪʃ/ 瑞典语</td></tr><tr><td>turkish</td><td>美 /ˈtɜːrkɪʃ/ 土耳其语</td></tr><tr><td>thai</td><td>美 /taɪ/ 泰语</td></tr></tbody></table> 
<h6 id="使用案例-8">使用案例</h6> 
<blockquote> 
 <p>下面我们使用英语进行分析</p> 
</blockquote> 
<h6 id="原始内容-8">原始内容</h6> 
<pre><code>COPY"In 2020, Java is the best language in the world."</code></pre> 
<h6 id="测试分词-8">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "text":"In 2020, Java is the best language in the world.",
  "analyzer":"english"
}</code></pre> 
<p><img src="https://images2.imgbox.com/cc/5f/2NSCUcmT_o.png" alt="image-20220808114216744"></p> 
<h5 id="自定义分词器">自定义分词器</h5> 
<blockquote> 
 <p>当内置的分词器无法满足需求时，可以创建<code>custom</code>类型的分词器。</p> 
</blockquote> 
<h6 id="配置参数">配置参数</h6> 
<table><thead><tr><th>参数</th><th>描述</th></tr></thead><tbody><tr><td>tokenizer</td><td>内置或定制的tokenizer.(必须)</td></tr><tr><td>char_filter</td><td>内置或定制的char_filter(非必须)</td></tr><tr><td>filter</td><td>内置或定制的token filter(非必须)</td></tr><tr><td>position_increment_gap</td><td>当值为文本数组时，设置改值会在文本的中间插入假空隙。设置该属性，对与后面的查询会有影响。默认该值为100.</td></tr></tbody></table> 
<h6 id="创建索引">创建索引</h6> 
<blockquote> 
 <p>上面的示例中定义了一个名为<code>my_custom_analyzer</code>的分词器</p> 
</blockquote> 
<p> 该分词器的<code>type</code>为<code>custom</code>，<code>tokenizer</code>为<code>standard</code>，<code>char_filter</code>为<code>hmtl_strip</code>,<code>filter</code>定义了两个分别为：<code>lowercase</code>和<code>asciifolding</code></p> 
<pre><code>COPYPUT my_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer":{
          "type":"custom",
          "tokenizer":"standard",
          "char_filter":["html_strip"],
          "filter":["lowercase","asciifolding"]
        }
      }
    }
  }
}</code></pre> 
<h6 id="使用案例-9">使用案例</h6> 
<h6 id="原始内容-9">原始内容</h6> 
<pre><code>COPYIs this &lt;b&gt;déjà vu&lt;/b&gt;?</code></pre> 
<h6 id="测试分词-9">测试分词</h6> 
<pre><code>COPYPOST my_index/_analyze
{
  "text": "Is this &lt;b&gt;déjà vu&lt;/b&gt;?",
  "analyzer": "my_custom_analyzer"
}</code></pre> 
<p><img src="https://images2.imgbox.com/d1/79/szlLIrgB_o.png" alt="image-20220808140137712"></p> 
<h4 id="中文分词器">中文分词器</h4> 
<h5 id="ikanalyzer">IKAnalyzer</h5> 
<blockquote> 
 <p>IKAnalyzer是一个开源的，基于java的语言开发的轻量级的中文分词工具包</p> 
</blockquote> 
<p> 从2006年12月推出1.0版开始，IKAnalyzer已经推出了3个大版本，在 2012 版本中，IK 实现了简单的分词歧义排除算法，标志着 IK 分词器从单纯的词典分词向模拟语义分词衍化</p> 
<h5 id="使用ik分词器">使用IK分词器</h5> 
<blockquote> 
 <p>IK提供了两个分词算法:</p> 
</blockquote> 
<ul><li>ik_smart：最少切分。</li><li>ik_max_word：最细粒度划分。</li></ul> 
<h5 id="ik_smart">ik_smart</h5> 
<h6 id="使用案例-10">使用案例</h6> 
<h6 id="原始内容-10">原始内容</h6> 
<pre><code>COPY传智教育的教学质量是杠杠的</code></pre> 
<h6 id="测试分词-10">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "analyzer": "ik_smart",
  "text": "传智教育的教学质量是杠杠的"
}</code></pre> 
<p><img src="https://images2.imgbox.com/5e/18/HwyjBwHI_o.png" alt="image-20220808115450647"></p> 
<h5 id="ik_max_word">ik_max_word</h5> 
<h6 id="使用案例-11">使用案例</h6> 
<h6 id="原始内容-11">原始内容</h6> 
<pre><code>COPY传智教育的教学质量是杠杠的</code></pre> 
<h6 id="测试分词-11">测试分词</h6> 
<pre><code>COPYGET _analyze
{
  "analyzer": "ik_max_word",
  "text": "传智教育的教学质量是杠杠的"
}</code></pre> 
<p><img src="https://images2.imgbox.com/30/73/vF5FjpAw_o.png" alt="image-20220808115513668"></p> 
<h5 id="自定义词库">自定义词库</h5> 
<blockquote> 
 <p>我们在使用IK分词器时会发现其实有时候分词的效果也并不是我们所期待的</p> 
</blockquote> 
<h6 id="问题描述">问题描述</h6> 
<p> 例如我们输入“传智教育的教学质量是杠杠的”，但是分词器会把“传智教育”进行拆开，分为了“传”，“智”，“教育”，但我们希望的是<strong>“传智教育”可以不被拆开</strong>。</p> 
<p><img src="https://images2.imgbox.com/a5/e3/i5uMK233_o.png" alt="image-20220808115543696"></p> 
<h6 id="解决方案">解决方案</h6> 
<blockquote> 
 <p>对于以上的问题，我们只需要将自己要保留的词，加到我们的分词器的字典中即可</p> 
</blockquote> 
<h6 id="编辑字典内容">编辑字典内容</h6> 
<blockquote> 
 <p>进入elasticsearch目录<code>plugins/ik/config</code>中，创建我们自己的字典文件<code>yixin.dic</code>，并添加内容：</p> 
</blockquote> 
<pre><code>COPYcd plugins/ik/config
echo "传智教育" &gt; custom.dic</code></pre> 
<h6 id="扩展字典">扩展字典</h6> 
<blockquote> 
 <p>进入我们的elasticsearch目录 ：<code>plugins/ik/config</code>，打开<code>IKAnalyzer.cfg.xml</code>文件，进行如下配置：</p> 
</blockquote> 
<pre><code>COPYvi IKAnalyzer.cfg.xml
#增加如下内容
&lt;entry key="ext_dict"&gt;custom.dic&lt;/entry&gt;</code></pre> 
<h6 id="再次测试">再次测试</h6> 
<blockquote> 
 <p>重启ElasticSearch，再次使用kibana测试</p> 
</blockquote> 
<pre><code>COPYGET _analyze
{
  "analyzer": "ik_max_word",
  "text": "传智教育的教学质量是杠杠的"
}</code></pre> 
<blockquote> 
 <p>可以发现，现在我们的词汇”传智教育”就不会被拆开了，达到我们想要的效果了</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/4b/06/0hkPHjYe_o.png" alt="image-20220808134046401"></p> 
<blockquote> 
 <p>本文由<code>传智教育博学谷狂野架构师</code>教研团队发布。</p> 
 <p>如果本文对您有帮助，欢迎<code>关注</code>和<code>点赞</code>；如果您有任何建议也可<code>留言评论</code>或<code>私信</code>，您的支持是我坚持创作的动力。</p> 
 <p><strong>转载请注明出处！</strong></p> 
</blockquote>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/98b11bef5d5ab63f84c4abe18a734d6a/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">控制台flask db init执行后，出现flask : 无法将“flask”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次。</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/b3d1485440c1219a648246b1d585fed6/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Java使用poi导出excel针对不同数据列配置设置不同单元格格式(适用于通用导出excel数据)</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>