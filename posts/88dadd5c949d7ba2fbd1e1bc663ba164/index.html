<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>ResNet论文解读及代码实现（pytorch） - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="ResNet论文解读及代码实现（pytorch）" />
<meta property="og:description" content="又重新看了一遍何凯明大神的残差网络，之前懵懵懂懂的知识豁然开朗了起来。然后，虽然现在CSDN和知乎的风气不是太好，都是一些复制粘贴别人的作品来给自己的博客提高阅读量的人，但是也可以从其中汲取到很多有用的知识，我们要取其精华，弃其糟粕。
我只是大概的记录一下ResNet论文讲了什么，希望大家还是可以自己去读几遍。
ResNet论文链接为：https://arxiv.org/abs/1512.03385
1.前言 在读这篇文章之前，希望可以思考一个问题。残差网络到底是用来干什么的？我想很多人思考过后后的回答就是“残差网络不就是解决过深的网络引起的梯度消失和梯度爆炸这种现象嘛。”
这个回答是没问题的，但是梯度消失和梯度爆炸可以通过归一化初始化或中间层归一化来解决，还有最重要的一个原因就是过深的网络会出现网络退化的问题。何为网络退化？这里拿出原论文中的一个插图来解释。
按照正常的逻辑来说，神经网络越深训练的效果应该会越好啊，但实验推翻了我们这个结论。
假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。
问题就出现在这里，什么都不做就是神经网络最难做到的东西。
2. 论文解读 1. 摘要 在摘要中，作者主要是提出了一种残差结构，这种结构不需要额外的参数也更容易被优化，结果证明加了这种结构后不管是在ImageNet分类数据集上还是在COCO目标检测数据集上都有很好的效果，啊，不对，不是很好，是在大赛中都取得了第一名。
2. 引言 第一段主要介绍了一些研究背景，深度卷积网络为图像分类带来了一系列的突破。网络深度对训练模型来说也是至关重要的。
作者在第二段就表明了，随着深度的增加会出现梯度消失、梯度爆炸，但是这个问题已经在很大程度上通过归一化初始化和中间归一化层得到了解决。
第三段提出：随着网络深度的增加，精度达到饱和(这可能并不奇怪)，然后迅速退化。然而，这种退化并不是由过拟合引起的，在适当深度的模型上添加更多的层会导致更高的训练误差。
第四段提出了一个名词identity mapping，我理解为输入为X，输出也为X。构造深的模型，虽然有解决方案，但是在可行时间内是不可能实现的。
第五段提出了残差网络的结构
X 为浅层网络的输出。如果我们想要得到的映射为H(X)，则我们让添加的非线性网络层去拟合残差映射F(X):=H(X)-X。原始的映射就可以写成F(X)&#43;X。图二中右侧连接为一个identity mapping。
第六段提出了shortcut connection的概念。这种连接可以跳过一层或更多层，因为F(X)和X是直接相加的。所以不需要额外的参数和更复杂的计算。
剩余部分都是在说这个模型有多好，在100层，甚至1000层的模型上效果都很好。
我们假设第一个网络在训练集和测试集上可以得到很好的性能（甚至可以理解为接近100%）。
那么在这个新的网络，由于我们copy了前四层的参数，理论上前四层已经足够满足我们的性能要求，那么新增加的层便显得有些多余，如果这个新的网络也要达到性能100%，则新增加的层要做的事情就是“恒等映射”，也即后面几个紫色的层要实现的效果为 。这样一来，网络的性能一样能达到100%。而退化现象也表明了，实际上新增加的几个紫色的层，很难做到恒等映射。又或者能做到，但在有限的时间内很难完成（即网络要用指数级别的时间才能达到收敛）。这时候，巧妙的通过添加”桥梁“，使得难以优化的问题瞬间迎刃而解。
可以看到通过添加这个桥梁，把数据原封不动得送到FC层的前面，而对于中间的紫色层，可以很容易的通过把这些层的参数逼近于0，进而实现的功能。
实际上，网络性能通常未能达到100%，可以假设最初的网络（只有前四层）的性能到了98%等等，如果不添加跳连接，增加三个紫色层之后的新网络同样难以进行优化（由上面极端情况的推广，也即前面四层的性能达到100%）。
而通过跳连接，可以把前四层的输出先送到FC层前面，也就相当于告诉紫色层：”兄弟你放心，我已经做完98%的工作了，你看看能不能在剩下的2%中发点力，你要是找不出提升性能的效果也没事的，我们可以把你的参数逼近于0，所以放心大胆的找吧。&#34;
我们把整个映射看成100%，则前面四层网络实现了98%的映射关系，而残余的映射由紫色层完成，Residual 另一个翻译就是&#34;残余，残留“的意思，也就是让每一个残差块，只关注残余映射的一小部分，真的是恰到好处。
当然了，实际上网络运行的时候，我们并不会知道哪几层就能达到很好的效果，然后在它们的后面接一个跳连接，于是一开始便在两个层或者三个层之间添加跳连接，形成残差块，每个残差块只关注当前的残余映射，而不会关注前面已经实现的底层映射。
不得不佩服他对神经网络的深入理解，从他灵感的来源，让我感觉他就是个数学大佬，结果一查还真是，本科是清华基础科学班的（研究物理数学的）（拿烟的手微微颤抖）。好了，废话不多说，让我们一起来理解什么是残差网络。我们先来看一个现象，假设我们有如下的一个网络，它可以在训练集和测试
这个例子来自于知乎大佬Sakura当时我看到后真的是茅塞顿开。
作者：Sakura
链接：https://www.zhihu.com/question/306135761/answer/2491142607
来源：知乎
3. 相关工作 相关工作是说在这篇论文之前已经有一些人在某些方向上使用本论文中使用的一些方法去做事。这句话很拗口，但是只要你理解了，就会发现所有论文的相关方向都是说的这东西。
4. 深度残差网络 这部分就是介绍了残差模块的构成还有各种ResNet的网络结构。
在上文中，我们说道F(X)和X直接相加，因此需要保证他们的维度一定要一样，否则就对X做投影。
关于残差网络结构（以34层为例），只截取了一部分
图右中的实线表示残差连接，虚线表示升维。
架构图：
更深的ResNet网络：
50层，101层，152层都使用图右中的结构，使用1X1的卷积主要是为了降维和升维。右边结构的参数量明显比左边的少。
总结 网络亮点：
超深的网络结构(突破1000层)提出residual模块使用Batch Normalization加速训练(丢弃dropout)
沐神很形象的描述出了为什么加一个残差连接就可以收敛。 代码实现 只是model模块
import torch.nn as nn import torch &#39;&#39;&#39; 对应18层，34层的残差结构 &#39;&#39;&#39; class BasicBlock(nn." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/88dadd5c949d7ba2fbd1e1bc663ba164/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-06-09T14:35:14+08:00" />
<meta property="article:modified_time" content="2022-06-09T14:35:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ResNet论文解读及代码实现（pytorch）</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="markdown_views prism-atom-one-dark">
                    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
                        <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
                    </svg>
                    <blockquote> 
 <p>又重新看了一遍何凯明大神的残差网络，之前懵懵懂懂的知识豁然开朗了起来。然后，虽然现在CSDN和知乎的风气不是太好，都是一些复制粘贴别人的作品来给自己的博客提高阅读量的人，但是也可以从其中汲取到很多有用的知识，我们要取其精华，弃其糟粕。</p> 
</blockquote> 
<p>我只是大概的记录一下ResNet论文讲了什么，希望大家还是可以自己去读几遍。<br> ResNet论文链接为：<a href="https://arxiv.org/abs/1512.03385" rel="nofollow">https://arxiv.org/abs/1512.03385</a></p> 
<h2><a id="1_7"></a>1.前言</h2> 
<p>在读这篇文章之前，希望可以思考一个问题。残差网络到底是用来干什么的？我想很多人思考过后后的回答就是“残差网络不就是解决过深的网络引起的<strong>梯度消失和梯度爆炸</strong>这种现象嘛。”<br> 这个回答是没问题的，但是梯度消失和梯度爆炸可以通过<strong>归一化初始化或中间层归一</strong>化来解决，还有最重要的一个原因就是过深的网络会出现<strong>网络退化</strong>的问题。何为网络退化？这里拿出原论文中的一个插图来解释。<br> <img src="https://images2.imgbox.com/0e/3e/HeG8gmWG_o.png" alt="在这里插入图片描述"><br> 按照正常的逻辑来说，神经网络越深训练的效果应该会越好啊，但实验推翻了我们这个结论。<br> 假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。<br> 问题就出现在这里，<strong>什么都不做</strong>就是神经网络最难做到的东西。</p> 
<h2><a id="2__14"></a>2. 论文解读</h2> 
<h3><a id="1__15"></a>1. 摘要</h3> 
<p>在摘要中，作者主要是提出了一种残差结构，这种结构不需要额外的参数也更容易被优化，结果证明加了这种结构后不管是在ImageNet分类数据集上还是在COCO目标检测数据集上都有很好的效果，啊，不对，不是很好，是在大赛中都取得了第一名。</p> 
<h3><a id="2__17"></a>2. 引言</h3> 
<p>第一段主要介绍了一些研究背景，深度卷积网络为图像分类带来了一系列的突破。网络深度对训练模型来说也是至关重要的。<br> 作者在第二段就表明了，随着深度的增加会出现梯度消失、梯度爆炸，但是这个问题已经在很大程度上通过归一化初始化和中间归一化层得到了解决。<br> 第三段提出：随着网络深度的增加，精度达到饱和(这可能并不奇怪)，然后迅速退化。然而，这种退化并不是由过拟合引起的，在适当深度的模型上添加更多的层会导致更高的训练误差。<br> 第四段提出了一个名词<code>identity mapping</code>，我理解为输入为X，输出也为X。构造深的模型，虽然有解决方案，但是在可行时间内是不可能实现的。<br> 第五段提出了<strong>残差网络的结构</strong><br> <img src="https://images2.imgbox.com/0d/4b/SxHaK3tV_o.png" alt="在这里插入图片描述"><br> X 为浅层网络的输出。如果我们想要得到的映射为H(X)，则我们让添加的非线性网络层去拟合残差映射F(X):=H(X)-X。原始的映射就可以写成F(X)+X。图二中右侧连接为一个identity mapping。<br> 第六段提出了<code>shortcut connection</code>的概念。这种连接可以跳过一层或更多层，因为F(X)和X是直接相加的。所以不需要额外的参数和更复杂的计算。<br> 剩余部分都是在说这个模型有多好，在100层，甚至1000层的模型上效果都很好。<br> 我们假设第一个网络在训练集和测试集上可以得到很好的性能（甚至可以理解为接近100%）。<br> <img src="https://images2.imgbox.com/65/ab/KKGgE32S_o.png" alt="在这里插入图片描述">那么在这个新的网络，由于我们copy了前四层的参数，理论上前四层已经足够满足我们的性能要求，那么新增加的层便显得有些多余，如果这个新的网络也要达到性能100%，则新增加的层要做的事情就是“恒等映射”，也即后面几个紫色的层要实现的效果为 <img src="https://images2.imgbox.com/36/5f/05ELlmZe_o.png" alt="在这里插入图片描述"><br> 。这样一来，网络的性能一样能达到100%。而退化现象也表明了，实际上新增加的几个紫色的层，很难做到恒等映射。又或者能做到，但在有限的时间内很难完成（即网络要用指数级别的时间才能达到收敛）。这时候，巧妙的通过添加”桥梁“，使得难以优化的问题瞬间迎刃而解。<br> <img src="https://images2.imgbox.com/43/6f/fD0dx4ec_o.png" alt="在这里插入图片描述"><br> 可以看到通过添加这个桥梁，把数据原封不动得送到FC层的前面，而对于中间的紫色层，可以很容易的通过把这些层的参数逼近于0，进而实现<img src="https://images2.imgbox.com/db/a4/hkXstMao_o.png" alt="在这里插入图片描述">的功能。<br> 实际上，网络性能通常未能达到100%，可以假设最初的网络（只有前四层）的性能到了98%等等，如果不添加跳连接，增加三个紫色层之后的新网络同样难以进行优化（由上面极端情况的推广，也即前面四层的性能达到100%）。<br> 而通过跳连接，可以把前四层的输出先送到FC层前面，也就相当于告诉紫色层：”兄弟你放心，我已经做完98%的工作了，你看看能不能在剩下的2%中发点力，你要是找不出提升性能的效果也没事的，我们可以把你的参数逼近于0，所以放心大胆的找吧。"</p> 
<p>我们把整个映射看成100%，则前面四层网络实现了98%的映射关系，而残余的映射由紫色层完成，Residual 另一个翻译就是"残余，残留“的意思，也就是让每一个残差块，只关注残余映射的一小部分，真的是恰到好处。</p> 
<p>当然了，实际上网络运行的时候，我们并不会知道哪几层就能达到很好的效果，然后在它们的后面接一个跳连接，于是一开始便在两个层或者三个层之间添加跳连接，形成残差块，每个残差块只关注当前的残余映射，而不会关注前面已经实现的底层映射。<br> 不得不佩服他对神经网络的深入理解，从他灵感的来源，让我感觉他就是个数学大佬，结果一查还真是，本科是清华基础科学班的（研究物理数学的）（拿烟的手微微颤抖）。好了，废话不多说，让我们一起来理解什么是残差网络。我们先来看一个现象，假设我们有如下的一个网络，它可以在训练集和测试</p> 
<p><strong>这个例子来自于知乎大佬Sakura</strong>当时我看到后真的是茅塞顿开。<br> 作者：Sakura<br> 链接：https://www.zhihu.com/question/306135761/answer/2491142607<br> 来源：知乎</p> 
<h3><a id="3__44"></a>3. 相关工作</h3> 
<p>相关工作是说在这篇论文之前<strong>已经有一些人在某些方向上使用本论文中使用的一些方法</strong>去做事。这句话很拗口，但是只要你理解了，就会发现所有论文的相关方向都是说的这东西。</p> 
<h3><a id="4__46"></a>4. 深度残差网络</h3> 
<p>这部分就是介绍了残差模块的构成还有各种ResNet的网络结构。<br> 在上文中，我们说道F(X)和X直接相加，因此需要保证他们的维度一定要一样，否则就对X做投影。<br> <img src="https://images2.imgbox.com/d4/23/YTtPREvz_o.png" alt="在这里插入图片描述"><br> 关于残差网络结构（以34层为例），只截取了一部分<br> <img src="https://images2.imgbox.com/23/3a/OmGbnJ5y_o.png" alt="在这里插入图片描述"><br> 图右中的实线表示残差连接，虚线表示升维。<br> 架构图：<br> <img src="https://images2.imgbox.com/d2/f8/YtpaAfqm_o.png" alt="在这里插入图片描述"><br> <strong>更深的ResNet网络</strong>：<br> <img src="https://images2.imgbox.com/2d/68/GsTyCcyv_o.png" alt="在这里插入图片描述"><br> 50层，101层，152层都使用图右中的结构，使用1X1的卷积主要是为了降维和升维。右边结构的参数量明显比左边的少。</p> 
<h2><a id="_58"></a>总结</h2> 
<p>网络亮点：</p> 
<ol><li>超深的网络结构(突破1000层)</li><li>提出residual模块</li><li>使用Batch Normalization加速训练(丢弃dropout)<br> 沐神很形象的描述出了为什么加一个残差连接就可以收敛。</li></ol> 
<p><img src="https://images2.imgbox.com/9c/10/Egaw1Os4_o.png" alt="在这里插入图片描述"></p> 
<h2><a id="_67"></a>代码实现</h2> 
<p>只是model模块</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token triple-quoted-string string">'''
对应18层，34层的残差结构
'''</span>
<span class="token keyword">class</span> <span class="token class-name">BasicBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    expansion <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment">#判断每一个卷积块中，卷积核的个数会不会有变化</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channel<span class="token punctuation">,</span> out_channel<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># downsample表示是否有升维操作</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># output = (input - kernel_size + 2*padding)/stride + 1</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># stride=1表示option A；stride=2表示optionB 使用BN不需要偏置bias</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        identity <span class="token operator">=</span> x
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            identity <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">+=</span> identity
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out

<span class="token triple-quoted-string string">'''
50层，101层，152层
'''</span>
<span class="token keyword">class</span> <span class="token class-name">Bottleneck</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。
    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，
    这么做的好处是能够在top1上提升大概0.5%的准确率。
    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch
    """</span>
    expansion <span class="token operator">=</span> <span class="token number">4</span>

    

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channel<span class="token punctuation">,</span> out_channel<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>


        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>in_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># squeeze channels</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        <span class="token comment"># -----------------------------------------</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token punctuation">)</span>
        <span class="token comment"># -----------------------------------------</span>
        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>out_channel<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>out_channel<span class="token operator">*</span>self<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span>
                               kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># unsqueeze channels</span>
        self<span class="token punctuation">.</span>bn3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channel<span class="token operator">*</span>self<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        identity <span class="token operator">=</span> x
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            identity <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        out <span class="token operator">+=</span> identity
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out


<span class="token keyword">class</span> <span class="token class-name">ResNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 block<span class="token punctuation">,</span> <span class="token comment"># 残差结构</span>
                 blocks_num<span class="token punctuation">,</span>
                 num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>
                 include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                 width_per_group<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ResNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>include_top <span class="token operator">=</span> include_top
        self<span class="token punctuation">.</span>in_channel <span class="token operator">=</span> <span class="token number">64</span>

        self<span class="token punctuation">.</span>groups <span class="token operator">=</span> groups
        self<span class="token punctuation">.</span>width_per_group <span class="token operator">=</span> width_per_group

        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                               padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>maxpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> blocks_num<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 对应结构图中conv2_x，下面同理</span>
        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> blocks_num<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> blocks_num<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer4 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>block<span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> blocks_num<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>include_top<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># output size = (1, 1)</span>
            self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span> <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>

        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>kaiming_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'fan_out'</span><span class="token punctuation">,</span> nonlinearity<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span>
    <span class="token triple-quoted-string string">'''
    block: BasicBlock或Bottleneck
    channel: 残差结构中的卷积核个数
    block_num：这一层有多少残差结构，例：34的第一层有三个，第二层有四个
    '''</span>
    <span class="token keyword">def</span> <span class="token function">_make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> block_num<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        downsample <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># 快捷连接虚线部分</span>
        <span class="token keyword">if</span> stride <span class="token operator">!=</span> <span class="token number">1</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>in_channel <span class="token operator">!=</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">:</span>
            downsample <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion<span class="token punctuation">)</span><span class="token punctuation">)</span>

        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 搭建每一个conv的第一层</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span>
                            channel<span class="token punctuation">,</span>
                            downsample<span class="token operator">=</span>downsample<span class="token punctuation">,</span>
                            stride<span class="token operator">=</span>stride<span class="token punctuation">,</span>
                            groups<span class="token operator">=</span>self<span class="token punctuation">.</span>groups<span class="token punctuation">,</span>
                            width_per_group<span class="token operator">=</span>self<span class="token punctuation">.</span>width_per_group<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>in_channel <span class="token operator">=</span> channel <span class="token operator">*</span> block<span class="token punctuation">.</span>expansion

        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> block_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_channel<span class="token punctuation">,</span>
                                channel<span class="token punctuation">,</span>
                                groups<span class="token operator">=</span>self<span class="token punctuation">.</span>groups<span class="token punctuation">,</span>
                                width_per_group<span class="token operator">=</span>self<span class="token punctuation">.</span>width_per_group<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>maxpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>include_top<span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x


<span class="token keyword">def</span> <span class="token function">resnet34</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>BasicBlock<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span> include_top<span class="token operator">=</span>include_top<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">resnet50</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span> include_top<span class="token operator">=</span>include_top<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">resnet101</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span> include_top<span class="token operator">=</span>include_top<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">resnext50_32x4d</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span>
    groups <span class="token operator">=</span> <span class="token number">32</span>
    width_per_group <span class="token operator">=</span> <span class="token number">4</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span>
                  include_top<span class="token operator">=</span>include_top<span class="token punctuation">,</span>
                  groups<span class="token operator">=</span>groups<span class="token punctuation">,</span>
                  width_per_group<span class="token operator">=</span>width_per_group<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">resnext101_32x8d</span><span class="token punctuation">(</span>num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> include_top<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span>
    groups <span class="token operator">=</span> <span class="token number">32</span>
    width_per_group <span class="token operator">=</span> <span class="token number">8</span>
    <span class="token keyword">return</span> ResNet<span class="token punctuation">(</span>Bottleneck<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                  num_classes<span class="token operator">=</span>num_classes<span class="token punctuation">,</span>
                  include_top<span class="token operator">=</span>include_top<span class="token punctuation">,</span>
                  groups<span class="token operator">=</span>groups<span class="token punctuation">,</span>
                  width_per_group<span class="token operator">=</span>width_per_group<span class="token punctuation">)</span>

</code></pre> 
<p>代码来自于B站大佬：<a href="https://space.bilibili.com/18161609" rel="nofollow">https://space.bilibili.com/18161609</a><br> 完整的代码和数据集在我的GitHub上。<a href="https://github.com/Glory-Peng/CV">https://github.com/Glory-Peng/CV</a></p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/2ef9982b89b116ca00d094721c779b6d/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【GD32学习】五、FMC Flash 单字节读写实验</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/7ec65ffd830a3b5e17c26dd37c3a74d8/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">【Linux】Ubuntu系统下用apt命令删除/卸载软件包</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>