<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Hadoop-day07(MapReduce三个小案例) - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Hadoop-day07(MapReduce三个小案例)" />
<meta property="og:description" content="MapReduce三个小案例 回顾一下 wordcount案例中map阶段 回顾一下 wordcount案例中的reduce阶段 1、IK分词器(统计三国演义指定词语个数) 步骤一：找到ik依赖,并添加到环境中
步骤二：在hadoop项目中创建子项目，并添加环境依赖
步骤三：小测试一下，结果如下：
步骤四：通过程序在 HDFS 中的 sgyy.txt 中统计 三国演义出现的次数
《三国演义》是中国文学史上第一部章回小说，是历史演义小说的开山之作，也是第一部文人长篇小说，明清时期甚至有“第一才子书”之称。 [1] 《三国演义》(全名为《三国志通俗演义》，又称《三国志演义》)是元末明初小说家罗贯中根据陈寿《三国志》和裴松之注解以及民间三国故事传说经过艺术加工创作而成的长篇章回体历史演义小说，与《西游记》《水浒传》《红楼梦》并称为中国古典四大名著。该作品成书后有嘉靖壬午本等多个版本传于世，到了明末清初，毛宗岗对《三国演义》整顿回目、修正文辞、改换诗文，该版本也成为诸多版本中水平最高、流传最广
等等 。。。。
首先：配置依赖开始第一次尝试，失败。原因是没有添加能够将 IKanalyzer 打包的依赖。下面是配置了将依赖一起打包的代码
&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;project xmlns=&#34;http://maven.apache.org/POM/4.0.0&#34; xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&#34;&gt; &lt;parent&gt; &lt;artifactId&gt;java-hadoop&lt;/artifactId&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;hadoop-sanguodemo&lt;/artifactId&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.janeluo/ikanalyzer --&gt; &lt;dependency&gt; &lt;groupId&gt;com.janeluo&lt;/groupId&gt; &lt;artifactId&gt;ikanalyzer&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/e95b0d391365ff0ad226ce9e3c100045/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-05-28T16:34:00+08:00" />
<meta property="article:modified_time" content="2022-05-28T16:34:00+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hadoop-day07(MapReduce三个小案例)</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <div id="cnblogs_post_body" class="blogpost-body cnblogs-markdown" style="font-size: 16px;"> 
 <h2 id="mapreduce三个小案例">MapReduce三个小案例</h2> 
 <h3 id="回顾一下-wordcount案例中map阶段">回顾一下 wordcount案例中<strong>map</strong>阶段</h3> 
 <p><img src="https://images2.imgbox.com/58/96/S35iJ0gH_o.png" alt="" style="outline: none;"></p> 
 <h3 id="回顾一下-wordcount案例中的reduce阶段">回顾一下 wordcount案例中的<strong>reduce</strong>阶段</h3> 
 <p><img src="https://images2.imgbox.com/f2/e7/wiG1he37_o.png" alt="" style="outline: none;"></p> 
 <h3 id="1ik分词器统计三国演义指定词语个数">1、IK分词器(统计三国演义指定词语个数)</h3> 
 <p>步骤一：找到ik依赖,并添加到环境中</p> 
 <p><img src="https://images2.imgbox.com/63/d5/vWm09C75_o.png" alt="" style="outline: none;"></p> 
 <p>步骤二：在hadoop项目中创建子项目，并添加环境依赖</p> 
 <p><img src="https://images2.imgbox.com/1a/59/RroOrQjF_o.png" alt="" style="outline: none;"></p> 
 <p>步骤三：小测试一下，结果如下：</p> 
 <p><img src="https://images2.imgbox.com/e0/4f/T07cEMXT_o.png" alt="" style="outline: none;"></p> 
 <p>步骤四：通过程序在 HDFS 中的 sgyy.txt 中统计 三国演义出现的次数</p> 
 <blockquote> 
  <p>《三国演义》是中国文学史上第一部章回小说，是历史演义小说的开山之作，也是第一部文人长篇小说，明清时期甚至有“第一才子书”之称。 [1] 《三国演义》(全名为《三国志通俗演义》，又称《三国志演义》)是元末明初小说家罗贯中根据陈寿《三国志》和裴松之注解以及民间三国故事传说经过艺术加工创作而成的长篇章回体历史演义小说，与《西游记》《水浒传》《红楼梦》并称为中国古典四大名著。该作品成书后有嘉靖壬午本等多个版本传于世，到了明末清初，毛宗岗对《三国演义》整顿回目、修正文辞、改换诗文，该版本也成为诸多版本中水平最高、流传最广</p> 
  <p>等等 。。。。</p> 
 </blockquote> 
 <p>首先：配置依赖<mark>开始第一次尝试，失败。原因是没有添加能够将 IKanalyzer 打包的依赖。</mark><strong>下面是配置了将依赖一起打包的代码</strong></p> 
 <pre class="has"><code class="language-java">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;java-hadoop&lt;/artifactId&gt;
        &lt;groupId&gt;org.example&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;hadoop-sanguodemo&lt;/artifactId&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/com.janeluo/ikanalyzer --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.janeluo&lt;/groupId&gt;
            &lt;artifactId&gt;ikanalyzer&lt;/artifactId&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.3.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;descriptorRefs&gt;
                        &lt;!--  打包出来的带依赖jar包名称 --&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
                &lt;!--下面是为了使用 mvn package命令，如果不加则使用mvn assembly--&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assemble&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;</code></pre> 
 <p>其次：编写主程序(main方法中的代码千篇一律，但是调用的map 和reduce 的代码是关键代码)</p> 
 <pre class="has"><code class="language-java">package com.shujia;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.wltea.analyzer.core.IKSegmenter;
import org.wltea.analyzer.core.Lexeme;
import java.io.IOException;
import java.io.StringReader;
/**
 * @author WangTao
 * @date 2022/5/27 21:04
 */
class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        //使用分词器将一行数据分词
        StringReader sr = new StringReader(value.toString());
        //使用IK分词器进行分词
        IKSegmenter ikSegmenter = new IKSegmenter(sr, true);
        Lexeme lexeme = null;
        while ((lexeme = ikSegmenter.next())!=null){
            String word = lexeme.getLexemeText();
            if("三国演义".equals(word)||"曹操".equals(word)){
                context.write(new Text(word),new LongWritable(1L));
            }

        }
    }
}
class MyReduce extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;{
    @Override
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        Long sum = 0L;
        for (LongWritable value : values) {
            long l = value.get();
            sum += l;
        }
        context.write(key,new LongWritable(sum));
    }
}
/*
        统计sgyy.txt中的 三国演义 出现的次数
 */
public class SanGuoCount {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setNumReduceTasks(2);
        job.setJobName("统计三国中三国演义和曹操出现的此时");

        job.setJarByClass(SanGuoCount.class);
        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReduce.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        job.waitForCompletion(true);
    }
}</code></pre> 
 <p>然后将写好的代码打包，上传到linux中，通过命令运行</p> 
 <blockquote> 
  <p>hadoop jar &lt;上传的jar包&gt; main方法的 Reference hdfs中源文件的路径 目标路径、</p> 
 </blockquote> 
 <p><mark>运行结果如下：</mark></p> 
 <p><img src="https://images2.imgbox.com/0a/8d/qDYrqbOV_o.png" alt="" style="outline: none;"></p> 
 <h3 id="2mapreduce案例">2、MapReduce案例</h3> 
 <h3 id="好友推荐系统">好友推荐系统</h3> 
 <blockquote> 
  <p>固定类别推荐</p> 
  <p>​ 莫扎特----&gt;钢琴----&gt;贝多芬-----&gt;命运交响曲</p> 
  <p>数据量</p> 
  <p>​ QQ好友推荐---&gt;</p> 
  <p>​ 每个QQ200个好友</p> 
  <p>​ 5亿QQ号</p> 
  <p>解决思路：</p> 
  <p>​ 需要按照行进行计算</p> 
  <p>​ 将相同推荐设置成相同的key，便于reduce统一处理</p> 
  <p>数据：</p> 
  <pre class="has"><code>tom hello hadoop cat
world hello hadoop hive
cat tom hive
mr hive hello
hive cat hadoop world hello mr
hadoop tom hive world hello
hello tom world hive mr</code></pre> 
 </blockquote> 
 <p><img src="https://images2.imgbox.com/44/84/j3iupNaA_o.png" alt="" style="outline: none;"></p> 
 <blockquote> 
  <p>分析：</p> 
  <p><strong>我们需要在map阶段根据用户的直接联系和间接关系列举出来，map输出的为tom：hadoop 1，hello：hadoop 0，0代表间接关系，1代表直接关系。在Reduce阶段把直接关系的人删除掉，再输出。</strong></p> 
  <p>具体实现</p> 
 </blockquote> 
 <p><strong>环境代码</strong></p> 
 <pre class="has"><code class="language-java">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;java-hadoop&lt;/artifactId&gt;
        &lt;groupId&gt;org.example&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;anli-tuijian&lt;/artifactId&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;


&lt;/project&gt;</code></pre> 
 <p>主程序代码：</p> 
 <pre class="has"><code class="language-java">import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;{
    private Text mkey = new Text();
    private  LongWritable mvalue = new LongWritable();
    @Override
    protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        //tom hello hadoop cat
        /**
         * tom:hello 1
         * tom:hadoop 1
         * tom:cat 1
         *
         * hello:hadoop -1
         * hello:cat -1
         * hadoop:cat -1
         */
        //hadoop tom hive world hello
        /**
         * hadoop:tom 1
         * hadoop:hive 1
         * hadoop:world 1
         * hello:hadoop 1
         */
        //将value类型的数据转换成String类型
        String line = value.toString();
        String[] strings = line.split(" ");
        for (int i = 1; i &lt; strings.length; i++) {
            //处理直接好友
            mkey.set(orderFriend(strings[0],strings[i]));
            mvalue.set(1L);
            context.write(mkey,mvalue);
            //处理间接好友
            for(int j =i+1; j &lt; strings.length; j++) {
                mkey.set(orderFriend(strings[i],strings[j]));
                mvalue.set(-1L);
                context.write(mkey, mvalue);
            }
        }
    }
    public static String orderFriend(String f1,String f2){
        int i = f1.compareTo(f2);
        if(i&gt;0){
            return f1+":"+f2;
        }else{
            return f2+":"+f1;
        }
    }
}
class MyReduce extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;{
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        //hadoop:hello 1
        //hadoop:hello -1
        int flag = 0;
        Long sum = 0L;
        for (LongWritable value : values) {
            if (value.get() == 1) {
                flag = 1;
            }
            sum += value.get();
        }

        if (flag == 0) {
            context.write(key, new LongWritable(-1L));
        } else {
            context.write(key, new LongWritable(0L));
        }
    }
}



/**
 * @author WangTao
 * @date 2022/5/28 10:07
 */
/*
    计算出给出的数据中有多少直接关系的组合和间接关系的组合，
    将组合作为key
    直接关系，value是1
    间接关系，value是-1
 */
public class TuiJianDemo {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(TuiJianDemo.class);
        job.setNumReduceTasks(1);
        job.setJobName("推荐任务");

        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReduce.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        job.waitForCompletion(true);
    }
}</code></pre> 
 <h5 id="结果如下">结果如下：</h5> 
 <p><img src="https://images2.imgbox.com/40/dc/hfYU2tft_o.png" alt="" style="outline: none;"></p> 
 <h3 id="4pm25平均值">4 PM2.5平均值</h3> 
 <blockquote> 
  <p>表格如下：.csv结尾的都是以 ， 间隔的</p> 
 </blockquote> 
 <p><img src="https://images2.imgbox.com/8f/aa/hKQnyC8Z_o.png" alt="" style="outline: none;"></p> 
 <p><mark>添加依赖</mark></p> 
 <pre class="has"><code class="language-java">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
    &lt;parent&gt;
        &lt;artifactId&gt;java-hadoop&lt;/artifactId&gt;
        &lt;groupId&gt;org.example&lt;/groupId&gt;
        &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
    &lt;/parent&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;artifactId&gt;airPM25avg&lt;/artifactId&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;
&lt;/project&gt;</code></pre> 
 <p><strong>代码如下：</strong></p> 
 <pre class="has"><code class="language-java">package com.shujia.airPM25;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;
//20220527:1001 20
//20220527:1001 30
//20220527:1001 40
//...
class MyMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt;{
    @Override
    protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
       //将一行的数据转换成 String 类型的
        String line = value.toString();
        String[] strings = line.split(",");
        //过滤出 Pm2.5 的对应的数据
        if(strings.length&gt;=4 &amp;&amp; "PM2.5".equals(strings[2])){
            for(int i = 3, j = 1001;i &lt; strings.length;i++,j++){
                //对一行的数据进行清洗，因为有的时间没有监控到PM2.5的值
                if("".equals(strings[i]) || strings[i] ==null || " ".equals(strings[i])){
                    strings[i] = "0";
                }
                context.write(new Text("date:"+strings[0]+"-城市编号："+j),new LongWritable(Long.parseLong(strings[i])));
            }
        }
    }
}
class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;{
    @Override
    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context) throws IOException, InterruptedException {
        Long sum = 0L;
        for (LongWritable value : values) {
            long l = value.get();
            sum += l;
        }
        //除以 24 得到该城市当天的 PM2.5 平均值
        long avg = sum/24;
        context.write(key,new LongWritable(avg));
    }
}
/**
 * @author WangTao
 * @date 2022/5/28 14:11
 */
public class AirPm25avg {
    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(AirPm25avg.class);
        job.setNumReduceTasks(2);
        job.setJobName("计算每个城市每天的pm2.5的平均值");

        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);

        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));

        job.waitForCompletion(true);
    }

}</code></pre> 
 <blockquote> 
  <p><mark>注意：</mark>args[] 的位置不仅可以填写文件，而且可以填写文件夹</p> 
 </blockquote> 
 <p><img src="https://images2.imgbox.com/cd/d2/6EwLSp1Z_o.png" alt="" style="outline: none;"></p> 
 <p><mark>结果：</mark></p> 
 <p><img src="https://images2.imgbox.com/24/e2/C1EXJ5XC_o.png" alt="" style="outline: none;"></p> 
</div>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/e4783427ba5ec8dbf1957acf7c138b28/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">【无标题】c# 在Emit代码中如何await一个异步方法学习通http://www.bdgxy.com/</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/dd6f7734de1bbd2a640b758667c5c794/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">信息学奥赛一本通 1387：搭配购买(buy) | 洛谷 P1455 搭配购买</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>