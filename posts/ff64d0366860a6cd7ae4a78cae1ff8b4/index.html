<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>03_反向传播算法（BP算法）· 原理&#43;演算 - 编程大白的博客</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="03_反向传播算法（BP算法）· 原理&#43;演算" />
<meta property="og:description" content="反向传播算法（Backpropagation Algorithm，BP算法）是深度学习的重要思想基础
目录
一、BP算法的推导
1、前向传播的计算
第一层隐藏层的计算
第二层隐藏层的计算
输出层计算
2、反向传播计算
计算偏导数
二、实际数据代入推导BP算法
1、前向传播计算
第一层隐藏层的计算
第二层隐藏层的计算
输出层计算
2、误差反向传播
输出层误差计算
第二层隐藏层误差计算
第一层隐藏层误差计算
3、更新参数
其实，大概意思就是在输入和输出之间构造神经网络（像人的大脑神经网络一样），当然这个神经网络里面会有很多的神经节点和节点之间相连的边（就想神经元由前到后相连）。然后，这个神经网络作为一个计算的网络，里面会有一些参数，要是参数选的好的话，那么这个模型就可以拿去用了，比如说，看到一张狗的照片，就知道这个是狗了。而反向传播算法就是从结果出发一步步去约束其中的参数，然后使得参数达到最优的状态。
看一看神经元，这里的：
a1、a2、a3指的是输入
w1、w2、w3指的是权重（对应输入的）
b 表示偏置（这里没有呢）
f 表示激活函数（你可以想成是平时数学课上的函数，一般都是比较简单的函数，表现出输入和输出的关系）
z 指的是输出
一、BP算法的推导 这个是一个简单的三层神经网络结构（两个隐藏层，一个输出层）
比如我们来解决一个二分类问题，给一个样本输入，通过前向运算得到输出
输出值域为（0，1），输出的结果越靠近0，就代表样本是0类的可能性越大，反之，1类的可能性越大
1、前向传播的计算 基本上都是矩阵运算
第一层隐藏层的计算 第二层隐藏层的计算 输出层计算 2、反向传播计算 假设我们使用随机梯度下降的方式来训练神经网络的参数：
损失函数（loss function）或代价函数（cost function），通常作为学习准则与优化问题相联系，即通过最小化损失函数求解和评估模
型。定义为，其中 y 定义为该样本的真实类标，而 是模型的输出。
这里有一些损失函数，我们一般用的是第二个平方损失函数：
当损失函数的值为0时，那么表示模型参数训练的炒鸡好的！因为没有损失了。
使用梯度下降法进行参数的学习，必须计算出损失函数关于神经网络中各层参数（权重w和偏置b）的偏导数。
计算偏导数 BP算法基本就是这些
二、实际数据代入推导BP算法 我们对上述的神经网络代值进行计算验证，所有的参数的初始值如下:
注意：这里面的所有的权值w和所有的偏置b的具体的值都是初始值，当然也可以换成其他数字
1、前向传播计算 第一层隐藏层的计算 第二层隐藏层的计算 输出层计算 2、误差反向传播 输出层误差计算 第二层隐藏层误差计算 第一层隐藏层误差计算 3、更新参数 我们已经计算出每一层的误差了，现在我们要利用每一层的误差和梯度来更新每一层的参数
权重w和偏置b的更新公式如下：
一般来说，权重w的更新会在前面加上一个正则化项来避免过拟合，这里为了简化运算，我们省去正则化项。
正则化：必须有一种自动的东西来告诉我们哪种程度将最适合所提供的数据，同时告诉我们需要把那些特征的影响降低至最低，以获得最好的预测。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bcdabai.github.io/posts/ff64d0366860a6cd7ae4a78cae1ff8b4/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-05-15T20:58:14+08:00" />
<meta property="article:modified_time" content="2020-05-15T20:58:14+08:00" />


	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程大白的博客" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程大白的博客</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">03_反向传播算法（BP算法）· 原理&#43;演算</h1>
			
		</header>
		<div id="gatop"></div>
		<div class="content post__content clearfix">
			
<div id="content_views" class="htmledit_views">
                    <p><strong>反向传播算法（Backpropagation Algorithm，BP算法）是深度学习的重要思想基础</strong></p> 
<p> </p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81BP%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC-toc" style="margin-left:0px;"><a href="#%E4%B8%80%E3%80%81BP%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC" rel="nofollow">一、BP算法的推导</a></p> 
<p id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E8%AE%A1%E7%AE%97" rel="nofollow">1、前向传播的计算</a></p> 
<p id="%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97" rel="nofollow">第一层隐藏层的计算</a></p> 
<p id="%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97" rel="nofollow">第二层隐藏层的计算</a></p> 
<p id="%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%A1%E7%AE%97" rel="nofollow">输出层计算</a></p> 
<p id="-toc" style="margin-left:40px;"> </p> 
<p id="2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97" rel="nofollow">2、反向传播计算</a></p> 
<p style="margin-left:80px;"> </p> 
<p id="%E8%AE%A1%E7%AE%97%E5%81%8F%E5%AF%BC%E6%95%B0-toc" style="margin-left:80px;"><a href="#%E8%AE%A1%E7%AE%97%E5%81%8F%E5%AF%BC%E6%95%B0" rel="nofollow">计算偏导数</a></p> 
<p id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%99%85%E6%95%B0%E6%8D%AE%E4%BB%A3%E5%85%A5%E6%8E%A8%E5%AF%BCBP%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#%E4%BA%8C%E3%80%81%E5%AE%9E%E9%99%85%E6%95%B0%E6%8D%AE%E4%BB%A3%E5%85%A5%E6%8E%A8%E5%AF%BCBP%E7%AE%97%E6%B3%95" rel="nofollow">二、实际数据代入推导BP算法</a></p> 
<p id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;"><a href="#1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97" rel="nofollow">1、前向传播计算</a></p> 
<p style="margin-left:80px;"><a href="#%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97" rel="nofollow">第一层隐藏层的计算</a></p> 
<p style="margin-left:80px;"><a href="#%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97" rel="nofollow">第二层隐藏层的计算</a></p> 
<p style="margin-left:80px;"><a href="#%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%A1%E7%AE%97" rel="nofollow">输出层计算</a></p> 
<p id="2%E3%80%81%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-toc" style="margin-left:40px;"><a href="#2%E3%80%81%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD" rel="nofollow">2、误差反向传播</a></p> 
<p id="%E8%BE%93%E5%87%BA%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E8%BE%93%E5%87%BA%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97" rel="nofollow">输出层误差计算</a></p> 
<p id="%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97" rel="nofollow">第二层隐藏层误差计算</a></p> 
<p id="%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97-toc" style="margin-left:80px;"><a href="#%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97" rel="nofollow">第一层隐藏层误差计算</a></p> 
<p id="3%E3%80%81%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0-toc" style="margin-left:40px;"><a href="#3%E3%80%81%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0" rel="nofollow">3、更新参数</a></p> 
<hr id="hr-toc"> 
<p>其实，大概意思就是在输入和输出之间构造神经网络（像人的大脑神经网络一样），当然这个神经网络里面会有很多的神经节点和节点之间相连的边（就想神经元由前到后相连）。然后，这个神经网络作为一个计算的网络，里面会有一些参数，要是参数选的好的话，那么这个模型就可以拿去用了，比如说，看到一张狗的照片，就知道这个是狗了。而<strong>反向传播算法就是从结果出发一步步去约束其中的参数，然后使得参数达到最优的状态。</strong></p> 
<p>看一看<strong>神经元</strong>，这里的：</p> 
<p><strong>a1、a2、a3指的是输入</strong></p> 
<p><strong>w1、w2、w3指的是权重（对应输入的）</strong></p> 
<p><strong>b 表示偏置（这里没有呢）</strong></p> 
<p><strong>f 表示激活函数（你可以想成是平时数学课上的函数，一般都是比较简单的函数，表现出输入和输出的关系）</strong></p> 
<p><strong>z 指的是输出</strong></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a9/aa/93ellu0H_o.jpg"></p> 
<p> </p> 
<h2 id="%E4%B8%80%E3%80%81BP%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC">一、BP算法的推导</h2> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/67/70/WUS4B81H_o.png"></p> 
<p>这个是一个简单的<strong>三层神经网络结构（两个隐藏层，一个输出层）</strong></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/af/17/Ad81Zo4e_o.png"></p> 
<p>比如我们来解决一个二分类问题，给一个样本输入，通过前向运算得到输出</p> 
<p>输出值域为（0，1），输出的结果越靠近0，就代表样本是0类的可能性越大，反之，1类的可能性越大</p> 
<p> </p> 
<h3 id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E8%AE%A1%E7%AE%97">1、前向传播的计算</h3> 
<p style="text-indent:33px;"><strong>基本上都是矩阵运算</strong></p> 
<ul><li> <h4 id="%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97">第一层隐藏层的计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/6b/34/dW3y6Ifq_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/89/4a/RuIMG484_o.png"></p> 
<ul><li> <h4 id="%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E8%AE%A1%E7%AE%97">第二层隐藏层的计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/79/0c/wIvtO0jb_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3c/e5/lSrYe5tQ_o.png"></p> 
<ul><li> <h4 id="%E8%BE%93%E5%87%BA%E5%B1%82%E8%AE%A1%E7%AE%97">输出层计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/dd/fa/ePmnm0t9_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ca/af/6IeFWBQU_o.png"></p> 
<h4> </h4> 
<h3 id="2%E3%80%81%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97">2、反向传播计算</h3> 
<p style="text-indent:33px;">假设我们使用<strong>随机梯度下降</strong>的方式来训练神经网络的参数：</p> 
<p style="text-indent:33px;"><strong>损失函数</strong>（loss function）或代价函数（cost function），通常作为学习准则与优化问题相联系，即<strong>通过最小化损失函数求解和评估模</strong></p> 
<p style="text-indent:0;"><strong>型。定义为<img alt="L\left ( y,\widehat{y} \right )" class="mathcode" src="https://images2.imgbox.com/42/fc/igDKg0WT_o.gif">，其中 y 定义为该样本的真实类标，而 <img alt="\widehat{y}" class="mathcode" src="https://images2.imgbox.com/79/d0/piBUSo7K_o.gif"> 是模型的输出。</strong></p> 
<p style="text-indent:0;">这里有一些损失函数，我们一般用的是第二个<strong>平方损失函数：</strong></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/bf/0f/PqZoZqS6_o.png"></p> 
<p style="text-indent:33px;"><strong>当损失函数的值为0时，那么表示模型参数训练的炒鸡好的！</strong>因为没有损失了。</p> 
<p style="text-indent:33px;">使用梯度下降法进行参数的学习，必须计算出损失函数关于神经网络中各层参数（权重w和偏置b）的偏导数。</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/c8/62/tFVoAbYk_o.png"></p> 
<h4> </h4> 
<ul><li> <h4 id="%E8%AE%A1%E7%AE%97%E5%81%8F%E5%AF%BC%E6%95%B0">计算偏导数</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/a2/3f/RUYSo66g_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/dd/d8/aOBJJJiX_o.png"></p> 
<p>BP算法基本就是这些</p> 
<p> </p> 
<h2 id="%E4%BA%8C%E3%80%81%E5%AE%9E%E9%99%85%E6%95%B0%E6%8D%AE%E4%BB%A3%E5%85%A5%E6%8E%A8%E5%AF%BCBP%E7%AE%97%E6%B3%95">二、实际数据代入推导BP算法</h2> 
<p>我们对上述的神经网络代值进行计算验证，所有的参数的初始值如下:</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/e4/b7/LXKIOxgc_o.png"></p> 
<p><strong>注意：这里面的所有的权值w和所有的偏置b的具体的值都是初始值，当然也可以换成其他数字</strong></p> 
<p> </p> 
<h3 id="1%E3%80%81%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97">1、前向传播计算</h3> 
<ul><li> <h4>第一层隐藏层的计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/3e/8e/jlaf4RnR_o.png"></p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/f9/66/9HRkinFS_o.png"></p> 
<ul><li> <h4>第二层隐藏层的计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/cb/b6/GTO9CRC9_o.png"></p> 
<p style="text-align:center;"><img alt="" height="195" src="https://images2.imgbox.com/3d/23/CMAonGoj_o.png" width="838"></p> 
<ul><li> <h4>输出层计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/fb/a6/lfFcMt8U_o.png"></p> 
<p> </p> 
<h3 id="2%E3%80%81%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD">2、误差反向传播</h3> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ea/2f/HRL8hLkH_o.png"></p> 
<ul><li> <h4 id="%E8%BE%93%E5%87%BA%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97">输出层误差计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/63/b5/SAZ8blAo_o.png"></p> 
<ul><li> <h4 id="%E7%AC%AC%E4%BA%8C%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97">第二层隐藏层误差计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/90/37/1ROSUp7R_o.png"></p> 
<ul><li> <h4 id="%E7%AC%AC%E4%B8%80%E5%B1%82%E9%9A%90%E8%97%8F%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97">第一层隐藏层误差计算</h4> </li></ul> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/49/fd/ywkbLE1H_o.png"></p> 
<h3 id="3%E3%80%81%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0">3、更新参数</h3> 
<p>我们已经计算出每一层的误差了，现在我们要利用每一层的误差和梯度来更新每一层的参数</p> 
<p>权重w和偏置b的更新公式如下：</p> 
<p style="text-align:center;"><img alt="" height="121" src="https://images2.imgbox.com/89/04/6ZgTPMhb_o.png" width="472"></p> 
<p>一般来说，权重w的更新会在前面加上一个<strong>正则化项来避免过拟合</strong>，这里为了简化运算，我们省去正则化项。</p> 
<p><strong><a href="https://cloud.tencent.com/developer/article/1625641" rel="nofollow">正则化</a></strong>：必须有一种自动的东西来告诉我们<strong>哪种程度将最适合所提供的数据</strong>，同时<strong>告诉我们需要把那些特征的影响降低至最低</strong>，以获得最好的预测。</p> 
<p><strong><img alt="\alpha" class="mathcode" src="https://images2.imgbox.com/e1/aa/SJVGlDE8_o.gif">是学习率</strong>：为了能够使得梯度下降法有较好的性能，我们需要把学习率的值设定在合适的范围内。<strong>学习率决定了参数移动到最优值的速度快慢</strong>。如果学习率过大，很可能会越过最优值；反而如果学习率过小，优化的效率可能过低，长时间算法无法收敛。所以学习率对于算法性能的表现至关重要。</p> 
<p><strong>在每次迭代中去调整学习率的值是另一种很好的学习率自适应方法。此类方法的基本思路是当你离最优值越远，你需要朝最优值移动的就越多，即学习率就应该越大；反之亦反。</strong>我们这里设置为0.1。</p> 
<p>每一层的参数更新的计算方法都是一样的，这里只给出第一层隐藏层的参数更新：</p> 
<p style="text-align:center;"><img alt="" src="https://images2.imgbox.com/ea/8f/EXhaa4mE_o.png"></p> 
<p> </p> 
<p> </p> 
<p>哈哈哈哈，具体还有很多东西，大家可以看看mooc复旦大学的商务数据教程！</p> 
<p>对原文进行了修正和添加！</p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p> 
<p> </p>
                </div>
		</div>
		<div id="gabottom"></div>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/3194c704704df9e0439449aa5b8b6aa0/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">Proxmox ve（Pve） 安装windows server</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/posts/6f0cb3753451543abaa7632768aec707/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">linux主机间传输数据，No route to host解决方法</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 编程大白的博客.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<div id="gafoot"></div>
<script src="https://101121.xyz/ga/app.js"></script>


	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>